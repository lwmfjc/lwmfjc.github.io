[{"id":0,"href":"/zh/docs/technology/","title":"技术","section":"Docs","content":"这里面都是放一些平常技术知识的学习，知识来源主要来自经典书籍，或是其他通俗易懂的系列视频。\n"},{"id":1,"href":"/zh/docs/problem/","title":"问题解决","section":"Docs","content":"主要是一些平常遇到的一些问题，或是经过一顿折腾后解决的，或是临时遇到的小问题。\n"},{"id":2,"href":"/zh/docs/technology/Review/java_guide/","title":"JavaGuide","section":"面试","content":"基本全部转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者! 不过不是整篇拷贝，也是一句句理解后，一个字一个字打下来的。\n"},{"id":3,"href":"/zh/docs/technology/Review/","title":"面试","section":"技术","content":"面试用的，面经。但是从另一个角度也可以说是梳理知识框架，爱恨交加。\n"},{"id":4,"href":"/zh/docs/life/","title":"生活","section":"Docs","content":"生活相关的一些随感而发\n"},{"id":5,"href":"/zh/docs/test/","title":"测试","section":"Docs","content":"没啥重要的，随便测试的一些东西，大部分跟博客架构相关的东西，之前用的hexo，现在改用hugo了，还有一些东西在摸索中。\n"},{"id":6,"href":"/zh/docs/technology/Linux/Terminal-Commands/00-x/","title":"0000-0000","section":"Terminal Commands","content":" 这个系列只有一个长达五个多小时的视频，所以以时间0102(第1小时第2分钟)这样的形式命名\n"},{"id":7,"href":"/zh/docs/technology/Linux/SHELLlearnLinuxTV_/16-18/","title":"16-18","section":"SHELL编程(learnLinuxTV)","content":" 向bash脚本添加参数 # basic # ─ ~/shellTest ly@vmmin 10:37:24 ╰─❯ cat ./16myscript_cls.sh #!/bin/bash echo \u0026#34;You entered the argument: $1,$2,$3, and $4.\u0026#34; 结果\n╭─ ~/shellTest 16s ly@vmmin 10:37:18 ╰─❯ ./16myscript_cls.sh Linux1 Linux2 You entered the argument: Linux1,Linux2,, and . 示例1 # ╭─ ~/shellTest ly@vmmin 10:41:45 ╰─❯ cat ./16myscript_cls.sh #!/bin/bash ls -lh $1 #echo \u0026#34;You entered the argument: $1,$2,$3, and $4.\u0026#34; ╭─ ~/shellTest ly@vmmin 10:41:28 ╰─❯ ./16myscript_cls.sh /etc total 792K -rw-r--r-- 1 root root 3.0K May 25 2023 adduser.conf -rw-r--r-- 1 root root 44 Dec 17 15:26 adjtime -rw-r--r-- 1 root root 194 Dec 23 22:38 aliases drwxr-xr-x 2 root root 4.0K Dec 23 22:38 alternatives drwxr-xr-x 2 root root 4.0K Dec 17 15:24 apparmor drwxr-xr-x 8 root root 4.0K Dec 17 15:25 apparmor.d drwxr-xr-x 9 root root 4.0K Dec 17 15:30 apt -rw-r----- 1 root daemon 144 Oct 16 2022 at.deny -rw-r--r-- 1 root root 2.0K Mar 30 2024 bash.bashrc 示例2 # #!/bin/bash lines=$(ls -lh $1 | wc -l) #行计数 echo \u0026#34;You hava $(($lines-1)) objects in the $1 directory.\u0026#34; #$(($lines-1))这里用到了子shell #echo \u0026#34;You entered the argument: $1,$2,$3, and $4.\u0026#34; ╭─ ~/shellTest ly@vmmin 10:48:06 ╰─❯ ls -lh logfiles total 12K -rw-r--r-- 1 ly ly 0 Dec 22 23:07 a.log -rw-r--r-- 1 ly ly 120 Dec 22 23:17 a.log.tar.gz -rw-r--r-- 1 ly ly 0 Dec 22 23:07 b.log -rw-r--r-- 1 ly ly 121 Dec 22 23:17 b.log.tar.gz -rw-r--r-- 1 ly ly 0 Dec 22 23:07 c.log -rw-r--r-- 1 ly ly 121 Dec 22 23:17 c.log.tar.gz -rw-r--r-- 1 ly ly 0 Dec 22 23:15 xx.txt -rw-r--r-- 1 ly ly 0 Dec 22 23:15 y.txt ╭─ ~/shellTest ly@vmmin 10:48:10 ╰─❯ ./16myscript_cls.sh logfiles You hava 8 objects in the logfiles directory. head，表示前十行，可以看出total这些被算作一行了，所以上面的shell中-1\n─ ~/shellTest ly@vmmin 10:57:19 ╰─❯ ls -l /etc | head total 792 -rw-r--r-- 1 root root 3040 May 25 2023 adduser.conf -rw-r--r-- 1 root root 44 Dec 17 15:26 adjtime -rw-r--r-- 1 root root 194 Dec 23 22:38 aliases drwxr-xr-x 2 root root 4096 Dec 23 22:38 alternatives drwxr-xr-x 2 root root 4096 Dec 17 15:24 apparmor drwxr-xr-x 8 root root 4096 Dec 17 15:25 apparmor.d drwxr-xr-x 9 root root 4096 Dec 17 15:30 apt -rw-r----- 1 root daemon 144 Oct 16 2022 at.deny -rw-r--r-- 1 root root 1994 Mar 30 2024 bash.bashrc 不输入参数的情形 # 可以看出，其实就是应用到当前文件夹了\n╭─ ~/shellTest ly@vmmin 11:02:35 ╰─❯ ./16myscript_cls.sh logfiles You hava 8 objects in the logfiles directory. ╭─ ~/shellTest ly@vmmin 11:02:39 ╰─❯ ./16myscript_cls.sh You hava 29 objects in the directory. ╭─ ~/shellTest ly@vmmin 11:02:44 ╰─❯ ls -l | wc -l 30 参数判断 # #!/bin/bash # $#表示用户传到脚本中的参数数量 if [ $# -ne 1 ] #[]左右两边都一定要有空格 then echo \u0026#34;This script requires xxxxone directory path passed to it.\u0026#34; echo \u0026#34;Please try again.\u0026#34; exit 1 fi lines=$(ls -lh $1 | wc -l) echo \u0026#34;You hava $(($lines-1)) objects in the $1 directory.\u0026#34; #echo \u0026#34;You entered the argument: $1,$2,$3, and $4.\u0026#34; 执行\n╭─ ~/shellTest 4m 3s ly@vmmin 11:09:41 ╰─❯ ./16myscript_cls.sh This script requires xxxxone directory path passed to it. Please try again. ╭─ ~/shellTest ly@vmmin 11:09:45 ╰─❯ ./16myscript_cls.sh logfiles You hava 8 objects in the logfiles directory. ╭─ ~/shellTest ly@vmmin 11:09:55 ╰─❯ ./16myscript_cls.sh logfiles x b This script requires xxxxone directory path passed to it. Please try again. 创建备份脚本 # ╭─ ~/shellTest ly@vmmin 12:41:11 ╰─❯ cat ./17myscript_cls.sh #!/bin/bash #如果参数个数不是2则退出，并指定exitCode为1 if [ $# -ne 2 ] then echo \u0026#34;Usage: backup.sh \u0026lt;source_directory\u0026gt; \u0026lt;target_directory\u0026gt;\u0026#34; echo \u0026#34;Please try again.\u0026#34; exit 1 fi # check rsync installed #发送标准错误和标准输出到/dev/null #command -v rsync \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 这条命令，若rsync存在则返回零（真），否则返回非零（假） if ! command -v rsync \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 then echo \u0026#34;This script requires rsync to be installed.\u0026#34; echo \u0026#34;Please use your distribution\u0026#39;s package manager to install it and try again.\u0026#34; #指定exitCode exit 2 fi #格式化date输出，即YYYY-MM-DD current_date=$(date +%Y-%m-%d) # -a 保留所有元数据，权限等 # -v 详细显示输出 #-b、--backup参数指定在删除或更新目标目录已经存在的文件时，将该文件更名后进行备份，默认行为是删除。更名规则是添加由--suffix参数指定的文件后缀名，默认是~。 #--backup-dir参数指定文件备份时存放的目录，比如--backup-dir=/path/to/backups # --delete 确保目标目录是源目录的克隆（完全克隆，不多不少） # --dry-run 尝试执行操作 rsync_options=\u0026#34;-avb --backup-dir $2/$current_date --delete --dry-run\u0026#34; #rsync_options=\u0026#34;-avb --backup-dir $2/$current_date --delete \u0026#34; # $1是源目录 $(which rsync) $rsync_options $1 $2/current \u0026gt;\u0026gt; backup_$current_date.log 运行脚本 # 会提示rsync还没有安装，需要安装\n╭─ ~/shellTest ly@vmmin 14:41:04 ╰─❯ nano 17myscript_cls.sh ╭─ ~/shellTest 27s ly@vmmin 14:41:35 ╰─❯ ./17myscript_cls.sh logfiles backup ╭─ ~/shellTest ly@vmmin 14:41:45 ╰─❯ ./17myscript_cls.sh logfiles/ backup ╭─ ~/shellTest ly@vmmin 14:41:49 ╰─❯ cat backup_2024-12-24.log sending incremental file list created directory backup/current logfiles/ logfiles/a.log logfiles/a.log.tar.gz logfiles/b.log logfiles/b.log.tar.gz logfiles/c.log logfiles/c.log.tar.gz logfiles/xx.txt logfiles/y.txt sent 279 bytes received 81 bytes 720.00 bytes/sec total size is 362 speedup is 1.01 (DRY RUN) sending incremental file list created directory backup/current ./ a.log a.log.tar.gz b.log b.log.tar.gz c.log c.log.tar.gz xx.txt y.txt sent 254 bytes received 80 bytes 668.00 bytes/sec total size is 362 speedup is 1.08 (DRY RUN) backup是空白，因为这只是试运行\n./17myscript_cls.sh logfiles backup和./17myscript_cls.sh logfiles/ backup的区别，后者备份文件夹logfiles下所有文件，而前者备份logfiles(包括文件夹自身)整个文件夹\n把 --dry-run去掉后运行 # 文件查看\n╭─ ~/shellTest ly@vmmin 14:42:53 ╰─❯ ls backup ╭─ ~/shellTest ly@vmmin 14:43:03 ╰─❯ nano 17myscript_cls.sh #第一次备份 ╭─ ~/shellTest 8s ly@vmmin 14:43:17 ╰─❯ ./17myscript_cls.sh logfiles/ backup ╭─ ~/shellTest ly@vmmin 14:43:28 ╰─❯ ls backup current ╭─ ~/shellTest ly@vmmin 14:43:42 ╰─❯ ls backup/current a.log a.log.tar.gz b.log b.log.tar.gz c.log c.log.tar.gz xx.txt y.txt 日志查看\n╭─ ~/shellTest ly@vmmin 14:43:46 ╰─❯ cat backup_2024-12-24.log sending incremental file list created directory backup/current logfiles/ logfiles/a.log logfiles/a.log.tar.gz logfiles/b.log logfiles/b.log.tar.gz logfiles/c.log logfiles/c.log.tar.gz logfiles/xx.txt logfiles/y.txt sent 279 bytes received 81 bytes 720.00 bytes/sec total size is 362 speedup is 1.01 (DRY RUN) sending incremental file list created directory backup/current ./ a.log a.log.tar.gz b.log b.log.tar.gz c.log c.log.tar.gz xx.txt y.txt sent 254 bytes received 80 bytes 668.00 bytes/sec total size is 362 speedup is 1.08 (DRY RUN) sending incremental file list created directory backup/current ./ a.log a.log.tar.gz b.log b.log.tar.gz c.log c.log.tar.gz xx.txt y.txt sent 916 bytes received 208 bytes 2,248.00 bytes/sec total size is 362 speedup is 0.32 此时在logfiles里新建一个文件以及更新一个文件\n╭─ ~/shellTest ly@vmmin 14:43:50 ╰─❯ touch logfiles/testfile.txt ╭─ ~/shellTest ly@vmmin 14:46:48 ╰─❯ touch logfiles/a.log ╭─ ~/shellTest ly@vmmin 14:47:20 ╰─❯ rm backup_2024-12-24.log #第二次备份 ╭─ ~/shellTest ly@vmmin 14:48:22 ╰─❯ ./17myscript_cls.sh logfiles/ backup ╭─ ~/shellTest ly@vmmin 14:49:16 ╰─❯ cat backup_2024-12-24.log sending incremental file list ./ a.log testfile.txt sent 339 bytes received 57 bytes 792.00 bytes/sec total size is 362 speedup is 0.91 查看此时真实目录\n╭─ ~/shellTest ly@vmmin 14:54:58 ╰─❯ ls 10_1myscript_cls.sh 17myscript_cls.sh 62myscript_cls.sh 91myscript_cls.sh 11_1myscript_cls.sh 2myscript_cls.sh 63myscript_cls.sh 92myscript_cls.sh 11_2myscript_cls.sh 31myscript_cls.sh 64myscript_cls.sh backup 12myscript_cls.sh 32myscript_cls.sh 65myscript_cls.sh backup_2024-12-24.log 13myscript_cls.sh 51myscript_cls.sh 71myscript_cls.sh logfiles 14myscript_cls.sh 52myscript_cls.sh 72myscript_cls.sh package_install_results.log 15myscript_cls.sh 53myscript_cls.sh 81myscript_cls.sh package_isntall_failure.log 16myscript_cls.sh 61myscript_cls.sh 82myscript_cls.sh ╭─ ~/shellTest ly@vmmin 14:55:37 ╰─❯ ls backup/current a.log backup b.log.tar.gz c.log.tar.gz xx.txt a.log.tar.gz b.log c.log testfile.txt y.txt #这里会发现，他在替换成新文件前，把旧的文件拷贝到备份文件夹中了 ╭─ ~/shellTest ly@vmmin 14:55:42 ╰─❯ ls backup/current/backup/2024-12-24 a.log 我又修改了一次a.log，变成了这样(深层次)\n╭─ ~/shellTest ly@vmmin 14:55:53 ╰─❯ touch logfiles/a.log ╭─ ~/shellTest ly@vmmin 14:57:52 ╰─❯ ./17myscript_cls.sh logfiles/ backup ╭─ ~/shellTest ly@vmmin 14:57:57 ╰─❯ cat backup_2024-12-24.log sending incremental file list ./ a.log testfile.txt sent 336 bytes received 57 bytes 786.00 bytes/sec total size is 362 speedup is 0.92 sending incremental file list deleting backup/2024-12-24/a.log cannot delete non-empty directory: backup/2024-12-24 cannot delete non-empty directory: backup a.log sent 295 bytes received 165 bytes 920.00 bytes/sec total size is 362 speedup is 0.79 ╭─ ~/shellTest ly@vmmin 14:58:00 ╰─❯ ls backup/current/backup/2024-12-24 a.log backup #注意，这里对a.log又进行了backup备份 ╭─ ~/shellTest ly@vmmin 14:58:17 ╰─❯ ls backup/current/backup/2024-12-24/backup 2024-12-24 ╭─ ~/shellTest ly@vmmin 14:58:23 ╰─❯ ls backup/current/backup/2024-12-24/backup/2024-12-24 a.log 继续Linux的学习 # https://ubuntuserverbook.com/ 作者写的书 https://www.youtube.com/c/LearnLinuxTV 作者的y2b https://learnlinux.tv 作者的网站 "},{"id":8,"href":"/zh/docs/technology/Linux/SHELLlearnLinuxTV_/12-15/","title":"12-15","section":"SHELL编程(learnLinuxTV)","content":" functions 函数 # 以update这个脚本为基础编改\n作用\n减少重复代码 #!/bin/bash release_file=/etc/os-release logfile=/var/log/updater.log errorlog=/var/log/updater_errors.log check_exit_status(){ if [ $? -ne 0 ] then echo \u0026#34;An error occured,please check the $errorlog file.\u0026#34; fi } if grep -q \u0026#34;Arch\u0026#34; $release_file then sudo pacman -Syu 1\u0026gt;\u0026gt;$logfile 2\u0026gt;\u0026gt;$errorlog check_exit_status fi if grep -q \u0026#34;Ubuntu\u0026#34; $release_file || grep -q \u0026#34;Debian\u0026#34; $release_file then sudo apt update 1\u0026gt;\u0026gt;$logfile 2\u0026gt;\u0026gt;$errorlog check_exit_status #默认yes sudo apt dist-upgrade -y 1\u0026gt;\u0026gt;$logfile 2\u0026gt;\u0026gt;$errorlog check_exit_status fi CaseStatements # 脚本 # ╭─ ~/shellTest ly@vmmin 22:32:52 ╰─❯ cat ./13myscript_cls.sh #!/bin/bash finished=0 while [ $finished -ne 1 ] do echo \u0026#34;What is your favorite Linux distribution?\u0026#34; echo \u0026#34;1 - Arch\u0026#34; echo \u0026#34;2 - CentOS\u0026#34; echo \u0026#34;3 - Debian\u0026#34; echo \u0026#34;4 - Mint\u0026#34; echo \u0026#34;5 - Something else..\u0026#34; echo \u0026#34;6 - exit\u0026#34; read distro; case $distro in 1) echo \u0026#34;Arch is xxx\u0026#34;;; 2) echo \u0026#34;CentOS is xbxxx\u0026#34;;; 3) echo \u0026#34;Debian is bbbxx\u0026#34;;; 4) echo \u0026#34;Mint is xxxxsss\u0026#34;;; 5) echo \u0026#34;Something els.xxxxx\u0026#34;;; 6) finished=1 echo \u0026#34;now will exit\u0026#34; ;; *) echo \u0026#34;you didn\u0026#39;t enter an xxxx choice.\u0026#34; esac done echo \u0026#34;Thanks for using this script.\u0026#34; 脚本执行 # ╭─ ~/shellTest ly@vmmin 22:32:11 ╰─❯ ./13myscript_cls.sh What is your favorite Linux distribution? 1 - Arch 2 - CentOS 3 - Debian 4 - Mint 5 - Something else.. 6 - exit 3 Debian is bbbxx What is your favorite Linux distribution? 1 - Arch 2 - CentOS 3 - Debian 4 - Mint 5 - Something else.. 6 - exit u you didn\u0026#39;t enter an xxxx choice. What is your favorite Linux distribution? 1 - Arch 2 - CentOS 3 - Debian 4 - Mint 5 - Something else.. 6 - exit 6 now will exit Thanks for using this script. ScheduleJobs # 作用 # 脚本在特定时间运行\n安装 # ─ ~/shellTest ly@vmmin 22:37:20 ╰─❯ which at at not found ╭─ ~/shellTest ly@vmmin 22:37:23 ╰─❯ sudo apt install at 查看 # ─ ~/shellTest 28s ly@vmmin 22:38:59 ╰─❯ which at /usr/bin/at 示例 # ╰─❯ cat 14myscript_cls.sh #!/bin/bash logfile=job_results.log echo \u0026#34;The script ran at the following time: $(date)\u0026#34; \u0026gt; $logfile ╭─ ~/shellTest ly@vmmin 23:06:05 ╰─❯ date Mon Dec 23 11:06:06 PM CST 2024 ╭─ ~/shellTest ly@vmmin 23:06:06 ╰─❯ at 23:07 -f /home/ly/shellTest/14myscript_cls.sh warning: commands will be executed using /bin/sh job 1 at Mon Dec 23 23:07:00 2024 ╭─ ~/shellTest ly@vmmin 23:06:34 ╰─❯ cat job_results.log The script ran at the following time: Mon Dec 23 11:01:23 PM CST 2024 ╭─ ~/shellTest ly@vmmin 23:06:46 ╰─❯ cat job_results.log The script ran at the following time: Mon Dec 23 11:07:00 PM CST 2024 ╭─ ~/shellTest ly@vmmin 23:07:03 ╰─❯ date Mon Dec 23 11:07:10 PM CST 2024 解释 # at 23:07 -f /home/ly/shellTest/14myscript_cls.sh 23:07没给日期说明是今天，-f表示运行的是一个文件\n查看待运行任务 # ╭─ ~/shellTest ly@vmmin 23:10:53 ╰─❯ at 23:12 -f ./14myscript_cls.sh warning: commands will be executed using /bin/sh job 2 at Mon Dec 23 23:12:00 2024 ╭─ ~/shellTest ly@vmmin 23:11:02 ╰─❯ atq 2\tMon Dec 23 23:12:00 2024 a ly ╭─ ~/shellTest ly@vmmin 23:11:04 ╰─❯ at 23:13 -f ./14myscript_cls.sh warning: commands will be executed using /bin/sh job 3 at Mon Dec 23 23:13:00 2024 ╭─ ~/shellTest ly@vmmin 23:11:12 ╰─❯ atq 3\tMon Dec 23 23:13:00 2024 a ly 2\tMon Dec 23 23:12:00 2024 a ly 删除作业 # ╭─ ~/shellTest ly@vmmin 23:13:09 ╰─❯ at 23:15 -f ./14myscript_cls.sh warning: commands will be executed using /bin/sh job 4 at Mon Dec 23 23:15:00 2024 ╭─ ~/shellTest ly@vmmin 23:13:14 ╰─❯ at 23:16 -f ./14myscript_cls.sh warning: commands will be executed using /bin/sh job 5 at Mon Dec 23 23:16:00 2024 ╭─ ~/shellTest ly@vmmin 23:13:20 ╰─❯ atq 5\tMon Dec 23 23:16:00 2024 a ly 4\tMon Dec 23 23:15:00 2024 a ly ╭─ ~/shellTest ly@vmmin 23:13:24 ╰─❯ atrm 4 ╭─ ~/shellTest ly@vmmin 23:13:31 ╰─❯ atq 5\tMon Dec 23 23:16:00 2024 a ly 日期 # ╭─ ~/shellTest ly@vmmin 23:13:33 ╰─❯ atq 5\tMon Dec 23 23:16:00 2024 a ly ╭─ ~/shellTest ly@vmmin 23:14:52 ╰─❯ at 23:16 122424 -f ./14myscript_cls.sh warning: commands will be executed using /bin/sh job 6 at Tue Dec 24 23:16:00 2024 ╭─ ~/shellTest ly@vmmin 23:15:08 ╰─❯ atq 5\tMon Dec 23 23:16:00 2024 a ly 6\tTue Dec 24 23:16:00 2024 a ly CronJobs # 命令的完整路径 # 安排你的bash脚本，在将来的某个时间执行\n下面使用完全限定名\n╭─ ~/shellTest 19s ly@vmmin 09:08:02 ╰─❯ cat 15myscript_cls.sh #!/bin/bash logfile=job_results.log /usr/bin/echo \u0026#34;The script ran at the following time: $(/uar/bin/date)\u0026#34; \u0026gt; $logfile ╭─ ~/shellTest ly@vmmin 09:08:06 ╰─❯ which查看命令\nly@vmmin:~/shellTest$ which echo /usr/bin/echo ly@vmmin:~/shellTest$ which date /usr/bin/date 能够保持安全性，还有涉及到路径变量（没法找到，或者找错）\n编辑任务 # ╭─ ~/shellTest ly@vmmin 09:17:35 ╰─❯ crontab -e no crontab for ly - using an empty one Select an editor. To change later, run \u0026#39;select-editor\u0026#39;. 1. /bin/nano \u0026lt;---- easiest 2. /usr/bin/vim.basic 3. /usr/bin/vim.tiny Choose 1-3 [1]: 1 #之后会进入nano编辑器并编辑/tmp/crontab.I0AOMk/crontab这个文件（用户自己的，不会干扰其他用户）(crontab.I0AOMk这个文件夹每次都不确定，每次运行crontab -e都是不同文件夹，不过crontab文件内容会跟上次的一样) 编辑内容\n# For more information see the manual pages of cro\u0026gt; # # m h dom mon dow command 30 1 * * 5 /home/ly/shellTest/15myscript_cls.sh 30分钟时执行(每一个小时，即0:30,1:30,2:30），后面两个星号，表示一个月中几号，每年的哪个月，最后这个5表示每星期几（这里是星期五，0跟7都代表星期日）。 这个脚本的意思，每周五凌晨1点30分运行\n30 1 10 7 4 /home/ly/shellTest/15myscript_cls.sh，如果改成这样，则运行的概率极低。即 每年7月10号且星期四，那天里每一个小时到达30分时执行 #用root用户为某个用户创建任务，不存在则创建新文件并编辑任务，存在则继续编辑之前的任务文件 sudo crontab -u ly -e "},{"id":9,"href":"/zh/docs/technology/Linux/SHELLlearnLinuxTV_/11DataStreams/","title":"11DataStreams","section":"SHELL编程(learnLinuxTV)","content":" 下面的输出中，涉及到标准输出的，有十几行的那些，只列举了其中四五行\n概念 # 标准输入，标准输出，标准错误 标准输出：打印到屏幕上的输出 ╭─ ~ ly@vmmin 12:31:44 ╰─❯ ls content.zh index.html myfile dufs.log install.sh shellTest ╭─ ~ ly@vmmin 12:32:21 ╰─❯ echo $? 0 标准错误 ╭─ ~ ly@vmmin 12:30:27 ╰─❯ ls /notexist ls: cannot access \u0026#39;/notexist\u0026#39;: No such file or directory ╭─ ~ ly@vmmin 12:30:59 ╰─❯ echo $? 2 标准输出和标准错误 # 部分重定向 # 标准错误重定向 2\u0026gt; # find，文件系统\nfind /etc -type f\n## 下面是附加知识，最后没用到 #新建一个用户，-m 让用户具有默认主目录，-d指定目录，-s指定用户登入后所使用的shell sudo useradd -d /home/ly1 -s /bin/bash -m ly1 #设置密码 sudo passwd ly1 为了演示错误，先创建几个文件\nroot@vmmin:/home/ly# mkdir a \u0026amp;\u0026amp; touch a/a1.txt a/a2.txt root@vmmin:/home/ly# mkdir b \u0026amp;\u0026amp; touch b/b1.txt b/b2.txt #去除所在组和其他人的所有权限 root@vmmin:/home/ly# chmod 700 a root@vmmin:/home/ly# chmod 700 b ╭─ ~ ly@vmmin 17:02:36 ╰─❯ ls -l total 80 drwx------ 2 root root 4096 Dec 23 16:13 a -rw-r--r-- 1 ly ly 17006 Dec 23 16:04 abc.txt drwx------ 2 root root 4096 Dec 23 16:13 b drwxr-xr-x 3 ly ly 4096 Dec 18 17:33 content.zh -rw-r--r-- 1 ly ly 90 Dec 20 11:20 dufs.log -rw-r--r-- 1 ly ly 19786 Dec 17 23:54 index.html -rw-r--r-- 1 ly ly 18369 Dec 19 15:01 install.sh -rw-r--r-- 1 ly ly 0 Dec 20 22:27 myfile drwxr-xr-x 3 ly ly 4096 Dec 23 10:34 shellTest 用root账号，在/home/ly下面创建了a，b文件夹，以及a1.txt，a2.txt，b1.txt，b2.txt\n，a，b文件夹的权限均为700\n使用find查找，会出现Permission错误\n这里使用-not -path \u0026quot;/home/ly/.**\u0026quot;忽略点开头的文件\n╭─ ~ ly@vmmin 17:02:38 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; find: ‘/home/ly/a’: Permission denied /home/ly/dufs.log find: ‘/home/ly/b’: Permission denied /home/ly/index.html /home/ly/abc.txt /home/ly/shellTest/2myscript_cls.sh /home/ly/shellTest/82myscript_cls.sh /home/ly/shellTest/62myscript_cls.sh /home/ly/shellTest/31myscript_cls.sh /home/ly/shellTest/92myscript_cls.sh 忽略显示错误的信息\n╭─ ~ ly@vmmin 17:05:37 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; 2\u0026gt; /dev/null /home/ly/dufs.log /home/ly/index.html /home/ly/abc.txt /home/ly/shellTest/2myscript_cls.sh /home/ly/shellTest/82myscript_cls.sh /home/ly/shellTest/62myscript_cls.sh /home/ly/shellTest/31myscript_cls.sh /home/ly/shellTest/92myscript_cls.sh /home/ly/shellTest/61myscript_cls.sh /home/ly/shellTest/32myscript_cls.sh /home/ly/shellTest/64myscript_cls.sh /home/ly/shellTest/65myscript_cls.sh /home/ly/shellTest/53myscript_cls.sh /home/ly/shellTest/72myscript_cls.sh /home/ly/shellTest/52myscript_cls.sh /home/ly/shellTest/81myscript_cls.sh ╭─ ~ ly@vmmin 17:05:47 ╰─❯ echo $? 1 echo $?结果为1说明其实出错了，但是没有显示。\n\u0026gt;号用来重定向 /dev/null dev null\n构成错误-标准错误的每一行将被发送到Dev null而不是屏幕\n标准输出重定向1\u0026gt;或\u0026gt; # ╭─ ~ ly@vmmin 17:12:43 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; \u0026gt; /dev/null find: ‘/home/ly/a’: Permission denied find: ‘/home/ly/b’: Permission denied 1\u0026gt; 和 \u0026gt; 是一样的结果，都是重定向标准输出\n╭─ ~ ly@vmmin 17:12:43 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; 1\u0026gt; /dev/null find: ‘/home/ly/a’: Permission denied find: ‘/home/ly/b’: Permission denied 重定向到文件 # ╭─ ~ ly@vmmin 17:15:07 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; 1\u0026gt; file.txt find: ‘/home/ly/a’: Permission denied find: ‘/home/ly/b’: Permission denied ╭─ ~ ly@vmmin 17:16:35 ╰─❯ cat file.txt /home/ly/dufs.log /home/ly/index.html /home/ly/file.txt /home/ly/abc.txt /home/ly/shellTest/2myscript_cls.sh /home/ly/shellTest/82myscript_cls.sh /home/ly/shellTest/62myscript_cls.sh /home/ly/shellTest/31myscript_cls.sh /home/ly/shellTest/92myscript_cls.sh /home/ly/shellTest/61myscript_cls.sh /home/ly/shellTest/32myscript_cls.sh /home/ly/shellTest/64myscript_cls.sh /home/ly/shellTest/65myscript_cls.sh /home/ly/shellTest/53myscript_cls.sh 同时重定向标准输出和标准错误 # 同时重定向到一个 # 在Shell中，标准错误写法为 2\u0026gt;, 标准输出为 1\u0026gt; 或者 \u0026gt;。如要要将标准输出和标准错误合二为一，都重定向到同一个文件，可以使用下面两种方式:\n\u0026amp;\u0026gt; # ╭─ ~ ly@vmmin 17:22:07 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; \u0026amp;\u0026gt; file.txt ╭─ ~ ly@vmmin 17:22:14 ╰─❯ cat file.txt find: ‘/home/ly/a’: Permission denied /home/ly/dufs.log find: ‘/home/ly/b’: Permission denied /home/ly/index.html /home/ly/file.txt /home/ly/abc.txt /home/ly/shellTest/2myscript_cls.sh /home/ly/shellTest/82myscript_cls.sh /home/ly/shellTest/62myscript_cls.sh /home/ly/shellTest/31myscript_cls.sh /home/ly/shellTest/92myscript_cls.sh /home/ly/shellTest/61myscript_cls.sh 2\u0026gt;\u0026amp;1 # ╭─ ~ ly@vmmin 17:24:41 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; \u0026gt; file.txt 2\u0026gt;\u0026amp;1 ╭─ ~ ly@vmmin 17:24:57 ╰─❯ cat file.txt find: ‘/home/ly/a’: Permission denied /home/ly/dufs.log find: ‘/home/ly/b’: Permission denied /home/ly/index.html /home/ly/file.txt /home/ly/abc.txt /home/ly/shellTest/2myscript_cls.sh /home/ly/shellTest/82myscript_cls.sh /home/ly/shellTest/62myscript_cls.sh /home/ly/shellTest/31myscript_cls.sh /home/ly/shellTest/92myscript_cls.sh 一条语句分别重定向到多个 # ╭─ ~ ly@vmmin 17:30:48 ╰─❯ find ~ -type f -not -path \u0026#34;/home/ly/.**\u0026#34; 1\u0026gt;find_results.txt 2\u0026gt;find_errors.txt ╭─ ~ ly@vmmin 17:31:06 ╰─❯ cat find_results.txt /home/ly/dufs.log /home/ly/index.html /home/ly/file.txt /home/ly/abc.txt /home/ly/shellTest/2myscript_cls.sh /home/ly/shellTest/82myscript_cls.sh /home/ly/shellTest/62myscript_cls.sh /home/ly/shellTest/31myscript_cls.sh /home/ly/shellTest/92myscript_cls.sh /home/ly/shellTest/61myscript_cls.sh /home/ly/shellTest/32myscript_cls.sh /home/ly/shellTest/64myscript_cls.sh ╭─ ~ ly@vmmin 17:31:24 ╰─❯ cat find_errors.txt find: ‘/home/ly/a’: Permission denied find: ‘/home/ly/b’: Permission denied 也可以使用find ~ -type f -not -path \u0026quot;/home/ly/.**\u0026quot; \u0026gt;find_results.txt 2\u0026gt;find_errors.txt\nupdate脚本（之前的） # ╭─ ~ ly@vmmin 17:36:08 ╰─❯ cat /usr/local/bin/update #!/bin/bash release_file=/etc/os-release if grep -q \u0026#34;Arch\u0026#34; $release_file then sudo pacman -Syu fi if grep -q \u0026#34;Ubuntu\u0026#34; $release_file || grep -q \u0026#34;Debian\u0026#34; $release_file then sudo apt update sudo apt dist-upgrade fi 修改\n╭─ ~/shellTest ly@vmmin 17:46:55 ╰─❯ cat 11_1_1myscript_cls.sh #!/bin/bash release_file=/etc/os-release logfile=/var/log/updater.log errorlog=/var/log/updater_errors.log if grep -q \u0026#34;Arch\u0026#34; $release_file then sudo pacman -Syu 1\u0026gt;\u0026gt;$logfile 2\u0026gt;\u0026gt;$errorlog if [ $? -ne 0 ] then echo \u0026#34;An error occured,please check the $errorlog file.\u0026#34; fi fi if grep -q \u0026#34;Ubuntu\u0026#34; $release_file || grep -q \u0026#34;Debian\u0026#34; $release_file then sudo apt update 1\u0026gt;\u0026gt;$logfile 2\u0026gt;\u0026gt;$errorlog if [ $? -ne 0 ] then echo \u0026#34;An error occured,please check the $errorlog file.\u0026#34; fi sudo apt dist-upgrade -y 1\u0026gt;\u0026gt;$logfile 2\u0026gt;\u0026gt;$errorlog if [ $? -ne 0 ] then echo \u0026#34;An error occured,please check the $errorlog file.\u0026#34; fi fi 切换到root用户并执行\n╭─ ~/shellTest ly@vmmin 17:50:25 ╰─❯ su root - Password: root@vmmin:/home/ly/shellTest# ./11_1_1myscript_cls.sh 查看\n#root用户下 root@vmmin:/home/ly/shellTest# cat /var/log/updater.log Hit:1 https://mirrors.tuna.tsinghua.edu.cn/debian bookworm InRelease Hit:2 https://mirrors.tuna.tsinghua.edu.cn/debian bookworm-updates InRelease Hit:3 https://mirrors.tuna.tsinghua.edu.cn/debian bookworm-backports InRelease Hit:4 https://mirrors.tuna.tsinghua.edu.cn/debian-security bookworm-security InRelease Hit:5 https://security.debian.org/debian-security bookworm-security InRelease Reading package lists... Building dependency tree... Reading state information... All packages are up to date. Reading package lists... Building dependency tree... Reading state information... Calculating upgrade... 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. root@vmmin:/home/ly/shellTest# cat var/log/updater_errors.log cat: var/log/updater_errors.log: No such file or directory #这里没有出错，所以甚至连错误文件都没有 附加知识，监听文本文件\n╭─ ~ ly@vmmin 18:00:17 ╰─❯ sudo tail -f /var/log/updater.log Hit:5 https://security.debian.org/debian-security bookworm-security InRelease Reading package lists... Building dependency tree... Reading state information... All packages are up to date. Reading package lists... Building dependency tree... Reading state information... Calculating upgrade... 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. 标准输入 # ╭─ ~/shellTest 5s ly@vmmin 18:07:53 ╰─❯ cat 11_2myscript_cls.sh #!/bin/bash echo \u0026#34;Please enter your name:\u0026#34; read myname echo \u0026#34;Your name is: $myname\u0026#34; ╭─ ~/shellTest 1m 6s ly@vmmin 18:07:42 ╰─❯ ./11_2myscript_cls.sh Please enter your name: JayH Your name is: JayH "},{"id":10,"href":"/zh/docs/technology/Linux/SHELLlearnLinuxTV_/07-10/","title":"07-10","section":"SHELL编程(learnLinuxTV)","content":" WhileLoops # 范例 # #!/bin/bash myvar=1 #小于或者等于10 while [ $myvar -le 10 ] do echo $myvar myvar=$(( $myvar + 1 )) sleep 0.5 done 运行\n╭─ ~/shellTest ≡ ly@vmmin 12:10:33 ╰─❯ ./71myscript_cls.sh 1 2 3 4 5 6 7 8 9 10 数字会每隔0.5s就输出一次\n对于myvar=$(( $myvar + 1 )) ，$((expression))形式表示算数运算，而且其中的空格是可以省略的\n范例2 # #!/bin/bash while [ -f ~/testfile ] do echo \u0026#34;As of $(date),the test file exists.\u0026#34; sleep 5 done echo \u0026#34;As of $(date), the test ....has gone missing.\u0026#34; 用来测试文件是否存在，运行前先新建一下文件touch ~/testfile 运行一会后把文件删除，如图\ndate命令包含在子shell中，因此date命令将在后台运行并将该命令的输出替换$(date)这部分\n更新相关的脚本 # 基本概念 # upgrade：系统将现有的Package升级，如果有相依性的问题，而此相依性需要安装其它新的Package或影响到其它Package的相依性时，此Package就不会被升级，会保留下来。\ndist-upgrade：可以聪明的解决相依性的问题，如果有相依性问题，需要安装/移除新的Package，就会试着去安装/移除它。\ngrep -q，安静模式，不打印任何标准输出。如果有匹配的内容则立即返回状态值0\nshell中，零为真，非零为假\n#!/bin/bash release_file=/etc/os-release #这里没有使用[]测试命令，而是使用Linux命令 # #号用来注释，除了第一行shebang比较特殊 if grep -q \u0026#34;Arch\u0026#34; $release_file then sudo pacman -Syu fi # ||或者，\u0026amp;\u0026amp; 与， if grep -q \u0026#34;Ubuntu\u0026#34; $release_file || grep -q \u0026#34;Debian\u0026#34; $release_file then sudo apt update sudo apt dist-upgrade fi for语句 # #!/bin/bash for current_number in 1 2 3 4 5 6 7 8 9 10 do echo $current_number sleep 1 done echo \u0026#34;This is outside of the for loop.\u0026#34; for语句进入do语句前，current_number指向1，1的do结束后current_number指向2\n─ ~/shellTest ly@vmmin 21:55:34 ╰─❯ ./9myscript_cls.sh 1 2 3 4 5 6 7 8 9 10 This is outside of the for loop. 简化\n#!/bin/bash for current_number in {1..10} #for current_number in {a..z} #字母也行 do echo $current_number sleep 1 done echo \u0026#34;This is outside of the for loop.\u0026#34; #!/bin/bash for n in {1..10} #for n in {a..z} #字母也行 do echo $n sleep 1 done echo \u0026#34;This is outside of the for loop.\u0026#34; 文件遍历 # ─ ~/shellTest ly@vmmin 23:15:41 ╰─❯ ls logfiles a.log b.log c.log xx.txt y.txt 脚本：\n#!/bin/bash for file in logfiles/*.log do tar -czvf $file.tar.gz $file done tar命令，tar -czvf c : create，z : zip，v: view，f: file\n结果：\n╭─ ~/shellTest ly@vmmin 23:26:15 ╰─❯ ls logfiles a.log b.log c.log xx.txt a.log.tar.gz b.log.tar.gz c.log.tar.gz y.txt 可以用来循环发送日志文件（提到，没例子） # 脚本保存位置 # 主要讨论脚本应该放在哪个公共位置才可以让所有人都可以访问\n为需要的人提供脚本\nfile system hierarchy standard，文件系统层次结构标准，简称FHS 这个东西存在的目的，\u0026ldquo;所有Linux发行版上都可以找到的每个典型目录\u0026rdquo;。\nFHS指出了与本地安装的程序一起使用的用户本地目录（给系统管理员使用），bin目录也位于用户本地，我们将在其中放置脚本\n─ ~/shellTest ly@vmmin 10:34:13 ╰─❯ sudo mv 10_1myscript_cls.sh /usr/local/bin/update ╭─ ~/shellTest 3s ly@vmmin 10:28:43 ╰─❯ ls -l /usr/local/bin total 77876 -rwxr-xr-x 1 root root 4488672 Dec 17 16:28 dufs -rwxr-xr-x 1 root root 75247968 Dec 17 16:44 hugo -rwxr-xr-x 1 root root 231 Dec 23 10:28 update ╭─ ~/shellTest ly@vmmin 10:34:58 ╰─❯ ls -l /usr/local/bin total 77876 -rwxr-xr-x 1 root root 4488672 Dec 17 16:28 dufs -rwxr-xr-x 1 root root 75247968 Dec 17 16:44 hugo -rwxr-xr-x 1 ly ly 231 Dec 23 10:33 update 现在需要让这个脚本由root拥有，以确保有人需要pseudo privileges 伪权限或者root permissions root权限才能修改该脚本，不能让（普通）用户修改\n╭─ ~/shellTest ly@vmmin 10:35:05 ╰─❯ sudo chown root:root /usr/local/bin/update ╭─ ~/shellTest ly@vmmin 10:40:32 ╰─❯ ls -l /usr/local/bin total 77876 -rwxr-xr-x 1 root root 4488672 Dec 17 16:28 dufs -rwxr-xr-x 1 root root 75247968 Dec 17 16:44 hugo -rwxr-xr-x 1 root root 231 Dec 23 10:33 update Linux中任何脚本其实都不需要后缀的，所以这里删除了 .sh 。\n因为第一行shebang已经指明了需要使用到什么解释器\n使用 # ╭─ ~ ly@vmmin 11:39:04 ╰─❯ ls content.zh dufs.log index.html install.sh myfile shellTest ╭─ ~ ly@vmmin 11:39:05 ╰─❯ update Hit:1 https://mirrors.tuna.tsinghua.edu.cn/debian bookworm InRelease Hit:2 https://mirrors.tuna.tsinghua.edu.cn/debian bookworm-updates InRelease Hit:3 https://mirrors.tuna.tsinghua.edu.cn/debian bookworm-backports InRelease Hit:4 https://mirrors.tuna.tsinghua.edu.cn/debian-security bookworm-security InRelease Hit:5 https://security.debian.org/debian-security bookworm-security InRelease Reading package lists... Done Building dependency tree... Done Reading state information... Done All packages are up to date. Reading package lists... Done Building dependency tree... Done Reading state information... Done Calculating upgrade... Done 0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded. ╭─ ~ 13s ly@vmmin 11:39:19 ╰─❯ which update /usr/local/bin/update 运行update命令的时候，是需要sudo权限的\n且不需要指定具体完整路径，就可以使用update文件\n有一个系统变量，告诉shell将在其中查找所有的目录\n全大写表示系统变量\n─ ~ ly@vmmin 11:44:21 ╰─❯ echo $PATH /usr/local/bin:/usr/bin:/bin:/usr/games 系统变量查看\n╭─ ~ ly@vmmin 11:45:48 ╰─❯ env USER=ly LOGNAME=ly HOME=/home/ly PATH=/usr/local/bin:/usr/bin:/bin:/usr/games SHELL=/usr/bin/zsh TERM=xterm DISPLAY=localhost:11.0 XDG_SESSION_ID=99 XDG_RUNTIME_DIR=/run/user/1000 DBUS_SESSION_BUS_ADDRESS=unix:path=/run/user/1000/bus XDG_SESSION_TYPE=tty XDG_SESSION_CLASS=user MOTD_SHOWN=pam LANG=en_US.UTF-8 SSH_CLIENT=192.168.1.201 52599 22 SSH_CONNECTION=192.168.1.201 52599 192.168.1.206 22 SSH_TTY=/dev/pts/2 SHLVL=1 PWD=/home/ly OLDPWD=/home/ly/shellTest P9K_TTY=old _P9K_TTY=/dev/pts/2 ZSH=/home/ly/.oh-my-zsh PAGER=less LESS=-R LSCOLORS=Gxfxcxdxbxegedabagacad LS_COLORS=rs=0:di=01;34:ln=01;36:mh=00:pi=40;33:so=01;35:do=01;35:bd=40;33;01:cd=40;33;01:or=40;31;01:mi=00:su=37;41:sg=30;43:ca=00:tw=30;42:ow=34;42:st=37;44:ex=01;32:*.tar=01;31:*.tgz=01;31:*.arc=01;31:*.arj=01;31:*.taz=01;31:*.lha=01;31:*.lz4=01;31:*.lzh=01;31:*.lzma=01;31:*.tlz=01;31:*.txz=01;31:*.tzo=01;31:*.t7z=01;31:*.zip=01;31:*.z=01;31:*.dz=01;31:*.gz=01;31:*.lrz=01;31:*.lz=01;31:*.lzo=01;31:*.xz=01;31:*.zst=01;31:*.tzst=01;31:*.bz2=01;31:*.bz=01;31:*.tbz=01;31:*.tbz2=01;31:*.tz=01;31:*.deb=01;31:*.rpm=01;31:*.jar=01;31:*.war=01;31:*.ear=01;31:*.sar=01;31:*.rar=01;31:*.alz=01;31:*.ace=01;31:*.zoo=01;31:*.cpio=01;31:*.7z=01;31:*.rz=01;31:*.cab=01;31:*.wim=01;31:*.swm=01;31:*.dwm=01;31:*.esd=01;31:*.avif=01;35:*.jpg=01;35:*.jpeg=01;35:*.mjpg=01;35:*.mjpeg=01;35:*.gif=01;35:*.bmp=01;35:*.pbm=01;35:*.pgm=01;35:*.ppm=01;35:*.tga=01;35:*.xbm=01;35:*.xpm=01;35:*.tif=01;35:*.tiff=01;35:*.png=01;35:*.svg=01;35:*.svgz=01;35:*.mng=01;35:*.pcx=01;35:*.mov=01;35:*.mpg=01;35:*.mpeg=01;35:*.m2v=01;35:*.mkv=01;35:*.webm=01;35:*.webp=01;35:*.ogm=01;35:*.mp4=01;35:*.m4v=01;35:*.mp4v=01;35:*.vob=01;35:*.qt=01;35:*.nuv=01;35:*.wmv=01;35:*.asf=01;35:*.rm=01;35:*.rmvb=01;35:*.flc=01;35:*.avi=01;35:*.fli=01;35:*.flv=01;35:*.gl=01;35:*.dl=01;35:*.xcf=01;35:*.xwd=01;35:*.yuv=01;35:*.cgm=01;35:*.emf=01;35:*.ogv=01;35:*.ogx=01;35:*.aac=00;36:*.au=00;36:*.flac=00;36:*.m4a=00;36:*.mid=00;36:*.midi=00;36:*.mka=00;36:*.mp3=00;36:*.mpc=00;36:*.ogg=00;36:*.ra=00;36:*.wav=00;36:*.oga=00;36:*.opus=00;36:*.spx=00;36:*.xspf=00;36:*~=00;90:*#=00;90:*.bak=00;90:*.old=00;90:*.orig=00;90:*.part=00;90:*.rej=00;90:*.swp=00;90:*.tmp=00;90:*.dpkg-dist=00;90:*.dpkg-old=00;90:*.ucf-dist=00;90:*.ucf-new=00;90:*.ucf-old=00;90:*.rpmnew=00;90:*.rpmorig=00;90:*.rpmsave=00;90: P9K_SSH=1 _P9K_SSH_TTY=/dev/pts/2 _=/usr/bin/env 如果想要修改与路径变量PATH不同的目录\n#如果/usr/bin/local/bin默认没有添加到path里面的情况 export PATH=/usr/bin/local/bin:$PATH "},{"id":11,"href":"/zh/docs/life/dailyExcerpt/","title":"每日摘抄","section":"生活","content":" 欲买桂花同载酒，终不似，少年游。 君埋泉下泥销骨，我寄人间雪满头。 吾不识青天高，黄地厚。唯见月寒日暖，来煎人寿。 \u0026ldquo;老妈看不到你变老的样子了\u0026rdquo; 我也曾闪亮如星，而非没入尘埃。 我并非一直无人问津，也曾有人对我寄予厚望。 \u0026ldquo;你要好好读书，将来让他们都有工作。\u0026rdquo; 人道洛阳花似锦，偏我来时不逢春。 "},{"id":12,"href":"/zh/docs/technology/Linux/SHELLlearnLinuxTV_/06ExitCode/","title":"06ExitCode","section":"SHELL编程(learnLinuxTV)","content":" 意义 # 用来确定代码是否执行成功\n例子 # ls -l /misc echo $? #输出2 ls -l ~ echo $? #输出0 $?用来显示最近一个命令的状态，零表示成功，非零表示失败\n#!/bin/bash #这个例子之前，作者用 sudo apt remove htop 命令把htop删除了 package=htop sudo apt install $package echo \u0026#34;The exit code for ....is $?\u0026#34; 安装完毕后显示返回0\n另一个示例\n#!/bin/bash package=notexist sudo apt install $package echo \u0026#34;The exit code for ....is $?\u0026#34; #执行后显示 #Reading package lists... Done #Building dependency tree... Done #Reading state information... Done #E: Unable to locate package notexist #The exit code for ....is 100 配合if语句 # 基本功能 # #!/bin/bash package=htop sudo apt install $package if [ $? -eq 0 ] then echo \u0026#34;The installation of $package success...\u0026#34; echo \u0026#34;new comman here:\u0026#34; which $package else echo \u0026#34;$package failed ...\u0026#34; fi 之前前作者用sudo apt remove htop又把htop删除了，不过其实不删除也是走的 echo \u0026quot;The installation of .....\u0026quot;这个分支\n结果：\nxxxxxx.... kB] Fetched 152 kB in 1s (292 kB/s) Selecting previously unselected package htop. (Reading database ... 38811 files and directories currently installed.) Preparing to unpack .../htop_3.2.2-2_amd64.deb ... Unpacking htop (3.2.2-2) ... Setting up htop (3.2.2-2) ... Processing triggers for mailcap (3.70+nmu1) ... Processing triggers for man-db (2.11.2-2) ... The installation of htop success... new comman here: /usr/bin/htop 修改后：\n#!/bin/bash package=notexit sudo apt install $package if [ $? -eq 0 ] then echo \u0026#34;The installation of $package success...\u0026#34; echo \u0026#34;new comman here:\u0026#34; which $package else echo \u0026#34;$package failed ...\u0026#34; fi 再次运行的结果：\nReading package lists... Done Building dependency tree... Done Reading state information... Done E: Unable to locate package notexit notexit failed ... 重定向到文件 # 先把htop再次提前卸载sudo apt remove htop 成功 # #!/bin/bash package=htop sudo apt install $package \u0026gt;\u0026gt; package_install_results.log if [ $? -eq 0 ] then echo \u0026#34;The installation of $package success...\u0026#34; echo \u0026#34;new comman here:\u0026#34; which $package else echo \u0026#34;$package failed ...\u0026#34; \u0026gt;\u0026gt; package_isntall_failure.log fi 结果：\n╭─ ~/shellTest ≡ ly@vmmin 11:14:08 ╰─❯ ./63myscript_cls.sh WARNING: apt does not have a stable CLI interface. Use with caution in scripts. The installation of htop success... new comman here: /usr/bin/htop 查看文件：\n─ ~/shellTest ≡ ly@vmmin 11:15:06 ╰─❯ cat package_install_results.log Reading package lists... Building dependency tree... Reading state information... Suggested packages: lm-sensors strace The following NEW packages will be installed: htop 0 upgraded, 1 newly installed, 0 to remove and 0 not upgraded. Need to get 152 kB of archives. After this operation, 387 kB of additional disk space will be used. Get:1 https://mirrors.tuna.tsinghua.edu.cn/debian bookworm/main amd64 htop amd64 3.2.2-2 [152 kB] Fetched 152 kB in 1s (202 kB/s) Selecting previously unselected package htop. (Reading database ... 38811 files and directories currently installed.) Preparing to unpack .../htop_3.2.2-2_amd64.deb ... Unpacking htop (3.2.2-2) ... Setting up htop (3.2.2-2) ... Processing triggers for mailcap (3.70+nmu1) ... Processing triggers for man-db (2.11.2-2) ... 失败 # #!/bin/bash package=notexit sudo apt install $package \u0026gt;\u0026gt; package_install_results.log if [ $? -eq 0 ] then echo \u0026#34;The installation of $package success...\u0026#34; echo \u0026#34;new comman here:\u0026#34; which $package else echo \u0026#34;$package failed ...\u0026#34; \u0026gt;\u0026gt; package_isntall_failure.log fi 结果：\n─ ~/shellTest ≡ ly@vmmin 11:17:23 ╰─❯ ./63myscript_cls.sh WARNING: apt does not have a stable CLI interface. Use with caution in scripts. E: Unable to locate package notexit 查看文件\n╭─ ~/shellTest ≡ ly@vmmin 11:17:26 ╰─❯ cat *fail* notexit failed ... 退出代码的测试 # 代码\n#!/bin/bash directory=/notexist if [ -d $directory ] then echo $? #测试失败，是非0 echo \u0026#34;The directory $directory exists.\u0026#34; else echo $? #测试失败，是非0 echo \u0026#34;The directory $directory doesn\u0026#39;t exist.\u0026#34; fi #最近一个命令是echo，echo确实正确执行并输出了，所以上一个指令执行成功，返回0 echo \u0026#34;The exit code ....is $?\u0026#34; 结果\n╭─ ~/shellTest ≡ ly@vmmin 11:43:24 ╰─❯ ./64myscript_cls.sh 1 The directory /notexist doesn\u0026#39;t exist. The exit code ....is 0 控制退出代码的结果 # ─ ~/shellTest ≡ ly@vmmin 11:50:29 ╰─❯ cat ./65myscript_cls.sh #!/bin/bash echo \u0026#34;Hello world\u0026#34; exit 199 #这行代码永远都不会执行 echo \u0026#34;never exec\u0026#34; 结果：\n╭─ ~/shellTest ✘ STOP 1m 16s ≡ ly@vmmin 11:50:16 ╰─❯ ./65myscript_cls.sh Hello world ╭─ ~/shellTest ✘ 199 ≡ ly@vmmin 11:50:23 ╰─❯ echo $? 199 执行失败 # 以最后一次exit返回的code为最终结果\n虽然执行失败了，但是返回值还是以我们给出的为结果\n╭─ ~/shellTest ✘ INT ≡ ly@vmmin 11:53:24 ╰─❯ cat 65myscript_cls.sh #!/bin/bash sudo apt install notexist exit 0 #这行代码永远都不会执行 echo \u0026#34;never exec\u0026#34; 结果：\n╭─ ~/shellTest ✘ STOP 8s ≡ ly@vmmin 11:55:04 ╰─❯ ./65myscript_cls.sh Reading package lists... Done Building dependency tree... Done Reading state information... Done E: Unable to locate package notexist ╭─ ~/shellTest ≡ ly@vmmin 11:55:07 ╰─❯ echo $? 0 exit最本质含义 # !/bin/bash directory=/notexist if [ -d $directory ] then echo \u0026#34;The directory $directory exists.\u0026#34; exit 0 else echo \u0026#34;The directory $directory doesn\u0026#39;t exist.\u0026#34; exit 1 fi #下面这三行永远不会执行，因为上面的任何一个分支都会导致退出程序执行 echo \u0026#34;The exit code for this ....is: $?\u0026#34; echo \u0026#34;You didn\u0026#39;t ..see this..\u0026#34; echo \u0026#34;You won\u0026#39;t see ...\u0026#34; "},{"id":13,"href":"/zh/docs/technology/Linux/SHELLlearnLinuxTV_/05If/","title":"05If","section":"SHELL编程(learnLinuxTV)","content":" 在shell中，零为真，非零为假。\nif then fi # mynum=200 #[和]前后都要有空格 if [ $mynum -eq 200 ] then echo \u0026#34;The condition is true.\u0026#34; fi 编辑之后，按ctrl + O 保存文件\nctrl + T + Z 保持在后台，fg+回车 恢复\n#!/bin/bash mynum=200 #[和]前后都要有空格 if [ $mynum -eq 200 ] then echo \u0026#34;The condition is true.\u0026#34; fi if [ $mynum -eq 300 ] then echo \u0026#34;The variable does not equal 200.\u0026#34; fi else if # #!/bin/bash mynum=300 #[和]前后都要有空格 if [ $mynum -eq 200 ] then echo \u0026#34;The condition is true.\u0026#34; else echo \u0026#34;The variable does not equal\u0026gt; fi ! # #!/bin/bash mynum=300 #[和]前后都要有空格 #!用来反转条件 if [ ! $mynum -eq 200 ] then echo \u0026#34;The condition is true.\u0026#34; else echo \u0026#34;The variable does not equal 200.\u0026#34; fi ne # #!/bin/bash mynum=300 #[和]前后都要有空格 if [ $mynum -ne 200 ] then echo \u0026#34;The condition is true.\u0026#34; else echo \u0026#34;The variable does not equal 200.\u0026#34; fi 其他 # -gt 大于\n-f 文件是否存在 # #!/bin/bash if [ -f ~/myfile ] then echo \u0026#34;The file exists.\u0026#34; else echo \u0026#34;The file does not exists.\u0026#34; fi touch # 文件不存在则创建文件；存在则更新修改时间\n-d查看是否存在某个目录\n配合install # which查看是否存在应用程序\n先是which htop，结果是空 说明暂时没有安装该程序\n编辑程序\n#!/bin/bash command=/usr/bin/htop #查看程序文件是否存在 if [ -f $command ] then echo \u0026#34;$command is available,let\u0026#39;s run it ...\u0026#34; else #不存在则进行安装 echo \u0026#34;$command is NOT available, installing it...\u0026#34; sudo apt update \u0026amp;\u0026amp; sudo apt install -y htop fi $command 首先apt update只是用来更新软件包列表，与镜像存储库同步，找出实际可用的软件包，并不会实际更新软件。这就是为什么上面要先更新列表之后再安装。 其次，经常有时候要apt update之后apt upgrade(这个命令才实际更新了软件) \u0026amp;\u0026amp;用来命令链接，如果第一个命令成功，将立即运行第二个命令。失败则不运行。-y表示不要确认提示，只需继续运行即可（-y：当安装过程提示选择全部为\u0026quot;yes\u0026quot; ） 还有一点，在此之前我已经将我该用户ly添加进了sudoer组，即使用root用户运行 sudo usermod -aG sudo ly 命令（sudo deluser ly sudo 移出sudo组）。解释：-a 参数表示附加，只和 -G 参数一同使用，表示将用户增加到组中；即将ly添加到sudo组中。 简化\n#!/bin/bash command=htop #这里删除了[]，因为command本身就是一个测试命令 if command -v $command then echo \u0026#34;$command is available,let\u0026#39;s run it ...\u0026#34; else echo \u0026#34;$command is NOT available, installing it...\u0026#34; sudo apt update \u0026amp;\u0026amp; sudo apt install -y $command fi $command man # man test 补充 # 我经常用的是[[ ]] 这个命令，感觉比较直观，很多运算符都能用上。[]这个命令有些运算符没法用\n"},{"id":14,"href":"/zh/docs/technology/Linux/SHELLlearnLinuxTV_/01-04/","title":"01-04","section":"SHELL编程(learnLinuxTV)","content":" 意义 # 执行一系列命令\n视频框架 # 介绍，欢迎 HelloWorld 变量 数学函数 if语句 退出代码 while循环 更新脚本，保持服务器最新状态 for循环 脚本应该存储在文件系统哪个位置 数据流，标准输入、标准输出、标准错误输出 函数 case语句 调度作业（SchedulingJobs）Part1 调度作业（SchedulingJobs）Part2 传递参数 备份脚本 准备 # 需要一台运行Linux系统的计算机（或虚拟机）\n一些基本操作 # 新建或编辑脚本 # nano myscript.sh 内容 # ctrl + o 保存，ctrl + x 退出\n如何执行脚本 # 权限 # #给脚本赋予执行的权限 sudo chmod +x myscript.sh 执行 # 执行前查看权限 # 运行 # ./myscript.sh 查看脚本 # cat myscript.sh 更多语句的脚本 # ls pwd 输出\nshebang # 告诉系统哪个解释器准备运行脚本(不特别指定的情况)，比如bash ./myscript.sh就特别指明了用bash运行脚本，所以这里指的是./myscript.sh这种情况使用的哪个默认解释器\n#!/bin/bash echo \u0026#34;Hello World!\u0026#34; echo \u0026#34;My current working directory is:\u0026#34; #结果中pwd会另取一行跟这里的显式换行没关系， 我猜是echo在最末尾加了\\n换行符 pwd 关于echo行末换行符 # echo -n abc;echo c 这里使用-n禁止输出默认换行符，所以两个c连接上了\n变量 # 变量左右两侧都不允许有空格！！ # nano快捷键 # ctrl + k ，删除当前行\n基本使用 # #!/bin/bash myname=\u0026#34;Jay\u0026#34; #myage=\u0026#34;40\u0026#34; my=\u0026#34;xxx\u0026#34; myage=\u0026#34;40\u0026#34; #\u0026#34;\u0026#34;和\u0026#39;\u0026#39;的区别 echo \u0026#39;Hello, my name is $myname.\u0026#39; echo \u0026#34;Hello, my name is $myname.\u0026#34; #注意下面这句，不会去找变量m，my或者mya(以word字符为界，即字母或下划线为开头，直到字母或数字或下划线终止) echo \u0026#34;I\u0026#39;m $myage years old.\u0026#34; #下面这句，将单引号进行了转义 #视频中的方法有点问题，这里貌似只能通过 #下面这种分段的方法 echo \u0026#39;I\u0026#39;\\\u0026#39;\u0026#39;m $myage years old.\u0026#39; 减少重复操作 # # myscript.sh #!/bin/bash word=\u0026#34;fun\u0026#34; echo \u0026#34;Linux is $word\u0026#34; echo \u0026#34;Vediogames are $word\u0026#34; echo \u0026#34;Sunny days are $word\u0026#34; 存储临时值 # now=$(date) echo \u0026#34;The system time and date is:\u0026#34; echo $now 系统环境变量(默认变量) # 视频中的 # 输出\n自己测试 # 系统变量字母全是大写英文 # #查看系统变量 env 数学函数 # 运算符左右两边都要有空格!! # shell中执行算术运算 # expr 3 + 3 expr 30 - 10 expr 30 / 10 乘法*号是通配符 # 反斜杠转义星号\nexpr 100 \\* 4 变量运算 # "},{"id":15,"href":"/zh/docs/technology/RegExp/baseCoreySchafer/base/","title":"基础","section":"基础(CoreySchafer)","content":" 环境 # 使用视频作者给出的示例，https://github.com/CoreyMSchafer/code_snippets/tree/master/Regular-Expressions 使用sublimeText打开的文件，ctrl+f时要确认勾选正则及区分大小写\nsimple.txt-基础操作 # 直接搜索 # 任意字符 # 这里默认不会显示所有，点击findAll才会出来\n有些字符需要加反斜杠转义，比如 . （点）以及 \\ （斜杠本身） # /////，从左到右，和书写方向一致的叫做(正)斜杠。\n反之，叫做反斜杠 \\\n一些元字符 # . - Any Character Except New Line 除了换行符的任意字符 \\d - Digit (0-9) 数字 \\D - Not a Digit (0-9) 非数字 \\w - Word Character (a-z, A-Z, 0-9, _) 单词字符，大小写字母+数字+下划线 \\W - Not a Word Character 非单词字符 \\s - Whitespace (space, tab, newline) 空白字符，空格+tab+换行符 \\S - Not Whitespace (space, tab, newline) 非空白字符 \\b - Word Boundary 边界字符-单词边界 \\B - Not a Word Boundary 非单词边界(没有单词边界) ^ - Beginning of a String $ - End of a String [] - Matches Characters in brackets [^ ] - Matches Characters NOT in brackets | - Either Or ( ) - Group Quantifiers: * - 0 or More + - 1 or More ? - 0 or One {3} - Exact Number {3,4} - Range of Numbers (Minimum, Maximum) #### Sample Regexs #### [a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+ 边界字符 # 非边界字符 # 事例 # 数字 # 方括号，或者 # 破折号是有特殊意义的（表示范围），比如1-9，a-z，但是处于方括号中的开头或者结尾，就是普通的破折号\n范围 # 任意的小写字母或者大写字母\n尖叫符号表示非，排除，否定 # 匹配多次(大括号，数字) # (|)组 或者关系，?出现或不出现 ，* 出现几次都行 # 综合事例 # 邮箱1 # 邮箱2 # URL # 匹配 # 分组并且反向引用 # 这里有个没展示，$0 表示匹配的内容，这里指的是从http一直到结束\n"},{"id":16,"href":"/zh/docs/test/hello2/","title":"pdfTest","section":"测试","content":"\n111\n"},{"id":17,"href":"/zh/docs/technology/Hugo/themes/PaperMod/01/","title":"使用PaperMode","section":"主题","content":" 地址 # 官方： https://github.com/adityatelange/hugo-PaperMod/wiki/Installation （有些东西没有同hugo官方同步） 非官方： https://github.com/vanitysys28/hugo-papermod-wiki/blob/master/Home.md （与hugo官方更同步）\n安装 # hugo new site blog.source --format yaml cd blog.source git init git submodule add --depth=1 https://github.com/adityatelange/hugo-PaperMod.git themes/PaperMod git submodule update --init --recursive # needed when you reclone your repo (submodules may not get cloned automatically) git submodule update --remote --merge "},{"id":18,"href":"/zh/docs/technology/Hugo/GiraffeAcademy/advanced20-23/","title":"hugo进阶学习20-23","section":"基础(Giraffe学院)_","content":"\nDateFiles # {% raw %} { \u0026#34;classA\u0026#34;:\u0026#34;json位置: data\\\\classes.json\u0026#34;, \u0026#34;classA\u0026#34;:{ \u0026#34;master\u0026#34;:\u0026#34;xiaoLi\u0026#34;, \u0026#34;number\u0026#34;:\u0026#34;05\u0026#34; }, \u0026#34;classB\u0026#34;:{ \u0026#34;master\u0026#34;:\u0026#34;aXiang\u0026#34;, \u0026#34;number\u0026#34;:\u0026#34;15\u0026#34; }, \u0026#34;classC\u0026#34;:{ \u0026#34;master\u0026#34;:\u0026#34;BaoCeng\u0026#34;, \u0026#34;number\u0026#34;:\u0026#34;20\u0026#34; } } {% endraw %} 模板代码\n{% raw %} {{/* layouts\\_default\\single.html */}} {{ define \u0026#34;main\u0026#34; }} {{ range .Site.Data.classes }} master:{{.master}}==number:{{.number}}\u0026lt;br\u0026gt; {{end}} {{end}} {% endraw %} PartialTemplates # 传递全局范围 # {% raw %} {{/*layouts\\partials\\header.html*/}} \u0026lt;h1\u0026gt;{{.Title}}\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;{{.Date}}\u0026lt;/p\u0026gt; {% endraw %} {% raw %} {{/*layouts\\_default\\single.html*/}} {{ define \u0026#34;main\u0026#34; }} {{ partial \u0026#34;header\u0026#34; . }} {{/*点.传递了当前文件的范围，代表了所有的范围，所有可以访问的变量*/}} \u0026lt;hr\u0026gt; {{end}} {% endraw %} 预览：\n传递字典 # {% raw %} {{/* layouts\\partials\\header.html */}} {{ partial \u0026#34;header\u0026#34; (dict \u0026#34;myTitle\u0026#34; \u0026#34;myCustomTitle\u0026#34; \u0026#34;myDate\u0026#34; \u0026#34;myCustomDate\u0026#34; ) }} {{/* partial \u0026#34;header\u0026#34; . 同一个partial只能在一个地方出现一次？这里会报错，不知道为啥*/}} \u0026lt;hr\u0026gt; {% endraw %} 使用:\n{% raw %} {{/*layouts\\partials\\header.html*/}} \u0026lt;h1\u0026gt;{{.myTitle}}\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;{{.myDate}}\u0026lt;/p\u0026gt; {% endraw %} 效果：\nShortCodeTemplate # 效果图 # 记得先在a相关的template把 .Content 补上 # 代码片段的使用 # {% raw %} --- title: \u0026#34;This is A\u0026#39;s title\u0026#34; date: 2004-12-04T12:42:49+08:00 draft: true author: \u0026#34;Mike\u0026#34; color: \u0026#34;blue\u0026#34; --- This is A. {{\u0026lt; myshortcode color=\u0026#34;blue\u0026#34; \u0026gt;}} {{\u0026lt; myshortcode2 red \u0026gt;}} {{\u0026lt; myshortcode-p \u0026gt;}} This is the test inside the shortcode tags.. sdf d---end {{\u0026lt; /myshortcode-p \u0026gt;}} 下面没有被渲染： {{\u0026lt; myshortcode-p \u0026gt;}} **bold text** {{\u0026lt; /myshortcode-p \u0026gt;}} 下面被渲染了，但是没有被片段处理： {{% myshortcode-p %}} **bold text**xxx {{% /myshortcode-p %}} {%/* endraw */%} 代码片段的编写 # 等号键值对 # {% raw %} \u0026lt;!--layouts\\shortcodes\\myshortcode.html--\u0026gt; \u0026lt;p style=\u0026#34;color:{{.Get `color`}}\u0026#34;\u0026gt;This is my shortcode text\u0026lt;/p\u0026gt; {% endraw %} 直接写值 # {% raw %} \u0026lt;!--layouts\\shortcodes\\myshortcode2.html--\u0026gt; \u0026lt;p style=\u0026#34;color:{{.Get 0}}\u0026#34;\u0026gt;This is my shortcode text\u0026lt;/p\u0026gt; {% endraw %} 获取多行大量文字 # {% raw %} \u0026lt;!--layouts\\shortcodes\\myshortcode-p.html--\u0026gt; \u0026lt;p style=\u0026#34;background-color: yellow;\u0026#34;\u0026gt;{{.Inner}}\u0026lt;/p\u0026gt; {% endraw %} 如何构建网站及托管 # 使用hugo server运行并打开网站（平常测试） 使用hugo生成静态网页文件夹/public/ 把上面的/public/下的所有文件传到网络服务器即可 进行第三步之前，得先把原先传到网络服务器上的/public/的内容清空 "},{"id":19,"href":"/zh/docs/technology/Hugo/GiraffeAcademy/advanced17-19/","title":"hugo进阶学习17-19","section":"基础(Giraffe学院)_","content":"\nVariable # 文件结构 # 实战 # {% raw %} {{/*layouts\\_default\\single.html*/}} {{ define \u0026#34;main\u0026#34; }} This is the single template\u0026lt;br\u0026gt; {{/* 常见变量 */}} title: {{ .Params.title }}\u0026lt;br\u0026gt; title: {{ .Title }}\u0026lt;br\u0026gt; date: {{ .Date }}\u0026lt;br\u0026gt; url: {{ .URL }}\u0026lt;br\u0026gt; myvar: {{ .Params.myVar }}\u0026lt;br\u0026gt; {{/* 定义变量 */}} {{ $myVarname := \u0026#34;aString\u0026#34; }} myVarname:{{ $myVarname }}\u0026lt;br\u0026gt; \u0026lt;h1 style=\u0026#34;color: {{ .Params.color }} ;\u0026#34; \u0026gt;Single Template\u0026lt;/h1\u0026gt; {{ end }} {% endraw %} {% raw %} --- title: \u0026#34;E-title\u0026#34; date: 2024-12-07T12:43:21+08:00 draft: true myVar: \u0026#34;myvalue\u0026#34; color: \u0026#34;red\u0026#34; --- This is dir3/e.md {% endraw %} 其他两个文件效果\n{% raw %} --- title: \u0026#34;F\u0026#34; date: 2024-12-07T12:43:21+08:00 draft: true color: \u0026#34;green\u0026#34; --- This is dir3/f.md {% endraw %} {% raw %} --- title: \u0026#34;This is A\u0026#39;s title\u0026#34; date: 2004-12-04T12:42:49+08:00 draft: true author: \u0026#34;Mike\u0026#34; color: \u0026#34;blue\u0026#34; --- This is A,/a. {% endraw %} 效果：\n官网详细默认变量 # hugo variables。\nFunctions函数 # 文件结构 # 代码(模板) # 注意，下面全是dir1下的模板，只对/dir1/及其下文件有效\nbaseof.html\n{% raw %} {{/*layouts\\_default\\baseof.html*/}} \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{ block \u0026#34;main\u0026#34; . }} 33 {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} single.html\n{% raw %} {{/*layouts\\dir1\\single.html*/}} \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document111\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Dir1Template,see! \u0026lt;h1\u0026gt;Header\u0026lt;/h1\u0026gt; \u0026lt;h3\u0026gt;{{.Title}}\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;{{.Date}}\u0026lt;/h4\u0026gt; \u0026lt;!--特殊项--\u0026gt; {{.Content}} \u0026lt;h1\u0026gt;Footer\u0026lt;/h1\u0026gt; \u0026lt;hr\u0026gt; {{ truncate 10 \u0026#34;This is a really long string\u0026#34;}}\u0026lt;br\u0026gt; {{ add 1 5 }}\u0026lt;br\u0026gt; {{ sub 1 5 }}\u0026lt;br\u0026gt; {{ singularize \u0026#34;dogs\u0026#34; }} \u0026lt;br\u0026gt; {{/*下面完全没有输出，因为不是list page*/}} {{ range .Pages }} {{ .Title }}\u0026lt;br\u0026gt; {{ end }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} 对于上面的single.html生成的html源码：\n{% raw %} \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document111\u0026lt;/title\u0026gt; \u0026lt;!--注意这里，说明完全使用layouts\\dir1\\single.html作为模板，跟baseof.html无关--\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Dir1Template,see! \u0026lt;h1\u0026gt;Header\u0026lt;/h1\u0026gt; \u0026lt;h3\u0026gt;B-title\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;2024-12-07 12:43:21 \u0026amp;#43;0800 CST\u0026lt;/h4\u0026gt; \u0026lt;p\u0026gt;This is dir1/b.md\u0026lt;/p\u0026gt; \u0026lt;h1\u0026gt;Footer\u0026lt;/h1\u0026gt; \u0026lt;hr\u0026gt; This is a …\u0026lt;br\u0026gt; 6\u0026lt;br\u0026gt; -4\u0026lt;br\u0026gt; dog \u0026lt;br\u0026gt; \u0026lt;script data-no-instant\u0026gt;document.write(\u0026#39;\u0026lt;script src=\u0026#34;/livereload.js?port=1313\u0026amp;mindelay=10\u0026#34;\u0026gt;\u0026lt;/\u0026#39; + \u0026#39;script\u0026gt;\u0026#39;)\u0026lt;/script\u0026gt;\u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} list.html\n{% raw %} {{/* layouts\\dir1\\list.html */}} {{ define \u0026#34;main\u0026#34; }} This is the listTemplate for dir1;\u0026lt;br\u0026gt; {{/*下面只输出了dir1下的所有文件(包括子文件夹)*/}} {{ range .Pages }} {{ .Title }}\u0026lt;br\u0026gt; {{ end }} {{ end }} {% endraw %} IfStatements # 文件结构 # if代码演示 # {% raw %} {{/* layouts\\_default\\single.html */}} {{ define \u0026#34;main\u0026#34; }} This is the listTemplate\u0026lt;br\u0026gt; {{ $var1 := \u0026#34;dog\u0026#34; }} {{ $var2 := \u0026#34;cat\u0026#34; }} {{ if ge $var1 $var2 }} True {{ else }} False {{ end }} \u0026lt;br\u0026gt; {{ $var3 := 6 }} {{ $var4 := 4 }} {{ $var5 := 1 }} {{ if and (le $var3 $var4) (lt $var3 $var5) }} var3 is minist {{ else if and (le $var4 $var3) (lt $var4 $var5)}} var4 is minist {{ else }} var5 is minist {{ end }} \u0026lt;br\u0026gt; {{ end }} {% endraw %} 其他代码展示 # {% raw %} \u0026lt;!--layouts\\dir1\\single.html--\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document111\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{ $title := .Title }} {{/* 注意，这里遍历的是整个网站(.Site)的文件 */}} {{ range .Site.Pages }} \u0026lt;a href=\u0026#34;{{.URL}}\u0026#34; style=\u0026#34; {{ if eq .Title $title }} background-color: red; {{ end }} \u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt; {{ end }} \u0026lt;hr\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} "},{"id":20,"href":"/zh/docs/technology/Hugo/GiraffeAcademy/advanced11-16/","title":"hugo进阶学习11-15","section":"基础(Giraffe学院)_","content":" 这里使用的版本是v0.26（很久之前的版本）\ntemplate basic # 模板分为list template和single template\n文件夹结构 # content目录结构\nlist template （列表模板） # single template （单页模板） # 特点 # 所有的列表之间都是长一样的（页眉，页脚，及内容（都是列表））\n所有的单页之间都是长一样的（一样的页眉页脚，一样的内容布局）\n部分代码解释 # 单页探索 # list page templates # 文件夹结构 # 文件内容 # #content/_index --- title: \u0026#34;_Index\u0026#34; --- This is the home page #content/dir1/_index --- title: \u0026#34;_Index\u0026#34; --- This is the landing page for dir1 当前效果 # 原因 # {% raw %} \u0026lt;!--themes\\ga-hugo-theme\\layouts\\_default\\list.html--\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{ partial \u0026#34;header\u0026#34; (dict \u0026#34;Kind\u0026#34; .Kind \u0026#34;Template\u0026#34; \u0026#34;List\u0026#34;) }} {{.Content}} {{ range .Pages }} \u0026lt;div style=\u0026#34;border: 1px solid black; margin:10px; padding:10px; \u0026#34;\u0026gt; \u0026lt;div style=\u0026#34;font-size:20px;\u0026#34;\u0026gt; \u0026lt;a href=\u0026#34;{{.URL}}\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;color:grey; font-size:16px;\u0026#34;\u0026gt;{{ dateFormat \u0026#34;Monday, Jan 2, 2006\u0026#34; .Date }}\u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;color:grey; font-size:16px;\u0026#34;\u0026gt;{{ if .Params.tags }}\u0026lt;strong\u0026gt;Tags:\u0026lt;/strong\u0026gt; {{range .Params.tags}}\u0026lt;a href=\u0026#34;{{ \u0026#34;/tags/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{end}}{{end}}\u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;color:grey; font-size:16px;\u0026#34;\u0026gt;{{ if .Params.categories }}\u0026lt;strong\u0026gt;Categories:\u0026lt;/strong\u0026gt; {{range .Params.categories}}\u0026lt;a href=\u0026#34;{{ \u0026#34;/categories/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{end}}{{end}}\u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;color:grey; font-size:16px;\u0026#34;\u0026gt;{{ if .Params.moods }}\u0026lt;strong\u0026gt;Moods:\u0026lt;/strong\u0026gt; {{range .Params.moods}}\u0026lt;a href=\u0026#34;{{ \u0026#34;/moods/\u0026#34; | relLangURL }}{{ . | urlize }}\u0026#34;\u0026gt;{{ . }}\u0026lt;/a\u0026gt; {{end}}{{end}}\u0026lt;/div\u0026gt; \u0026lt;p style=\u0026#34;font-size:18px;\u0026#34;\u0026gt;{{.Summary}}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {{ end }} {{ partial \u0026#34;footer\u0026#34; . }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} 覆盖默认的list template # 编辑文件并保存\n{% raw %} \u0026lt;!--layouts\\_default\\list.html--\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{.Content}} \u0026lt;!--显示对应的目录下的_index.md内容--\u0026gt; {{ range .Pages }} \u0026lt;!--枚举对应目录下所有页面(.md)--\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;!--.URL 文件路径，类似 /a或者/dir1/b--\u0026gt; \u0026lt;!--.Title md中的前言-title字段--\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href=\u0026#34;{{.URL}}\u0026#34;\u0026gt;{{.Title}}\u0026lt;/a\u0026gt;\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; {{end}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} 效果 # list template简易版\nsingle template # 当前效果 # 主题默认代码 # {% raw %} \u0026lt;!-- themes\\ga-hugo-theme\\layouts\\_default\\single.html --\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; {{ partial \u0026#34;header\u0026#34; (dict \u0026#34;Kind\u0026#34; .Kind \u0026#34;Template\u0026#34; \u0026#34;Single\u0026#34;) }} \u0026lt;p\u0026gt;Test test\u0026lt;/p\u0026gt; \u0026lt;div style=\u0026#34;margin:25px;\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;{{.Title}}\u0026lt;/h1\u0026gt; \u0026lt;div style=\u0026#34;color:grey; font-size:16px;\u0026#34;\u0026gt;{{ dateFormat \u0026#34;Monday, Jan 2, 2006\u0026#34; .Date }}\u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;color:grey; font-size:16px;\u0026#34;\u0026gt;{{if .Params.author}}Author: {{.Params.Author}}{{end}}\u0026lt;/div\u0026gt; \u0026lt;div style=\u0026#34;font-size:18px;\u0026#34;\u0026gt;{{.Content}}\u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {{ partial \u0026#34;footer\u0026#34; . }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} 改编 # {% raw %} \u0026lt;!--layouts\\_default\\single.html--\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Header\u0026lt;/h1\u0026gt; \u0026lt;h3\u0026gt;{{.Title}}\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;{{.Date}}\u0026lt;/h4\u0026gt; \u0026lt;!--特殊项--\u0026gt; {{.Content}} \u0026lt;h1\u0026gt;Footer\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} 效果\nhome template # 是什么 # 前面学到，页面分为“列表页面list page”和“单页页面”。其实再细分还有一种“主页页面home page”。 主页，即 localhost:1313 是先使用homepage，找不到的情况，才会使用list page 目录结构 # 当前效果 # 修改文件代码 # {% raw %} \u0026lt;!--layouts\\index.html--\u0026gt; Home Page Template {% endraw %} 效果 # SectionTemplate # 当前目录结构 # 目的 # 不用理会a.md使用哪个当single template。而dir1文件夹下的所有md，都是用同一个single template。\n目前content下所有md文件详情：a.md使用layouts/index.html当模板（没有的话则找layouts/_default/index.html当模板）。b.md和c.md、e.md、d.md、f.md均使用layouts/_default/index.html当模板\n代码\n{% raw %} \u0026lt;!--layouts\\dir1\\single.html--\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;meta http-equiv=\u0026#34;X-UA-Compatible\u0026#34; content=\u0026#34;ie=edge\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Dir1Template,see! \u0026lt;h1\u0026gt;Header\u0026lt;/h1\u0026gt; \u0026lt;h3\u0026gt;{{.Title}}\u0026lt;/h3\u0026gt; \u0026lt;h4\u0026gt;{{.Date}}\u0026lt;/h4\u0026gt; \u0026lt;!--特殊项--\u0026gt; {{.Content}} \u0026lt;h1\u0026gt;Footer\u0026lt;/h1\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} 结果 # 其他的走默认模板 layouts\\_default\\single.html\nBase Templates \u0026amp;\u0026amp; Blocks Hugo # 是什么 # BaseTemplate就是这个网站的总体模板\n案例 # 目录结构 # 编辑文件 # baseof.html\n{% raw %} \u0026lt;!--layouts\\_default\\baseof.html--\u0026gt; \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1.0\u0026#34;\u0026gt; \u0026lt;title\u0026gt;Document\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;!--Hugo实体快，Block--\u0026gt; {{ block \u0026#34;main\u0026#34; . }} {{end}} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; {% endraw %} single.html\n不要用html5的\u0026lt;!----!\u0026gt;注释，会出问题\n{% raw %} {{ define \u0026#34;main\u0026#34; }} This is the single template {{ end }} {% endraw %} list.html\n{% raw %} {{/* layouts\\_default\\single.html */}} {{ define \u0026#34;main\u0026#34; }} This is the listTemplate {{ end }} {% endraw %} 效果\n"},{"id":21,"href":"/zh/docs/technology/Hugo/GiraffeAcademy/advanced01-10/","title":"hugo进阶学习01-10","section":"基础(Giraffe学院)_","content":" 系列视频地址介绍\nhttps://www.youtube.com/watch?v=qtIqKaDlqXo\u0026list=PLLAZ4kZ9dFpOnyRlyS-liKL5ReHDcj4G3\n介绍 # hugo是用来构建静态网站的 但是也可以稍微做点动态生成的事 这里使用的版本是v0.26（很久之前的版本） 备注：标题短代码之前（不包括短代码这篇）的笔记是回溯的，所以没有复制源代码下来，直接在视频再次截图的\n在Windows上安装hugo # 到github release下载，然后放到某个文件夹中\n设置环境变量\n验证环境变量\n最后验证hugo版本 hugo version 创建一个新的网站 # 使用代码生成 hugo new site 文件夹结构\n使用主题 # 这里是https://themes.gohugo.io\n这里使用的是ga-hugo-theme（github中查找），并放到themes文件夹中\n之后在config.toml中使用主题\nbaseURL = \u0026#34;http://example.org/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;ga-hugo-theme\u0026#34; #添加这句话 启动博客\nhugo serve 地址\nlocalhost:1313 创建md文件 # 使用hugo new a.md把文件创建在content/a.md或者hugo new dir2/d.md把文件创建在content/dir2.md下，这讲创建后的结构目录为\n总共5个文件，可以使用localhost:1313访问博客（默认列举所有（包括子文件夹）文件 可以使用 localhost:1313/dir3访问dir3下所有文件列表(list)，localhost:1313/dir1访问dir1下所有文件列表 （都是content的直接子文件夹） 如果没有dir1/dir2/_index.md这个文件 ，则不能直接使用localhost:1313/dir1/dir2访问dir1/dir2下所有文件 查看dir1/dir2/index.md文件及效果\nfrontmatter (前言) # 可以使用YAML，TOML，或者JSON md编码及效果\narchetypes（原型） # 默认的原型文件 # archetypes/default.md\n{% raw %} --- title: \u0026#34;{{ replace .TranslationBaseName \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date: {{ .Date }} draft: true author: \u0026#34;Mike\u0026#34; --- {% endraw %} 使用命令行hugo new b.md结果\n和文件夹结构相关的原型文件 # 使用命令行hugo new dir1/c.md结果\n如果hugo new dir1/c.md时archetypes/dir1.md不存在，则才会去找archetypes/default.md当模板创建文件\nshortcodes 短代码 # 代码 # 放到markdown文件中（这个youtube是官方支持的内嵌的）\n{% raw %} {{/*\u0026lt; youtube w7Ft2ymGmfc \u0026gt;*/}} {% endraw %} 效果 # taxonomies（分类法） # 默认的两个分类 # 比如修改了总共三个文件 （隐去其他前言数据）\n{% raw %} --- # a.md title: \u0026#34;A\u0026#34; tags: [\u0026#34;tag1\u0026#34;,\u0026#34;tag2\u0026#34;,\u0026#34;tag3\u0026#34;] categories: [\u0026#34;cat1\u0026#34;] --- # b.md --- title: \u0026#34;B\u0026#34; tags: [\u0026#34;tag2\u0026#34; ] categories: [\u0026#34;cat2\u0026#34;] --- # c.md --- title: \u0026#34;C\u0026#34; tags: [\u0026#34;tag3\u0026#34;] categories: [\u0026#34;cat2\u0026#34;] --- {% endraw %} 效果：\n点击tag2时效果\n点击cat1时的效果\n自定义分类 # {% raw %} # a.md添加最后一行，最后代码（忽略其他属性） --- title: \u0026#34;A tags: [\u0026#34;tag1\u0026#34;,\u0026#34;tag2\u0026#34;,\u0026#34;tag3\u0026#34;] categories: [\u0026#34;cat1\u0026#34;] moods: [\u0026#34;Happy\u0026#34;,\u0026#34;Upbeat\u0026#34;] --- {% endraw %} 以及修改config.toml文件\n{% raw %} baseURL = \u0026#34;http://example.org/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;My New Hugo Site\u0026#34; theme = \u0026#34;ga-hugo-theme\u0026#34; [taxonomies] #添加这行及以下三行 tag = \u0026#34;tags\u0026#34; category = \u0026#34;categories\u0026#34; mood = \u0026#34;moods\u0026#34; {% endraw %} 效果：\n"},{"id":22,"href":"/zh/docs/technology/Obsidian/border-theme/","title":"border-theme背景图片问题","section":"Obsidian","content":" svg格式作为背景图片（简单图片可行） # 以下面这张图片为例\n最简单的方式，用记事本/文本编辑器，打开svg图片，全选，复制，即\n\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 1920 1080\u0026#34;\u0026gt;\u0026lt;g transform=\u0026#34; rotate(0 960 540) translate(-0 -0) scale(1) \u0026#34;\u0026gt;\u0026lt;rect width=\u0026#34;1920\u0026#34; height=\u0026#34;1080\u0026#34; fill=\u0026#34;rgb(184, 171, 255)\u0026#34;\u0026gt;\u0026lt;/rect\u0026gt;\u0026lt;g transform=\u0026#34;translate(0, 0)\u0026#34;\u0026gt;\u0026lt;path fill=\u0026#34;rgb(131, 114, 218)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,352.943L45.714,350.075C91.429,347.207,182.857,341.471,274.286,340.581C365.714,339.692,457.143,343.65,548.571,344.095C640,344.54,731.429,341.472,822.857,303.183C914.286,264.894,1005.714,191.383,1097.143,185.175C1188.571,178.967,1280,240.06,1371.429,221.336C1462.857,202.612,1554.286,104.069,1645.714,98.48C1737.143,92.892,1828.571,180.258,1874.286,223.941L1920,267.624L1920,1080L1874.286,1080C1828.571,1080,1737.143,1080,1645.714,1080C1554.286,1080,1462.857,1080,1371.429,1080C1280,1080,1188.571,1080,1097.143,1080C1005.714,1080,914.286,1080,822.857,1080C731.429,1080,640,1080,548.571,1080C457.143,1080,365.714,1080,274.286,1080C182.857,1080,91.429,1080,45.714,1080L0,1080Z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;g transform=\u0026#34;translate(0, 360)\u0026#34;\u0026gt;\u0026lt;path fill=\u0026#34;rgb(79, 57, 180)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,136.093L45.714,117.434C91.429,98.774,182.857,61.455,274.286,80.719C365.714,99.983,457.143,175.829,548.571,189.505C640,203.181,731.429,154.687,822.857,130.414C914.286,106.141,1005.714,106.09,1097.143,141.274C1188.571,176.458,1280,246.877,1371.429,284.697C1462.857,322.517,1554.286,327.739,1645.714,284.675C1737.143,241.611,1828.571,150.263,1874.286,104.589L1920,58.914L1920,720L1874.286,720C1828.571,720,1737.143,720,1645.714,720C1554.286,720,1462.857,720,1371.429,720C1280,720,1188.571,720,1097.143,720C1005.714,720,914.286,720,822.857,720C731.429,720,640,720,548.571,720C457.143,720,365.714,720,274.286,720C182.857,720,91.429,720,45.714,720L0,720Z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;g transform=\u0026#34;translate(0, 720)\u0026#34;\u0026gt;\u0026lt;path fill=\u0026#34;rgb(26, 0, 143)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,107.121L45.714,134.307C91.429,161.493,182.857,215.866,274.286,254.33C365.714,292.794,457.143,315.35,548.571,300.514C640,285.679,731.429,233.452,822.857,180.313C914.286,127.174,1005.714,73.123,1097.143,43.365C1188.571,13.606,1280,8.141,1371.429,41.079C1462.857,74.017,1554.286,145.358,1645.714,167.782C1737.143,190.206,1828.571,163.713,1874.286,150.467L1920,137.221L1920,360L1874.286,360C1828.571,360,1737.143,360,1645.714,360C1554.286,360,1462.857,360,1371.429,360C1280,360,1188.571,360,1097.143,360C1005.714,360,914.286,360,822.857,360C731.429,360,640,360,548.571,360C457.143,360,365.714,360,274.286,360C182.857,360,91.429,360,45.714,360L0,360Z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;/svg\u0026gt; 之后打开https://codepen.io/yoksel/details/MWKeKK 网站，在 Insert your SVG中粘贴，得到\n最后把url(\u0026quot;\u0026quot;) 这块复制【没有分号】，即\nurl(\u0026#39;data:image/svg+xml,%3Csvg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 1920 1080\u0026#34;%3E%3Cg transform=\u0026#34; rotate(0 960 540) translate(-0 -0) scale(1) \u0026#34;%3E%3Crect width=\u0026#34;1920\u0026#34; height=\u0026#34;1080\u0026#34; fill=\u0026#34;rgb(184, 171, 255)\u0026#34;%3E%3C/rect%3E%3Cg transform=\u0026#34;translate(0, 0)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(131, 114, 218)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,352.943L45.714,350.075C91.429,347.207,182.857,341.471,274.286,340.581C365.714,339.692,457.143,343.65,548.571,344.095C640,344.54,731.429,341.472,822.857,303.183C914.286,264.894,1005.714,191.383,1097.143,185.175C1188.571,178.967,1280,240.06,1371.429,221.336C1462.857,202.612,1554.286,104.069,1645.714,98.48C1737.143,92.892,1828.571,180.258,1874.286,223.941L1920,267.624L1920,1080L1874.286,1080C1828.571,1080,1737.143,1080,1645.714,1080C1554.286,1080,1462.857,1080,1371.429,1080C1280,1080,1188.571,1080,1097.143,1080C1005.714,1080,914.286,1080,822.857,1080C731.429,1080,640,1080,548.571,1080C457.143,1080,365.714,1080,274.286,1080C182.857,1080,91.429,1080,45.714,1080L0,1080Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3Cg transform=\u0026#34;translate(0, 360)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(79, 57, 180)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,136.093L45.714,117.434C91.429,98.774,182.857,61.455,274.286,80.719C365.714,99.983,457.143,175.829,548.571,189.505C640,203.181,731.429,154.687,822.857,130.414C914.286,106.141,1005.714,106.09,1097.143,141.274C1188.571,176.458,1280,246.877,1371.429,284.697C1462.857,322.517,1554.286,327.739,1645.714,284.675C1737.143,241.611,1828.571,150.263,1874.286,104.589L1920,58.914L1920,720L1874.286,720C1828.571,720,1737.143,720,1645.714,720C1554.286,720,1462.857,720,1371.429,720C1280,720,1188.571,720,1097.143,720C1005.714,720,914.286,720,822.857,720C731.429,720,640,720,548.571,720C457.143,720,365.714,720,274.286,720C182.857,720,91.429,720,45.714,720L0,720Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3Cg transform=\u0026#34;translate(0, 720)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(26, 0, 143)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,107.121L45.714,134.307C91.429,161.493,182.857,215.866,274.286,254.33C365.714,292.794,457.143,315.35,548.571,300.514C640,285.679,731.429,233.452,822.857,180.313C914.286,127.174,1005.714,73.123,1097.143,43.365C1188.571,13.606,1280,8.141,1371.429,41.079C1462.857,74.017,1554.286,145.358,1645.714,167.782C1737.143,190.206,1828.571,163.713,1874.286,150.467L1920,137.221L1920,360L1874.286,360C1828.571,360,1737.143,360,1645.714,360C1554.286,360,1462.857,360,1371.429,360C1280,360,1188.571,360,1097.143,360C1005.714,360,914.286,360,822.857,360C731.429,360,640,360,548.571,360C457.143,360,365.714,360,274.286,360C182.857,360,91.429,360,45.714,360L0,360Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3C/g%3E%3C/svg%3E\u0026#39;) 加上\ntop/cover no-repeat fixed 即\nurl(\u0026#39;data:image/svg+xml,%3Csvg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 1920 1080\u0026#34;%3E%3Cg transform=\u0026#34; rotate(0 960 540) translate(-0 -0) scale(1) \u0026#34;%3E%3Crect width=\u0026#34;1920\u0026#34; height=\u0026#34;1080\u0026#34; fill=\u0026#34;rgb(184, 171, 255)\u0026#34;%3E%3C/rect%3E%3Cg transform=\u0026#34;translate(0, 0)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(131, 114, 218)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,352.943L45.714,350.075C91.429,347.207,182.857,341.471,274.286,340.581C365.714,339.692,457.143,343.65,548.571,344.095C640,344.54,731.429,341.472,822.857,303.183C914.286,264.894,1005.714,191.383,1097.143,185.175C1188.571,178.967,1280,240.06,1371.429,221.336C1462.857,202.612,1554.286,104.069,1645.714,98.48C1737.143,92.892,1828.571,180.258,1874.286,223.941L1920,267.624L1920,1080L1874.286,1080C1828.571,1080,1737.143,1080,1645.714,1080C1554.286,1080,1462.857,1080,1371.429,1080C1280,1080,1188.571,1080,1097.143,1080C1005.714,1080,914.286,1080,822.857,1080C731.429,1080,640,1080,548.571,1080C457.143,1080,365.714,1080,274.286,1080C182.857,1080,91.429,1080,45.714,1080L0,1080Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3Cg transform=\u0026#34;translate(0, 360)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(79, 57, 180)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,136.093L45.714,117.434C91.429,98.774,182.857,61.455,274.286,80.719C365.714,99.983,457.143,175.829,548.571,189.505C640,203.181,731.429,154.687,822.857,130.414C914.286,106.141,1005.714,106.09,1097.143,141.274C1188.571,176.458,1280,246.877,1371.429,284.697C1462.857,322.517,1554.286,327.739,1645.714,284.675C1737.143,241.611,1828.571,150.263,1874.286,104.589L1920,58.914L1920,720L1874.286,720C1828.571,720,1737.143,720,1645.714,720C1554.286,720,1462.857,720,1371.429,720C1280,720,1188.571,720,1097.143,720C1005.714,720,914.286,720,822.857,720C731.429,720,640,720,548.571,720C457.143,720,365.714,720,274.286,720C182.857,720,91.429,720,45.714,720L0,720Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3Cg transform=\u0026#34;translate(0, 720)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(26, 0, 143)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,107.121L45.714,134.307C91.429,161.493,182.857,215.866,274.286,254.33C365.714,292.794,457.143,315.35,548.571,300.514C640,285.679,731.429,233.452,822.857,180.313C914.286,127.174,1005.714,73.123,1097.143,43.365C1188.571,13.606,1280,8.141,1371.429,41.079C1462.857,74.017,1554.286,145.358,1645.714,167.782C1737.143,190.206,1828.571,163.713,1874.286,150.467L1920,137.221L1920,360L1874.286,360C1828.571,360,1737.143,360,1645.714,360C1554.286,360,1462.857,360,1371.429,360C1280,360,1188.571,360,1097.143,360C1005.714,360,914.286,360,822.857,360C731.429,360,640,360,548.571,360C457.143,360,365.714,360,274.286,360C182.857,360,91.429,360,45.714,360L0,360Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3C/g%3E%3C/svg%3E\u0026#39;) top/cover no-repeat fixed 之后粘贴即可\n通过对比发现：\n就是把下面的内容\nurl(\u0026#39;data:image/svg+xml,\u0026lt;svg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 1920 1080\u0026#34;\u0026gt;\u0026lt;g transform=\u0026#34; rotate(0 960 540) translate(-0 -0) scale(1) \u0026#34;\u0026gt;\u0026lt;rect width=\u0026#34;1920\u0026#34; height=\u0026#34;1080\u0026#34; fill=\u0026#34;rgb(184, 171, 255)\u0026#34;\u0026gt;\u0026lt;/rect\u0026gt;\u0026lt;g transform=\u0026#34;translate(0, 0)\u0026#34;\u0026gt;\u0026lt;path fill=\u0026#34;rgb(131, 114, 218)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,352.943L45.714,350.075C91.429,347.207,182.857,341.471,274.286,340.581C365.714,339.692,457.143,343.65,548.571,344.095C640,344.54,731.429,341.472,822.857,303.183C914.286,264.894,1005.714,191.383,1097.143,185.175C1188.571,178.967,1280,240.06,1371.429,221.336C1462.857,202.612,1554.286,104.069,1645.714,98.48C1737.143,92.892,1828.571,180.258,1874.286,223.941L1920,267.624L1920,1080L1874.286,1080C1828.571,1080,1737.143,1080,1645.714,1080C1554.286,1080,1462.857,1080,1371.429,1080C1280,1080,1188.571,1080,1097.143,1080C1005.714,1080,914.286,1080,822.857,1080C731.429,1080,640,1080,548.571,1080C457.143,1080,365.714,1080,274.286,1080C182.857,1080,91.429,1080,45.714,1080L0,1080Z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;g transform=\u0026#34;translate(0, 360)\u0026#34;\u0026gt;\u0026lt;path fill=\u0026#34;rgb(79, 57, 180)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,136.093L45.714,117.434C91.429,98.774,182.857,61.455,274.286,80.719C365.714,99.983,457.143,175.829,548.571,189.505C640,203.181,731.429,154.687,822.857,130.414C914.286,106.141,1005.714,106.09,1097.143,141.274C1188.571,176.458,1280,246.877,1371.429,284.697C1462.857,322.517,1554.286,327.739,1645.714,284.675C1737.143,241.611,1828.571,150.263,1874.286,104.589L1920,58.914L1920,720L1874.286,720C1828.571,720,1737.143,720,1645.714,720C1554.286,720,1462.857,720,1371.429,720C1280,720,1188.571,720,1097.143,720C1005.714,720,914.286,720,822.857,720C731.429,720,640,720,548.571,720C457.143,720,365.714,720,274.286,720C182.857,720,91.429,720,45.714,720L0,720Z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;g transform=\u0026#34;translate(0, 720)\u0026#34;\u0026gt;\u0026lt;path fill=\u0026#34;rgb(26, 0, 143)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,107.121L45.714,134.307C91.429,161.493,182.857,215.866,274.286,254.33C365.714,292.794,457.143,315.35,548.571,300.514C640,285.679,731.429,233.452,822.857,180.313C914.286,127.174,1005.714,73.123,1097.143,43.365C1188.571,13.606,1280,8.141,1371.429,41.079C1462.857,74.017,1554.286,145.358,1645.714,167.782C1737.143,190.206,1828.571,163.713,1874.286,150.467L1920,137.221L1920,360L1874.286,360C1828.571,360,1737.143,360,1645.714,360C1554.286,360,1462.857,360,1371.429,360C1280,360,1188.571,360,1097.143,360C1005.714,360,914.286,360,822.857,360C731.429,360,640,360,548.571,360C457.143,360,365.714,360,274.286,360C182.857,360,91.429,360,45.714,360L0,360Z\u0026#34;\u0026gt;\u0026lt;/path\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;/g\u0026gt;\u0026lt;/svg\u0026gt;\u0026#39;) top/cover no-repeat fixed ,左尖括号 \u0026lt; 替换成 %3C，把 \u0026lt; 替换成 %3E （补充下，如果是其他图片可能不止这几个，但是这张图只替换了这几个。不太了解前端到底需要转哪些特殊字符，因为我发现有些空格 / \u0026quot; 也都没有转），即\nurl(\u0026#39;data:image/svg+xml,%3Csvg xmlns=\u0026#34;http://www.w3.org/2000/svg\u0026#34; viewBox=\u0026#34;0 0 1920 1080\u0026#34;%3E%3Cg transform=\u0026#34; rotate(0 960 540) translate(-0 -0) scale(1) \u0026#34;%3E%3Crect width=\u0026#34;1920\u0026#34; height=\u0026#34;1080\u0026#34; fill=\u0026#34;rgb(184, 171, 255)\u0026#34;%3E%3C/rect%3E%3Cg transform=\u0026#34;translate(0, 0)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(131, 114, 218)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,352.943L45.714,350.075C91.429,347.207,182.857,341.471,274.286,340.581C365.714,339.692,457.143,343.65,548.571,344.095C640,344.54,731.429,341.472,822.857,303.183C914.286,264.894,1005.714,191.383,1097.143,185.175C1188.571,178.967,1280,240.06,1371.429,221.336C1462.857,202.612,1554.286,104.069,1645.714,98.48C1737.143,92.892,1828.571,180.258,1874.286,223.941L1920,267.624L1920,1080L1874.286,1080C1828.571,1080,1737.143,1080,1645.714,1080C1554.286,1080,1462.857,1080,1371.429,1080C1280,1080,1188.571,1080,1097.143,1080C1005.714,1080,914.286,1080,822.857,1080C731.429,1080,640,1080,548.571,1080C457.143,1080,365.714,1080,274.286,1080C182.857,1080,91.429,1080,45.714,1080L0,1080Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3Cg transform=\u0026#34;translate(0, 360)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(79, 57, 180)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,136.093L45.714,117.434C91.429,98.774,182.857,61.455,274.286,80.719C365.714,99.983,457.143,175.829,548.571,189.505C640,203.181,731.429,154.687,822.857,130.414C914.286,106.141,1005.714,106.09,1097.143,141.274C1188.571,176.458,1280,246.877,1371.429,284.697C1462.857,322.517,1554.286,327.739,1645.714,284.675C1737.143,241.611,1828.571,150.263,1874.286,104.589L1920,58.914L1920,720L1874.286,720C1828.571,720,1737.143,720,1645.714,720C1554.286,720,1462.857,720,1371.429,720C1280,720,1188.571,720,1097.143,720C1005.714,720,914.286,720,822.857,720C731.429,720,640,720,548.571,720C457.143,720,365.714,720,274.286,720C182.857,720,91.429,720,45.714,720L0,720Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3Cg transform=\u0026#34;translate(0, 720)\u0026#34;%3E%3Cpath fill=\u0026#34;rgb(26, 0, 143)\u0026#34; fill-opacity=\u0026#34;1\u0026#34; d=\u0026#34;M0,107.121L45.714,134.307C91.429,161.493,182.857,215.866,274.286,254.33C365.714,292.794,457.143,315.35,548.571,300.514C640,285.679,731.429,233.452,822.857,180.313C914.286,127.174,1005.714,73.123,1097.143,43.365C1188.571,13.606,1280,8.141,1371.429,41.079C1462.857,74.017,1554.286,145.358,1645.714,167.782C1737.143,190.206,1828.571,163.713,1874.286,150.467L1920,137.221L1920,360L1874.286,360C1828.571,360,1737.143,360,1645.714,360C1554.286,360,1462.857,360,1371.429,360C1280,360,1188.571,360,1097.143,360C1005.714,360,914.286,360,822.857,360C731.429,360,640,360,548.571,360C457.143,360,365.714,360,274.286,360C182.857,360,91.429,360,45.714,360L0,360Z\u0026#34;%3E%3C/path%3E%3C/g%3E%3C/g%3E%3C/svg%3E\u0026#39;) top/cover no-repeat fixed PNG格式作为背景图片（不可行） # 试着将这张图片用base64编码\n这里之所以不放字符了，是因为我放了之后obsidian卡死了（30多万个字符），所以突然联想到一个问题，这就是为什么我把这个base64放到StyleSettings里面的时候，没效果而且卡死的原因把，哈哈，之前一直没注意\n试着把png先转为svg，之后再按照svg作为背景的方法做一遍。\n转换后的svg图片(不知道为什么图片方向变了，我第一次下载这张图【png】确实是这个方向，后面我调整正了，现在又歪了)\n有两百多万多个字符。。。照样没效果，不过这次没卡死了 用了个纯色png转svg，有效果了，但是图片变了一些。我觉得可能是转换的问题？不过可以确定的一点就是，png没效果的一个原因是字符太多\n最后尝试用一张纯色png，转base64，并用url(\u0026quot;\u0026quot;)的形式放入设置，照样没效果。不知道是不支持png，还是我的设置方法错了，这次只有9000个字符，但是卡顿了，tab界面花屏，如图\n文章中用到的（可能用到的）url如下\nhttps://codepen.io/yoksel/details/MWKeKK svg-\u0026gt;encode-\u0026gt;css https://stackoverflow.com/questions/41405884/svg-data-image-as-css-background 关于svg转base64并嵌入css的做法（可能可行，没试过，本文中采用稍微简单的办法） https://meyerweb.com/eric/tools/dencoder/ url encode（没用上，因为发现他把所有字符都转码了，实际上只转了 \u0026lt; 和 \u0026gt; https://www.base64-image.de/ 图片转base64\nhttps://tool.chinaz.com/tools/urlencode.aspx url编码解码\nhttps://www.asciim.cn/m/tools/convert_ascii_to_string.html ascaii与字符串的转换\nhttps://forever-z-133.github.io/demos/single/svg-to-base64.html svg转base64\nhttps://products.aspose.app/pdf/zh/conversion/png-to-svg png转svg\nhttps://github.com/Akifyss/obsidian-border/issues/251 obsidian-border主题，issue中作者的相关回复\n"},{"id":23,"href":"/zh/docs/technology/Obsidian/obsidian-theme/","title":"obsidian-theme","section":"Obsidian","content":" 主题推荐 # Neumorphism-dark.json\nSunset-base64.json ✔ Obsidian-default-dark-alt ✔ 4. Obsidian-default-light-alt Neumorphism.json eyefriendly ✔ boundy ✔ flexoki-light Borderless-light 关于obsidian主题border的背景图片设置 # 配合StyleSettings，在StyleSettings的这里设置\n暂不明确 # background中貌似存在转换规则，不是直接用url(\u0026quot;\u0026quot;)这个形式把图片base64放进来就可以了，目前觉得可能的转换规则\n%3c 48+12=60 \u0026lt; %3e 48+14=62 \u0026gt; %23 32+3=35 # #下面的好像没用到，也不确定 %2b 32+11=43 + %3b ; %2c , 后续见另一篇文章\nborder-theme {% post_link \u0026lsquo;study/obsidian/border-theme\u0026rsquo; \u0026lsquo;helo\u0026rsquo; %}\n"},{"id":24,"href":"/zh/docs/technology/Obsidian/plugin/","title":"plugin","section":"Obsidian","content":"obsidian-custom-attachment-location v.28.1文件批量重命名有效，再往上都是无效的\n"},{"id":25,"href":"/zh/docs/technology/Redis/rsync-use/","title":"rsync使用","section":"Redis","content":"其实就是linux的cp功能（带增量复制） #推送到rsync-k40 rsync -avz \u0026ndash;progress \u0026ndash;delete source-rsync -e \u0026lsquo;ssh -p 8022\u0026rsquo; ly@192.168.1.101:/storage/emulated/0/000Ly/git/blog.source/source/attachments/rsync-k40 #推送到rsync-tabs8 rsync -avz \u0026ndash;progress \u0026ndash;delete source-rsync -e \u0026lsquo;ssh -p 8022\u0026rsquo; ly@192.168.1.106:/storage/emulated/0/000Ly/git/blog.source/source/attachments/rsync-tabs8 #推送到rsync-pc rsync -avz \u0026ndash;progress \u0026ndash;delete source-rsync -e \u0026lsquo;ssh -p 22\u0026rsquo; ly@192.168.1.206:/mnt/hgfs/gitRepo/blog.source/source/attachments/rsync-pc\n#从手机上拉取 rsync -avz \u0026ndash;progress \u0026ndash;delete -e \u0026lsquo;ssh -p 8022\u0026rsquo; ly@192.168.1.101:/storage/emulated/0/000Ly/git/blog.source/source/attachments/rsync-k40 source-rsync\n"},{"id":26,"href":"/zh/docs/life/20240626/","title":"知命不惧  日日自新","section":"生活","content":"视频分享\n"},{"id":27,"href":"/zh/docs/problem/Other/01/","title":"如何搜索","section":"Other","content":" 原则 # 搜索的时候，要简约，且尽量把关键词分散，不要有\u0026quot;的\u0026quot;，\u0026ldquo;地\u0026rdquo;，或者其他动词什么的，尽量是名词。 关键词之间用空格隔开 比如想要看一部电影，那么关键词有\u0026quot;电影名\u0026quot;，\u0026ldquo;bt\u0026rdquo;，\u0026ldquo;迅雷\u0026rdquo;，\u0026ldquo;阿里云盘\u0026rdquo;，\u0026ldquo;百度网盘\u0026rdquo;\n解释一下，这里来源（迅雷|阿里云盘|百度网盘），资源类型（bt）也是关键词\n所以搜索就是**\u0026ldquo;孤注一掷 bt\u0026rdquo;，\u0026ldquo;孤注一掷 阿里云盘\u0026rdquo;，\u0026ldquo;孤注一掷 百度网盘\u0026rdquo;，\u0026ldquo;孤注一掷 迅雷\u0026rdquo;，注意，中间都有空格**\n例子 # bt种子形式 # 点进来之后，滑到最下面（一般链接都是以浅蓝色标识，点过一次后就变成暗红色）\nbt文件一般要用\u0026quot;迅雷\u0026quot;这个软件下载，上面随便点击一个，迅雷这个软件就会跳出来\n然后选择好目录点击确认就可以下载了\n阿里云盘形式 # 第一个链接有人提出质疑了，我们点下面那个，这是进入之后的画面：\n再点击\u0026quot;阿里xxxxxxxxxx\u0026quot;这个链接，进入阿里云盘：\n点进来看视频文件还在不在，在的话，保存就可以了\n之后到自己的阿里云盘下载就行了\n百度云盘形式 # 百度云盘被限速了，不得已的情况下，不要用百度云盘，基本上前面两种形式的资源没找到的话，百度云盘大概率也不会有\n"},{"id":28,"href":"/zh/docs/life/20231227/","title":"起床临感","section":"生活","content":"所谓贵人，并不是封建迷信，而是指对你成长有帮助的人，不单单是直观的好。\n在你蒸蒸日上的时候打压你，让你有所收敛；在你颓废堕落的时候鼓励你，使你积极向上。他们都是贵人，一阴一阳之谓道，如是而已。\n"},{"id":29,"href":"/zh/docs/life/20231101/","title":"20231101","section":"生活","content":" 附(20231102)\n融入我中华文化的，才是自己人。想消灭我中华文化的，即使占有了这片土地，也不能称之“功臣”。\n当一个民族的文化被摧毁的时候，那个民族就是彻底灭亡了。 不过，我觉得，只要是在中国这片土地上(地理)，无论谁来，都会产生这样的文化，无例外。(地理决定论) "},{"id":30,"href":"/zh/docs/life/archive/20231026/","title":"成就","section":"往日归档","content":" 任何事情的成功，都没有什么可骄傲的，不过是一物降一物，无他尔。 人生最大的问题，是不想，而不是不能。 "},{"id":31,"href":"/zh/docs/life/archive/20231013/","title":"沉没","section":"往日归档","content":" 努力不一定有用，但是虚度光阴难道就是对的吗？ 即使你一时找不到正确的路，但是你应该能一眼看出哪些是错的，及时避开。 "},{"id":32,"href":"/zh/docs/problem/Linux/20230919/","title":"Linux操作符问题","section":"Linux","content":" 函数退出 # 函数退出状态：0（成功），非零（非正常，失败）\n引号 # 双引号中使用转义字符可以防止展开\n这意味着单词分割(空格制表换行分割单词)、路径名展开(*星号)、波浪线展开和花括号展开都将失效，然而参数展开、 算术展开和命令替换仍然执行\necho \u0026#34;text ~/*.txt {a,b} $(echo foo) $((2+2)) $USER\u0026#34; #禁止部分 text ~/*.txt {a,b} foo 4 me echo \u0026#39;text ~/*.txt {a,b} $(echo foo) $((2+2)) $USER\u0026#39; #全部禁止 text ~/*.txt {a,b} $(echo foo) $((2+2)) $USER 各种操作符 # [ expression ] / test [[ expression ]] $(( expression )) $var $( termi ) 文件表达式 -e file，字符串表达式 -n string，整数表达式 integer1 -eq integer2 test增强，增加 [ str =~ regex ]，增加 == [[ $FILE == foo.* ]] 整数加减乘除取余 取变量 执行命令/函数 termi取变量$必加，里面被看作命令参数，\u0026lt; \u0026gt; ( ) 必须转义 否则 小于号 \u0026lt; 大于号\u0026gt;被认为重定向 与[ ] 一致 取变量$可加可不加 termi取变量$必加 if [ -x \u0026#34;$FILE\u0026#34; ] #引号可以防止空参数，空值。而\u0026#34;\u0026#34;被解释成空字符串 "},{"id":33,"href":"/zh/docs/life/archive/20230913/","title":"鲇鱼后思","section":"往日归档","content":" 鲇鱼事件其实出来很久了，一直没有太大关注，这几天突发兴致（某乎评论提到），就去了解了下。可能我对“官”这种东西，从小到大就定了性，所以如果查出个大清官，省吃俭用破衣烂衫，倒可算得上新闻。 对于其言论，确实听了未尝不免义愤填膺。于是我就花了大半个小时义愤填膺\u0026hellip; 一代人只能做一代人的事\u0026mdash;《走向共和》，官如此，民亦如此。如果作为普通老百姓，不能够跻身仕途，那就是先老老实实做好自己的本分\u0026ndash;照顾父母，照顾自己，照顾妻子，照顾儿女。总有人会替天行道，如果不是你，那就做好自己，教育好自己的子女，足以。不要三心二意，事物发展有其必然规律，有盛必有衰，自古皆如此。穷则独善其身，达则兼济天下。 没有必要把自己带入高高在上的角色。也许自己到了那个地位，贪得更凶。真小人好过伪君子，伪君子往往会迷失自己，既做不了君子，又成不了小人。 "},{"id":34,"href":"/zh/docs/life/archive/20230912/","title":"病愈 有感","section":"往日归档","content":" 一个人只有真正意识到事情的发展，是自己的错误导致，才会真正改过自新。否则就会怨天尤人，甚至掩耳盗铃。 世界万事万物，有优有劣。劣并不代表罪恶，不过是事物发展的某个过程，如同生病的头疼脑热，“现象”，不过是提醒世人罢了。切勿以过程盖棺定论，自暴自弃，及时止损即可。 "},{"id":35,"href":"/zh/docs/problem/Linux/20230819/","title":"Debian问题处理3","section":"Linux","content":" fcitx配合各种软件出现的问题 # 本文章中出现的引号都是英文状态下的引号，切记！\n安装完毕后环境变量设置 # /etc/profile 和/etc/enviroment 均可，profile针对用户，environment针对系统。一般都是放profile里面\n不行的话 # 如果修改profile无效，则在/etc/enviroment添加修改\n#/etc/enviroment 末尾添加 fcitx \u0026amp; #这行要添加 export XIM_PROGRAM=fcitx export XIM=fcitx export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=\u0026#34;@im=fcitx\u0026#34; export LANG=zh_CN.UTF-8 source后再重启一下哦\n装了zsh后(从终端打开)idea等各种软件不出现fcitx输入法的问题 # 在/.zshrc最后添加\nexport XIM_PROGRAM=fcitx export XIM=fcitx export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=\u0026#34;@im=fcitx\u0026#34; export LANG=zh_CN.UTF-8 export LC_MESSAGES=en_US.UTF-8 #让终端报错时，显示英文 而不是中文 也可以不在/.zshrc中追加这些，而是直接追加 source /etc/profile或者/etc/enviroment即可\n如果还有问题，就要在idea的配置文件idea.vmoptions添加\n-Drecreate.x11.input.method=true 如果使用系统默认终端的情况下出的问题 # 可以在 ~/.bashrc最后添加这段话，重启试试\nexport XIM_PROGRAM=fcitx export XIM=fcitx export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=\u0026#34;@im=fcitx\u0026#34; export LANG=zh_CN.UTF-8 各个文件的解释 # /etc/profile //用户级，所有用户登陆时才会执行 对于fcitx没效果(firefox无效)\n/etc/enviroment //系统级，一般不修改 这里有效果\n~/.bashrc //系统默认终端打开时执行 ~/.zshrc //zsh使用前执行\nsource命令是一个内置的shell命令，用于从当前shell会话中的文件读取和执行命令。source命令通常用于保留、更改当前shell中的环境变量。简而言之，source一个脚本，将会在当前shell中运行execute命令。 source命令可用于：\n刷新当前的shell环境 在当前环境使用source执行Shell脚本 从脚本中导入环境中一个Shell函数 从另一个Shell脚本中读取变量\nzsh卸载后账号无法登录 # 参考https://lwmfjc.github.io/2023/05/23/problem/linux/20230523/ 这篇文章\n如果不是root用户就简单多了，直接\nvim /etc/passwd # xx(账户名)......zsh，中/bin/zsh，改为/bin/bash 即可 xfce4的安装及gnome卸载 # gnome完全卸载\naptitude purge `dpkg --get-selections | grep gnome | cut -f 1` aptitude -f install aptitude purge `dpkg --get-selections | grep deinstall | cut -f 1` aptitude -f install xfce4安装\nsudo apt install task-xfce-desktop 蓝牙问题 # 最后是装了blueman 连接蓝牙耳机出现这个问题，为了连接装了这个。之后想用扬声器发现用不了，拔了耳机可以了却发现破音了\u0026hellip;.\n最后解决方案是把这两个删了，而且此时蓝牙耳机也可以连上了\u0026hellip;原因不明\n备份 # 如果是vm下学习linux，要多利用vmware，养成习惯，每进行一次大操作之前，都要进行vmware的快照备份。避免大操作导致出问题\n"},{"id":36,"href":"/zh/docs/problem/Linux/20230817/","title":"Debian问题处理2","section":"Linux","content":" 代理 # Vmware里面的debian,连接外面物理机的v2ray。\n对于浏览器 # 无论是firefox还是chromium，都可以直接通过v2ray允许局域网，然后使用ProxySwitchOmege代理访问\n对于命令 # 可以使用proxychains，直接用apt-get 安装即可，注意事项\n作用范围 # 对tcp生效，ping是不生效的，不要白费力气\n需要修改两个地方 # libproxychains.so.3 提示不存在 ly\nwhereis libproxychains.so.3 #libproxychains.so.3: /usr/lib/x86_64-linux-gnu/libproxychains.so.3 #修改/usr/bin/proxychains #export LD_PRELOAD = libproxychains.so.3 修改为： export LD_PRELOAD = /usr/lib/x86_64-linux-gnu/libproxychains.so.3 l\u0026rsquo;y配置修改\n#修改文件/etc/proxychains.conf，在最后一行添加 socks5 192.168.1.201 1082 使用\nproxychains git pull #直接在命令最前面输入proxychains即可 直接网络（gui）配置代理 # 这个对于终端不生效\nzsh安装 # proxychains wget https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh proxychains sh install.sh zsh主题安装 # proxychains git clone --depth=1 https://github.com/romkatv/powerlevel10k.git ${ZSH_CUSTOM:-$HOME/.oh-my-zsh/custom}/themes/powerlevel10k #修改 vim ~/.zshrc ZSH_THEME=\u0026#34;powerlevel10k/powerlevel10k\u0026#34; 重新配置 p10k configure\n程序环境设置 # 环境变量\n#java环境变量 export JAVA_HOME=/usr/local/jdk1.8.0_281 export CLASSPATH=$:CLASSPATH:$JAVA_HOME/lib/ export PATH=$PATH:$JAVA_HOME/bin #maven环境变量 export MAVEN_HOME=/usr/local/apache-maven-3.3.9 export PATH=$PATH:$MAVEN_HOME/bin typora破解 # 声明：破解可耻，尊重正版。这里仅以学习为目的\n来自文章 https://ccalt.cn/2023/04/07/%5BWindows%7CLinux%5DTypora%E6%9C%80%E6%96%B0%E7%89%88%E9%80%9A%E7%94%A8%E7%A0%B4%E8%A7%A3-%E8%87%B3%E4%BB%8A%E5%8F%AF%E7%94%A8/\n程序 # https://github.com/DiamondHunters/NodeInject_Hook_example/actions/runs/4180836116\n我自己fork了一份，不知道哪天就没了\nhttps://github.com/lwmfjc/NodeInject_Hook_example/actions/runs/5888943386\n步骤 # 将linux版本的文件，按下面的结构解压放入Typora文件夹中\n这里盗（借）用文章图片说明，不想截图了\n之后先运行node_inject，后运行license-gen ，即可得到序列号\n字体 # 很多程序都偏小，系统字体基本正常。\nfirefox中，要设置最小字体。 Typor没找到。\npicgo问题 # 没有什么特别注意的，基本问题搜索引擎都有。对了，把windows下的picgo卸载了，只留下了picgo-core，还安装了 super-prefix，自定义上传路径及文件名\n参考文章 https://connor-sun.github.io/posts/38835.html\n#自定义文件夹及文件名 picgo install super-prefix #super-prefix地址 https://github.com/gclove/picgo-plugin-super-prefix picgo-core 配置手册 https://picgo.github.io/PicGo-Core-Doc/zh/guide/config.html\n#插件配置 ~/.picgo/config.json ,在根结构里面添加 \u0026#34;picgoPlugins\u0026#34;: { \u0026#34;picgo-plugin-super-prefix\u0026#34;: true }, \u0026#34;picgo-plugin-super-prefix\u0026#34;: { \u0026#34;prefixFormat\u0026#34;: \u0026#34;YYYY/MM/DD/\u0026#34;, \u0026#34;fileFormat\u0026#34;: \u0026#34;YYYYMMDD-HHmmss\u0026#34; } Typora中文件上传对picgo-core的设置 # 自定义命令格式：picgo upload，windows中可以使用绝对路径(加双引号)\n截图工具 # apt install flameshot 使用flameshot gui 启动\n"},{"id":37,"href":"/zh/docs/problem/Linux/20230815/","title":"Debian问题处理1","section":"Linux","content":" 清华源设置 # vim /etc/apt/sources.list #注释掉原来的，并添加 # 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释 deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm main contrib non-free non-free-firmware deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-updates main contrib non-free non-free-firmware deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian/ bookworm-backports main contrib non-free non-free-firmware deb https://mirrors.tuna.tsinghua.edu.cn/debian-security bookworm-security main contrib non-free non-free-firmware # deb-src https://mirrors.tuna.tsinghua.edu.cn/debian-security bookworm-security main contrib non-free non-free-firmware deb https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware # deb-src https://security.debian.org/debian-security bookworm-security main contrib non-free non-free-firmware 中文环境 # su sudo apt-get install locales #配置中文环境 1.选择zh开头的 2 后面选择en(cn也行，不影响输入法) sudo dpkg-reconfigure locales #设置上海时区 sudo timedatectl set-timezone Asia/Shanghai 中文输入法 # #清除旧的环境 apt-get remove ibus #不兼容问题 apt-get remove fcitx5 fcitx5-chinese-addons apt-get autoremove ly # gnome-shell-extension-kimpanel sudo apt install fcitx5 fcitx5-chinese-addons fcitx5-frontend-gtk4 fcitx5-frontend-gtk3 fcitx5-frontend-gtk2 fcitx5-frontend-qt5 im-config #配置使用fcitx5 #环境变量添加 export XMODIFIERS=@im=fcitx export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx #退出root用户权限，使用普通用户权限再终端 fcitx5-configtool #配置中文输入法即可 #附加组件-经典用户界面--这里可以修改字体及大小 其他 # 应用程序-优化 修改默认字体大小\n桌面任务栏\u0026ndash; https://extensions.gnome.org/extension/1160/dash-to-panel\n参考文章 https://itsfoss.com/gnome-shell-extensions/\n之后设置一下任务栏的位置即可\n参考文章 # https://zhuanlan.zhihu.com/p/508797663\n"},{"id":38,"href":"/zh/docs/problem/Linux/20230803/","title":"安卓手机及平板安装linuxDeploy的问题简记","section":"Linux","content":" 为什么是简记呢，因为这几天折腾这些太累了，等以后回过头来重新操作再详细记载\n前言 # 初衷 # 一开始的初衷是为了在平板上使用idea，之前看了一篇docker使用idea的文章，心血来潮。所以想直接在平板的termux安装docker然后使用，结果一堆问题。后面妥协了，在手机上装，然后开远程吧\n这年头机在人在，所以装手机还是平板，还真没有很大的问题。后面使用情况证明：手机不需要开热点的情况（开热点是为了保证网络联通，在同一局域网），其实不怎么发热也不怎么耗电的。\n平板上 # 本来想在tab s8平板上通过termux安装linux（无root权限），但是总会遇到一堆问题\u0026ndash;连系统都装不上。因为root会有两个问题，所以一开始没有考虑使用linuxDeploy（需要root）\n保修失效 无法通过系统直接更新（需要线刷） 手机上（root） # 配置 # 后面尝试在root过的手机上安装linuxDeploy，照样有一堆问题，这里配上能使用的配置（能进系统）：\n我用的时候ssh端口改了一下，不过不影响，第一次用的22端口也是能连上的。初始用户写的root，这里也是设置的root。\n最好挂载一下\n问题 # 用其他桌面环境，可能会导致图标没有（应该是就没有那个应用，比如浏览器），不过我这个配置完也没有浏览器，不过好在图标也没，不用自己再去移除了。\n装完之后vns有出错过一次，突然就蹦了，死活连不上。后面我直接重装系统了（linux deploy），没有再出现问题。装完之后需要在etc/rc.local添加:\n#删除vns临时文件，保证每次启动都是使用端口:5901 #(linux上显示:1，连接使用时要+5900，即使用5901端口) rm -rf /tmp/.X[1-9]-lock rm -rf /tmp/.X11-unix/X[1-9] #保证系统每次启动后都自动启动vncserver vncserver 电脑上随便找了个VNCServer 绿色免安装程序可以连上\n平板上使用AVNC，电脑不方便截图，就不截了.. 类似长这样\n#常用命令(也不常，这两天用的最多的) vncserver -kill :1 #强制关闭端口1 vncserver #启动 安装idea，也不用安装，就是去官网下载解压即可。问题：需要jdk11以上才能打开（疑惑，貌似之前在windows安装的时候没这要求，反正后面我妥协了，装了11，之后就是配置环境变量什么的）\n一开始linuxDeploy的Ubuntu，然后..发现openjdk11装完之后，java -version显示的10，一脸蒙圈，搞得后面又重装了Debian（中途还试了centos）\n装完没有中文输入法，系统装完就是要用的，如果随便打打命令倒是不需要中文输入法，但是如果打点代码写点注解，那蹩脚英语就\u0026hellip;总不能句句good good study,day day up..真是one day day de\u0026hellip;\n问题处理 # 其实解决方案前面好像都说了，输入法单独开一块吧，比较恶心，主要是让我意识到了自己水平有多菜\u0026hellip;\n某些机器（平板）省电模式下，默认的那个用户会断网，原因不明 # 所以那个用户就别用了，再创建一个新的\nuseradd -d /home/ly -s /bin/bash -m ly 然后设置下密码\npasswd ly 配置中文环境 # sudo dpkg-reconfigure locales #前面选英文和中文，后面选英文 #设置时区 sudo timedatectl set-timezone Asia/Shanghai 中文字体安装 # apt-get install ttf-wqy-zenhei apt-get install xfonts-intl-chinese wqy* 输入法相关安装 # #fcitx安装 apt install fcitx -y #输入法安装 apt install fcitx-googlepinyin fcitx-sunpinyin fcitx-pinyin #中文字体包，简体繁体 apt install fonts-arphic-bsmi00lp fonts-arphic-gbsn00lp fonts-arphic-gkai00mp apt install fcitx-table* 输入法bug # 网上一堆教程，找了很多，最后的解决方案 fcitx+googlepinyin\n没用fcitx5和sougou输入法，因为尝试了很多次实在装不上，不知道是arm64的架构问题还是什么，装完老是输入法状态栏闪啊闪\u0026hellip;bug？玄学？\n这个网上都有教程（抄袭），就不写（抄）了。写下重要的问题\n输入法在terminal终端、idea中不能切换出来、切换后打字不能上去 # tabs8没有这个bug，手机的miui系统有这个bug，不知道为啥\n这个问题其实在wiki里面有说到，不过我是google之后才定位到这里的\nhttps://wiki.archlinux.org/title/fcitx\n需要修改 ~/.xinitrc文件\nfcitx \u0026amp; #add export GTK_IM_MODULE=fcitx #add export QT_IM_MODULE=fcitx #add export XMODIFIERS=@im=fcitx #add XAUTHORITY=$HOME/.Xauthority export XAUTHORITY LANG_MESSAGE=zh_CN.UTF-8 #add LANG=en_US.UTF-8 #add export LANG #add export LANG_MESSAGE #add echo $$ \u0026gt; /tmp/xsession.pid . $HOME/.xsession 其他的不要改，安装系统原来怎么样就怎么样就行\u0026hellip;\n在这之前什么中文字体、还有区域设置都要先搞定，前面的设置选择en_US.UTF-8+zh_CN所有（有四五个），后面的设置选择系统默认语言（中英文都可，我选的英文，方便我这样的菜鸟报错时google查解决方案，只能选一个）\n突然想起来souhupinyin闪啊闪可能跟这里设置了fcitx有关系？以后有空再研究\n还有这个地方要改\nvim /etc/locale.conf #LANG=zh_CN.UTF-8 #LC_MESSAGES=en_US.UTF-8 LANG=en_US.UTF-8 LC_MESSAGES=zh_CN.UTF-8 之前我还改了idea.*vmoptions的配置，不过可能跟这个没有太大关系\nidea中输入法位置会出现在左下角 # 看了fcitx官方的issue，说是不关它的事\u0026hellip;.\n博客 # Obsidian # 到官方github下载即可\nhttps://github.com/obsidianmd/obsidian-releases/releases\n下载其中一个就行，不过都要以 \u0026ndash;no-sandbox方式才能运行\n这期间会有很多问题，需要安装下列软件\napt-get install zlib1g-dev apt-get install fuse libfuse2 apt install libnss3 ./obsidian --no-sandbox #以这种方式运行 Typora # linux 的arm64只存在于1.0以上版本，众所周知\u0026hellip;\n以下链接，仅供学习\nhttps://www.cnblogs.com/youngyajun/p/16661980.html \u0026ndash;1.0.3有效，不过图片上传有点问题，直接拖曳进去是能上传的，复制后粘贴不行（1.6可以，不过未授权，不太好弄）\npython安装过程也挺曲折，不要装python2，直接装3。pip也是直接装版本3即可。记得使用清华源/或者阿里源，要不得下载半天\npicgo # 这里配合的是picgo-core，用官方教程安装即可\n直接picgo不行，没有找到对应平台的安装包\nPicHoro # 最后放弃在linux上写博客的想法了，勉强能用，不过实在是勉强。\n后面采用直接在平板安装obsidian+pichoro的办法\nhttps://github.com/Kuingsmile/PicHoro\n还行。不过2.1.2报毒，不知道为啥，下完被自动删除了。使用的是2.1.1\n浏览器 # chromium # 直接apt安装就好了，装完之后ui那个图标是打不开的，同样的问题，在terminal终端那里，使用命令chromium --no-sandbox即可打开。有一些问题，比如没声音啦、一些pdf插件（博客的）显示不出来之类。目前没需求，暂不处理了\nps: 还有q的问题，目前也没需求。暂不处理\n参考链接 # https://cloud.tencent.com/developer/article/1159972\nhttps://blog.csdn.net/qysh123/article/details/117288055\nhttps://blog.csdn.net/weixin_42122432/article/details/116703457\nhttps://www.cnblogs.com/jtianlin/p/4230527.html\nhttps://blog.csdn.net/sinat_42483341/article/details/104104441\nhttps://blog.csdn.net/ma726518972/article/details/121034994?ydreferer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8%3D\nhttps://www.cnblogs.com/shanhubei/p/17517381.html\nhttps://blog.csdn.net/sandonz/article/details/106877555\nhttps://archlinuxarm.org/packages/aarch64/vim\nhttps://bbs.archlinuxcn.org/viewtopic.php?id=12685\nhttps://soft.zol.com.cn/126/1262460.html\nhttps://bbs.archlinuxcn.org/viewtopic.php?id=10498\nhttps://wiki.archlinux.org/title/fcitx\nhttps://www.reddit.com/r/archlinux/comments/rl1ncw/fcitx_input_in_terminal_window_doesnt_work/\nhttps://stackoverflow.com/questions/20705089/why-can-not-i-use-fcitx-input-method-in-gnome-terminal\nhttps://blog.csdn.net/u011166277/article/details/106287587/\nhttps://www.reddit.com/r/swaywm/comments/t09udp/anyone_using_the_input_method_fcitx5rime_cant_get/\nhttps://github.com/fcitx/fcitx5/issues/79\n其实不止这些，不过有用的可能就这些\n成果 # 疑惑 # 其实科学这种东西，在你不精通的情况下，也会出现玄而又玄的事。那传统意义上所谓的玄学，是否是因为自己不精通/失传导致的呢\u0026hellip;人类是退化还是进化..不过有一点是肯定的，所有的事情都是人为的 \u0026ndash; 自作自受。\n用window系统打完了这篇博客真的爽，没有各种莫名其妙的问题。果然，跌到谷底的时候，怎么走都是向上。\n"},{"id":39,"href":"/zh/docs/problem/JVM/20230526/","title":"JDK代理和CGLIB代理","section":"Jvm","content":" 完全转载自https://juejin.cn/post/7011357346018361375 ，以防丢失故作备份 。\n一、什么是代理模式 # 代理模式（Proxy Pattern）给某一个对象提供一个代理，并由代理对象控制原对象的引用。代理对象在客户端和目标对象之间起到中介作用。\n代理模式是常用的结构型设计模式之一，当直接访问某些对象存在问题时可以通过一个代理对象来间接访问，为了保证客户端使用的透明性，所访问的真实对象与代理对象需要实现相同的接口。代理模式属于结构型设计模式，属于GOF23种设计模式之一。\n代理模式可以分为静态代理和动态代理两种类型，而动态代理中又分为JDK动态代理和CGLIB代理两种。 代理模式包含如下角色:\nSubject (抽象主题角色) 抽象主题角色声明了真实主题和代理主题的共同接口,这样一来在任何使用真实主题 的地方都可以使用代理主题。客户端需要针对抽象主题角色进行编程。 Proxy (代理主题角色) 代理主题角色内部包含对真实主题的引用，从而可以在任何时候操作真实主题对象。 在代理主题角色中提供一个与真实主题角色相同的接口，以便在任何时候都可以替代真实 主体。代理主题角色还可以控制对真实主题的使用，负责在需要的时候创建和删除真实主 题对象,并对真实主题对象的使用加以约束。代理角色通常在客户端调用所引用的真实主 题操作之前或之后还需要执行其他操作，而不仅仅是单纯的调用真实主题对象中的操作。 RealSubject (真实主题 角色) 真实主题角色定义了代理角色所代表的真实对象，在真实主题角色中实现了真实的业 务操作,客户端可以通过代理主题角色间接调用真实主题角色中定义的方法。 代理模式的优点 # 代理模式能将代理对象与真实被调用的目标对象分离。 一定程度上降低了系统的耦合度，扩展性好。 可以起到保护目标对象的作用。 可以对目标对象的功能增强。 代理模式的缺点 # 代理模式会造成系统设计中类的数量增加。 在客户端和目标对象增加一个代理对象，会造成请求处理速度变慢。 二、JDK动态代理 # 在java的动态代理机制中，有两个重要的类或接口，一个是 InvocationHandler(Interface)、另一个则是 Proxy(Class)，这一个类和接口是实现我们动态代理所必须用到的。\nInvocationHandler # 每一个动态代理类都必须要实现InvocationHandler这个接口，并且每个代理类的实例都关联了一个handler，当我们通过代理对象调用一个方法的时候，这个方法的调用就会被转发为由InvocationHandler这个接口的 invoke 方法来进行调用。\nInvocationHandler这个接口的唯一一个方法 invoke 方法：\njava 复制代码Object invoke(Object proxy, Method method, Object[] args) throws Throwable 这个方法一共接受三个参数，那么这三个参数分别代表如下：\nproxy:指代JDK动态生成的最终代理对象 method:指代的是我们所要调用真实对象的某个方法的Method对象 args:指代的是调用真实对象某个方法时接受的参数 Proxy # Proxy这个类的作用就是用来动态创建一个代理对象的类，它提供了许多的方法，但是我们用的最多的就是newProxyInstance 这个方法：\njava 复制代码public static Object newProxyInstance(ClassLoader loader, Class\u0026lt;?\u0026gt;[] interfaces, InvocationHandler handler) throws IllegalArgumentException 这个方法的作用就是得到一个动态的代理对象，其接收三个参数，我们来看看这三个参数所代表的含义：\nloader：ClassLoader对象，定义了由哪个ClassLoader来对生成的代理对象进行加载，即代理类的类加载器。 interfaces：Interface对象的数组，表示的是我将要给我需要代理的对象提供一组什么接口，如果我提供了一组接口给它，那么这个代理对象就宣称实现了该接口(多态)，这样我就能调用这组接口中的方法了。 Handler：InvocationHandler对象，表示的是当我这个动态代理对象在调用方法的时候，会关联到哪一个InvocationHandler对象上。 所以我们所说的DynamicProxy（动态代理类）是这样一种class：它是在运行时生成的class，在生成它时你必须提供一组interface给它，然后该class就宣称它实现了这些 interface。这个DynamicProxy其实就是一个Proxy，它不会做实质性的工作，在生成它的实例时你必须提供一个handler，由它接管实际的工作。\nJDK动态代理实例 # 创建接口类\njava复制代码public interface HelloInterface { void sayHello(); } 创建被代理类，实现接口\njava复制代码/** * 被代理类 */ public class HelloImpl implements HelloInterface{ @Override public void sayHello() { System.out.println(\u0026#34;hello\u0026#34;); } } 创建InvocationHandler实现类\njava复制代码/** * 每次生成动态代理类对象时都需要指定一个实现了InvocationHandler接口的调用处理器对象 */ public class ProxyHandler implements InvocationHandler{ private Object subject; // 这个就是我们要代理的真实对象，也就是真正执行业务逻辑的类 public ProxyHandler(Object subject) {// 通过构造方法传入这个被代理对象 this.subject = subject; } /** *当代理对象调用真实对象的方法时，其会自动的跳转到代理对象关联的handler对象的invoke方法来进行调用 */ @Override public Object invoke(Object obj, Method method, Object[] objs) throws Throwable { Object result = null; System.out.println(\u0026#34;可以在调用实际方法前做一些事情\u0026#34;); System.out.println(\u0026#34;当前调用的方法是\u0026#34; + method.getName()); result = method.invoke(subject, objs);// 需要指定被代理对象和传入参数 System.out.println(method.getName() + \u0026#34;方法的返回值是\u0026#34; + result); System.out.println(\u0026#34;可以在调用实际方法后做一些事情\u0026#34;); System.out.println(\u0026#34;------------------------\u0026#34;); return result;// 返回method方法执行后的返回值 } } 测试\njava复制代码public class Mytest { public static void main(String[] args) { //第一步：创建被代理对象 HelloImpl hello = new HelloImpl(); //第二步：创建handler,传入真实对象 ProxyHandler handler = new ProxyHandler(hello); //第三步：创建代理对象，传入类加载器、接口、handler HelloInterface helloProxy = (HelloInterface) Proxy.newProxyInstance( HelloInterface.class.getClassLoader(), new Class[]{HelloInterface.class}, handler); //第四步：调用方法 helloProxy.sayHello(); } } 结果\nmarkdown复制代码可以在调用实际方法前做一些事情 当前调用的方法是sayHello hello sayHello方法的返回值是null 可以在调用实际方法后做一些事情 ------------------------ JDK动态代理步骤 # JDK动态代理分为以下几步：\n拿到被代理对象的引用，并且通过反射获取到它的所有的接口。 通过JDK Proxy类重新生成一个新的类，同时新的类要实现被代理类所实现的所有的接口。 动态生成 Java 代码，把新加的业务逻辑方法由一定的逻辑代码去调用。 编译新生成的 Java 代码.class。 将新生成的Class文件重新加载到 JVM 中运行。 所以说JDK动态代理的核心是通过重写被代理对象所实现的接口中的方法来重新生成代理类来实现的，那么假如被代理对象没有实现接口呢？那么这时候就需要CGLIB动态代理了。\n三、CGLIB动态代理 # JDK动态代理是通过重写被代理对象实现的接口中的方法来实现，而CGLIB是通过继承被代理对象来实现，和JDK动态代理需要实现指定接口一样，CGLIB也要求代理对象必须要实现MethodInterceptor接口，并重写其唯一的方法intercept。\nCGLib采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。（利用ASM开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理）\n注意：因为CGLIB是通过继承目标类来重写其方法来实现的，故而如果是final和private方法则无法被重写，也就是无法被代理。\nxml复制代码\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;cglib\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;cglib-nodep\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; CGLib核心类 # 1、 net.sf.cglib.proxy.Enhancer：主要增强类，通过字节码技术动态创建委托类的子类实例；\nEnhancer可能是CGLIB中最常用的一个类，和Java1.3动态代理中引入的Proxy类差不多。和Proxy不同的是，Enhancer既能够代理普通的class，也能够代理接口。Enhancer创建一个被代理对象的子类并且拦截所有的方法调用（包括从Object中继承的toString和hashCode方法）。Enhancer不能够拦截final方法，例如Object.getClass()方法，这是由于Java final方法语义决定的。基于同样的道理，Enhancer也不能对fianl类进行代理操作。这也是Hibernate为什么不能持久化final class的原因。\n2、net.sf.cglib.proxy.MethodInterceptor：常用的方法拦截器接口，需要实现intercept方法，实现具体拦截处理；\njava复制代码 public java.lang.Object intercept(java.lang.Object obj, java.lang.reflect.Method method, java.lang.Object[] args, MethodProxy proxy) throws java.lang.Throwable{} obj：动态生成的代理对象 method:实际调用的方法 args：调用方法入参 net.sf.cglib.proxy.MethodProxy：java Method类的代理类，可以实现委托类对象的方法的调用；常用方法：methodProxy.invokeSuper(proxy, args)；在拦截方法内可以调用多次。 CGLib代理实例 # 创建被代理类\njava复制代码public class SayHello { public void say(){ System.out.println(\u0026#34;hello\u0026#34;); } } 创建代理类\njava复制代码/** *代理类 */ public class ProxyCglib implements MethodInterceptor{ private Enhancer enhancer = new Enhancer(); public Object getProxy(Class clazz){ //设置需要创建子类的类 enhancer.setSuperclass(clazz); enhancer.setCallback(this); //通过字节码技术动态创建子类实例 return enhancer.create(); } //实现MethodInterceptor接口方法 public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { System.out.println(\u0026#34;可以在调用实际方法前做一些事情\u0026#34;); //通过代理类调用父类中的方法 Object result = proxy.invokeSuper(obj, args); System.out.println(\u0026#34;可以在调用实际方法后做一些事情\u0026#34;); return result; } } 测试\njava复制代码public class Mytest { public static void main(String[] args) { ProxyCglib proxy = new ProxyCglib(); //通过生成子类的方式创建代理类 SayHello proxyImp = (SayHello)proxy.getProxy(SayHello.class); proxyImp.say(); } } 结果\n复制代码可以在调用实际方法前做一些事情 hello 可以在调用实际方法后做一些事情 CGLIB动态代理实现分析 # CGLib动态代理采用了FastClass机制，其分别为代理类和被代理类各生成一个FastClass，这个FastClass类会为代理类或被代理类的方法分配一个 index(int类型)。这个index当做一个入参，FastClass 就可以直接定位要调用的方法直接进行调用，这样省去了反射调用，所以调用效率比 JDK 动态代理通过反射调用更高。\n但是我们看上面的源码也可以明显看到，JDK动态代理只生成一个文件，而CGLIB生成了三个文件，所以生成代理对象的过程会更复杂。\n四、JDK和CGLib动态代理对比 # JDK 动态代理是实现了被代理对象所实现的接口，CGLib是继承了被代理对象。 JDK和CGLib 都是在运行期生成字节码,JDK是直接写Class字节码，CGLib 使用 ASM 框架写Class字节码，Cglib代理实现更复杂，生成代理类的效率比JDK代理低。\nJDK 调用代理方法，是通过反射机制调用,CGLib 是通过FastClass机制直接调用方法,CGLib 执行效率更高。\n原理区别： # java动态代理是利用反射机制生成一个实现代理接口的匿名类，在调用具体方法前调用InvokeHandler来处理。核心是实现InvocationHandler接口，使用invoke()方法进行面向切面的处理，调用相应的通知。\n而cglib动态代理是利用asm开源包，对代理对象类的class文件加载进来，通过修改其字节码生成子类来处理。核心是实现MethodInterceptor接口，使用intercept()方法进行面向切面的处理，调用相应的通知。\n1、如果目标对象实现了接口，默认情况下会采用JDK的动态代理实现AOP\n2、如果目标对象实现了接口，可以强制使用CGLIB实现AOP\n3、如果目标对象没有实现了接口，必须采用CGLIB库，spring会自动在JDK动态代理和CGLIB之间转换\n性能区别： # 1、CGLib底层采用ASM字节码生成框架，使用字节码技术生成代理类，在jdk6之前比使用Java反射效率要高。唯一需要注意的是，CGLib不能对声明为final的方法进行代理，因为CGLib原理是动态生成被代理类的子类。\n2、在jdk6、jdk7、jdk8逐步对JDK动态代理优化之后，在调用次数较少的情况下，JDK代理效率高于CGLIB代理效率，只有当进行大量调用的时候，jdk6和jdk7比CGLIB代理效率低一点，但是到jdk8的时候，jdk代理效率高于CGLIB代理。\n各自局限： # 1、JDK的动态代理机制只能代理实现了接口的类，而不能实现接口的类就不能实现JDK的动态代理。\n2、cglib是针对类来实现代理的，他的原理是对指定的目标类生成一个子类，并覆盖其中方法实现增强，但因为采用的是继承，所以不能对final修饰的类进行代理。\n类型 机制 回调方式 适用场景 效率 JDK动态代理 委托机制，代理类和目标类都实现了同样的接口，InvocationHandler持有目标类，代理类委托InvocationHandler去调用目标类的原始方法 反射 目标类是接口类 效率瓶颈在反射调用稍慢 CGLIB动态代理 继承机制，代理类继承了目标类并重写了目标方法，通过回调函数MethodInterceptor调用父类方法执行原始逻辑 通过FastClass方法索引调用 非接口类、非final类，非final方法 第一次调用因为要生成多个Class对象，比JDK方式慢。多次调用因为有方法索引比反射快，如果方法过多，switch case过多其效率还需测试 五、静态代理和动态的本质区别 # 静态代理只能通过手动完成代理操作，如果被代理类增加新的方法，代理类需要同步新增，违背开闭原则。 动态代理采用在运行时动态生成代码的方式，取消了对被代理类的扩展限制，遵循开闭原则。 若动态代理要对目标类的增强逻辑扩展，结合策略模式，只需要新增策略类便可完成，无需修改代理类的代码。 作者：ycf 链接：https://juejin.cn/post/7011357346018361375 来源：稀土掘金 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n"},{"id":40,"href":"/zh/docs/technology/JVM/understanding_the_jvm/03/","title":"03垃圾收集器与内存分配策略","section":"_深入理解Java虚拟机_","content":" 学习《深入理解Java虚拟机》，感谢作者！\n代码清单3-9 -XX:MaxTenuringThreshod=1说明 # Eden[8M] Survivor1[1M] Survivor2[1M] Old {10M} 初始 allocation1[0.25M]，allocation2[4MB] 3执行时gc导致的变化 +allocation1[0.25M] +allocation2[4MB] 3执行后 +allocation3[4MB] +allocation1[0.25M] +allocation2[4MB] 5执行时gc导致的变化 allocation2[4MB]，+allocation1[0.25M] 5执行后 +allocation3[4MB] allocation2[4MB]，+allocation1[0.25M] 代码清单3-9 -XX:MaxTenuringThreshod=15说明 # Eden[8M] Survivor1[1M] Survivor2[1M] Old {10M} 初始 allocation1[0.25M]，allocation2[4MB] 3执行时gc导致的变化 +allocation1[0.25M] +allocation2[4MB] 3执行后 +allocation3[4MB] +allocation1[0.25M] +allocation2[4MB] 5执行时gc导致的变化 +allocation1[0.25M] allocation2[4MB] 5执行后 +allocation3[4MB] +allocation1[0.25M] allocation2[4MB]，+allocation1[0.25M] 代码清单3-10 说明 # Eden[8M] Survivor1[1M] Survivor2[1M] Old {10M} 初始 allocation1[0.25M]，allocation2[[0.25M]，allocation3[4M] 4执行时gc导致的变化 +allocation1[0.25M]，+allocation2[[0.25M]， +allocation3[4MB] 4执行后 +allocation4[4MB] +allocation1[0.25M]，+allocation2[[0.25M]， +allocation3[4MB] 6执行时gc导致的变化 allocation3[4MB]，+allocation1[0.25M]，+allocation2[[0.25M]， 6执行后 +allocation4[4MB] allocation3[4MB]，+allocation1[0.25M]，+allocation2[[0.25M]， 代码清单3-11 说明 # -XX:-HandlePromotionFailure 关 # Eden[8M] Survivor1[1M] Survivor2[1M] Old {10M} 初始 allocation1[2M]，allocation2[2M]，allocation3[2M]allocation1[null]，allocation4[2M] 5执行时gc导致的变化 +allocation2[2M]，+allocation3[2M] //总共4M 5执行后 +allocation4[2M] +allocation2[2M]，+allocation3[2M] //总共4M 6-\u0026gt;11 allocation4[2M]+allocation5[2M]，+allocation6[2M] allocation2[2M]，allocation3[2M] //总共4M，此时老年代连续可用空间在6M（或者说小于6M） 11执行时gc导致的变化 allocation3[4MB]，+allocation1[0.25M]，+allocation2[[0.25M]， 11执行后 +allocation7[2MB] allocation3[4MB]，+allocation1[0.25M]，+allocation2[[0.25M]， 说明 # 书籍版权归著者和出版社所有\n本PDF来自于各个广泛的信息平台，经过整理而成\n本PDF仅限用于非商业用途或者个人交流研究学习使用\n本PDF获得者不得在互联网上以任何目的进行传播，违规者造成的法律责任和后果，违规者自负\n如果觉得书籍内容很赞，请一定购买正版实体书，多多支持编写高质量的图书的作者和相应的出版社!当然，如果图书内容不堪入目，质量低下，你也可以选择狠狠滴撕裂本PDF\n技术类书籍是拿来获取知识的，不是拿来收藏的，你得到了书籍不意味着你得到了知识，所以请不要得到书籍后就觉得沾沾自喜，要经常翻阅!!经常翻阅\n请于下载PDF后24小时内研究使用并删掉本PDF\n"},{"id":41,"href":"/zh/docs/problem/JVM/2023052302/","title":"linux中调试open jdk","section":"Jvm","content":" 完全转载自https://lin1997.github.io/2020/07/19/debug-openjdk-on-ubuntu.html ，以防丢失故作备份，目前还没看懂。\n在Ubuntu中编译和调试OpenJDK # OpenJDK\nUbuntu\nCLion\n2020年 07月19日\n构建编译环境 # 安装GCC编译器：\nsudo apt install build-essential 安装OpenJDK依赖库：\n工具 库名称 安装命令 FreeType The FreeType Project sudo apt install libfreetype6-dev CUPS Common UNIX Printing System sudo apt install libcups2-dev X11 X Window System sudo apt install libx11-dev libxext-dev libxrender-dev libxrandr-dev libxtst-dev libxt-dev ALSA Advanced Linux Sound Architecture sudo apt install libasound2-dev libffi Portable Foreign Function Interface sudo apt install libffi-dev Autoconf Extensible Package of M4 Macros sudo apt install autoconf zip/unzip unzip sudo apt install zip unzip fontconfig fontconfig sudo apt install libfontconfig1-dev 假设要编译大版本号为N的JDK，我们还要安装一个大版本号至少为N-1的、已经编译好的JDK作为“Bootstrap JDK”：\nsudo apt install openjdk-11-jdk 获取源码 # 可以直接访问准备下载的JDK版本的仓库页面（譬如本例中OpenJDK 11的页面为https://hg.openjdk.java.net/jdk-updates/jdk11u/），然后点击左边菜单中的“Browse”，再点击左边的“zip”链接即可下载当前版本打包好的源码，到本地直接解压即可。\n也可以从Github的镜像Repositories中获取（https://github.com/openjdk），进入所需版本的JDK的页面，点击Clone按钮下的Download ZIP按钮下载打包好的源码，到本地直接解压即可。\n进行编译 # 首先进入解压后的源代码目录，本例解压到的目录为~/openjdk/：\ncd ~/openjdk 要想带着调试、定制化的目的去编译，就要使用OpenJDK提供的编译参数，可以使用bash configure --help查看. 本例要编译SlowDebug版、仅含Server模式的HotSpot虚拟机，同时我们还可以禁止压缩生成的调试符号信息，方便gdb调试获取当前正在执行的源代码和行号等调试信息. 对应命令如下：\nbash configure --with-debug-level=slowdebug --with-jvm-variants=server --disable-zip-debug-info 对于版本较低的OpenJDK，编译过程中可能会出现了源码deprecated的错误，这是因为\u0026gt;=2.24版本的glibc中 ，readdir_r等方法被标记为deprecated。若读者也出现了该问题，请在configure命令加上--disable-warnings-as-errors参数，如下：\nbash configure --with-debug-level=slowdebug --with-jvm-variants=server --disable-zip-debug-info --disable-warnings-as-errors 此外，若要重新编译，请先执行make dist-clean\n执行make命令进行编译：\nmake 生成的JDK在build/配置名称/jdk中，测试一下，如：\ncd build/linux-x86_64-normal-server-slowdebug/jdk/bin ./java -version 生成Compilation Database # CLion可以通过Compilation Database来导入项目. 在OpenJDK 11u及之后版本中，OpenJDK官方提供了对于IDE的支持，可以使用make compile-commands命令生成Compilation Database：\nmake compile-commands 对于版本较低的OpenJDK，可以使用一些工具来生成Compilation Database，比如：\nBear scan-build compiled 然后检查一下build/配置名称/下是否生成了compile_commands.json.\ncd build/linux-x86_64-normal-server-slowdebug ls -l 导入项目至CLion # 优化CLion索引速度 # 提高Inotify监视文件句柄上限，以优化CLion索引速度：\n在/etc/sysctl.conf中或 /etc/sysctl.d/目录下新建一个*.conf文件，添加以下内容：\nfs.inotify.max_user_watches = 524288 应用更改：\nsudo sysctl -p --system 重新启动CLion\n导入项目 # 打开CLion，选择Open Or Import，选择上文生成的build/配置名称/compile_commands.json文件，弹出框选择Open as Project，等待文件索引完成.\n接着，修改项目的根目录，通过Tools -\u0026gt; Compilation Database -\u0026gt; Change Project Root功能，选中你的源码目录.\n为了减少CLion索引文件数，提高CLion效率，建议将非必要的文件夹排除：Mark Directory as -\u0026gt; Excluded. 大部分情况下，我们只需要索引以下文件夹下的源码：\nsrc/hotspot src/java.base 配置调试选项 # 创建自定义Build Target # 点击File菜单栏，Settings -\u0026gt; Build, Execution, Deployment -\u0026gt; Custom Build Targets，点击+新建一个Target，配置如下：\nName：Target的名字，之后在创建Run/Debug配置的时候会看到这个名字\n点击Build或者Clean右边的三点，弹出框中点击+新建两个External Tool配置如下：\n# 第一个配置如下，用来指定构建指令 # Program 和 Arguments 共同构成了所要执行的命令 \u0026#34;make\u0026#34; Name: make Program: make Arguments: Working directory: {项目的根目录} # 第二个配置如下，用来清理构建输出 # Program 和 Arguments 共同构成了所要执行的命令 \u0026#34;make clean\u0026#34; Name: make clean Program: make Arguments: clean Working directory: {项目的根目录} ToolChain选择Default；Build选择make（上面创建的第一个External Tool）；Clean选择make clean（上面创建的第二个External Tool）\n创建自定义的Run/Debug configuration # 点击Run菜单栏，Edit Configurations， 点击+，选择Custom Build Application，配置如下：\n# Executable 和 Program arguments 可以根据需要调试的信息自行选择 # Name：Configure 的名称 Name: OpenJDK # Target：选择上一步创建的 “Custom Build Target” Target: {上一步创建的 “Custom Build Target”} # Executable：程序执行入口，也就是需要调试的程序 Executable: 这里我们调试`java`，选择`{source_root}/build/{build_name}/jdk/bin/java`。 # Program arguments: 与 “Executable” 配合使用，指定其参数 Program arguments: 这里我们选择`-version`，简单打印一下`java`版本。 # Before luanch：这个下面的Build可去可不去，去掉就不会每次执行都去Build，节省时间，但其实OpenJDK增量编译的方式，每次Build都很快，所以就看个人选择了。 配置GDB # 由于HotSpot JVM内部使用了SEGV等信号来实现一些功能（如NullPointerException、safepoints等），所以调试过程中，GDB可能会误报Signal: SIGSEGV (Segmentation fault). 解决办法是，在用户目录下创建.gdbinit，让GDB捕获SEGV等信号：\nvi ~/.gdbinit 将以下内容追加到文件中并保存：\nhandle SIGSEGV nostop noprint pass 开始调试 # 使用CLion调试C++层面的代码 # 完成以上配置之后，一个可修改、编译、调试的HotSpot工程就完全建立起来了。HotSpot虚拟机启动器的执行入口是${source_root}/src/java.base/share/native/libjli/java.c的JavaMain()方法，读者可以设置断点后点击Debug可开始调试.\n使用GDB调试汇编层面的代码 # 这里提供两个方法，一个是使用-XX:StopInterpreterAt=\u0026lt;n\u0026gt;虚拟机参数来实现中断，缺点是需要找到你所感兴趣的字节码在程序中的序号；第二个方法是直接去寻找记录生成的机器指令的入口(EntryPoint)的表，即Interpreter::_normal_table，在对应的字节码入口地址打断点，但是这需要读者对模板解释器有一定了解。\n使用虚拟机参数进行中断 # 对于汇编级别的调试，我们可以手动使用GDB进行调试：\ngdb build/linux-x86_64-normal-server-slowdebug/jdk/bin/java 由于目前HotSpot在主流的操作系统上，都采用模板解释器来执行字节码，它与即时编译器一样，最终执行的汇编代码都是运行期间产生的，无法直接设置断点，所以HotSpot增加了一些参数来方便开发人员调试解释器。\n我们可以先使用参数-XX:+TraceBytecodes，打印并找出你所感兴趣的字节码位置，中途可以使用Ctrl + C退出：\nset args -XX:+TraceBytecodes run 然后，再使用参数-XX:StopInterpreterAt=\u0026lt;n\u0026gt;，当遇到程序的第n条字节码指令时，便会进入${source_root}/src/os/linux/vm/os_linux.cpp中的空函数breakpoint()：\nset args -XX:+TraceBytecodes -XX:StopInterpreterAt=\u0026lt;n\u0026gt; 再通过GDB在${source_root}/src/hotspot/os/linux/os_linux.cpp中的breakpoint()函数上打上断点:\nbreak breakpoint 为什么要将断点打在这里？\n去看${source_root}/src/hotspot/share/interpreter/templateInterpreterGenerator.cpp里，函数TemplateInterpreterGenerator::generate_and_dispatch中对stop_interpreter_at()的调用就知道了.\n接着我们开始运行hotspot：\nrun 当命中断点时，我们再跳出breakpoint()函数：\nfinish 这样就会返回到真正的字节码的执行了。\n不过，我们还要跳过函数TemplateInterpreterGenerator::generate_and_dispatch中插入到字节码真正逻辑前的一些用于debug的逻辑：\nif (PrintBytecodeHistogram) histogram_bytecode(t); // debugging code if (CountBytecodes || TraceBytecodes || StopInterpreterAt \u0026gt; 0) count_bytecode(); if (PrintBytecodePairHistogram) histogram_bytecode_pair(t); if (TraceBytecodes) trace_bytecode(t); if (StopInterpreterAt \u0026gt; 0) stop_interpreter_at(); 比如开启了参数-XX:+TraceBytecodes和-XX:StopInterpreterAt=\u0026lt;n\u0026gt;，应该跳过的指令如下：\n# count_bytecode()对应指令: 0x7fffe07e8261:\tincl 0x16901039(%rip) # 0x7ffff70e92a0 \u0026lt;BytecodeCounter::_counter_value\u0026gt; # trace_bytecode(t)对应指令: 0x7fffe07e8267:\tmov %rsp,%r12 0x7fffe07e826a:\tand $0xfffffffffffffff0,%rsp 0x7fffe07e826e:\tcallq 0x7fffe07c5edf 0x7fffe07e8273:\tmov %r12,%rsp 0x7fffe07e8276:\txor %r12,%r12 # stop_interpreter_at()对应指令: 0x7fffe07e8279:\tcmpl $0x66,0x1690101d(%rip) # 0x7ffff70e92a0 \u0026lt;BytecodeCounter::_counter_value\u0026gt; 0x7fffe07e8283:\tjne 0x7fffe07e828e 0x7fffe07e8289:\tcallq 0x7ffff606281a \u0026lt;os::breakpoint()\u0026gt; #\t......................... #\t......真正的字节码逻辑...... #\t......................... # dispatch_epilog(tos_out, step)对应指令，用来取下一条指令执行... 进入真正的字节码逻辑后，我们就可以使用指令级别的stepi, nexti命令来进行跟踪调试了。（由于汇编代码都是运行期产生的，GDB中没有与源代码的对应符号信息，所以不能用C++源码行级命令step以及next）\n寻找字节码机器指令的入口手动打断点 # 关于模板解释器相关知识，可以阅读：JVM之模板解释器.\n还是一样，运行GDB：\ngdb build/linux-x86_64-normal-server-slowdebug/jdk/bin/java start break JavaMain continue 我们先在${source_root}/src/hotspot/share/interpreter/templateInterpreter.cpp的DispatchTable::set_entry(...)函数上打条件断点，条件是函数实参i == \u0026lt;字节码对应十六进制\u0026gt;，字节码对应的十六进制见：${source_root}/src/hotspot/share/interpreter/bytecodes.hpp的Bytecodes::Code.\nbreak DispatchTable::set_entry if i==\u0026lt;字节码对应十六进制\u0026gt; 然后继续运行\ncontinue 命中断点后，查看函数实参entry所指向的内存地址\nprint entry 在这个地址上打断点。\nbreak *\u0026lt;内存地址\u0026gt; 然后继续运行\ncontinue 命中断点后，就跟前一个方法一样可以直接使用指令级别的stepi, nexti命令来进行跟踪调试了。\n配置IDEA # 为项目的绑定JDK源码路径 # 打开IDEA，新建一个项目。然后选择File -\u0026gt; Project Structure，选到SDKs选项，新添加上自己刚刚编译生成的JDK，JDK home path为${source_root}/build/配置名称/jdk. 然后在Sourcepath下移除原本的源码路径（如果有），并添加为前面的源代码，如${source_root}/src/java.base/share/classes等. 这样以来，我们就可以在IDEA中编辑JDK的JAVA代码，添加自己的注释了。\n重新编译JDK的JAVA代码 # 在添加中文注释后，再编译JDK时会报错：\nerror: unmappable character (0x??) for encoding ascii\n我们可以在${source_root}/make/common/SetupJavaCompilers.gmk中，修改两处编码方式的设置，替换原内容：\n-encoding ascii 为：\n-encoding utf-8 这样编译就不会报错了。\n而且，如果我们只修改了JAVA代码，无需使用make命令重新编译整个OpenJDK，而只需要使用以下命令仅编译JAVA模块：\nmake java 使用IDEA的Step Into跟踪调试源码 # 我们发现，在IDEA调试JDK源码时，无法使用Step Into(F7)跟进JDK中的相关函数，这是因为IDEA默认设置不步入这些内置的源码。可以在File -\u0026gt; Settings -\u0026gt; Build, Execution, Deployment -\u0026gt; Debugger -\u0026gt; Stepping中，取消勾选Do not step into the classes来取消限制。\n参考文章 # Tips \u0026amp; Tricks: Develop OpenJDK in CLion with Pleasure OpenJDK 编译调试指南(Ubuntu 16.04 + MacOS 10.15) JVM-在MacOS系统上使用CLion编译并调试OpenJDK12 深入理解Java虚拟机：JVM高级特性与最佳实践(第3版) 编译JDK源码踩坑纪实 How to to debug the HotSpot interpreter JVM之模板解释器 "},{"id":42,"href":"/zh/docs/problem/Linux/20230523/","title":"zsh卸载后root无法登录及vm扩容centos7报错处理","section":"Linux","content":" zsh卸载后root无法登录 # 主要参考文档 https://blog.csdn.net/Scoful/article/details/119746150\n重启，开机引导进入下面的那个，按e进入编辑模式 移动光标，找到ro crashkernel=auto，修改为 rw init=sysroot/bin/sh\n按ctrl+x进入单用户模式界面\n输入chroot /sysroot 获取权限 vim /etc/passwd 第一行 ，root \u0026hellip;\u0026hellip;zsh，中/bin/zsh，改为/bin/bash 用touch /.autorelabel更新SELinux信息 两次exit 推出chroot reboot 重启：需要一定时间，耐心等待 vm扩容centos7 # 这里是因为我在vm手动扩容后，进入centos7系统\u0026mdash;用了 可视化界面中的disk软件直接扩容，发生错误（具体错误我没注意，一闪而过了），后面呢我再使用命令resize2fs /dev/sda3的时候，发现总是提示 busy\n解决办法 # 按照上面的办法，进入到第3步结束之后（按ctrl+x进入单用户模式界面 要做）\n输入 umount /dev/sda3 进行卸载\n然后输入下面进行修复（极为重要），然后出现问题是否修复一直按\u0026rsquo;y\u0026rsquo;即可\nxfs_repair /dev/sda4 注：如果你当前文件系统是ext4，可以执行fsck.ext4 /dev/sda4 然后输入 mount /dev/sda3 / 进行挂载（这步可能不需要）\n最后 reboot 重启 重启之后，再执行 resize2fs /dev/sda3 即可\n"},{"id":43,"href":"/zh/docs/problem/Hexo/01/","title":"hexo在线查看pdf","section":"Hexo","content":" 场景 # 由于在看《mysql是如何运行的》，做md文件笔记时，发现好多都是按pdf一字不漏打出来。所以想着能不能直接本地编辑pdf，然后博客上支持在线查看。\n事后觉得这个方式有待斟酌，电脑上/平板上查看没啥问题，手机上查看字有点小，但也还能接受。==\u0026gt;待斟酌\n不过下面的方案是可行的。\n准备 # 需要到官网下载 pdf.js\nhttps://github.com/mozilla/pdf.js/releases ，这里选择 v3.4.120中的pdfjs-3.4.120-dist.zip ，最新版本好像有问题\n操作 # pdfjs处理 # 在source/下创建myjs/pdfjs文件夹，并解压到这个文件夹下\n修改pdfjs/web/viewer.js\nif (fileOrigin !== viewerOrigin) {//1563行左右 throw new Error(\u0026#34;file origin does not match viewer\u0026#39;s\u0026#34;); } //注释掉，为了处理跨域问题，注释掉后允许在线访问其他网站的pdf // if (fileOrigin !== viewerOrigin) { //\tthrow new Error(\u0026#34;file origin does not match viewer\u0026#39;s\u0026#34;); //} hexo配置修改 # # 找到# Directory下的skip_render项，添加忽略渲染的文件夹 skip_render: [\u0026#39;myjs/pdfjs/**/*\u0026#39;] 清理hexo中public及其他缓存文件 # hexo clean \u0026amp; hexo g 文件预览测试 # 本地文件 # 我们在hexo的source文件夹下，放置这样一个文件： source/pdf/my.pdf\nMD文件修改 # \u0026lt;iframe src=\u0026#39;/myjs/pdfjs/web/viewer.html?file=/pdf/my.pdf\u0026#39; style=\u0026#34;padding: 0;width:100%;\u0026#34; marginwidth=\u0026#34;0\u0026#34; frameborder=\u0026#34;no\u0026#34; scrolling=\u0026#34;no\u0026#34; height=\u0026#34;2000px\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; 操作并查看 # hexo g \u0026amp; hexo s 远程文件 # ‘\n也就是在我的账号(lwmfjc)下，创建一个仓库（仓库名 pdfs），然后创建一个文件夹及文件 temp/01.pdf ，这个地址是 https://raw.githubusercontent.com/lwmfjc/pdfs/main/temp/01.pdf\n注意修改账号名及仓库名 ：lwmfjc/pdfs/\n文件夹及文件：temp/01.pdf\nMD文件修改 # \u0026lt;iframe src=\u0026#39;/myjs/pdfjs/web/viewer.html?file=https://raw.githubusercontent.com/lwmfjc/pdfs/main/mysql/01.pdf\u0026#39; style=\u0026#34;padding: 0;width:100%;\u0026#34; marginwidth=\u0026#34;0\u0026#34; frameborder=\u0026#34;no\u0026#34; scrolling=\u0026#34;no\u0026#34; height=\u0026#34;2000px\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; 操作并查看 # hexo g \u0026amp; hexo s "},{"id":44,"href":"/zh/docs/technology/MySQL/how_mysql_run/07/","title":"07B+数索引的使用","section":"_MySQL是怎样运行的_","content":" 学习《MySQL是怎样运行的》，感谢作者！\nInnoDB存储引擎的B+树索引：结论 # 每个索引对应一颗B+树。B+树有好多层，最下边一层是叶子节点，其余是内节点。所有用户记录都存在B+树的叶子节点，所有目录项记录都存在内节点 InnoDB 存储引擎会自动为主键建立聚簇索引(如果没有显式指定主键或者没有声明不允许存储NULL的UNIQUE 键，它会自动添加主键) ， 聚簇索引的叶子节点包含完整的用户记录 我们可以为感兴趣的列建立二级索引，二级索引的叶子节点包含的用户记录由索引列 和主键组成。如果想通过二级索引查找完整的用户记录，需要执行回表操作， 也就是在通过二级索引找到主键值之后，再到聚簇索引中查找完整的用户记录 B+ 树中的每层节点都按照索引列的值从小到大的顺序排序组成了双向链表，而且每个页内的记录(无论是用户记录还是目录项记录)都按照索引列的值从小到大的顺序形成了一个单向链表。如果是联合索引， 则页面和记录 先按照索引列中前面的列的值排序：如果该列的值相同，再按照索引列中后面的列的值排序。比如， 我们对列c2 和c3建立了联合索引 idx_c2_c3(c2， c3)，那么该索引中的页面和记录就先按照c2 列的值进行排序；如果c2 列的值相同， 再按照c3 列的值排序 通过索引查找记录时，是从B+ 树的根节点开始一层一层向下搜索的。由于每个页面(无论是内节点页面还是叶子节点页面〉中的记录都划分成了若干个组， 每个组中索引列值最大的记录在页内的偏移量会被当作槽依次存放在页目录中(当然， 规定Supremum 记录比任何用户记录都大) ，因此可以在页目录中通过二分法快速定位到索引列等于某个值的记录 如果大家在阅读上述结论时哪怕有点疑惑， 那么下面的内容就不适合你，请回过头去反复阅读前面的章节\nB+树索引示意图的简化 # #创建新表 mysql\u0026gt; CREATE TABLE single_table( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1(key1), UNIQUE KEY uk_key2(key2), KEY idx_key3(key3), KEY idx_key_part(key_part1,key_part2,key_part3) ) Engine=InnoDB CHARSET = utf8; 如上，建立了1个聚簇索引，4个二级索引\n为id列建立的聚簇索引 为key1列建立的idx_key1二级索引 为key2列建立的uk_key2二级索引，而且该索引是唯一二级索引 为key3列建立的idx_key3二级索引 为key_part1、key_part2、key_part3列建立的idx_key_part二级索引，是一个联合索引 接下来为这个表插入10,000行记录\n除了id，其余的列取随机值：该表后面会频繁用到\n需要用程序写，这里暂时跳过（不会\u0026hellip;，书上也没写）\n回顾：B+树包括内节点和叶子节点，以及各个节点中的记录。B+树其实是一个矮矮的大胖子，能够利用B+树快速地定位记录，下面简化一下B+树的示意图：\n忽略页结构，直接把所有叶子节点中的记录放一起 为了方便，把聚簇索引叶子节点的记录称为聚簇索引记录，把二级索引叶子节点称为二级索引记录 回顾一下：\n核心要点：把下一层每一页的最小值，放到上一级的目录项记录，以key值+页号这样的组合存在\n精简：\n如上，聚簇索引记录是按照主键值由小到大的顺序排列的 如下图，通过B+树定位到id值为1438的记录\n二级索引idx_key1对应的B+树中保留了叶子结点的记录。以key1排序，如果key1相同，则按照id列排序\n为了方便，把聚簇索引叶子节点的记录称为聚簇索引记录，把二级索引叶子节点称为二级索引记录\n如果要查找key1值等于某个值的二级索引记录，通过idx_key1对应的B+树，可以很容易定位到第一条key1列的值等于某个值的二级索引记录，然后沿着单向链表向后扫描即可。\n索引的代价 # 空间上的代价 # 每建立一个索引，都要为他建立一颗B+树。每一颗B+树的每一个节点都是一个数据页（一个数据页默认占用16KB），而一颗很大的B+树由许多数据页组成，这将占用很大的片存储空间\n时间上的代价 # 每当对表中数据进行增上改查时，都要修改各个B+树索引 执行查询语句前，都要生成一个执行计划。一般情况下，一条查询语句在执行过程中最多用到一个二级索引（有例外，10章），在生成执行计划时需要计算使用不同索引执行查询时所需要的成本，最后选取成本最低的那个索引执行查询（12章：如何计算查询成本）==\u0026gt; 索引太多导致分析时间过长 总结 # 索引越多，存储空间越多，增删改记录或者生成执行计划时性能越差\n为了建立又好又少的索引，得先了解索引在查询执行期间到底是如何发挥作用的\n应用B+树索引 # 对于某个查询来说，最简单粗暴的执行方案就是扫描表中的所有记录。判断每一条记录是否符合搜索条件。如果符合，就将其发送到客户端，否则就跳过该记录。这种执行方案也称为全表扫描。\n对于使用 InnoDB 存储引擎的表来说，全表扫描意味着从聚簇索引第一个叶子节点的第一条记录开始，沿着记录所在的单向链表向后扫描 直到最后一个叶子节点的最后一条记录(叶子节点：页，16KB；即页内最后一条)。虽然全表扫描是一种很笨的执行方案，但却是一种万能的执行方案，所有的查询都可以使用这种方案来执行。\n扫描区间和边界条件 # 可以利用B+树查找索引值等于某个值的记录=\u0026gt;减少需要扫描的记录数量。由于*B+树叶子节点中的记录是按照索引列值由小到大的顺序排序的，所以只扫描某个区间或者某些区间中的记录也可以明显减少需要扫描的记录数量。\n简单例子 # 例子1（聚簇索引） # 例子：SELECT * FROM single_table WHERE id\u0026gt;=2 AND id \u0026lt;=100\n这个语句其实是要找id值在**[2,100]区间中的所有聚簇索引**记录。\n可以通过聚簇索引对应的B+树快速地定位到id值为2的那条聚簇索引记录，然后沿着记录所在的单向链表向后扫描，直到某条聚簇索引记录的id值不在[2,100]区间中为止（即id不再符合id\u0026lt;=100条件） 与扫描全部的聚簇索引记录相比，扫描id 值在**[2,100]** 区间中的记录已经很大程度地减少了需要扫描的记录数量， 所以提升了查询效率。简便起见，我们把这个例子中待扫描记录的id 值所在的区间称为扫描区间，把形成这个扫描区间的搜索条件(也就是id \u0026gt;= 2AND \u0026gt; id \u0026lt;= 100 ) 称为形成这个扫描区间的边界条件. 对于全表扫描来说，相当于扫描id在**(-∞,+∞)** 区间中的记录，也就是说全表扫描对应的扫描区间是**(-∞,+∞)**\n例子2（二级索引） # SELECT * FROM single_table WHERE key2 IN (1438,6328 ) OR (key2 \u0026gt;=38 AND key2 \u0026lt;=79) 可以直接使用全表扫描的方式执行该查询。\n但是我们发现该查询的搜索条件涉及key2列，而我们又正好为key2列建立了uk_key2索引。如果使用uk_key2索引执行这个查询，则相当于从下面的3个扫描区间中获取二级索引记录：\n[1438,1438] ：对应的边界条件就是key2 IN (1438) [6328,6328]：对应的边界条件就是key2 IN (6328) [38,79]：对应的边界条件就是key2 \u0026gt;= 38 AND key2 \u0026lt;= 79 这些扫描区间对应到数轴上时，如图\n方便起见，我们把像[1438,1438]、[6328, 6328] 这样只包含一个值的扫描区间称为单点扫描区间， 把[38， 79] 这样包含多个值的扫描区间称为范围扫描区间。另外，由于我们的查询列表是 * ，也就是需要读取完整的用户记录，所以从上述扫描区间中每获取一条二级索引记录， 就需要根据该二级索引记录id列的值执行回表操作，也就是到聚簇索引中找到相应的聚簇索引记录。\n其实我们不仅仅可以使用uk_key2 执行上述查询， 还可以使用idx_key1、idx_key3 、idx_key_part 执行上述查询。以idx_key_1 为例，很显然无法通过搜索条件形成合适的扫描区间来减少需要扫描的idx_key1 二级索引记录的数量，只能扫描idx_keyl 的全部二级索引记录。针对获取到的每一条二级索引记录，都需要执行回表操作来获取完整的用户记录.。我们也可以说，使用idx_key1 执行查询时对应的扫描区间就是**(-∞,+∞)** 这样虽然行得通，但我们图啥呢，最简单粗暴的全表扫描方式已经需要扫描全部的聚簇索引记录， 这里除了需要访问全部的聚簇索引记录，还要扫描全部的idx_key1二级索 引记录，这不是费力不讨好么。可见， 在这个过程中并没有减少需要扫描的记录数量，效 率反而比全表扫描差。所以如果想使用某个索引来执行查询，但是又无法通过搜索条件 形成合适的扫描区间来减少需要扫描的记录数量时， 则不考虑使用这个索引执行查询 例3 不是索引的搜索条件都可以成为边界条件 # SELECT * FROM single_table WHERE key1 \u0026lt; \u0026#39;a\u0026#39; AND key3 \u0026gt; \u0026#39;z\u0026#39; AND common_field = \u0026#39;abc\u0026#39; 如果使用idx_key1 执行查询，那么相应的扫描区间就是(-∞,\u0026lsquo;a\u0026rsquo;)，形成该扫描区间的边界条件就是key1 \u0026lt; \u0026lsquo;a\u0026rsquo;。而 key3 \u0026gt; \u0026lsquo;z\u0026rsquo; AND common_field = \u0026lsquo;abc\u0026rsquo;就是普通的搜索条件，这些普通的搜索条件需要在获取到idx_key1的二级索引记录后，再执行回表操作，在获取到完整的用户记录后才能去判断它们是否成立 而如果使用idx_key3 执行查询，那么相应的扫描区间就是\u0026rsquo;z\u0026rsquo;，形成该扫描区间的边界条件就是key3\u0026gt;\u0026lsquo;z\u0026rsquo;。而key1\u0026lt;\u0026lsquo;a\u0026rsquo; AND common_field=\u0026lsquo;abc\u0026rsquo;就是普通的搜索条件，这些普通的搜索条件需要在获取到idx_key3的二级索引记录后，再执行回表操作，在获取到完整的用户记录后才能去判断它们是否成立 总结 # 在使用某个索引执行查询时，关键的问题就是通过搜索条件找出合适的扫描区间，然后再到对应的B+ 树中扫描索引列值在这些扫描区间的记录。对于每个扫描区间来说，仅需要通过B+ 树定位到该扫描区间中的第一条记录，然后就可以沿着记录所在的单向链表向后扫描，直到某条记录不符合形成该扫描区间的边界条件为止。其实对于B+ 树索引来说，只要索引列和常数使用**=、\u0026lt;=\u0026gt;、lN、NOT IN、IS NULL、IS NOT NULL、\u0026gt; 、\u0026lt;、=、\u0026lt;=、BETWEEN 、! = (也可以写成\u0026lt; \u0026gt;)或者LIKE 操作符连接起来，就可以产生所谓的扫描区间**。不过有下面几点需要注意：\nlN操作符的语义与若干个等值匹配操作符( =)之间用OR 连接起来的语义是一样的，都会产生多个单点扫描区间。比如下面这两个语句的语义效果是一样的：\nSELECT * FROM single_table WHERE key2 IN (1438,6328); #与上面的语义效果一样 SELECT * FROM single_table WHERE key2 = 1438 OR key2 = 6328 != 产生的扫描区间比较有趣，如：\nSELECT * FROM single_table key1 != \u0026#39;a\u0026#39;; 此时idx_key1执行查询时对应的扫描区间就是(-∞,\u0026lsquo;a\u0026rsquo;) 和(\u0026lsquo;a\u0026rsquo;,+∞)\nLIKE操作符比较特殊，只有在匹配完整的字符串或者匹配字符串前缀时才产生合适的扫描区间\n比较字符串的大小，其实就相当于一次比较每个字符的大小。字符串的比较过程如下所示：\n先比较字符串的第一个字符：第一个字符小的那个字符串就比较小 如果两个字符串的第一个字符相同，再比较第二个字符；第二个字符比较小的那个字符串就比较小 如果两个字符串的前两个字符都相同，那么就接着比较第三个字符：依此类推 对于某个索引列来说，字符串前缀相同的记录在由记录组成的单向链表中肯定是相邻的。\n比如我们有一个搜索条件是key1 LIKE \u0026lsquo;a%\u0026rsquo;。 对于二级索引 idx_key1 来说，所有字符串前缀为\u0026rsquo;a\u0026rsquo;的二级索引记录肯定是相邻的。这也就意味着我们只要定位 key1 值的字符串前缀为\u0026rsquo;a\u0026rsquo; 的第一条记录，就可以沿着记录所在的单向链表向后扫描， 直到某条二级索引记录的字符串前缀不为\u0026rsquo;a\u0026rsquo; 为止，如图7-7 所示。很显然 key1 LIKE \u0026lsquo;a%\u0026rsquo; 形成的扫描区间相当于**[\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;)** 稍复杂例子 # 日常工作中，一个查询语句中的WHERE子句可能有多个小的搜索条件，这些搜索条件使用AND 或者OR 操作符连接起来。虽然大家都知道这两个操作符的作用，但这里还是要再强调一遍：\ncond1 AND cond2 只有当cond1和cond2都为TRUE 时，整个表达式才为TRUE cond1 OR cond2 ， 只要cond1 或者cond2 中有一个为TRUE， 整个表达式就为TRUE 在我们执行一个查询语句时，首先需要找出所有可用的索引以及使用它们时对应的扫描区间。下面我们来看一下怎么从包含若干个AND 或OR 的复杂搜索条件中提取出正确的扫描区间：\n所有搜索条件都可以生成合适的扫描区间的情况 # AND结合 # SELECT * FROM single_table WHERE key2 \u0026gt; 100 AND key2 \u0026gt; 200; 其中，每个小的搜索条件都可以生成一个合适的扫描区间来减少需要扫描的记录数量，最终的扫描区间就是对这两个小的搜索条件形成的扫描区间取交集后的结果，取交集的过程：\n上面查询语句使用uk_key2索引执行查询时对应的扫描区间就是**(200,+∞)，形成该扫描区间的边界条件就是key2 \u0026gt; 200**\nOR结合 # 使用OR操作符将多个搜索条件连接在一起：\nSELECT * FROM single_table WHERE key2 \u0026gt; 100 OR key2 \u0026gt; 200 OR意味着需要取各个扫描区间的并集，取并集的过程如图所示：\n即，上面的查询语句在使用uk_key2索引执行查询时，对应的扫描区间就是**(100,+∞)，形成扫描区间的边界条件就是key2 \u0026gt; 100**\n有的搜索条件不能生成合适的扫描区间的情况 # AND情况 # 有的搜索条件不能生成合适的扫描区间来减少需要扫描的记录数量\nSELECT * FROM single_table WHERE key2 \u0026gt; 100 AND common_field = \u0026#39;abc\u0026#39; 分析：使用uk_key2执行查询时，搜索条件key2\u0026gt;100可以形成扫描区间(100,+∞)。但是由于uk_key2的二级索引并不按照common_field列进行排序（uk_key2二级索引记录中压根儿不包含common_field列），所以仅凭搜索条件common_field = \u0026lsquo;abc\u0026rsquo;并不能减少需要扫描的二级索引记录数量。即该搜索条件生成的扫描区间其实是**(-∞,+∞)。由于这两个小的搜索条件是使用AND操作符连接，所以对(100,+∞)** 和 (-∞,+∞)这两个搜索区间取交集后得到的结果自然是**（100，+∞）。即使用uk_key2执行上述查询，最终对应的扫描区间就是（100，+∞），形成该扫描区间的条件就是key2\u0026gt;100\n简化：使用uk_key2执行查询时，在寻找对应的扫描区间**的过程中，搜索条件 common_field = \u0026lsquo;abc\u0026rsquo;没起到任何作用，我们可以直接把 common_field = \u0026lsquo;abc\u0026rsquo; 搜索条件替换为TRUE，（TRUE对应的扫描区间也是（-∞，+∞））,如下：\nSELECT * FROM single_table WHERE key2 \u0026gt; 100 AND TRUE # 简化之后 SELECT * FROM single_table WHERE key2 \u0026gt; 100 即上面的查询语句使用uk_key2执行查询时对应的扫描区间是**（100，+∞）**\nOR情况 # SELECT * FROM single_table WHERE key2 \u0026gt; 100 OR common_field = \u0026#39;abc\u0026#39; #替换之后 SELECT * FROM single_table WHERE key2 \u0026gt; 100 OR TRUE 所以，如果强制使用uk_key2执行查询，由于这两个小的搜索条件是使用OR操作符连接，所以对**(100,+∞)** 和 (-∞,+∞)这两个搜索区间取并集后得到的结果自然是**（-∞，+∞）。也就是需要扫描uk_key2的全部二级索引记录**，并且对于获取到的每一条二级索引记录，都需要执行回表操作。这个代价比执行全表扫描的代价都大。这种情况下，不考虑使用uk_key2来执行查询\n从复杂的搜索条件中找出扫描区间 # SELECT * FROM single_table WHERE (key1 \u0026gt; \u0026#39;xyz\u0026#39; AND key2 =748) OR (key1\u0026lt;\u0026#39;abc\u0026#39; AND key1 \u0026gt; \u0026#39;lmn\u0026#39;) OR (key1 LIKE \u0026#39;%suf\u0026#39; AND key1 \u0026gt; \u0026#39;zzz\u0026#39; AND (key2 \u0026lt; 8000 OR common_field = \u0026#39;abc\u0026#39;)) 分析：\n涉及到的列，以及为哪些列建立了索引\n设计key1，key2，common_field这三个列，其中key1列有普通二级索引idx_key1，key2列有唯一二级索引uk_key2 对于可能用到的索引，分析它们的扫描区间 假设使用idx_key1执行查询 # 把不能形成合适扫描区间的搜索条件暂时移除掉：直接替换为TRUE\n除了有关key2和common_field列的搜索条件不能形成合适的扫描区间，还有key1 LIKE \u0026lsquo;%suf\u0026rsquo;形成的扫描区间是（-∞，+∞）,所以也需要替换成TRUE，这些不能形成合适扫描区间的搜索条件替换成TRUE之后，搜索条件如下所示：\nSELECT * FROM single_table WHERE (key1 \u0026gt; \u0026#39;xyz\u0026#39; AND TRUE) OR (key1\u0026lt;\u0026#39;abc\u0026#39; AND key1 \u0026gt; \u0026#39;lmn\u0026#39;) OR (TRUE AND key1 \u0026gt; \u0026#39;zzz\u0026#39; AND (TRUE OR TRUE)) #简化 SELECT * FROM single_table WHERE (key1 \u0026gt; \u0026#39;xyz\u0026#39; ) OR (key1\u0026lt;\u0026#39;abc\u0026#39; AND key1 \u0026gt; \u0026#39;lmn\u0026#39;) OR (key1 \u0026gt; \u0026#39;zzz\u0026#39; AND (TRUE OR TRUE) ) #进一步简化 SELECT * FROM single_table WHERE (key1 \u0026gt; \u0026#39;xyz\u0026#39; ) OR (key1\u0026lt;\u0026#39;abc\u0026#39; AND key1 \u0026gt; \u0026#39;lmn\u0026#39;) OR (key1 \u0026gt; \u0026#39;zzz\u0026#39;) #由于key1\u0026lt;\u0026#39;abc\u0026#39; AND key1 \u0026gt;\u0026#39;lmn\u0026#39; 永远为FALSE，所以进一步简化 SELECT * FROM single_table WHERE (key1 \u0026gt; \u0026#39;xyz\u0026#39; ) OR (key1 \u0026gt; \u0026#39;zzz\u0026#39;) #继续简化(取范围大的，即并集) SELECT * FROM single_table WHERE key1 \u0026gt; \u0026#39;xyz\u0026#39; 即如果使用idx_key1索引执行查询，则对应扫描区间为(\u0026lsquo;xyz\u0026rsquo;,+∞)。\n也就是需要把所有满足key1\u0026gt;\u0026lsquo;xyz\u0026rsquo;条件的所有二级索引记录都取出来，针对获取到的每一条二级索引记录，都要用它的主键值再执行回表操作，在得到完整的用户记录之后再使用其他的搜索条件进行过滤\n使用idx_key1执行上述查询时，搜索条件key1 LIKE %suf比较特殊，虽然不能作为形成扫描区间的边界条件，但是idx_key1的二级索引记录是包括key1列的，因此可以*先判断获取到的二级索引记录是否符合这个条件，如果符合再执行回表操作，如果不符合则不执行回表操作。这可减少因回表操作带来的性能损耗，这种优化方式称为索引条件下推\n假设使用idx_key2执行查询 # 对于：\nSELECT * FROM single_table WHERE (key1 \u0026gt; \u0026#39;xyz\u0026#39; AND key2 =748) OR (key1\u0026lt;\u0026#39;abc\u0026#39; AND key1 \u0026gt; \u0026#39;lmn\u0026#39;) OR (key1 LIKE \u0026#39;%suf\u0026#39; AND key1 \u0026gt; \u0026#39;zzz\u0026#39; AND (key2 \u0026lt; 8000 OR common_field = \u0026#39;abc\u0026#39;)) 简化\nSELECT * FROM single_table WHERE (TRUE AND key2 =748) OR (TRUE AND TRUE) OR (TRUE AND TRUE AND (key2 \u0026lt; 8000 OR TRUE)) #简化 key2 = 748 OR TRUE #进一步简化 TRUE 意味着如果要使用uk_key2索引执行查询，则对应的扫描区间就是**（-∞，+∞），即需要扫描uk_key2的全部二级索引记录，针对每一条二级索引记录还需要回表**，所以这种情况下不会使用uk_key2索引\n使用联合索引执行查询时对应的扫描区间 # 联合索引的索引包含多个列，B+树中的每一层页面以及每个页面中采用的排序规则较为复杂，以single_table表的idx_key_part联合索引为例，采用的排序规则如下所示：\n先按照key_part1列的值进行排序 在key_part1列的值相同的情况下，再按照key_part2列的值进行排序 在key_part1列和key_part2列值都相同的情况下，再按照key_part3列的值进行排序，画一下idx_key_part索引的示意图，如图所示：\n对于查询语句Q1(单条件) # SELECT * FROM single_table WHERE key_part1 = \u0026#39;a\u0026#39;; 由于二级索引记录是先按照key_part1列排序的，所以符合key_part1=\u0026lsquo;a\u0026rsquo;条件的所有记录肯定是相邻的。我们可以定位到符合key_part1=\u0026lsquo;a\u0026rsquo;条件的第一条记录，然后沿着记录所在的单向链表向后扫描(如果本页面中的记录扫描完了，就根据叶子节点的双向链表找到下一个页面中的第一条记录，继续沿着记录所在的单向链表向后扫描。我们之后就不强调叶子节点的双向链表了），直到某条记录不符合key_part=\u0026lsquo;a\u0026rsquo;条件为止（当然，对于获取到的每一条二级索引记录都要执行回表操作）。过程如图所示\n也就是说，如果使用idx_key_part索引执行查询语句Q1，对应的扫描区间是**[\u0026lsquo;a\u0026rsquo;,\u0026lsquo;a\u0026rsquo;]，形成这个扫描区间的边界条件**就是key_part=\u0026lsquo;a\u0026rsquo;\n对于查询条件Q2(顺序2条件) # SELECT * FROM single_table WHERE key_part1=\u0026#39;a\u0026#39; AND key_part2=\u0026#39;b\u0026#39;; 由于二级索引记录是先按照key_part1列的值排序的， 在key_part1列的值相等的情况下再按照key_part2列进行排序，所以符合key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2=\u0026lsquo;b\u0026rsquo;条件的二级索引记录肯定是相邻的。 我们可以定位到符合key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2=\u0026lsquo;b\u0026rsquo;条件的第一条记录，然后沿着记录所在的单向链表向后扫描，直到某条记录不符合key_part1=\u0026lsquo;a\u0026rsquo;条件或者key_part2=\u0026lsquo;b\u0026rsquo;条件为止(当然，对于获取到的每一条二级索引记录都要执行回表操作，这里就不展示了) ，如图7-12 所示。也就是说，如果使用idx_key_part索引执行查询语句Q2 ，可以形成扫描区间**[(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;),(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;)]**，形成这个扫描区间的边界条件就是 key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2=\u0026lsquo;b\u0026rsquo;\n[(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;),(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;)] 代表在idx_key_part索引中，从第一条符合key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2=\u0026lsquo;b\u0026rsquo; 条件的记录开始，到最后一条符合key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2=\u0026lsquo;b\u0026rsquo;条件的记录为止的所有二级索引记录。\n对于查询条件Q3(顺序3条件) # SELECT * FROM single_table WHERE key_part1=\u0026#39;a\u0026#39; AND key_part2=\u0026#39;b\u0026#39; AND key_part3=\u0026#39;c\u0026#39;; 由于二级索引记录是先按照 key_part1列的值排序的，在key_part1列的值相等的情况下再按照key_part2列进行排序：在key_part1和key_part2列的值都相等的情况下， 再按照key_part3列进行排序，所以符合key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2=\u0026lsquo;b\u0026rsquo; AND key_part3=\u0026lsquo;c\u0026rsquo;条件的二级索引记录肯定是相邻的。\n我们可以定位到符合key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2 = \u0026lsquo;b\u0026rsquo; AND key_part3=\u0026lsquo;c\u0026rsquo;条件的第一条记录，然后沿着记录所在的单向链表向后扫描，直到某条记录不符合key_part1=\u0026lsquo;a\u0026rsquo;条件或者key_part2=\u0026lsquo;b\u0026rsquo;条件或者key_part3条件为止(当然，对于获取到的每一条二级索引记录都要执行回表操作)。这里就不再画示意图了。\n如果使用idx_key_part索引执行查询语句Q3 ，可以形成扫描区间**[(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;,\u0026lsquo;c\u0026rsquo;)，(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;,\u0026lsquo;c\u0026rsquo;)]** ,形成这个扫描区间的边界条件就是key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2 = \u0026lsquo;b\u0026rsquo; AND key_part3=\u0026lsquo;c\u0026rsquo;\n对于查询语句Q4(单条件范围) # SELECT * FROM single_table WHERE key_part1 \u0026lt; \u0026#39;a\u0026#39;; 由于二级索引记录是先按照key_part1列的值进行排序的，所以符合key_part1\u0026lt;\u0026lsquo;a\u0026rsquo;条件的所有记录肯定是相邻的。我们可以定位到符合key_part1\u0026lt;\u0026lsquo;a\u0026rsquo;条件的第一条记录(其实就是idx_key_part 索引第一个叶子节点的第一条记录) ，然后沿着记录所在的单向链表向后扫描，直到某条记录不符合key_part1\u0026lt; \u0026lsquo;a\u0026rsquo; 条件为止(当然，对于获取到的每一条二级索引记录都要执行回表操作，这里就不展示了) ，如图7- 13 所示\n也就是说，如果使用idx_key_part索引执行查询语句Q4，可以形成扫描区间（-∞，\u0026lsquo;a\u0026rsquo;)，形成这个扫描区间的边界条件就是key_part1\u0026lt;\u0026lsquo;a\u0026rsquo;\n查询语句Q5（条件1等值，条件2范围） # SELECT * FROM single_table WHERE key_part1=\u0026#39;a\u0026#39; AND key_part2 \u0026gt; \u0026#39;a\u0026#39; AND key_part2 \u0026lt; \u0026#39;d\u0026#39;; 由于二级索引记录是先按照key_part1列的值进行排序的，在key_part1列的值相等的情况下再按照key_part2列进行排序。也就是说，在符合key_part1=\u0026lsquo;a\u0026rsquo;条件的二级索引记录中，这些记录是按照key_part2 列的值排序的， 那么此时符合key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2\u0026gt;\u0026lsquo;a\u0026rsquo; AND key_part2 \u0026lt; \u0026rsquo;d\u0026rsquo;条件的二级索引记录肯定是相邻的。我们可以定位到符合 key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2\u0026gt;\u0026lsquo;a\u0026rsquo; AND key_part2 \u0026lt; \u0026rsquo;d\u0026rsquo;条件的第-条记录(其实第一条就是key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2 = \u0026lsquo;a\u0026rsquo; )， 然后沿着记录所在的单向链表向后扫描， 直到某条记录不符合key_part1=\u0026lsquo;a\u0026rsquo; 或者key_part2\u0026gt;\u0026lsquo;a\u0026rsquo; 或者**key_part2 \u0026lt; \u0026rsquo;d\u0026rsquo;**条件为止(当然，对于获取到的每一条二级索引记录都要执行回表操作， 这里就不展示了) ，如图7- 1 4 所示\n也就是说，如果使用idx_key_part索引执行查询语句Q5，可以形成扫描区间**((\u0026lsquo;a\u0026rsquo;,\u0026lsquo;a\u0026rsquo;),(\u0026lsquo;a\u0026rsquo;,\u0026rsquo;d\u0026rsquo;))，形成这个扫描区间的边界条件就是key_part1=\u0026lsquo;a\u0026rsquo; AND key_part2\u0026gt;\u0026lsquo;a\u0026rsquo; AND key_part2\u0026lt;\u0026rsquo;d\u0026rsquo;**。\n查询语句Q6（条件2等值=\u0026gt;用不上索引） # 由于二级索引记录不是直接按照key_part2列的值排序的，所以符合key_part2列的二级索引记录可能并不相邻，也就意味着我们不能通过这个key_part2=\u0026lsquo;a\u0026rsquo; 搜索条件来减少需要扫描的记录数量。在这种情况下，我们是不会使用idx_key_part 索引执行查询的\n查询语句Q7（条件1等值，条件3等值=\u0026gt;只有前面的条件是边界条件） # SELECT * FROM single_table WHERE key_part=\u0026#39;a\u0026#39; AND key_part3=\u0026#39;c\u0026#39; 由于二级索引记录是先按照key_part1列的值排序的，所以符合key_part1=\u0026lsquo;a\u0026rsquo;条件的二级索引记录肯定是相邻的。但是对于符合key_part3 =\u0026lsquo;c\u0026rsquo;条件的二级索引记录来说，并不是直接按照key_part3列进行排序的，也就是说我们不能根据搜索条件key_part3=\u0026lsquo;c\u0026rsquo;来进一步减少需要扫描的记录数量。那么，如果使用idx_key_part 索引执行查询，可以定位到符合key_part1 = \u0026lsquo;a\u0026rsquo;条件的第一条记录，然后沿着记录所在的单向链表向后扫描，直到某条记录不符合key_part1 = \u0026lsquo;a\u0026rsquo;条件为止。所以在使用idx_key_part索引执行查询语句Q7 的过程中，对应的扫描区间其实是**[\u0026lsquo;a\u0026rsquo;,\u0026lsquo;a\u0026rsquo;]，形成该扫描区间的边界条件是 key_part1=\u0026lsquo;a\u0026rsquo;，与key_part3=\u0026lsquo;c\u0026rsquo;**无关。\n索引条件下推特性，MySQL5.6中引入，默认开启\n针对获取到的每一条二级索引记录，如果没有开启索引条件下推特性，则必须先执行回表操作，在获取到完整的用户记录后再判断key_part3=\u0026lsquo;c\u0026rsquo;条件是否成立。如呆开启了索引条件下推特性，可以立即判断该二级索引记录是否符合key_part3=\u0026lsquo;c\u0026rsquo;条件。如果符合该条件，则再执行回表操作；如果不符合则不执行回农操作，直接跳到下一条二级索引记录。\n查询语句Q8（条件1范围，条件2等值=\u0026gt;只有前面的条件是边界条件） # SELECT * FROM single_table WHERE key_part \u0026lt; \u0026#39;b\u0026#39; AND key_part2=\u0026#39;a\u0026#39; 由于二级索引记录是先按照key_part1列的值排序的，所以符合key_part1\u0026lt;\u0026lsquo;b\u0026rsquo;条件的二级索引记录肯定是相邻的。但是对于符合key_part2 =\u0026lsquo;a\u0026rsquo;条件的二级索引记录来说，并不是直接按照key_part2列进行排序的，也就是说我们不能根据搜索条件key_part2=\u0026lsquo;a\u0026rsquo;来进一步减少需要扫描的记录数量。那么，如果使用idx_key_part 索引执行查询，可以定位到符合key_part1 \u0026lt; \u0026lsquo;b\u0026rsquo;\u0026lsquo;条件的第一条记录（其实就是idx_key_part索引的第一个叶子节点的第一条记录），然后沿着记录所在的单向链表向后扫描，直到某条记录不符合key_part1\u0026lt;\u0026lsquo;b\u0026rsquo;条件为止。如图：\n所以在使用idx_key_part索引执行查询语句Q8 的过程中，对应的扫锚区间其实是[- ∞，\u0026lsquo;b\u0026rsquo;]，形成该扫描区间的边界条件是key_part1 \u0026lt; \u0026lsquo;b\u0026rsquo; ， 与 key_part2=\u0026lsquo;a\u0026rsquo;无关\n查询语句Q9（条件1范围(包括等号)，条件2等值） # SELECT * FROM single_table WHERE key_part1 \u0026lt;= \u0026#39;b\u0026#39; AND key_part2=\u0026#39;a\u0026#39; Q8和Q9很像，但是在涉及key_part1条件时，Q8中的条件是key_part1\u0026lt;\u0026lsquo;b\u0026rsquo;，Q9中的条件是key_part1\u0026lt;=\u0026lsquo;b\u0026rsquo;。很显然，符合key_part1=\u0026lsquo;b\u0026rsquo;条件的二级索引记录是相邻的。但是对于符合key_part1\u0026lt;=\u0026lsquo;b\u0026rsquo;条件的二级索引记录来说，并不是直接按照key_part2列排序的。但是，对于符合key_part1=\u0026lsquo;b\u0026rsquo;的二级索引记录来说，是按照key_part2列的值排序的。那么在确定需要扫描的二级索引记录的范围时，当二级索引记录的key_part1列值为\u0026rsquo;b\u0026rsquo; 时，也可以通过key_part2=\u0026lsquo;b\u0026rsquo; 条件减少需要扫描的二级索引记录范围。也就是说， 当扫描到不符合key_part1=\u0026lsquo;b\u0026rsquo; AND key_part2=\u0026lsquo;a\u0026rsquo; 条件的第一条记录时，就可以结束扫描，而不需要将所有key_part1列值为\u0026rsquo;b\u0026rsquo;的记录扫描完。\n注意，当扫描到记录的列key_part1值为b时，不能直接定位到**key_part2=\u0026lsquo;a\u0026rsquo;的数据了，但是可以扫描到key_part2=\u0026lsquo;a\u0026rsquo;**停止\n也就是说，如果使用idx_key_part索引执行查询语句Q9，可以形成扫描区间（(-∞，-∞),(\u0026lsquo;b\u0026rsquo;,\u0026lsquo;a\u0026rsquo;)），形成这个扫描区间的边界条件就是key_part1\u0026lt;=\u0026lsquo;b\u0026rsquo; AND key_part2=\u0026lsquo;a\u0026rsquo;。而在执行查询语句Q8时，我们必须将所有符合**key_part1\u0026lt;\u0026lsquo;b\u0026rsquo;**的记录都扫描完，**key_part2=\u0026lsquo;a\u0026rsquo;**条件在查询语句Q8中并不能起到减少需要扫描的二级索引范围的作用\n注意，对于Q9，key_part1\u0026lt;\u0026lsquo;b\u0026rsquo;的记录也是要扫描完的。这里仅仅对key_part1=\u0026lsquo;b\u0026rsquo;起了减少扫描二级索引范围的作用。\n索引用于排序 # 我们在编写查询语句时，经常需要使用ORDERBY子句对查询出来的记录按照某种规则进行排序。在一般情况下，我们只能把记录加载到内存中，然后再用一些排序算法在内存中对这些记录进行排序。有时查询的结果集可能太大以至于无法在内存中进行排序，此时就需要暂时借助磁盘的空间来存放中间结果，在排序操作完成后再把排好序的结果集返回客户端。在MySQL 中，这种在内存或者磁盘中进行排序的方式统称为文件排序(fìlesort)。但是，如果ORDERBY子句中使用了索引列，就有可能省去在内存或磁盘中排序的步骤。\n举例：\nSELECT * FROM single_table ORDER BY key_part1,key_part2,key_part3 LIMIT 10; 这个查询语句的结果集需要先按照key_part1 值排序。如果记录的key_part1 值相同，再按照key_part2值排序，如果记录的key_part1 和key_part2值都相同，再按照key_part3 值排序。大家可以回过头去看看图7-10。\n该二级索引的记录本身就是按照上述规则排好序的，所以我们可 以从第一条idx_key_part二级索引记录开始，沿着记录所在的单向链表向后扫描，取10 条二级索引记录即可。当然，针对获取到的每一条二级索引记录都执行一次回表操作，在获取到完整的用户记录之后发送给客户端就好了。这样是不是就变得简单多了，还省去了我们给10000条记录排序的时间\u0026ndash;索引就是这么厉害!\n关于回表操作： 请注意，本例的查询语句中加了LIMIT 子句，这是因为如果不限制需要获取的记录数量，会导致为大量二级索引记录执行回表操作，这样会影响整体的查询性能。关于回表操作造成的影响，我们稍后再详细唠叨\n使用联合索引进行排序时的注意事项 # ORDER BY子句后面的列顺序也必须按照索引列的顺序给出\n如果给出ORDER BY key_part3,key_part2,key_part1的顺序，则无法使用B+树索引。\n如果是ORDER BY key_part1 DESC,key_part2 DESC,key_part3 DESC ，那么应该是可以的，也就是ORDER BY key_part1,key_part2,key_part3的全反序\n之所以颠倒的排序列顺序不能使用索引，原因还是联合索引中页面和记录的排序规则是固定的，也就是先按照key_part1值排序，如果key_part1值相同，再按照key_part2值排序；如果key_part1和key_part2值都相同，再按照key_part2值排序。\n如果ORDER BY子句的内容是ORDER BY key_part3 , key_part2 , key_part,那就要求先要key_part3值排序（升序），如果key_part3相同，再按key_part2升序，如果key_part3和key_part3都相同，再按照key_part1升序\n同理，这些仅对联合索引的索引列中左边连续的列进行排序的形式（如ORDER BY key_part1和ORDER BY key_part1,key_part2），也是可以利用B+树索引的。另外，当连续索引的索引列左边连续的列为常量时，也可以使用联合索引对右边的列进行排序\nSELECT * FROM single_table WHERE key_part1=\u0026#39;a\u0026#39; AND key_part2=\u0026#39;b\u0026#39; ORDER BY key_part3 LIMIT 10 能使用联合索引排序，原因是key_part1值为\u0026rsquo;a\u0026rsquo;、key_part2值为\u0026rsquo;b\u0026rsquo;的二级索引记录本身就是按照key_part3列的值进行排序的\n不能使用索引进行排序的几种情况 # ASC、DESC混用 # 我们要求各个排序列的排序顺序规则是一致的，要么各个列都是按照ASC(升序)，要么都是按照DESC（降序）规则排序\n为什么呢：\nidx_key_part联合索引中的二级索引记录的排序规则：先key_part1升序，key_part1相同则key_part2升序，如果都相同则key_part3升序\n如果ORDER BY key_part1,key_part2 LIMIT10,那么直接从联合索引最左边的二级索引记录开始，向右读取10条即可\n如果ORDER BY key_part1 DESC,key_part2 DESC LIMIT 10，可以从联合索引最右边的那条二级索引记录开始，向左读10条\n注意，这里没有key_part3，也可以的。可以理解成，key_part3不要求排序。而按照key_part1 DESC,key_part2DESC顺序的记录一定是连续的\n如果是先key_part1列升序，再key_part2列降序，如：\nSELECT * FROM single_table ORDER BY key_part1,key_part2 DESC LIMIT 10; 此时联合索引的查询过程如下，算法较为复杂，不能高效地使用索引，所以这种情况下是不会使用联合索引执行排序操作的\nMySQL8.0引入了称为Descending Index的特性，支持ORDER BY 子句中ASC、DESC混用的情况\n排序列包含非同一个索引的列，这种情况也不能使用索引进行排序 # SELECT * FROM single_table ORDER BY key1,key2 LIMIT 10 对于idx_key1的二级索引来说，只按照key1列排序。且key1值相同的情况下是不按照key2列的值进行排序的，所以不能使用idx_key1索引执行上述查询\n排序列是某个联合索引的索引列，但是这些排序列再联合索引中并不连续 # SELECT * FROM single_table ORDER BY key_part1,key_part3 LIMIT 10; key_part1值相同的记录并不按照key_part3排序，所以不能使用idx_key_part执行上述查询\n用来形成扫描区间的索引列与排序列不同 # SELECT * FROM single_table WHERE key1=\u0026#39;a\u0026#39; ORDER BY key2 LIMIT 10; 如果使用key1=\u0026lsquo;1\u0026rsquo;作为边界条件来形成扫描区间，也就是再使用idx_key1执行该查询，仅需要扫描key1值为\u0026rsquo;a\u0026rsquo;的二级索引记录。此时无法使用uk_key2执行上述查询\n5：排序列不是以单独列名的形式出现在ORDER BY 子句中\n要想使用索引排序，必须保证索引列是以单独列名的形式（而不是修饰过）：\nSELECT * FROM single_table ORDER BY UPPER(key1) LIMIT 10; 因为key1列以UPPER(key1)函数调用的形式出现在ORDER BY子句，所以不能使用idx_key1执行上述查询\n索引用于分组 # 为了方便统计，会把表中记录按照某些列进行分组，如：\nSELECT key_part1,key_part2,key_part3,COUNT(*) FROM single_table GROUP BY key_part1,key_part2,key_part3; 对这些小分组进行统计，上面的查询，即统计每个小小分组包含的记录条数。\n如果没有idx_key_part索引，就得建立一个用于统计的临时表，在扫描聚簇索引的记录时将统计的中间结果填入这个临时表。当扫描完记录后， 再把临时表中的结果作为结果集发送给客户端。 如果有了索引idx_key_part ，恰巧这个分组顺序又与idx_key_part 的索引列的顺序是一致的，而idx_key_part 的二级索引记录又是按照索引列的值排好序的，这就正好了。所以可以直接使用idx_key_part 索引进行分组，而不用再建立临时表了 与使用B+ 树索引进行排序差不多， 分组列的顺序也需要与索引列的顺序一致，也可以只使用索引列中左边连续的列迸行分组\n如上，就是统计 (\u0026lsquo;0\u0026rsquo;,\u0026lsquo;0\u0026rsquo;,\u0026lsquo;0\u0026rsquo;)的有几条， (\u0026lsquo;0\u0026rsquo;,\u0026lsquo;a\u0026rsquo;,\u0026lsquo;a\u0026rsquo;)的有几条， (\u0026lsquo;0\u0026rsquo;,\u0026lsquo;a\u0026rsquo;,\u0026lsquo;b\u0026rsquo;)的有几条等\n回表的代价 # SELECT * FROM single_table WHERE key1 \u0026gt; 'a' AND key1 \u0026lt; 'c'\n有两种方式来执行上面语句\n以全表扫描的方式 # 直接扫描全部的聚簇索引记录， 针对每一条聚簇索引记录，都判断搜索条件是否成立， 如果成立则发送到客户端， 否则跳过该记录.\n使用idx_key1执行该查询 # 可以根据搜索条件key1 \u0026gt; \u0026lsquo;a\u0026rsquo; AND key1 \u0026lt; \u0026lsquo;c\u0026rsquo; 得到对应的扫描区间( \u0026lsquo;a\u0026rsquo;，\u0026lsquo;c\u0026rsquo; )，然后扫描该扫描区间中的二级索引记录。由于idx_key1索引的叶子节点存储的是不完整的用户记录，仅包含key1 、id 这两个列，而查询列表是*， 这意味着我们需要获取每条二级索引记录对应的聚簇索引记录， 也就是执行回表操作，在获取到完整的用户记录后再发送到客户端。\n分析 # 对于使用InnoDB 存储引擎的表来说， 索引中的数据页都必须存放在磁盘中， 等到需要时再加载到内存中使用。这些数据页会被存放到磁盘中的一个或者多个文件中， 页面的页号对应着该页在磁盘文件中的偏移量。以16KB大小的页面为例，页号为0 的页面对应着这些文件中偏移量为0 的位置，页号为1的页面对应着这些文件中偏移量为16KB 的位置。前面章节讲过， B+ 树的每层节点会使用双向链表连接起来， 上一个节点和下一个节点的页号可以不必相邻。\n不过在实际实现中， 设计Inno DB 的大叔还是尽量让同一个索引的叶子节点的页号按照顺序排列，这一点会在稍后讨论表空间时再详细嘴叨\n也就是说，idx_key1在扫描区间( \u0026lsquo;a\u0026rsquo;, \u0026lsquo;c\u0026rsquo; )中的二级索引记录所在的页面的页号会尽可能相邻\n即使这些页面的页号不相邻， 但起码一个页可以存放很多记录，也就是说在执行完一次页面I/O 后，就可以把很多二级索引记录从磁盘加载到内存中。 总而言之，就是读取在扫描区间( \u0026lsquo;a\u0026rsquo;， \u0026lsquo;c\u0026rsquo; ) 中 的二级索引记录时，所付出的代价还是较小的。不过扫描区间( \u0026lsquo;a\u0026rsquo;, \u0026lsquo;c\u0026rsquo; )中的二级索引记录对应 的id 值的大小是毫无规律的， 我们每读取一条二级索引记录，就需要根据该二级索引记录的id 值到聚簇索引中执行回表操作。如果对应的聚簇索引记录所在的页面不在内存中，就需要将该 页面从磁盘加载到内存中.。由于要读取很多id 值并不连续的聚簇索引记录，而且这些聚簇索引 记录分布在不同的数据页中， 这些数据页的页号也毫无规律，因此会造成大量的随机I/O . 需要执行回表操作的记录越多， 使用二级索引进行查询的性能也就越低，某些查询宁愿使 用全表扫描也不使用二级索引。比如， 假设key1值在\u0026rsquo;a\u0026rsquo;~\u0026lsquo;c\u0026rsquo; 之间的用户记录数量****占全部记录** 数量的99%** 以上，如果使用idx_key1索引，则会有99% 以上的id 值需要执行回表操作。这 不是吃力不讨好么， 还不如直接执行全表扫描\n什么时候采用全表扫描， 什么时候使用二级索引+回表的方式 # 这是查询优化器应该做的工作：\n查询优化器会事先针对表中的记录计算一些统计数据，然后再利用这些统计数据或者访问表中的少量记录来计算需要执行回表操作的记录数，如果需要执行回表操作的记录数越多，就越倾向于使用全表扫描， 反之则倾向于使用二级索引+回表的方式。当然，查询优化器所做的分析工作没有这么简单， 但大致上是这样一个过程。\n一般情况下，可以给查询语句指定LIMIT 子句来限制查询返回的记录数， 这可能会让查 询优化器倾向于选择使用二级索引+回表的方式进行查询， 原因是回表的记录越少， 性能提升 就越高。比如，上面的查询语句可以改写成下面这样\nSELECT * FROM single_table WHERE key1 \u0026gt; 'a' AND key1\u0026lt;'c' LIMIT 10\n添加了LIMlT10 子句后的查询语句更容易让查询优化器采用二级索引+回表的方式来执行。 对于需要对结果进行排序的查询，如果在采用二级索引执行查询时需要执行回表操作的记 录特别多，也倾向于使用全表扫描+文件排序的方式执行查询。比如下面这个查询语句 SELECT * FROM single_table ORDER BY key1 由于查询列表是 *，如果使用二级索引进行排序，则需要对所有二级索引记录执行回表操作. 这样操作的成本还不如直接遍历聚簇索引然后再进行文件排序低， 所以查询优化器会倾向于使 用全表扫描的方式执行查询。如果添加了LIMIT子句，比如下面这个查询语句:\nSELECT * FROM single_table ORDER BY key1 LIMIT 10; 这个查询语句需要执行回表操作的记录特别少，查询优化器就会倾向于使用二级索引+回表的 方式来执行\n更好地创建和使用索引 # 只为用于搜索、排序或分组的列创建索引 # SELECT common_field,key_part3 FROM single_table WHERE key1= \u0026#39;a\u0026#39;; 没必要为common_field,key_part3创建索引\n考虑索引列中不重复值的个数 # 前文在唠叨回表的知识时提提到， 在通过二级索引+回表的方式执行查询时，某个扫描区间中包含的二级索引记录数量越多， 就会导致回表操作的代价越大。我们在为某个列创建索引时，需要考虑该列中不重复值的个数占全部记录条数的比例。如果比例太低，则说明该列包含 过多重复值，那么在通过二级索引+回表的方式执行查询时，就有可能执行太多次回表操作\n索引列的类型尽量小 # 在定义表结构时，要显式地指定列的类型 以整数类型为例， 有 TINIINT、MEDIUMINT、INT、BIGINT这几种，它们占用的存储空间的大小依次递增。下面所说的类型大小指的就是该类型占用的存储空间的大小。刚才提到的这几个整数类型，它们能表示的整数范围当然也是依次递增。如果想要对某个整数类型的列建立索引，在表示的整数范围允许的情况下，尽量让索引列使用较小的类型，比如能使用INT就不要使用BIGINT。 能使用MEDIUMINT 就不要使用的INT。 因为数据类型越小， 索引占用的存储空间就越少，在一个数据页内就可以存放更多的记录，磁盘1/0 带来的性能损耗也就越小(一次页面I/O 可以将更多的记录加载到内存中) 读写效率也就越高 这个建议对于表的主键来说更加适用，因为不仅聚簇索引会存储主键值，其他所有的二级索引的节点都会存储一份记录的主键值。如果主键使用更小的数据类型，也就意味着能节省更多的存储空间\n为列前缀建立索引 # 我们知道，一个字符串其实是由若干个字符组成的。如果在MySQL 中使用utf8 字符集存储字符串，则需要1 - 3 字节来编码一个字符。假如字符串很长，那么在存储这个字符串时就需要占用很大的存储空间。在需要为这个字符串所在的列建立索引时，就意味着在对应的B+ 树中的记录中， 需要把该列的完整字符串存储起来。字符串越长，在索引中占用的存储空间越大。 前文说过， 索引列的字符串前缀其实也是排好序的，所以索引的设计人员提出了一个方案。 只将字符串的前几个字符存放到索引中，也就是说在二级索引的记录中只保留字符串的前几个字符。比如我们可以这样修改idx_key1索引，让索引中只保留字符串的前10个字符:\nALTER TABLE single_table DROP INDEX idx_key1; ALTER TABLE single_table ADD INDEX idx_key1(key1(10)); 再执行下面的语句\nSELECT * FROM single_table WHERE key1= \u0026#39;abcdefghijklmn\u0026#39; 由于在idx_key1 的二级索引记录中只保留字符串的前10 个字符，所以我们只能定位到前缀为\u0026rsquo;abcdefghij\u0026rsquo; 的二级索引记录，在扫描这些二级索引记录时再判断它们是否满足key1=\u0026lsquo;abcdefghijklmn\u0026rsquo; 条件。当列中存储的字符串包含的字符较多时，这种为列前缀建立索引的方式可以明显减少索引大小。\n注意，上面说的是扫描这些二级索引记录，是“些”。 可以减少索引大小，但不一样减少索引数量。如果有重复的照样会在索引中出现，因为不是UNIQUE约束。二级索引值大小相同时，会按照聚簇索引大小排列 不过，在只对列前缀建立索引的情况下， 下面这个查询语句就不能使用索引来完成排序需求了： SELECT * FROM single_table ORDER BY key1 LIMIT 10;\n因为二级索引idx_key1中不包含完整的key1列信息，所以在仅使用idx_key1索引执行查询时，无法对key1 列前10 个字符相同但其余字符不同的记录进行排序。也就是说，只为列前缀建立索引的方式无法支持使用索引进行排序的需求。上述查询语句只好乖乖地使用全表扫描+文件排序的方式来执行了。\n只为列前缀创建索引的过程我们就介绍完了，还是将idx_key1 改回原来的样式：\nALTER TABLE single_table DROP INDEX idx_key1; ALTER TABLE single_table ADD INDEX idx_key1(key1); 覆盖索引 # 为了彻底告别回表操作带来的性能损耗，建议最好在查询列表中只包含索引列，比如这个查询语句:\nSELECT key1,id FROM single_table WHERE key1 \u0026gt; 'a' AND key1 \u0026lt; 'c'\n由于只查询key1列和id列的值，这里使用idx_key1索引来扫描(\u0026lsquo;a\u0026rsquo;,\u0026lsquo;c\u0026rsquo;)区间的二级索引记录时，可以直接从获取到的二级索引记录中读出key1列和id列的值，不需要通过id值到聚簇索引执行回表，省去回表操作带来的性能损耗。\n把这种已经包含所有需要读取的列的查询方式称为覆盖索引\n排序操作也优先使用覆盖索引进行查询：\nSELECT * FROM single_table ORDER BY key1 虽然这个查询语句中没有LIMIT子旬，但是由于可以采用覆盖索引，所以查询优化器会直接使用idx_key1索引进行排序 而不需要执行回表操作。 当然，如果业务需要查询索引列以外的列，那还是以保证业务需求为重。如无必要， 最好仅把业务中需要的列放在查询列表中，而不是简单地以*替代\n让索引列以列名的形式在搜索条件中单独出现 # 注意，是单独\n如下面两个语义一样的搜索条件\nSELECT * FROM single_table WHERE key2 * 2 \u0026lt; 4; SELECT * FROM single_table WHERE key2 \u0026lt; 4/2; 在第一个查询语句的搜索条件中， key2列并不是以单独列名的形式出现的，而是以key2 * 2这样的表达式的形式出现的。 MySQL 并不会尝试简化key2*2\u0026lt;4 表达式，而是直接认为这个搜索条件不能形成合适的扫描区间来减少需要扫描的记录数量，所以该查询语句只能以全表扫锚的方式来执行。 在第二个查询语句的搜索条件中， key2 列是以单独列名的形式出现的， MySQL 可以分析出如果使用uk_key2 执行查询，对应的扫描区间就是（-∞，2） ，这可以减少需要扫描的记录数量。 所以MySQL 可能使用uk_key2 来执行查询。 所以，如果想让某个查询使用索引来执行，请让索引列以列名的形式单独出现在搜索条件中 新插入记录时主键大小对效率的影响 # 我们知道，对于一个使用lnnoDB 存储引擎的表来说，在没有显式创建索引时， 表中的数据实际上存储在聚簇索引的叶子节点中，而且B+ 树的每一层数据页以及页面中的记录都是按照主键值从小到大的顺序排序的。如果新插入记录的主键值是依次增大的话，则每插满一个数据页就换到下一个数据页继续插入。如果新插入记录的主键值忽大忽小，就比较麻烦了\n假设某个数据页存储的聚簇索引记录已经满了， 它存储的主键值在1 - 100之间，如图：\n此时，如果再插入一条主键值为8的记录，则它插入的位置如图：\n可这个数据页已经满了啊， 新记录该插入到哪里呢?我们需要把当前页面分裂成两个页面， 把本页中的一些记录移动到新创建的页中。页面分裂意味着什么?意味着性能损耗!所以， 如果想尽量避免这种无谓的性能损耗，最好让插入记录的主键值依次递增。就像single_table的主键id 列具有AUTO_INCREMENT 属性那样。 MySQL 会自动为新插入的记录生成递增的主键值\n冗余和重复索引 # 针对single_table 表， 可以单独针对key_part1列建立一个idx_key_part1索引\nALTER TABLE single_table ADD INDEX idx_key_part(key_part1); 其实现在我们已经有了一个针对key_part1、key_part2 、key_part3列建立的联合索引idx_key_part。idx_key_part索引的二级索引记录本身就是按照key_part1 列的值排序的， 此时再单独为key_part1列建立一个索引其实是没有必要的。我们可以把这个新建的idx_key_part1索引看作是一个冗余索引， 该冗余索引是没有必要的\n有时，我们可能会对同一个列创建多个索引，比如这两个添加索引的语句：\nALTER TABLE single_table ADD UNIQUE KEY uk_id(id); ALTER TABLE single_table ADD INDEX idx_id(id); 我们针对id 列又建立了一个唯一二级索引uk_id，. 还建立了一个普通二级索引idx_id。 可是id 列本身就是single_table 表的主键， InnoDB 自动为该列建立了聚簇索引， 此时uk_id 和idx_id 就是重复的，这种重复索引应该避免\n"},{"id":45,"href":"/zh/docs/technology/MySQL/how_mysql_run/06/","title":"06B+树索引","section":"_MySQL是怎样运行的_","content":" 学习《MySQL是怎样运行的》，感谢作者！\n概述 # 数据页由7个组成部分，各个数据页可以组成一个双向链表，每个数据页中的记录会按照主键值从小到大的顺序组成一个单向链表。每个数据页都会为它里面的记录生成一个页目录，在通过主键查找某条记录的时候可以在页目录中使用二分法快速定位到对应的槽，然后再遍历该槽对应分组中的记录即可快速找到指定的记录。页和记录的关系\n页a，页b 可以不在物理结构上相连，只要通过双向链表相关联即可\n没有索引时进行查找 # 假设我们要搜索某个列等于某个常数的情况：\nSELECT [查询列表] FROM 表名 WHERE 列名 = xxx\n在一个页中查找 # 假设记录极少，所有记录可以存放到一个页中\n以主键位搜索条件：页目录中使用二分法快速定位到对应的槽，然后在遍历槽对应分组中的记录，即可快速找到指定记录 其他列作为搜索条件：对于非主键，数据页没有为非主键列建立所谓的页目录，所以无法通过二分法快速定位相应的槽。只能从Infimum依次遍历单向链表中的每条记录，然后对比，效率极低 在很多页中查找 # 两个步骤：\n定位到记录所在的页 从所在页内查找相应的记录 没有索引情况下，不能快速定位到所在页，只能从第一页沿着双向链表一直往下找，而如果是主键，每一页则可以在页目录二分查找。\n不过由于要遍历所有页，所以超级耗时\n索引 # #例子 mysql\u0026gt; CREATE TABLE index_demo( c1 INT, c2 INT, c3 CHAR(1), PRIMARY KEY(c1) ) ROW_FORMAT=COMPACT; 完整的行格式\n简化的行格式\nrecord_type：记录头信息的一项属性，表示记录的类型。0：普通记录，2：Infimum记录，3：Supremum记录，1还没用过等会再说 next_record：记录头信息的一项属性，表示从当前记录的真实数据到下一条记录真实数据的距离 各个列的值：这里只展示在index_demo表中的3个列，分别是c1、c2、c3 其他信息：包括隐藏列及记录的额外信息 改为竖着查看：\n上面图6-4的箭头其实有一点点出入，应该是指向z真实数据第1列那个位置，如下 一个简单的索引方案 # 思考：在根据某个条件查找一些记录，为什么要遍历所有的数据页呢？因为各个页中的记录没有规律，不知道搜索条件会匹配哪些页\n思路：为快速定位记录所在的数据页而建立一个别的目录\n有序 # 下一个数据页中用户记录的主键值必须大于上一页用户记录的主键值\n假设一页只能存放3条记录\n#插入3条记录 mysql\u0026gt; INSERT INTO index_demo VALUES(1,4,\u0026#39;u\u0026#39;),(3,9,\u0026#39;d\u0026#39;),(5,3,\u0026#39;y\u0026#39;); Query OK, 3 rows affected (0.02 sec) Records: 3 Duplicates: 0 Warnings: 0 此时页的情况\n记录组成了单链表\n再插入一条记录\nmysql\u0026gt; INSERT INTO index_demo VALUES(4,4,\u0026#39;a\u0026#39;); Query OK, 1 row affected (0.01 sec) (注意，页之间可能不是连续的)\n由于页10中最大记录是5，而页28中有一条记录是4，因为5\u0026gt;4，不符合下一个数据页中用户记录的主键值必须大于上一页中用户记录的主键值，所以在插入主键值为4的记录时需要伴随着一次记录移动，也就是把5的记录移动到页28中，再把主键值4的记录插入到页10中\n这个过程表明，在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。这个过程也可以称为页分裂。\n给所有的页建立一个目录项 # 前提：index_demo表中有多条记录的效果\n1页有16KB，这些页在磁盘上可能并不连续，要想从这么多页中根据主键值快速定位某些记录所在的页，需要给他们编制一个目录，每个页对应一个目录项，每个目录项包括两部分：\n页的用户记录中最小的主键值，用key表示 页号，page_no表示 假设我们此时把目录项在物理存储器上连续存储，比如放到数组中，此时就可以根据主键值快速查找某条记录\n先用二分法快速定位主键20的记录在目录项3中（因为12\u0026lt;20\u0026lt;209)，对应页是9\n根据前面说的方式在页9中定位具体记录\n先在页目录中二分查找，找到对应的组后沿链表遍历\n这个目录项的别名：索引\nInnoDB中的索引方案 # 上述方案的问题 # InnoDB使用页作为管理存储空间的基本单位，即只能保证16KB的连续存储空间，如果记录非常多，则需要的连续存储空间就非常大 增删改是很频繁的，如果页28的记录全部移除，那么目录项2就没有出现的必要，即要删除目录项2，那么所有的目录项都需要左移/或者不移动，作为冗余放到目录项列表中，浪费空间 方案 # 复用之前存储用户记录的数据页来存储目录项，用了和用户记录进行区分，把这些用来表示目录项的记录称为目录项记录。如何区分一条记录是普通用户记录，还是目录项记录：使用记录头信息中的record_type属性\n0：普通用户记录 1：目录项记录 2：Infimum记录 3：Supremum记录 将目录项放到数据页中\n新分配了一个编号为30的页来专门存储目录项记录\n目录项记录和普通的用户记录的不同点\n目录项记录的record_type值为1，普通用户记录record_type值为0\n目录项记录只有主键值和页的编号这两个列，而普通用户记录的列是用户自己定义的，可能包含许多列，另外还有InnoDB自己添加的隐藏列\n记录头信息中有一个名为min_rec_flag的属性，只有目录项记录的min_rec_flag属性才可能为1，普通记录的min_rec_flag属性都是0\nB+ 树中每层非叶子节点中的最小的目录项记录都会添加该标记\n其他：\n它们用的是一样的数据页(页面类型都是Ox45BF ，这个属性在File Header 中)；页的组成结构也是一样的〈就是我们前面介绍过的7 个部分)；都会为主键值生成Page Directory（页目录）从而在按照主键值进行查找时可以使用 二分法来加快查询速度。\n举例 # 单个 目录项记录页 # 其中，页30中存储的主键值分别为1，5，12，209\n假设我们现在要查找主键值为20的记录：\n先到存储目录项记录的页（这里是页30）中，（由于有页目录）通过二分法快速定位到对应的目录项记录，因为12\u0026lt;20\u0026lt;209，所以定位到对应的用户记录所在的页就是页9 再到存储用户记录的页9中根据二分法（由于有页目录））快速定位到主键值为20的用户记录 目录项记录中只存储主键值和对应的页号，存储空间极小，但一个页只有16KB，存放的目录项记录有限。如果表中数据太多（页太多），以至于一个数据页不足以存放所有的目录项记录\n多个 目录项记录的页 # 解决方案：新增一个存储目录项记录的页\n此时再进行查找\n确定存储目录项记录的页\n现在存储目录项记录的页有2个，即页30和页32。又因为页30表示的目录项记录主键值范围是**[1，320)，页32表示的目录项记录主键值范围 \u0026gt; 320。所以确定主键值为20的记录对应的目录项记录**在页30中 按照单个 目录项记录页的方案查找 多个目录项记录页 # 如果数据再增加，则再生成存储更高级目录项记录的数据页\n无论是存放用户记录的数据页，还是存放目录项记录的数据页，都放到B+树数据结构中，我们也将这些数据页称为B+树的节点\n如图，我们真正的用户记录其实都存放在B+树最底层的节点上，这些节点也称为叶子节点或页节点，其余用来存放目录项记录的节点称为非叶子节点或者内节点，其中B+树最上边的那个节点也称为根节点 这里我们规定，最下面那层（存放用户记录的那层）为0层，之后层级依次往上加。\n这里我们假设所有存放用户记录的叶子节点所代表的数据页可以存放100条用户记录（16KB=16 * 1024 ≈10000 字节，差不多一条记录100字节），假设所有存放目录项记录的内节点所代表的数据页可以存放1000条目录项记录（10000字节，假设1个目录项10字节），那么如果\n如果B+树有1层，那么只有一个用于存放用户记录的节点，那么能存放100条用户记录（1百） 如果B+树有2层，那么能存放 1000 * 100=100,000条用户记录(10万) 如果B+树有3层，那么能存放 1000 * 1000 * 100=100,000,000条用户记录(1亿) 如果B+树有4层，那么能存放 1000 * 1000 * 1000 * 100=100,000,000,000条用户记录 (1000亿) 所以一般情况下，我们用到的B+树不会超过4层。\n当我们要通过主键值查找**某条记录 **\n最多只需要进行4个页面内的查找（查找3个存储目录项记录的页和1个存储用户记录的页） 每个页面内存在PageDirectory（页目录），所以在页面内也可以通过二分法快速定位记录 PageHeader中，有一个名为PAGE_LEVEL的属性，代表着这个数据页作为节点在B+树中的层级\n聚簇索引 # 前面介绍的B+树本身就是一个记录，或者说本身就是一个索引，有以下两个特点\n使用记录主键值的大小进行记录和页的排序 页（包括叶子节点和内节点）内的记录，按照主键大小顺序排成一个单向链表，页内的记录被划分成若干个组，每个组中主键值最大的记录在页内的偏移量会被当作槽一次存放在页目录中（Supremum记录比任何用户记录都大）之后可以在页目录中通过二分法快速定位到主键列等于某个值的记录 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表 存放目录项记录的页分为不同的层级，在同一层级中也是根据页目录项记录的主键大小顺序排成一个双向链表 B+树的叶子节点存储的是完整的用户记录（指的是这个记录中存储了所有列的值（包括隐藏列）） 具有上面两个特点的B+树称为聚簇索引。所有完整的用户记录都存放在这个聚簇索引的叶子节点处。这种聚簇索引，不需要我们在MySQL语句中显示使用INDEX语句去创建，InnoDB会自动为我们创建聚簇索引\nInnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录都存储在了叶子节点）。索引即数据，数据即索引\n二级索引 # 聚簇索引只能在搜索条件是主键值时才发挥作用，如果要以别的列作为搜索条件，可以多建几颗B+树，而且不同B+树中的数据，采用不同的排序规则\n比如用c2列的大小作为数据页、页中记录的排序规则，再建一颗B+树\n\u0026ldquo;前言：c1已经是主键了\u0026rdquo;\n#例子 mysql\u0026gt; CREATE TABLE index_demo( c1 INT, c2 INT, c3 CHAR(1), PRIMARY KEY(c1) ) ROW_FORMAT=COMPACT; 下面是聚簇索引特点：\n二级索引说明：\n使用记录c2列的大小进行记录和页的排序 页（包括叶子节点和内节点）内的记录，按照c2列大小顺序排成一个单向链表，页内的记录被划分成若干个组，每个组中c2列值最大的记录在页内的偏移量会被当作槽一次存放在页目录中（Supremum记录比任何用户记录都大）之后可以在页目录中通过二分法快速定位到c2列值等于某个值的记录 各个存放用户记录的页也是根据页中用户记录的c2列大小顺序排成一个双向链表 存放目录项记录的页分为不同的层级，在同一层级中也是根据页目录项记录的c2列大小顺序排成一个双向链表 B+树的叶子节点存储的是并不是完整的用户记录，而只是c2列+主键这两个列的值 目录项记录中不再是主键+页号的匹配，而变成了c2列+页号的搭配 B+树如下\n举例 # 假设要查找c2=4的记录，可以使用上面的B+树。由于c2没有唯一性约束，所以可能会有很多条：我们只需要在该B+树的叶子节点处定位到第一条满足搜索条件c2=4的那条，然后由记录组成的单向链表一直向后扫描即可。另外，各个叶子节点组成了双向链表，搜索完了本页面的记录后可以顺利跳到下一个页面中的第一条记录，然后沿着记录组成的单向链表向后扫描\n查找过程 # 确定第一条符合c2=4条件的目录项记录所在的页\n根据**根页面（44）**可以快速定位到第一条符合c2=4条件的目录项记录所在页为页42（因为2\u0026lt;4\u0026lt;9)\n通过第一条符合c2=4条件的目录项记录所在的页面确定第一条符合c2=4条件的用户记录所在的页\n根据页42可以快速定位（通过页目录）到第一条符合条件的用户记录所在页为34或35，因为2\u0026lt;4\u0026lt;=4\n在真正存储第一条符合c2=4条件的用户记录的页中定位到具体的记录\n页34和页35中定位到具体的用户记录（如果页34使用页目录定位到第一条符合条件的用户记录，就不需要再到35中去再定位，因为直接一直往后查找到不等的记录即可）\n由于这个B+树的叶子节点的记录只存储了c2和c1（即主键）两个列。在叶子节点定位到第一条符合条件的那条用户记录之后，我们需要根据该纪录中的主键信息，到聚簇索引中查找到完整的用户记录，这个通过携带主键信息到聚簇索引中重新定位完整的用户记录的过程也称为回表 。\n然后再返回到这棵B+ 树的叶子节点处，找到刚才定位到的符合条件的那条用户记录，并沿着记录组成的单向链表向后继续搜索其他也满足c2=4的记录**，每找到一条的话就继续进行回表操作。重复这个过程，直到下一条记录不满足c2 =4**的这个条件为止.\n如果把完整的用户记录放到叶子节点是可以不用回表，但是太占地方了\n因为这种以非主键列的大小为排序规则而建立的B+ 树需要执行回表操作才可以定位到完整的用户记录，所以这种B+ 树也称为二级索引(Secondary Index) 或辅助索引。由于我们是以c2 列的大小作为B+ 树的排序规则，所以我们也称这棵B+ 树为为c2 列建立的索引，把c2列称为索引列。二级索引记录和聚簇索引记录使用的是一样的记录行格式，只不过二级索引记录存储的列不像聚簇索引记录那么完整。\n把聚簇索引或者二级索引的叶子节点中的记录称为用户记录。为了区分，也把聚簇索引叶子节点中的记录称为完整的用户记录，把二级索引叶子节点中的记录称为不完整的用户记录\n如果为一个存储字符串的列建立索引，别忘了前面说的字符集和比较规则，字符串也是可以比较大小的\n联合索引 # 同时以多个列的大小作为排序规则，也就是同时为多个列建立索引，含义：\n先把各个记录和页按照c2列进行排序 记录的c2列相同的情况下，再采用c3进行排序 每条目录项记录都由c2列、c3列、页号这3个部分组成。各条记录先按照c2列的值进行排序。如果记录的c2列相同，则按照c3列的值进行排序\n这里说的是极特殊的情况，也就是c2列相同的记录有很多很多条，导致好几个页都有c2 = x的记录，而且其中c3列的值还不同，那么就会出现目录项记录页中的目录项c2相同而c3不相同\nB+树叶子节点处的用户记录由c2列、c3列、和主键c1列组成\n以c2 和c3 列的大小为排序规则建立的B+ 树称为联合索引，也称为复合索 引或多列索引。它本质上也是一个二级索引，它的索引列包括c2、c3.需要注意的是\u0026quot;以c2和c3列的大小为排序规则建立联合索引\u0026ldquo;和\u0026rdquo;分别为c2和d 列建立索引\u0026quot; 的表述是不同的， 不同点如下：\n建立联合索引只会建立如图6-15 所示的一棵B+ 树 为c2 和c3 列分别建立索引时，则会分别以c2 和c3 列的大小为排序规则建立两棵B+ 树 Inno中B+树索引的注意事项 # 根页面万年不动窝 # 前面为了理解方便，我们先把存储用户记录的叶子节点都画出来，然后再画出存储目录项记录的内节点。而实际上是这样的：\n每当为某个表创建一个B+树索引（聚簇索引不是人为创建的，默认就存在）时，都会为这个索引创建一个根节点页面。\n一开始表中没有数据的时候，每个B+树索引对应的根节点中既没有用户记录，也没有目录项记录 随后向表中插入用户记录时，先把用户记录存储到这个根节点中 当根节点可用空间用完时，继续插入记录，此时会将根节点中的所有记录复制到一个新分配的页（比如页a）中，然后对这个新页进行页分裂操作，得到另一个新页（比如页b）[因为一个页放不下，所以还要这个新页]。这时新插入的记录会根据键值（也就是聚簇索引中的主键值，或者二级索引中对应的索引列的值）的大小分配到页a或者页b。根节点此时，便升级为存储目录项记录的页，也就需要把页a和页b对应的目录项记录插入到根节点中 在这个过程中，需要特别注意的是， 一个B+ 树索引的根节点自创建之日起便不会再移动(也就是页号不再改变)。\n由于这个特性，只要我们对某个表建立一个索引，那么它的根节点的页号便会被记录到某个地方，后续凡是InnoDB引擎需要用到这个索引，会从那个固定的地方取出根节点的页号，从而访问这个索引\n\u0026ldquo;存储某个索引的根节点在哪个页面中\u0026rdquo;，就是传说中的数据字典中的一项信息\n这里还有一个问题，书上没说，就是根节点作为a，b页的存储目录项记录的页，一旦后面页越来越多，根节点放不下了，接下来\n我猜是这样的，也是再新分配一个页X，然后对页X页分裂，得到页Y。把根节点此时的所有目录项全复制到页X，然后新插入的目录项记录根据键值分配到页X，或Y，然后根节点又变为存储目录项记录的页\n内节点中目录项记录的唯一性 # 目前为止，我们说B+树索引的内节点中，目录项记录的内容是索引列加页号的搭配，但是这个搭配对二级索引来说有点儿不太严谨。以下面这个表为例（c1是主键，c2是二级索引）\nc1 c2 c3 1 1 \u0026lsquo;u\u0026rsquo; 3 1 \u0026rsquo;d' 5 1 \u0026lsquo;y\u0026rsquo; 7 1 \u0026lsquo;a\u0026rsquo; 如果二级索引中，目录项记录的内容只是索引列+页号的匹配，那么为c2列建立索引后的B+树如下图6-16\n如果此时再插入一条记录 c1=9，c2=1，c3=\u0026lsquo;c\u0026rsquo;，那么在修改为c2列建立的二级索引对应的B+树时：由于页3中存储的目录项记录由c2列+页号构成，页3中两条目录项记录对应的c2列都是1，而新插入的这条记录中，c2列也是1，那么这条新插入的记录应该放在页4还是页5？\n为了保证B+树同一层内节点的目录项记录除了页号这个字段以外是唯一，所以二级索引的内节点的目录项记录内容实际上由3部分构成： 索引列的值，主键值，页号 ，如上图6-17\n插入记录（9，1，’c\u0026rsquo;）时，由于页3 中存储的目录项记录是由c2 列+ 主键+页号构成的， 因此可以先把新记录的c2 列的值和页3 中各目录项记录的c2 列的值进行比较， 如果c2 列的值相同，可以接着比较主键值。因为B+ 树同一层中不同目录项记录的c2 列+主键的值肯定是不一样的，所以最后肯定能定位到唯一的一条目录项记录。 在本例中， 最后确定新记录应该插入到页5 中\n对于二级索引，先按照二级索引列的值进行排序，如果相同，再按照主键值进行排序。所以，为c2列建立索引，相当于为（c2，c1）列建立了一个联合索引。另外，对于唯一二级索引来说（当我们为某个列或列组合声明UNIQUE属性时，便会为这个列或组合建立唯一索引），也可能出现多条记录键值相同的情况（1. 声明为UNIQUE的列可能存储多个NULL 2. 后面要讲的MVCC服务），唯一二级索引的内节点的目录项记录也会包含记录的主键值\n注意，书上没有讲到删除的情况，也就是假设有一种情形：索引值1的行被删了，后面又重新添加了。我的理解是不会出现两条索引值一样的记录在树上（根据前面记录行的delete_flag，有可能重复，但是我猜会覆盖掉，所以这里没讲到那个情况，暂时没找到资料证明）\n一个页面至少容纳2条记录 # 如果一个大的目录中只存放一个子目录，那么目录层级会非常多，而且最后那个存放真正数据的目录中只能存放一条记录\n如果让B+树的叶子节点只存储一条记录，让内节点存储多条记录，也还是可以发挥B+树作用的。为了避免B+树的层级增长过高，要求所有数据页都至少可以容纳2条记录（也就是说，会极力避免因为列值过大、或者过多导致容纳不了2条记录）\nInnoDB对列的数量有所限制，而如果在最大限制下，结合04章的结论：\n如果一条记录的某个列中存储的数据占用字节数非常多，导致一个页没有办法存储两条记录，该列就可能会成为溢出列\nMyISAM中的索引方案简介 # 为了内容完整性，介绍一下MyISAM存储引擎中的索引方案\nInnoDB中，索引即数据，也就是聚簇索引的那颗B+树的叶子节点中包含了完整的用户记录。MyISAM虽然也是树形，但是索引和数据是分开的\n数据文件 # 表中的记录按照记录的插入顺序单独存储在一个文件中（称之为数据文件）\n该文件不划分若干个数据页，有多少记录就往文件中塞多少。通过行号快速访问到一条记录\nMyISAM记录需要记录头信息来存储额外数据，以index_demo表为例，看一下这个表在使用MyISAM作为存储引擎时，它的记录如何在存储空间表示\n由于是按插入顺序，没有按主键大小排序，所以不能在这些数据上使用二分法\n索引 # MyISAM存储引擎会把索引信息单独存储到另一个文件中（即索引文件）\nMyISAM会为表的主键单独创建一个索引，只不过在索引的叶子节点中存储的不是完整用户记录，而是主键值与行号的结合。即先通过索引找到对应的行号，再通过行号去找对应的记录\n与InnoDB不同，InnoDB存储隐情中，只需根据主键值对聚簇索引进行依次查找就能找到对应记录，而MyISAM中却需要进行一次回表操作。意味着MyISAM中建立的索引相当于都是二级索引\n其他索引 # 可以为其他列分别建立索引或者建立联合索引，原理与InnoDB差不多，只不过叶子节点存储的是相应的列+行号（InnoDB中存储的则是主键）。这些索引也都是二级索引。\nMyISAM行格式有定长记录格式Static、变长记录格式Dynamic、压缩记录格式 Compressed。图6-18就是定长记录格式，即一条记录占用的存储空间是固定的，这样就可以使用行号轻松算出某条记录在数据文件中的地址偏移量，但是变长记录格式就不行乐，MyISAM会直接在索引叶子节点处存储该记录在数据文件中的偏移量。==\u0026gt; MyISAM回表快速，因为是拿着地址偏移量直接到文件中取数据。而InnoDB则是获取主键后，再去从聚簇索引中查找。\n总结 # InnoDB：索引即数据\nMyISAM：索引是索引，数据是数据\nMySQL中创建和删除索引的语句 # InnoDB会自动为主键或者**带有UNIQUE **属性的列建立索引\nInnoDB不会自动为每个列创建索引，因为每建立一个索引都会建立一颗B+树，且增删改都要维护各个记录、数据页的排序关系，费性能和存储空间\n#语法 CREATE TABLE 表名( 各个列的信息..., (KEY|INDEX) 索引名 (需要被索引的单个列或多个列); ) #修改表结构 ALTER TABLE 表名 ADD (INDEX|KEY) 索引名 (需要被索引的单个列或多个列); #修改表结构的时候删除索引 ALTER TABLE 表名 DROP (INDEX|KEY) 索引名; 实例：\nmysql\u0026gt; CREATE TABLE index_demo( c1 INT, c2 INT, c3 CHAR(1), PRIMARY KEY (c1), INDEX idx_c2_c3 (c2,c3) ); #查看建表语句 mysql\u0026gt; SHOW CREATE TABLE index_demo; +------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | index_demo | CREATE TABLE `index_demo` ( `c1` int(11) NOT NULL, `c2` int(11) DEFAULT NULL, `c3` char(1) DEFAULT NULL, PRIMARY KEY (`c1`), KEY `idx_c2_c3` (`c2`,`c3`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) "},{"id":46,"href":"/zh/docs/technology/MySQL/how_mysql_run/05/","title":"05InnoDB数据页结构","section":"_MySQL是怎样运行的_","content":" 学习《MySQL是怎样运行的》，感谢作者！\n不同类型的页简介 # 页是InnoDB管理存储空间的基本单位，1个页的大小一般是16KB\nInnoDB为了不同目的设计多种不同类型的页，包括存放表空间头部信息 的页、存放Change Buffer 信息的页、存放INODE信息的页、存放undo 日志信息的页\n这里说的是存放表中记录的那种类型的页，这种存放记录的页称为索引页（INDEX页）\n暂时称之为数据页\n数据页结构快览 # 1个页有16KB，这部分存储空间被划分为了多个部分（7部分），不同部分有不同的功能\n名称 中文名 占用空间 大小 File Header 文件头部 38 字节 页的一些通用信息 Page Header 页面头部 56 字节 数据页专有的一些信息 Infimum + Supremum 页面中的最小记录和最大记录 26 字节 两个虚拟的记录 User Records 用户记录 不确定 用户存储的记录内容 Free Space 空闲空间 不确定 页中尚未使用的空间 Page Directory 页目录 不确定 某些记录的相对位置 File Trailer 文件尾部 8 字节 校验页是否完整 记录在页中的存储 # 每插入一条记录，从Free Space申请一个记录大小的空间，并将这个空间划分到UserRecords部分。当FreeSpace部分的空间全部被UserRecords部分替代掉后，意味着该页用完。如果再插入，就需要申请新的页\n记录头信息的秘密 # mysql\u0026gt; CREATE TABLE page_demo( c1 INT, c2 INT, c3 VARCHAR(10000), PRIMARY KEY(c1) ) CHARSET=ascii ROW_FORMAT=COMPACT; Query OK, 0 rows affected (0.03 sec) 名称 大小（比特） 描述 预留位1 1 没有使用 预留位2 1 没有使用 deleted_flag 1 标志该记录是否被删除 min_rec_flag 1 B+ 树中每层非叶子节点中的最小的目录项记录都会添加该标记 n_owned 4 一个页面中的记录会被分成若干个组，每个组中有一个记录是\u0026quot;带头大哥“，其余的记录都是\u0026quot;小弟\u0026quot;。带头大哥\u0026quot;记录的n_owned值代表该组中所有的记录条数，\u0026ldquo;小弟\u0026quot;记录的n_owned值都为0 heap_no 13 表示当前记录在页面堆中的相对位置 record_type 3 表示当前记录的类型，0表示普通记录. 1 表示B+ 树非叶节点的目录项记录. 2 表示Infimum 记录. 3 表示Supremum 记录 next_record 16 表示下一条记录的相对位置 简化一下（忽略其他非讲解的部分信息）\n#插入4条记录 mysql\u0026gt; INSERT INTO page_demo VALUES(1,100,\u0026#39;aaaa\u0026#39;),(2,200,\u0026#39;bbbb\u0026#39;),(3,300,\u0026#39;cccc\u0026#39;),(4,400,\u0026#39;dddd\u0026#39;); Query OK, 4 rows affected (0.01 sec) Records: 4 Duplicates: 0 Warnings: 0 UserRecords部分的存储结构\ndeleted_flag # 标记当前记录是否删除：0表示没有被删除，1表示记录被删除\n被删除的记录不从磁盘溢出，因为移除后还需要在磁盘上重新排列其他的记录，带来性能消耗\n被删除掉的记录会组成一个垃圾链表，记录在这个链表中占用的空间称为可重用空间，如果之后有新纪录插入到表中，就可能覆盖掉被删除的记录所占用的存储空间\ndelete_flag设置为1和将被删除的记录加入到垃圾链表其实是两个阶段，后面介绍undo日志会详细讲解删除操作的详细执行过程\nmin_rec_flag # B+树每层非叶子节点中的最小的目录项记录都会添加该标记\nn_onwed # heap_no # 记录一条一条亲密无间排列的结构称之为堆（heap）。把一条记录在堆中的相对位置称之为heap_no\n为了管理这个堆，每一条记录在堆中的相对位置称为heap_no。\n页面前面的记录heap_no比后面的小，且每新申请一条记录的存储空间，该条记录比物理位置在它前面的那条记录的heap_no大1\n由上可知，4条记录的heap_no为2，3，4，5\nInnoDB的设计者自动给每个页添加了两条记录（称之伪记录或虚拟记录）。一条代表页面中的最小记录（也称Infimum记录美 [ɪn'faɪməm]），一条代表页面中的最大记录（也称Supremumsu'pri: m en)。这两条伪记录也算作堆的一部分\n比较完整记录的大小就是比较主键的大小\n规定，用户的任何记录都比Infimum记录大，比supremum记录小\nInfimum和Supremum记录 # 单独放在一个称为Infimum和Supremum的部分\n堆中记录的heap_no值在分配之后就不会发生改动了（即使删除了堆中某条记录）\nrecord_type # 表示当前记录的类型，0表示普通记录（上面自己插入的记录是），1表示B+树非叶节点的目录项记录（后面索引会讲到），2表示Infimum记录，3表示Supremum记录\nnext_record # 表示从当前记录的真实数据到下一条记录的真实数据的距离\n如果该属性值为正数， 说明当前记录的下一条记录在当前记录的后面: 如果该属性值为负数，说明当前记录的下一条记录在当前记录的前面\n下一条记录，指的是按照主键值由小到大的顺序排列的下一条记录\nInfimum的下一条记录是本页中主键值最小的用户记录，本页中主键值最大的用户记录的下一条记录就是Supremum记录\n如上，记录按照主键从小到大的顺序形成了一个单向链表\nSupremum记录的next_record值为0，即没有下一条记录了，如果删除其中一条记录\nSupremum记录的n_owned由5变成了4\nInnoDB始终维护记录的一个单向链表，链表中的各个节点是按照主键值由小到大的顺序链接起来的\n为啥next_record是指向记录头信息和真实数据之间的位置，而不是整条记录的开头。\n这个位置刚好，向左是记录头信息，向右是真实数据 由于变长字段长度列表、NULL值列表中的信息都是逆序存放，这样可以使记录中靠前的字段和他们对应的字段长度信息在内存中的距离更近，提高高速缓存命中率 如果第2条记录被重新插入\nPageDirectory（页目录） # 解释 # 直接遍历的话，时间复杂度太高\n说明： 将所有记录（包括Infimum和Supremum记录，不包括已经移除到垃圾链表的记录划分为几个组\n每个组的最后一条记录（组内最大的那条记录）相当于带头大哥，其余记录相当于小弟。\n带头大哥记录的头信息中的n_owned属性表示改组内共有几条记录\n操作：\n将每个组中最后一条记录（组内最大记录）在页面中的地址偏移量(该记录的真实数据与页面中第0个字节之间的距离)单独提取出来，按顺序存储倒靠近页尾部的地方（这个地方就是PageDirectory）\n页目录的偏移地址称为槽(Slot)，每个槽占用2字节，页目录由多个槽组成\n1页有16KB，即16384字节，而2字节可以表示的地址偏移量为2^16-1=65535 \u0026gt;16384，所以用2字节表示一个槽足够了\n举例 # 假设page_demo表中有6条记录（包括Infimum和Supremum）\n注意，Infimum记录的n_owned值为1，Supremum记录的n_owned值为5\n且槽对应的记录（值）越小，越靠近FileTrailer\n用指针形式表示\n划分依据\n规定：对于Infimum记录所在的分组只能有1条记录，Supremum记录所在分组记录数在18条之间，剩下的分组中记录的条数范围只能是48条 简化：\n步骤：\n初始情况，数据页中只有Infimum和Supremum两条记录，分属两个分组\n页目录也只有两个槽：分别代表Infimum记录和Supremum记录在页中的偏移量\n之后每插入一条记录，都会从页目录中找到对应记录的主键值比待插入记录的主键值大并且差值最小的槽（从本质上看，槽是一个组内最大那条记录在页面中的地址偏移量，通过槽可以快速找到对应的记录的主键值)。然后把该槽对应的记录的n_owned值加1，表示本组内又添加了一条记录，直到该组中的记录数等于8\n当一个组中的记录数等于8后，再插入一条记录，会将组中的记录拆分成两个组，其中一个组中4条记录，另一个5条记录。且会在页目录中新增一个槽，记录这个新增分组中最大的那条记录的偏移量\n为了演示快速查找，再添加12条记录 ，总共16条\n一个槽占用2个字节，且槽之间是挨着的，每个槽代表的主键值都是从小到大排序的，所以可以使用二分法快速查找\n这里给槽编号：0，1，2，3，4。最低的槽就是low=0，最高的槽就是high=4\n假设我们要查找主键值为6的记录\n(0+4)/2=2，槽2代表的主键值8\u0026gt;6，所以high=2,low不变=0 (0+2)/2=1，槽1代表的主键值4\u0026lt;6，所以low=1,high不变=2 high-low=1，又因为槽记录的是最大值，所以不在槽1中，而是在槽2中\n沿着单项列表遍历槽2中的记录：如何遍历，先找到槽1的地址，然后它的下一条记录就是槽2中的最小记录 值为5，从值5的记录出发遍历即可（由于一个组中包含的记录条数最多是8，所以代价极小 总结\n通过二分法确定槽，找到槽所在分组中主键值最小的那条记录\n然后通过记录的next_record属性遍历该槽所在记录的各个记录\nPageHeader（页面头部） # 页结构的第2部分，占用固定的56字节，专门存储各种状态信息\nPageHeader的结构及描述\n状态名称 占用空间大小 描述 PAGE_N_DlR SLOTS 2字节 在页目录中的槽数量 PAGE_HEAP_TOP 2字节 还未使用的空间最小地址， 也就是说从该地址之后就是FreeSpace PAGE_N_HEAP 2字节 第1位表示本记录是否为紧凑型的记录， 剩余的15 位表示本页的堆中记录的数量（包括lnfimum 和Supremum 记录以及标记为\u0026quot;己删除\u0026quot;的记录） PAGE_FREE 2字节 各个己删除的记录通过next_record 组成一个单向链表，这个单向链表中的记录所占用的存储空间可以被重新利用；PAGE FREE 表示该链表头节点对应记录在页面中的偏移量 PAGE_GARBAGE 2字节 己删除记录占用的字节数 PAGE_LAST_INSERT 2字节 最后插入记录的位置 PAGE_DIRECTION 2字节 最后一条记录插入的方向 PAGE_N_DIRECTION 2字节 一个方向连续插入的记录数量 PAGE_N_RECS 2字节 该页中用户记录的数量〈不包括Infimum 和Supremum记录以及被删除的记录) PAGE_MAX_TRX_ID 8字节 修改当前页的最大事务id. 该值仅在二级索引页面中定义 PAGE_LEVEL 2字节 当前页在B+ 树中所处的层级 PAGE_INDEX_ID 8字节 索引ID， 表示当前页属于哪个索引 PAGE_BTR_SEG_LEAF 10字节 B+ 树叶子节点段的头部信息，仅在B+ 树的根页面中定义 PAGE_BTR_SEG_TOP 10字节 B+ 树非叶子节点段的头部信息，仅在B+ 树的根页面中定义 PAGE_N_DlR SLOTS - PAGE_N_RECS 的作用应该是清除的，这里有两个解释一下：\nPAGE_DIRECTION：加入新插入的一条记录的主键值比上一条记录的主键值大，我们说这条记录的插入方向是右边，反之则是左边。用来表示最后一条记录插入方向的状态就是PAGE_DIRECTION\nPAGE_N_DIRECTION：假设连续插入新记录的方向都是一致，InnoDB会把沿着同一个方向插入记录的条数记下来，用PAGE_N_DIRECTION表示。如果最后一条记录的插入方向发生了改变，这个状态的值会被清零后重新统计\n其他的暂时不讨论\nFileHeader（文件头部） # PageHeader专门针对的是数据页记录的各种状态信息，比如页有多少条记录，多少个槽。\nFileHeader通用于各种类型的页，描述了一些通用于各种页的信息，比如这个页的编号是多少，它的上一个页和下一个页是谁，固定占用38字节\n校验和（checksum)：对于很长的字节串，通过某种算法计算出比较短的值来代编这个字节串，比较之前先比较这个字节串。省去了直接比较这两个长字节串的时间损耗\nInnoDB通过页号来唯一定位一个页\n页号（第n个号），4字节，2^(4*8)=2^32次方位 =4294967296 个页\n4294967296 * (16KB/页) =64T，这也是InnoDB 单表限制的大小\n页有好几种类型，前面介绍的是存储记录的数据页，还有其他类型的页\n存放记录的数据页的类型其实是FIL_PAGE_INDEX，也就是索引页\n前面说记录的存储结构时，所说的溢出页是FIL_PAGE_TYPE_BLOB\n对于FIL_PAGE_PREV和FIL_PAGE_NEXT：当占用空间非常大时，无法一次性为这么多数据分配一个非常大的存储空间，如果分散到多个不连续的页中存储，则需要把这些页关联起来。FIL_PAGE_PREV和FIL_PAGE_NEXT就分别代表本数据页的上一个页和下一个页的页号。不是所有类型的页都有上一个页和下一个页属性的，不过数据页（FIL_PAGE_INDEX的页）有这两个属性，所以存储记录的数据页其实可以组成一个双向链表\nFileTrailer（文件尾部） # InnoDB存储引擎会把数据存储倒磁盘，但磁盘速度太慢，需要以页为单位把数据加载到内存中处理\n如果在该页中的数据在内存中修改了，在修改后某个时间还需要把数据刷新到磁盘中，但在刷新还没结束的时候断电了怎么办。为了检测一个页是否完整（判断刷新时有没有之刷新了一部分），为每个页尾部添加一个FileTriler部分，由8个字节组成，又分两小部分\n前4 字节代表页的校验和。这个部分与File Header 中的校验和相对应。每当一个页面在内存中发生修改时，在刷新之前就要把页面的校验和算出来。因为File Header 在页面的前边，所以File Header 中的校验和会被首先刷新到磁盘，当完全写完后，校验和也会被写到页的尾部。如果页面刷新成功，则页首和页尾的校验和应该是一致的。如果刷新了一部分后断电了，那么File Header 中的校验和就代表着己经修改过的页，而File Trailer 中的校验和代表着原先的页（因为断电了，所以没有完全写完），二者不同则意味着刷新期间发生了错误. 后4 字节代表页面被最后修改时对应的LSN 的后4 字节，正常情况下应该与FileHeader 部分的FIL_PAGE_LSN的后4 字节相同。这个部分也是用于校验页的完整性，不过我们目前还没说LSN 是什么意思，所以大家可以先不用管这个属性。 这个File Trailer 与File Header 类似，都通用于所有类型的页\n"},{"id":47,"href":"/zh/docs/technology/MySQL/how_mysql_run/04/","title":"04InnoDB记录存储结构","section":"_MySQL是怎样运行的_","content":" 学习《MySQL是怎样运行的》，感谢作者！\n问题 # 表数据存在哪，以什么格式存放，MySQL以什么方式来访问\n存储引擎：对表中数据进行存储和写入\nInnoDB是MySQL默认的存储引擎，这章主要讲InnoDB存储引擎的记录存储结构\nInnoDB页简介 # 注意，是简介\nInnoDB：将表中的数据存储到磁盘上\n真正处理数据的过程：内存中。所以需要把磁盘中数据加载到内存中，如果是写入或修改请求，还需要把内存中的内容刷新到磁盘上\n获取记录：不是一条条从磁盘读，InnoDB将数据划分为若干个页，以页作为磁盘和内存之间交互的基本单位。页大小-\u0026gt; 一般是16KB\n一般情况：一次最少从磁盘读取16KB的内容到内存中，一次最少把内存中的16KB内容刷新到磁盘中\nmysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;innodb_page_size\u0026#39;; +------------------+-------+ | Variable_name | Value | +------------------+-------+ | innodb_page_size | 16384 | +------------------+-------+ 1 row in set (0.00 sec) 只能在第一次初始化MySQL数据目录时指定，之后再也不能更改（通过mysqld \u0026ndash;initialize初始化数据目录[旧版本]）\nInnoDB行格式 # 以记录为单位向表中插入数据，而这些记录在磁盘上的存放形式也被称为行格式或者记录格式\n目前有4中不同类型的行格式：COMPACT、REDUNDANT、DYNAMIC和COMPRESSED\ncompact [kəmˈpækt]契约\nredundant[rɪˈdʌndənt] 冗余的\ndynamic[daɪˈnæmɪk]动态的\ncompressed [kəmˈprest] 压缩的\n指定行格式的语法 # CREATE TABLE 表名(列的信息) ROW_FORMAT=行格式名称\nALTER TABLE 表名 ROW_FORMATE=行格式名称\n如下，在数据库xiaohaizi下创建一个表\nCREATE TABLE record_format_demo( c1 VARCHAR(10), c2 VARCHAR(10) NOT NULL, c3 CHAR(10), c4 VARCHAR(10) ) CHARSET=ascii ROW_FORMAT=COMPACT; #回顾：ascii每个字符1字节即可表示，且只有空格标点数字字母不可见字符 #插入两条数据 INSERT INTO record_format_demo(c1,c2,c3,c4) VALUES(\u0026#39;aaaa\u0026#39;,\u0026#39;bbb\u0026#39;,\u0026#39;cc\u0026#39;,\u0026#39;d\u0026#39;),(\u0026#39;eeee\u0026#39;,\u0026#39;fff\u0026#39;,NULL,NULL); 查询\n#查询 mysql\u0026gt; SELECT * FROM record_format_demo; +------+-----+------+------+ | c1 | c2 | c3 | c4 | +------+-----+------+------+ | aaaa | bbb | cc | d | | eeee | fff | NULL | NULL | +------+-----+------+------+ 2 rows in set (0.01 sec) COMPACT行格式 # [kəmˈpækt]契约\n额外信息 # 包括变长字段长度列表、NULL值列表、记录头信息\n记录的真实数据 # REDUNDANT行格式 # [rɪˈdʌndənt] 冗余的 MySQL5.0之前使用的一种行格式（古老）\n如图\n下面主要和COMPACT行格式做比较\n字段长度偏移列表 # 记录了所有列 偏移，即不是直接记录，而是通过加减 同样是逆序，如第一条记录\n06 0C 13 17 1A 24 25，则\n第1列(RD_ROW_ID)：6字节\n第2列(DB_TRX_ID)：6字节 0C-06=6 第3列(DB_ROLL_POINTER)：7字节 13-0C=7 第4列(c1)：4字节\n第5列(c2)：3字节\n第6列(c3)：10字节\n第7列(c4)：1字节\n记录头信息 # 相比COMPACT行格式，多出了2个，少了一个\n没有了record_type这个属性 多了n_field和1byte_offs_flag这两个属性：\n#查询 mysql\u0026gt; SELECT * FROM record_format_demo; +------+-----+------+------+ | c1 | c2 | c3 | c4 | +------+-----+------+------+ | aaaa | bbb | cc | d | | eeee | fff | NULL | NULL | +------+-----+------+------+ 2 rows in set (0.01 sec) 第一条记录的头信息为：00 00 10 0F 00 BC\n即：00000000 00000000 00010000 00001111 00000000 1011 1100\n前面2字节都是0，即预留位1，预留位2，deleted_flag，min_rec_flag，n_owned都是0\nheap_no前面8位是0，再取5位：即 00000000 0001 0，即0x02\nn_field：000 0000111，即0x07\n1byte_offs_flag：0x01\nnext_record：00000000 1011 1100，即0xBC\n记录头信息中的1byte_offs_flag的值是怎么选择的 # 字段长度偏移列表存储的偏移量指的是每个列的值占用的空间在记录的真 实数据处结束的位置\n如上，0x06代表第一列(DB_ROW_ID)在真实数据的第6字节处结束；0x0C 代表第二列(DB_TRX_ID)在真实数据的第12字节处结束\u0026hellip;.\n讨论：每个列对应的偏移量可以使用1字节或2字节来存储，那么什么时候1什么时候2\n根据REDUNDANT行格式记录的真实数据占用的总大小来判断\n如果真实数据占用的字节数不大于127时，每个列对应的偏移量占用1字节**[注意，这里只用到了1字节的7位，即max=01111111]**\n如果大于127但不大于32767 （2^15-1，也就是15位的最大表示）时，使用2字节。\n如果超过32767，则本页中只保留前768字节和20字节的溢出页面地址（20字节还有别的信息）。这种情况下只是用2字节存储每个列对应的偏移量即可（127\u0026lt;768\u0026lt;=32767)\n在头信息中放置了一个1byte_offs_flag属性，值为1时表明使用1字节存储偏移量；值为0时表明使用2字节存储偏移量\nREDUNDANT行格式中NULL值的处理 # REDUNDANT行格式并没有NULL值列表\n将列对应的偏移量值的第一个比特位，作为是否为NULL的依据，也称之为NULL比特位 不论是1字节还是2字节，都要使用第1个比特位来标记该列值是否为NULL\n对于NULL列来说，该列的类型是否为变长类型决定了该列在记录的真实数据处的存储方式。\n分析第2条数据\n字段长度偏移列表-\u0026gt;按照列的顺序排放：06 0C 13 17 1A A4 A4\nc3=NULL，且c3类型-\u0026gt;CHAR(10) ==\u0026gt;真实数据部分占用10字节,0x00\nc3 原偏移量为36=32+4 = 00100100-\u0026gt;0x24，由于为NULL，所以首位(比特)为1，所以（真实）偏移量为10100100，0xA4\nc2偏移量为0x1A，则c2字节数为0x24-0x1A=36-26=10\n如果存储NULL值的字段为变长数据类型，则不在记录的真实数据部分占用任何存储空间\n所以c4的偏移量应该和c3相同，都是00100100，且由于是NULL，所以首位为1-\u0026gt;10100100,0xA4\n从结果往回推理，c4也是0xA4，和c3相同，说明c4和c3一样都是NULL\nCOMPACT行格式的记录占用的空间更少\nCHAR(m)列的存储格式 # COMPACT中，当定长类型CHAR(M)的字符集的每个字符占用字节不固定时，才会记录CHAR列的长度；而REDUNDANT行格式中，该列真实数据占用的存储空间大小，就是该字符集表示一个字符最多需要的字节数和M的乘积：utf8的CHAR(10)类型的列，真实数据占用存储空间大小始终为30字节；使用gbk字符集的CHAR(10)，始终20字节\n溢出列 # 溢出列 # #举例 mysql\u0026gt; CREATE TABLE off_page_demo( c VARCHAR(65532) ) CHARSET=ascii ROW_FORMAT=COMPACT; #插入一条数据 mysql\u0026gt; INSERT INTO off_page_demo(c) VALUES(REPEAT(\u0026#39;a\u0026#39;,65532)); Query OK, 1 row affected (0.06 sec) ascii字符集中1字符占用1字节，REPEAT(\u0026lsquo;a\u0026rsquo;,65532)生成一个把字符\u0026rsquo;a\u0026rsquo;重复65532次数的字符串\n1页有16kb=1024*16=16384字节，65532字节远超1页大小\nCOMPACT和REDUNDANT行格式中，对于存储空间占用特别多的列，真实数据处只会存储该列一部分数据，剩余数据存储在几个其他的页中，在记录的真实数据处用20字节存储指向这些页的地址（当然，这20字节还包括分散在其他页面中的数据所占用的字节数）\n原书加了括号里的话，不是很理解，我的理解是：这20字节指向的页中，包括了溢出的那部分数据\n如上，如果列数据非常大，只会存储该列前768字节的数据以及一个指向其他页的地址（存疑，应该不止一个，有时候1个是放不下所有溢出页数据的吧？）\n简化：\n例子中列c的数据需要使用溢出页来存储，我们把这个列称为溢出列，不止VARCHAR(M)，TEXT和BLOB也可能成为溢出列\n产生溢出列的临界点 # MySQL中规定一个页至少存放2条记录\n16KB=16384字节\n每个页除了记录，还有额外信息，这些额外信息需要132字节。\n每个记录需要27字节，包括\n针对下面的表\nmysql\u0026gt; CREATE TABLE off_page_demo( c VARCHAR(65532) ) CHARSET=ascii ROW_FORMAT=COMPACT;\n注意，是COMPACT行格式\n对于每一行记录 存储真实数据长度（2字节）\n存储列是否为NULL值（1字节）\n5字节大小的头信息\n6字节的row_id列\n6字节的row_id列\n7字节的roll_pointer列\n132+2*(27+n) \u0026lt;16384\n至于为社么不能等于，这是MySQL设计时规定的，未知。\n正常记录的页和溢出页是两种不同的页，没有规定一个溢出页页面中至少存放两条记录\n对于该表，得出的解是n\u0026lt;8099，也就是如果一个列存储的数据小于8099，就不会成为溢出页\n结论 # 如果一条记录的某个列中存储的数据占用字节数非常多，导致一个页没有办法存储两条记录，该列就可能会成为溢出列\nDYNAMIC行格式和COMPRESSED行格式 # 这两个与COMPACT行记录挺像，对于处理溢出列的数据有分歧：\n他们不会在记录真实处存储真实数据的前768字节，而是把该列所有真实数据都存储到溢出页，只在真实记录处存储20字节（指向溢出页的地址）。COMPRESSED行格式不同于DYNAMIC行格式的一点：COMPRESSED行格式会采用压缩算法对页面进行压缩\nMySQL5.7默认使用DYNAMIC行记录\n总结 # REDUNDANT是一个比较原始的行格式，较为紧凑；而COMPACT、DYNAMIC以及COMPRESSED行格式是较新的行格式，它们是紧凑的（占用存储空间更少）\n"},{"id":48,"href":"/zh/docs/technology/MySQL/how_mysql_run/03/","title":"03字符集和比较规则","section":"_MySQL是怎样运行的_","content":" 学习《MySQL是怎样运行的》，感谢作者！\n字符集 # 把哪些字符映射成二进制数据：字符范围\n怎么映射：字符-\u0026gt;二进制数据，编码；二进制-\u0026gt;字符，解码\n字符集：某个字符范围的编码规则\n同一种字符集可以有多种比较规则\n重要的字符集 # ASCAII字符集：128个，包括空格标点数字大小写及不可见字符，使用一个字节编码\nISO 8859-1字符集：256个，ASCAII基础扩充128个西欧常用字符（包括德法），使用1个字节，别名Latin1\nGB2312字符集：收录部分汉字，兼容ASCAII字符集，如果字符在ASCAII字符集中则采用1字节，否则两字节。即变长编码方式\n区分某个字节，代表一个单独字符，还是某个字符的一部分。\n比如0xB0AE75，由于是16进制，所有两个代表1个字节。所以这里有三个字节，其中最后那个字节为7*16+5=117 \u0026lt; 127 所以代表一个单独字符。而AE=10 * 16 +15=175 \u0026gt;127 ，所以是某个字符的一部分\nGBK字符集：对GB2312字符集扩充，编码方式兼容GB2312\nUTF-8字符集：几乎收录所有字符，且不断扩充，兼容ASCAII字符集。变长：采用14字节\nL-\u0026gt;0x4C 1字节，啊-\u0026gt;0xE5958A，两字节\nUTF-8是Unicode字符集的一种编码方案，Unicode字符集有三种方案：UTF-8(14字节编码一个字符)，UTF-16(2或4字节编码一个字符)，UTF-32(4字节编码一个字符)\n对于**“我”**，ASCLL中没有，UTF-8中采用3字节编码，GB22312采用2字节编码\nMySQL中支持的字符集和比较规则 # MySQL中，区分utf8mb3和utf8mb4，前者只是用13字节表示字符；后者使用14字节表示字符。MySQL中，utf8代表utf8mb3。\n#查看当前MySQL支持的字符集(注意，是字符集，名称都是小写) #Default collation 默认比较规则 mysql\u0026gt; SHOW CHARSET; +----------+---------------------------------+---------------------+--------+ | Charset | Description | Default collation | Maxlen | +----------+---------------------------------+---------------------+--------+ | big5 | Big5 Traditional Chinese | big5_chinese_ci | 2 | | dec8 | DEC West European | dec8_swedish_ci | 1 | | cp850 | DOS West European | cp850_general_ci | 1 | | hp8 | HP West European | hp8_english_ci | 1 | | koi8r | KOI8-R Relcom Russian | koi8r_general_ci | 1 | | latin1 | cp1252 West European | latin1_swedish_ci | 1 | \u0026lt;--- | latin2 | ISO 8859-2 Central European | latin2_general_ci | 1 | \u0026lt;--- | swe7 | 7bit Swedish | swe7_swedish_ci | 1 | | ascii | US ASCII | ascii_general_ci | 1 | \u0026lt;--- | ujis | EUC-JP Japanese | ujis_japanese_ci | 3 | | sjis | Shift-JIS Japanese | sjis_japanese_ci | 2 | | hebrew | ISO 8859-8 Hebrew | hebrew_general_ci | 1 | | tis620 | TIS620 Thai | tis620_thai_ci | 1 | | euckr | EUC-KR Korean | euckr_korean_ci | 2 | | koi8u | KOI8-U Ukrainian | koi8u_general_ci | 1 | | gb2312 | GB2312 Simplified Chinese | gb2312_chinese_ci | 2 | \u0026lt;--- | greek | ISO 8859-7 Greek | greek_general_ci | 1 | | cp1250 | Windows Central European | cp1250_general_ci | 1 | | gbk | GBK Simplified Chinese | gbk_chinese_ci | 2 | \u0026lt;--- | latin5 | ISO 8859-9 Turkish | latin5_turkish_ci | 1 | | armscii8 | ARMSCII-8 Armenian | armscii8_general_ci | 1 | | utf8 | UTF-8 Unicode | utf8_general_ci | 3 | \u0026lt;--- | ucs2 | UCS-2 Unicode | ucs2_general_ci | 2 | | cp866 | DOS Russian | cp866_general_ci | 1 | | keybcs2 | DOS Kamenicky Czech-Slovak | keybcs2_general_ci | 1 | | macce | Mac Central European | macce_general_ci | 1 | | macroman | Mac West European | macroman_general_ci | 1 | | cp852 | DOS Central European | cp852_general_ci | 1 | | latin7 | ISO 8859-13 Baltic | latin7_general_ci | 1 | | utf8mb4 | UTF-8 Unicode | utf8mb4_general_ci | 4 | \u0026lt;--- | cp1251 | Windows Cyrillic | cp1251_general_ci | 1 | | utf16 | UTF-16 Unicode | utf16_general_ci | 4 | \u0026lt;--- | utf16le | UTF-16LE Unicode | utf16le_general_ci | 4 | | cp1256 | Windows Arabic | cp1256_general_ci | 1 | | cp1257 | Windows Baltic | cp1257_general_ci | 1 | | utf32 | UTF-32 Unicode | utf32_general_ci | 4 | \u0026lt;--- | binary | Binary pseudo charset | binary | 1 | | geostd8 | GEOSTD8 Georgian | geostd8_general_ci | 1 | | cp932 | SJIS for Windows Japanese | cp932_japanese_ci | 2 | | eucjpms | UJIS for Windows Japanese | eucjpms_japanese_ci | 3 | | gb18030 | China National Standard GB18030 | gb18030_chinese_ci | 4 | +----------+---------------------------------+---------------------+--------+ 41 rows in set (0.00 sec) 字符集的比较规则（这里先看utf8的）\nmysql\u0026gt; SHOW COLLATION LIKE \u0026#39;utf8\\_%\u0026#39;; +--------------------------+---------+-----+---------+----------+---------+ | Collation | Charset | Id | Default | Compiled | Sortlen | +--------------------------+---------+-----+---------+----------+---------+ | utf8_general_ci | utf8 | 33 | Yes | Yes | 1 | | utf8_bin | utf8 | 83 | | Yes | 1 | | utf8_unicode_ci | utf8 | 192 | | Yes | 8 | | utf8_icelandic_ci | utf8 | 193 | | Yes | 8 | | utf8_latvian_ci | utf8 | 194 | | Yes | 8 | | utf8_romanian_ci | utf8 | 195 | | Yes | 8 | | utf8_slovenian_ci | utf8 | 196 | | Yes | 8 | | utf8_polish_ci | utf8 | 197 | | Yes | 8 | | utf8_estonian_ci | utf8 | 198 | | Yes | 8 | | utf8_spanish_ci | utf8 | 199 | | Yes | 8 | | utf8_swedish_ci | utf8 | 200 | | Yes | 8 | | utf8_turkish_ci | utf8 | 201 | | Yes | 8 | | utf8_czech_ci | utf8 | 202 | | Yes | 8 | | utf8_danish_ci | utf8 | 203 | | Yes | 8 | | utf8_lithuanian_ci | utf8 | 204 | | Yes | 8 | | utf8_slovak_ci | utf8 | 205 | | Yes | 8 | | utf8_spanish2_ci | utf8 | 206 | | Yes | 8 | | utf8_roman_ci | utf8 | 207 | | Yes | 8 | | utf8_persian_ci | utf8 | 208 | | Yes | 8 | | utf8_esperanto_ci | utf8 | 209 | | Yes | 8 | | utf8_hungarian_ci | utf8 | 210 | | Yes | 8 | | utf8_sinhala_ci | utf8 | 211 | | Yes | 8 | | utf8_german2_ci | utf8 | 212 | | Yes | 8 | | utf8_croatian_ci | utf8 | 213 | | Yes | 8 | | utf8_unicode_520_ci | utf8 | 214 | | Yes | 8 | | utf8_vietnamese_ci | utf8 | 215 | | Yes | 8 | | utf8_general_mysql500_ci | utf8 | 223 | | Yes | 1 | +--------------------------+---------+-----+---------+----------+---------+ 27 rows in set (0.00 sec) utf8_polish_ci 波兰语比较规则； utf8_spanish_ci班牙语的比较规则；utf8_general_ci一种通用的比较规则 （utf8的默认比较规则）\n一些后缀的解释：\n字符集和比较规则的应用 # MySQL有4个级别的字符集和比较规则：服务器级别、数据库级别、表级别、列级别\n服务器级别 # mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;character_set_server\u0026#39;; +----------------------+--------+ | Variable_name | Value | +----------------------+--------+ | character_set_server | latin1 | +----------------------+--------+ 1 row in set (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;collation_server\u0026#39;; +------------------+-------------------+ | Variable_name | Value | +------------------+-------------------+ | collation_server | latin1_swedish_ci | +------------------+------------------- 1 row in set (0.00 sec) #centos7(英语语言)默认情况下如上 #所以比较时，是不区分大小写的 mysql\u0026gt; select * from test; +-------+ | name | +-------+ | hello | | Hello | +-------+ 2 rows in set (0.00 sec) mysql\u0026gt; select * from test where name = \u0026#39;hello\u0026#39;; +-------+ | name | +-------+ | hello | | Hello | +-------+ 修改为utf8\nvim /etc/my.cnf #新增 [server] character_set_server=utf8 collation_server=utf8_general_ci #重启并查看 systemctl restart mysqld; mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;character_set_server\u0026#39;; +----------------------+-------+ | Variable_name | Value | +----------------------+-------+ | character_set_server | utf8 | +----------------------+-------+ 1 row in set (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;character_set_server\u0026#39;;^C mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;collation_server\u0026#39;; +------------------+-----------------+ | Variable_name | Value | +------------------+-----------------+ | collation_server | utf8_general_ci | +------------------+-----------------+ 1 row in set (0.00 sec) 数据库级别 # #创建数据库并指定字符集及比较规则 #如果不设置，则使用上级（服务器级别）的字符集和比较规则作为数据库的字符集和比较规则 CREATE DATABASE db_test CHARACTER SET gb2312 COLLATE gb2312_chinese_ci; #此时切换到db_test数据库 #再查看，发现变了 mysql\u0026gt; use db_test; Database changed mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;character_set_database\u0026#39;; +------------------------+--------+ | Variable_name | Value | +------------------------+--------+ | character_set_database | gb2312 | +------------------------+--------+ 1 row in set (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;collation_database\u0026#39;; +--------------------+-------------------+ | Variable_name | Value | +--------------------+-------------------+ | collation_database | gb2312_chinese_ci | +--------------------+-------------------+ 1 row in set (0.00 sec) 表级别 # mysql\u0026gt; CREATE TABLE t(col VARCHAR(10)) CHARACTER SET utf8 COLLATE utf8_general_ci; Query OK, 0 rows affected (0.06 sec) mysql\u0026gt; SHOW CREATE TABLE t; +-------+------------------------------------------------------------------------------------------+ | Table | Create Table | +-------+------------------------------------------------------------------------------------------+ | t | CREATE TABLE `t` ( `col` varchar(10) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8 | +-------+------------------------------------------------------------------------------------------+ 1 row in set (0.00 sec) 如果不指定，那么将继承所在数据库的字符集和比较规则\n列级别 # mysql\u0026gt; ALTER TABLE t MODIFY col VARCHAR(10) CHARACTER SET gbk COLLATE gbk_chinese_ci; ## 修改为字符集gbk和对应的排序规则，如果不指定，则使用表的字符集及表的排序规则 其他 # 如果仅修改字符集，不修改比较规则，则比较规则会设置为默认该字符集的比较规则；\n如果仅修改比较规则，则字符集会设置为该比较规则对应的字符集\n#查看当前服务器对应规则 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;character_set_server\u0026#39;; +----------------------+-------+ | Variable_name | Value | +----------------------+-------+ | character_set_server | utf8 | +----------------------+-------+ 1 row in set (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;collation_server\u0026#39;; +------------------+-----------------+ | Variable_name | Value | +------------------+-----------------+ | collation_server | utf8_general_ci | +------------------+-----------------+ 1 row in set (0.00 sec) #只修改字符集 mysql\u0026gt; SET character_set_server = gb2312; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;collation_server\u0026#39;; +------------------+-------------------+ | Variable_name | Value | +------------------+-------------------+ | collation_server | gb2312_chinese_ci | +------------------+-------------------+ 1 row in set (0.00 sec) #仅修改比较规则 mysql\u0026gt; SET collation_server = utf8_general_ci; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;character_set_server\u0026#39;; +----------------------+-------+ | Variable_name | Value | +----------------------+-------+ | character_set_server | utf8 | +----------------------+-------+ 1 row in set (0.00 sec) 根据各个列的字符集和比较规则是什么，从而根据这个列的类型来确认每个列存储的实际数据所占用的存储空间大小\n#例子 mysql\u0026gt; describe t; +-------+-------------+------+-----+---------+-------+ | Field | Type | Null | Key | Default | Extra | +-------+-------------+------+-----+---------+-------+ | col | varchar(10) | YES | | NULL | | +-------+-------------+------+-----+---------+-------+ 1 row in set (0.01 sec) mysql\u0026gt; INSERT INTO t(col) VALUES(\u0026#39;我我\u0026#39;); Query OK, 1 row affected (0.01 sec) ## 如果列col使用的字符集是gbk，则每个字符占用2字节，两个字符占用4字节；如果列col使用字符集为utf8，则两个字符实际占用的存储空间为6字节 客户端和服务端通信过程中使用的字符集 # 编码和解码使用的字符集不一样 # 字符集转换的概念 # “我\u0026quot; utf8\u0026ndash;\u0026gt; 编码成0xE68891\n接收到0xE68891后，对它解码，然后又按照GBK字符集编码，编码后是0xCED2。过程称为**”字符集的转换“**\nMySQL中字符集转换过程 # 从用户角度，客户发送请求和服务器返回响应都是字符串；从机器角度，客户端发送的请求和服务端返回的响应本质上就是一个字节序列，这个过程经历了多次的字符集转换\n客户端发送请求 # #查看当前系统使用的字符集 #linux #以第一个优先，没有依次往下取 ▶ echo $LC_ALL root@centos7101:~ ▶ echo $LC_CTYPE root@centos7101:~ ▶ echo $LANG zh_CN.UTF-8 #window C:\\Users\\ly\u0026gt;chcp 活动代码页: 936 -\u0026gt; GBK windows中，使用mysql客户端时，可以使用mysql --default-character-set=utf8，客户端将以UTF-8字符集对请求的字符串进行编码\n服务端接收请求 # 服务端接收到的应该是一个字节序列，是系统变量character_set_client代表的字符集进行编码后的字节序列\n每个客户端与服务端建立连接后，服务器会为该客户端维护一个独立的character_set_client变量，是SESSION级别的 客户端在编码请求字符串时实际使用的字符集，与服务器在收到一个字节序列后认为该序列采用的编码字符集，是两个独立的字符集。要尽量保证这两个字符集是一致的 例子：如果客户端用的UTF8编码\u0026quot;我\u0026quot;为0xE68891，并发给服务端，而服务端的character_set_client=latin1，则服务端会理解为\n如果character_set_client对应的字符集不能解释请求的字节序列，那么服务器发生警告\nmysql\u0026gt; SET character_set_client =ascii; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; select \u0026#39;我\u0026#39;; #utf8将它编码成了0xE68891发给服务端，而服务端以ASCII字符集解码 +-----+ | ??? | +-----+ | ??? | +-----+ 1 row in set, 1 warning (0.00 sec) mysql\u0026gt; SHOW WARNINGS\\G *************************** 1. row *************************** Level: Warning Code: 1300 Message: Invalid ascii character string: \u0026#39;\\xE6\\x88\\x91\u0026#39; 1 row in set (0.00 sec) 服务器处理请求 # 服务端会将请求的字节序列当作采用character_set_client对应的字符集进行编码，不过真正处理请求时会将其转换为SESSION级别的系统变量character_set_connection对应的字符集进行编码的字符序列\n假设character_set_client=utf8，character_set_connection=gbk，则对于\u0026quot;我\u0026quot;\u0026ndash;\u0026gt;0xE68891\u0026ndash;\u0026gt;0xCED2\n情形1 # 例子： SELECT 'a' = 'A'\n#默认情况 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;character_connection\u0026#39;; Empty set (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;collation_connection\u0026#39;; +----------------------+-----------------+ | Variable_name | Value | +----------------------+-----------------+ | collation_connection | utf8_general_ci | +----------------------+-----------------+ 1 row in set (0.00 sec) mysql\u0026gt; SELECT \u0026#39;A\u0026#39;=\u0026#39;a\u0026#39;; +---------+ | \u0026#39;A\u0026#39;=\u0026#39;a\u0026#39; | +---------+ | 1 | +---------+ 1 row in set (0.00 sec) 其他情况：\nmysql\u0026gt; SET character_set_connection = gbk; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SET collation_connection=gbk_chinese_ci; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SELECT \u0026#39;a\u0026#39;=\u0026#39;A\u0026#39;; +---------+ | \u0026#39;a\u0026#39;=\u0026#39;A\u0026#39; | +---------+ | 1 | +---------+ 1 row in set (0.00 sec) 另一种情况：\nmysql\u0026gt; SET character_set_connection = gbk; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SET collation_connection=gbk_bin; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SELECT \u0026#39;a\u0026#39;=\u0026#39;A\u0026#39;; +---------+ | \u0026#39;a\u0026#39;=\u0026#39;A\u0026#39; | +---------+ | 0 | +---------+ 1 row in set (0.00 sec) gbk_bin 简体中文, 二进制 gbk_chinese_ci 简体中文, 不区分大小写\n情形2 # #创建一个表 CREATE TABLE tt(c VARCHAR(100)) ENGINE=INNODB CHARSET=utf8; #先改回来 mysql\u0026gt; SET character_set_connection = utf8; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;collation_connection\u0026#39;; +----------------------+-----------------+ | Variable_name | Value | +----------------------+-----------------+ | collation_connection | utf8_general_ci | +----------------------+-----------------+ 1 row in set (0.00 sec) #插入一条记录 INSERT INTO tt(c) VALUES(\u0026#39;我\u0026#39;); 当前数据库、表、列使用的是utf8，现在把collection改为gbk：\nmysql\u0026gt; SET character_set_connection = gbk; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SET collation_connection=gbk_chinese_ci; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SELECT * FROM tt WHERE c=\u0026#39;我\u0026#39;; +------+ | c | +------+ | 我 | +------+ 1 row in set (0.00 sec) 此时SELECT * FROM tt WHERE c='我';中，\u0026lsquo;我\u0026rsquo;是使用gbk字符集进行编码的，比较规则是gbk_chinese_ci；而列c使用utf8字符集编码，比较规则为utf8_general_ci。这种情况下列的字符集和排序规则的优先级更高，会将gbk字符集转换成utf8字符集，然后使用列c的比较规则utf8_general_ci进行比较\n服务器生成响应 # 继续以最近的上面例子为例SELECT * FROM tt,是否是直接将0xE68891读出来发送到客户端呢？不是的，取决于SESSION级别的系统变量character_set_result的值\nSET character_set_results=gbk; 服务器会将字符串\u0026rsquo;我\u0026rsquo;从utf8字符集编码的0xE68891转换为character_set_results系统变量对应的字符集编码后的字节序列，再发给客户端\n此时传给客户端的响应中，字符串\u0026rsquo;我\u0026rsquo;对应的就是字节序列0xCED2(\u0026lsquo;我\u0026rsquo;的gbk编码字节序列)\n总结 # 每个客户端在服务器建立连接后，服务器都会为这个连接维护这3个变量\n每个MySQL客户端都维护着一个客户端默认字符集，客户端在启动时会自动检测所在系统(是客户端所在系统)当前使用的字符集，并按照一定规则映射成（最接近）MySQL支持的字符集，然后将该字符集作为客户端默认的字符集。\n如果启动MySQL客户端时设置了default-character-set 则忽略操作系统当前使用的字符集，直接将default-character-set启动选项中指定的值作为客户端默认字符集\n连接服务器时，客户端将默认的字符集信息与用户密码等发送给服务端，服务端在收到后会将character_set_client、character_set_connection、character_set_results这3个系统变量的值初始化为客户端的默认字符集\n客户端连接到服务器之后，可以使用SET分别修改character_set_client、character_set_connection、character_set_results这3个系统变量的值(或者使用SET NAMES charset_name一次性修改)\n不会改变客户端在编码请求字符串时使用的字符集，也不会修改客户端的默认字符集\n客户端接收到请求 # 客户端收到的响应其实也是一个字节序列\n对于类UNIX系统，收到的字节序列相当于写到黑框框中，再由黑框框将这个字节序列解释为人类能看懂的字符（用操作系统当前使用的字符集来解释）；对于Windows，客户端使用客户端的默认字符集来解释\n也就是说，如果在linux下指定的default-character-set和系统不一致，就会导致乱码\n整个过程，五件事：\n客户端发送的请求字节序列是采用哪种字符集进行编码的 [由客户端启动选项\u0026ndash;default-character-set/其次是系统默认的(linux可能是utf-8/windows可能是gbk)] 服务端接收到请求字节序列后会认为它是采用哪种字符集进行编码的[由character_set_client决定，而character_set_client[还有character_set_connection、character_set_results这2个系统变量]的值初始化为客户端的默认字符集，也就是上面1中的] 服务器在运行过程中会把请求的字符序列转换为以哪种字符集编码的字节序列[character_set_connection] 服务器在向客户端返回字节序列时，是采用哪种字符集进行编码的[character_set_results] 上面的character_set_clien、character_set_connection、character_set_results这3个系统变量]的值在客户端连接到服务端之后，都是可以修改的，且都是SESSION级别的 客户端在接收到响应字节序列后，是怎么把他们写到黑框框中的[windows中由default-character-set/其次是系统默认] 比较规则的应用 # 通常用来比较大小和排序\n例子：\nmysql\u0026gt; CREATE TABLE t(col VARCHAR(100)) ENGINE=INNODB CHARSET=gbk; Query OK, 0 rows affected (0.02 sec) mysql\u0026gt; INSERT INTO t(col) VALUES(\u0026#39;a\u0026#39;),(\u0026#39;b\u0026#39;),(\u0026#39;A\u0026#39;),(\u0026#39;B\u0026#39;); Query OK, 4 rows affected (0.01 sec) Records: 4 Duplicates: 0 Warnings: 0 mysql\u0026gt; SELECT * FROM t ORDER BY col; +------+ | col | +------+ | a | | A | | b | | B | | 我 | +------+ 5 rows in set (0.00 sec) # 如上，排序规则gbk_chinese_ci是不区分大小写的 # 修改为gbk_bin; mysql\u0026gt; ALTER TABLE t MODIFY col VARCHAR(10) COLLATE gbk_bin; Query OK, 5 rows affected (0.06 sec) Records: 5 Duplicates: 0 Warnings: 0 mysql\u0026gt; SELECT * FROM t ORDER BY col; +------+ | col | +------+ | A | | B | | a | | b | | 我 | +------+ 5 rows in set (0.00 sec) 解释：\n列col各个字符在使用gbk字符集编码后对应的数字如下：\n\u0026lsquo;A\u0026rsquo; -\u0026gt; 65\n\u0026lsquo;B\u0026rsquo;-\u0026gt;66\n\u0026lsquo;a\u0026rsquo;-\u0026gt;97\n\u0026lsquo;b\u0026rsquo;-\u0026gt;98\n\u0026lsquo;我\u0026rsquo;-\u0026gt;52946\n"},{"id":49,"href":"/zh/docs/technology/MySQL/how_mysql_run/02/","title":"02启动选项和系统变量","section":"_MySQL是怎样运行的_","content":" 学习《MySQL是怎样运行的》，感谢作者！\n启动选项和配置文件 # 在程序启动时指定的设置项，也称之为启动选项startup option（可以在命令行中/配置文件中 指定）\n由于在centos7中使用systemctl start mysqld启动mysql，所以好像没法用命令行指定启动选项了\n程序(可能有些程序新版本已经没有了)的对应类别和能读取的组：\n这里讲配置文件的方式设置启动选项：\n#添加配置 vim /etc/my.cnf [server] skip-networking #禁止tcp网络连接 default-storage-engine=MyISAM #建表默认使用M有ISAM存储引擎 #效果 ▶ mysql -h127.0.0.1 -uroot -p Enter password: ERROR 2003 (HY000): Can\u0026#39;t connect to MySQL server on \u0026#39;127.0.0.1\u0026#39; (111) #去除tcp网络连接限制后新建一个表 ▶ mysql -h127.0.0.1 -uroot -p #可以连接上 mysql\u0026gt; create table default_storage_engine_demo(i int); Query OK, 0 rows affected (0.01 sec) mysql\u0026gt; show create table default_storage_engine_demo; +-----------------------------+----------------------------------------------------------------------------------------------------------------+ | Table | Create Table | +-----------------------------+----------------------------------------------------------------------------------------------------------------+ | default_storage_engine_demo | CREATE TABLE `default_storage_engine_demo` ( `i` int(11) DEFAULT NULL ) ENGINE=MyISAM DEFAULT CHARSET=latin1 | 如果多个配置文件都配置了某个选项，如/etc/my.cnf /etc/mysql/my.cnf /usr/etc/my.cnf ~/.my.cnf都配置了，则以最后一个配置的为主\n如果同一个配置文件，比如[server]组和[mysqld]组都出现了default-storage-engine配置，则以后出现的组中的配置为准\n如果一个启动选项既在命令行中出现，又在配置文件中配置，则以命令行中的为准\n系统变量 # 查看系统变量\nmysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;default_storage_engine\u0026#39;; +------------------------+--------+ | Variable_name | Value | +------------------------+--------+ | default_storage_engine | InnoDB | +------------------------+--------+ 1 row in set (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;default%\u0026#39;; +-------------------------------+-----------------------+ | Variable_name | Value | +-------------------------------+-----------------------+ | default_authentication_plugin | mysql_native_password | | default_password_lifetime | 0 | | default_storage_engine | InnoDB | | default_tmp_storage_engine | InnoDB | | default_week_format | 0 | +-------------------------------+-----------------------+ 5 rows in set (0.00 sec) mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;max_connections\u0026#39;; +-----------------+-------+ | Variable_name | Value | +-----------------+-------+ | max_connections | 151 | +-----------------+-------+ 1 row in set (0.00 sec) 大部分系统变量，可以在服务器程序运行过程中动态修改而无须停止并重启服务器\n不同作用范围的系统变量\nGLOBAL（全局范围）：影响服务器的整体操作\nSESSION（会话范围）：影响某个客户端连接的操作\n让之后新连接到服务器的客户端都用MyISAM作为默认的存储引擎\n#不会对这之前已连接的客户端产生影响 SET GLOBAL default_storage_engine=MyISAM; #systemctl restart mysqld时候，该配置就失效了 只对本客户端使用\nSET SESSION default_storage_engine=MyISAM;#或 SET default_storage_engine=MyISAM; 查看系统变量\nSHOW [GLOBAL|SESSION] VARIABLES [LIKE \u0026#39;匹配的模式\u0026#39;]; 状态变量 # 状态变量用来显示服务器程序运行状态\n状态变量也分GLOBAL|SESSION\nSHOW [GLOBAL|SESSION] STATUS [LIKE 匹配的模式];\nmysql\u0026gt; SHOW STATUS LIKE \u0026#39;thread%\u0026#39;; +-------------------+-------+ | Variable_name | Value | +-------------------+-------+ | Threads_cached | 0 | | Threads_connected | 2 | | Threads_created | 2 | | Threads_running | 1 | +-------------------+-------+ "},{"id":50,"href":"/zh/docs/technology/MySQL/how_mysql_run/01/","title":"01初识MySQL","section":"_MySQL是怎样运行的_","content":" 学习《MySQL是怎样运行的》，感谢作者！\n原文 # 下载与安装 # 环境Centos7\n添加MySQL5.7仓库\nsudo rpm -ivh https://dev.mysql.com/get/mysql57-community-release-el7-11.noarch.rpm 解决证书问题\nrpm --import https://repo.mysql.com/RPM-GPG-KEY-mysql-2022 查看是否添加成功\nsudo yum repolist all | grep mysql | grep 启用 mysql-connectors-community/x86_64 MySQL Connectors Community 启用: 213 mysql-tools-community/x86_64 MySQL Tools Community 启用: 96 mysql57-community/x86_64 MySQL 5.7 Community Server 启用: 642 MySQL安装\nsudo yum -y install mysql-community-server 运行与密码修改 # Centos7中安装目录查看，在/usr/bin中，与Max有所不同\nwhereis mysql mysql: /usr/bin/mysql /usr/lib64/mysql /usr/share/mysql /usr/share/man/man1/mysql.1.gz ls /usr/bin |grep mysql mysql mysqladmin mysqlbinlog mysqlcheck mysql_config_editor mysqld_pre_systemd mysqldump mysqldumpslow mysqlimport mysql_install_db mysql_plugin mysqlpump mysql_secure_installation mysqlshow mysqlslap mysql_ssl_rsa_setup mysql_tzinfo_to_sql mysql_upgrade 添加mysqld目录到环境变量中（这里可省略，因为mysqld默认在/usr/bin中了\n启动MySQL(和书上说的启动方式有点不一样，查资料得知，从5.7.6起，不再支持mysql_safe的启动方式)\n# 启动MySQL root@centos7101:~ ▶ systemctl start mysqld # 查看MySQL状态 root@centos7101:~ ▶ systemctl status mysqld ● mysqld.service - MySQL Server Loaded: loaded (/usr/lib/systemd/system/mysqld.service; enabled; vendor preset: disabled) Active: active (running) since 一 2023-04-17 11:43:42 CST; 19s ago Docs: man:mysqld(8) http://dev.mysql.com/doc/refman/en/using-systemd.html Main PID: 2182 (mysqld) CGroup: /system.slice/mysqld.service └─2182 /usr/sbin/mysqld --daemonize --pid-file=/var/run/mysqld/mysqld.pid 4月 17 11:43:37 centos7101 systemd[1]: Starting MySQL Server... 4月 17 11:43:42 centos7101 systemd[1]: Started MySQL Server. # 设置为开机启动 root@centos7101:~ ▶ systemctl enable mysqld 查看MySQL默认密码\ncat /var/log/mysqld.log |grep -i \u0026#39;temporary password\u0026#39; 2023-04-17T03:43:38.995935Z 1 [Note] A temporary password is generated for root@localhost: ampddi9+fpyQ 连接\nmysql -uroot -p123456 #或者 mysql -uroot -p #或者 mysql -hlocalhost -uroot -p123456 为了方便起见，修改密码为123456\n# 修改密码强度 set global validate_password_policy=LOW; #修改密码长度 set global validate_password_length=6; #修改密码 ALTER USER \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; #刷新权限 flush privileges; 退出\nquit #或者 exit #或者 \\q 客户端与服务端连接过程 # 采用TCP作为服务端和客户端之间的网络通信协议\n远程连接前提\n#添加一个远程用户 CREATE USER \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;123456.\u0026#39;; grant all on *.* to \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#34;123456.\u0026#34; with grant option; #修改用户密码 SET PASSWORD FOR \u0026#39;root\u0026#39;@\u0026#39;host\u0026#39; = password(\u0026#39;123456.\u0026#39;); 端口号修改与远程连接\n#修改MySQL启动的端口 vim /etc/my.cnf [mysqld] port=33062 #新增该行即可 #重启 systemctl restart mysqld #查看状态 systemctl status mysqld #查看服务是否启动 netstat -lntup |grep mysql tcp6 0 0 :::33062 :::* LISTEN 4612/mysqld #远程连接 mysql -hnode2 -uroot -P33062 -p 处理客户端请求\n常用存储引擎：Innodb和MyISAM\n查看当前服务器支持的存储引擎\n只有InnoDB是支持事务的且支持分布式事务、部分回滚\n存储引擎是负责对表中数据进行读取和写入的\n-- 创建表时指定存储引擎 CREATE TABLE engine_demo_table(i int) ENGINE = MyISAM -- 查看建表语句 mysql\u0026gt; SHOW CREATE TABLE engine_demo_table \\G *************************** 1. row *************************** Table: engine_demo_table Create Table: CREATE TABLE `engine_demo_table` ( `i` int(11) DEFAULT NULL ) ENGINE=MyISAM DEFAULT CHARSET=latin1 1 row in set (0.00 sec) -- 修改建表时指定的存储引擎 ALTER TABLE engine_demo_table ENGINE=InnoDB -- 修改编码 ALTER TABLE engine_demo_table CHARSET=UTF8 "},{"id":51,"href":"/zh/docs/technology/Redis/redis-cluster/","title":"redis集群搭建","section":"Redis","content":" 转载自https://www.cnblogs.com/Yunya-Cnblogs/p/14608937.html（添加小部分笔记）感谢作者!\n部分参考自 https://www.cnblogs.com/ysocean/p/12328088.html\n基本准备 # 架构 # 采用Centos7，Redis版本为6.2，架构如下：\nhosts修改 # vim /etc/hosts #添加 192.168.1.101 node1 192.168.1.102 node2 192.168.1.103 node3 集群准备 # 对每个节点 # 下载redis并解压到 /usr/local/redis-cluster中\ncd /usr/local mkdir redis-cluster tar -zxvf redis* -C /usr/local/redis* 进入redis根目录\nmake make install 安装完毕\nhosts修改\nvim /etc/hosts #添加 192.168.1.101 node1 192.168.1.102 node2 192.168.1.103 node3 配置文件修改(6个节点中的每一个) # 创建多级目录\nmkdir -p /usr/local/redis_cluster/redis_63{79,80}/{conf,pid,logs} 编写配置文件\nvim /usr/local/redis_cluster/redis_6379/conf/redis.conf # 命令行状态下输入 :%d 回车，清空文件 # 再输入 :set paste 处理多出的行带#的问题 # 再输入i ####内容##### # 快速修改：:%s/6379/6380/g # 守护进行模式启动 daemonize yes # 设置数据库数量，默认数据库为0 databases 16 # 绑定地址，需要修改 # bind 192.168.1.101 bind node1 # 绑定端口，需要修改 port 6379 # pid文件存储位置，文件名需要修改 pidfile /usr/local/redis_cluster/redis_6379/pid/redis_6379.pid # log文件存储位置，文件名需要修改 logfile /usr/local/redis_cluster/redis_6379/logs/redis_6379.log # RDB快照备份文件名，文件名需要修改 dbfilename redis_6379.rdb # 本地数据库存储目录，需要修改 dir /usr/local/redis_cluster/redis_6379 # 集群相关配置 # 是否以集群模式启动 cluster-enabled yes # 集群节点回应最长时间，超过该时间被认为下线 cluster-node-timeout 15000 # 生成的集群节点配置文件名，文件名需要修改 cluster-config-file nodes_6379.conf 复制粘贴配置文件\ncp /usr/local/redis_cluster/redis_6379/conf/redis.conf /usr/local/redis_cluster/redis_6380/conf/redis.conf vim /usr/local/redis_cluster/redis_6380/conf/redis.conf #命令行模式下 :%s/6379/6380/g 查看文件夹当前情况\n运行 # 查看端口是否运行\nnetstat -lntup | grep 6379 运行\nredis-server /usr/local/redis_cluster/redis_6379/conf/redis.conf \u0026amp; redis-server /usr/local/redis_cluster/redis_6380/conf/redis.conf \u0026amp; 结果\nnetstat -lntup |grep 6379 tcp 0 0 192.168.1.101:6379 0.0.0.0:* LISTEN 6538/redis-server 1 tcp 0 0 192.168.1.101:16379 0.0.0.0:* LISTEN 6538/redis-server 1 #+10000端口出现，说明集群各个节点之间可以互相通信 结果\ncat *6379/pid/*pid* 6538 ##就是上面的进程id cat *6379/logs/*log 6538:C 14 Apr 2023 16:37:04.893 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo 6538:C 14 Apr 2023 16:37:04.893 # Redis version=6.2.6, bits=64, commit=00000000, modified=0, pid=6538, just started 6538:C 14 Apr 2023 16:37:04.893 # Configuration loaded 6538:M 14 Apr 2023 16:37:04.895 * Increased maximum number of open files to 10032 (it was originally set to 1024). 6538:M 14 Apr 2023 16:37:04.895 * monotonic clock: POSIX clock_gettime 6538:M 14 Apr 2023 16:37:04.898 * No cluster configuration found, I\u0026#39;m e13c04818944108ee3b0690d836466b4c0eb69fd 6538:M 14 Apr 2023 16:37:04.929 * Running mode=cluster, port=6379. 6538:M 14 Apr 2023 16:37:04.929 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128. 6538:M 14 Apr 2023 16:37:04.929 # Server initialized 6538:M 14 Apr 2023 16:37:04.929 # WARNING overcommit_memory is set to 0! Background save may fail under low memory condition. To fix this issue add \u0026#39;vm.overcommit_memory = 1\u0026#39; to /etc/sysctl.conf and then reboot or run the command \u0026#39;sysctl vm.overcommit_memory=1\u0026#39; for this to take effect. 6538:M 14 Apr 2023 16:37:04.930 * Ready to accept connections 集群节点配置文件，会发现生成了一组集群信息\n# 第一段信息是这个Redis服务作为集群节点的一个身份编码 # 别名为集群的node-id\ncat *6379/*nodes*conf e13c04818944108ee3b0690d836466b4c0eb69fd :0@0 myself,master - 0 0 0 connected vars currentEpoch 0 lastVoteEpoch 0 ## 当后续所有节点都连接上时，内容会变成： ls conf logs nodes_6379.conf pid redis_6379.rdb root@centos7101:local/redis_cluster/redis_6379 cat nodes_6379.conf f6cf3978d3397582c87480f8c335297675d4354a 192.168.1.103:6380@16380 master - 1681470597368 1681470597337 6 connected 0-5461 f1151c2350820b35e117d3c32b59b64917688745 192.168.1.103:6379@16379 master - 1681470597369 1681470597337 5 connected 10923-16383 83bfb30e39e3040397d995a7b1f560e8fb53c6a9 192.168.1.102:6380@16380 slave f1151c2350820b35e117d3c32b59b64917688745 1681470597369 1681470597337 5 connected 4e9452afe7a8f53dc546b0436109bef570e03888 192.168.1.101:6380@16380 slave 95b2dcd681674398d22817728af08c31d4bd4872 1681470597370 1681470597337 4 connected 95b2dcd681674398d22817728af08c31d4bd4872 192.168.1.102:6379@16379 master - 1681470597370 1681470597337 4 connected 5462-10922 fd6acb4af8afa5ddd31cf559ee2c80ffcbea456f 192.168.1.101:6379@16379 myself,slave f6cf3978d3397582c87480f8c335297675d4354a 0 1681470597337 6 connected vars currentEpoch 6 lastVoteEpoch 0 集群搭建 # 手动搭建集群 # 加入集群 # 在node1:6379 查看当前cluster\nredis-cli -h node1 -p 6379 node1:6379\u0026gt; cluster nodes e13c04818944108ee3b0690d836466b4c0eb69fd :6379@16379 myself,master - 0 0 0 connected node1:6379\u0026gt; cluster meet 192.168.1.102 6379 OK node1:6379\u0026gt; cluster nodes e13c04818944108ee3b0690d836466b4c0eb69fd 192.168.1.101:6379@16379 myself,master - 0 0 1 connected fbe66448ee1baefa6e9fbd55e778c1d09054b59a 192.168.1.102:6379@16379 master - 0 1681464479300 0 connected node1:6379\u0026gt; 此时在node2:6379查看当前cluster\nredis-cli -h node2 -p 6379 node2:6379\u0026gt; cluster nodes fbe66448ee1baefa6e9fbd55e778c1d09054b59a 192.168.1.102:6379@16379 myself,master - 0 0 0 connected e13c04818944108ee3b0690d836466b4c0eb69fd 192.168.1.101:6379@16379 master - 0 1681464547007 1 connected 切回node1:6379，将剩下的节点meet上\nnode1:6379\u0026gt; cluster meet 192.168.1.103 6379 OK node1:6379\u0026gt; cluster meet 192.168.1.101 6380 OK node1:6379\u0026gt; cluster meet 192.168.1.102 6380 OK node1:6379\u0026gt; cluster meet 192.168.1.103 6380 OK node1:6379\u0026gt; clear node1:6379\u0026gt; cluster nodes 84384230f256fae73ab5bbaf34b0479b67602d6e 192.168.1.102:6380@16380 master - 0 1681464635860 4 connected e13c04818944108ee3b0690d836466b4c0eb69fd 192.168.1.101:6379@16379 myself,master - 0 1681464633000 1 connected fbe66448ee1baefa6e9fbd55e778c1d09054b59a 192.168.1.102:6379@16379 master - 0 1681464636894 0 connected 43cdb0cd626a0341cf0c9fa31832735c5341a89b 192.168.1.103:6380@16380 master - 0 1681464635000 5 connected a20b6da956145cfa06ed55159456de8259d9f246 192.168.1.103:6379@16379 master - 0 1681464637923 2 connected 4aeeaa0d87b91712576c6e995b355fe4a87b24e0 192.168.1.101:6380@16380 master - 0 1681464637000 3 connected 主从配置 # 上面发现的node-id\nhostname 节点 node-id node1 192.168.1.101:6379 e13c04818944108ee3b0690d836466b4c0eb69fd node2 192.168.1.102:6379 fbe66448ee1baefa6e9fbd55e778c1d09054b59a node3 192.168.1.103:6379 a20b6da956145cfa06ed55159456de8259d9f246 主从配置\n#node1:6380-\u0026gt;node2:6379 node1:6380\u0026gt; cluster replicate 95b2dcd681674398d22817728af08c31d4bd4872 OK #node2:6380-\u0026gt;node3:6379 node2:6380\u0026gt; cluster replicate f1151c2350820b35e117d3c32b59b64917688745 OK #node3:6380-\u0026gt;node1:6379 node3:6380\u0026gt; cluster replicate fd6acb4af8afa5ddd31cf559ee2c80ffcbea456f OK 再一次查看节点信息，出现了master，slave\nnode3:6380\u0026gt; cluster nodes 4aeeaa0d87b91712576c6e995b355fe4a87b24e0 192.168.1.101:6380@16380 slave fbe66448ee1baefa6e9fbd55e778c1d09054b59a 0 1681465221000 0 connected 43cdb0cd626a0341cf0c9fa31832735c5341a89b 192.168.1.103:6380@16380 myself,slave e13c04818944108ee3b0690d836466b4c0eb69fd 0 1681465222000 1 connected fbe66448ee1baefa6e9fbd55e778c1d09054b59a 192.168.1.102:6379@16379 master - 0 1681465223000 0 connected e13c04818944108ee3b0690d836466b4c0eb69fd 192.168.1.101:6379@16379 master - 0 1681465222000 1 connected a20b6da956145cfa06ed55159456de8259d9f246 192.168.1.103:6379@16379 master - 0 1681465221814 2 connected 84384230f256fae73ab5bbaf34b0479b67602d6e 192.168.1.102:6380@16380 slave a20b6da956145cfa06ed55159456de8259d9f246 0 1681465223880 2 connected 分配槽位 # 只对主库分配，从库不进行分配\nexpr 16384 / 3 5461\n下面平均分配到3个master中，其中:\n节点 槽位数量 node1:6379 0 - 5461 【多分配了一个】 node2:6379 5461 - 10922 node3:6379 10922 - 16383 redis-cli -h node1 -p 6379 cluster addslots {0..5461} redis-cli -h node2 -p 6379 cluster addslots {5462..10922} redis-cli -h node3 -p 6379 cluster addslots {10923..16383} redis-cli -h node3 -p 6379 node1:6379\u0026gt; cluster nodes 84384230f256fae73ab5bbaf34b0479b67602d6e 192.168.1.102:6380@16380 slave a20b6da956145cfa06ed55159456de8259d9f246 0 1681467951000 2 connected e13c04818944108ee3b0690d836466b4c0eb69fd 192.168.1.101:6379@16379 myself,master - 0 1681467948000 1 connected 0-5461 fbe66448ee1baefa6e9fbd55e778c1d09054b59a 192.168.1.102:6379@16379 master - 0 1681467949000 0 connected 5462-10922 43cdb0cd626a0341cf0c9fa31832735c5341a89b 192.168.1.103:6380@16380 slave e13c04818944108ee3b0690d836466b4c0eb69fd 0 1681467949690 1 connected a20b6da956145cfa06ed55159456de8259d9f246 192.168.1.103:6379@16379 master - 0 1681467947626 2 connected 10923-16383 4aeeaa0d87b91712576c6e995b355fe4a87b24e0 192.168.1.101:6380@16380 slave fbe66448ee1baefa6e9fbd55e778c1d09054b59a 0 1681467951745 0 connected 检查集群状态是否OK\nnode1:6379\u0026gt; cluster info cluster_state:ok cluster_slots_assigned:16384 cluster_slots_ok:16384 cluster_slots_pfail:0 cluster_slots_fail:0 cluster_known_nodes:6 cluster_size:3 cluster_current_epoch:5 cluster_my_epoch:1 cluster_stats_messages_ping_sent:3461 cluster_stats_messages_pong_sent:3530 cluster_stats_messages_meet_sent:5 cluster_stats_messages_sent:6996 cluster_stats_messages_ping_received:3530 cluster_stats_messages_pong_received:3466 cluster_stats_messages_received:6996 自动集群搭建 # 假设所有的节点都已经重置过，没有主从状态，也未加入任何集群。\nRedis5之前使用redis-trib.rb脚本搭建\nredis-trib.rb脚本使用ruby语言编写，所以想要运行次脚本，我们必须安装Ruby环境。安装命令如下：\nyum -y install centos-release-scl-rh yum -y install rh-ruby23 scl enable rh-ruby23 bash gem install redis 安装完成后，我们可以使用 ruby -v 查看版本信息。\nRuby环境安装完成后。运行如下命令：\nredis-trib.rb create --replicas 1 192.168.14.101:6379 192.168.14.102:6380 192.168.14.103:6381 192.168.14.101:6382 192.168.14.102:6383 192.168.14.103:6384\n前面我们就说过，redis5.0之后已经将redis-trib.rb 脚本的功能全部集成到redis-cli中了，所以我们直接使用如下命令即可：\nredis-cli -h node3 -p 6379 cluster reset hard\n此时所有节点都是master且已经在运行中\n此时运行\n# redis-cli -a ${password} --cluster create 192.168.1.101:6379 192.168.1.102:6379 192.168.1.103:6379 192.168.1.103:6380 192.168.1.101:6380 192.168.1.102:6380 --cluster-replicas 1 # 如果有密码，一般情况下集群下的所有节点使用同样的密码 redis-cli --cluster create 192.168.1.101:6379 192.168.1.102:6379 192.168.1.103:6379 192.168.1.103:6380 192.168.1.101:6380 192.168.1.102:6380 --cluster-replicas 1 \u0026gt;\u0026gt;\u0026gt; Performing hash slots allocation on 6 nodes... Master[0] -\u0026gt; Slots 0 - 5460 Master[1] -\u0026gt; Slots 5461 - 10922 Master[2] -\u0026gt; Slots 10923 - 16383 Adding replica 192.168.1.102:6380 to 192.168.1.101:6379 Adding replica 192.168.1.103:6380 to 192.168.1.102:6379 Adding replica 192.168.1.101:6380 to 192.168.1.103:6379 M: 24ea7569f0a433eb9706d991f21ae49ec21e48cf 192.168.1.101:6379 slots:[0-5460] (5461 slots) master M: 518fc32f556b10d4b8f83bda420d01aaeeb25f51 192.168.1.102:6379 slots:[5461-10922] (5462 slots) master M: c021bdbaf1c3a476616781c25dbc2b3042ed6f10 192.168.1.103:6379 slots:[10923-16383] (5461 slots) master S: 25e44d3ff2d94400b3c53d66993fc99332adffe4 192.168.1.103:6380 replicates 518fc32f556b10d4b8f83bda420d01aaeeb25f51 S: a6159c5dda95017ba5433f597ea4d18780868dfc 192.168.1.101:6380 replicates c021bdbaf1c3a476616781c25dbc2b3042ed6f10 S: a0e986c4cfb914f34efc8f6ea07cb9b72b615593 192.168.1.102:6380 replicates 24ea7569f0a433eb9706d991f21ae49ec21e48cf Can I set the above configuration? (type \u0026#39;yes\u0026#39; to accept): yes \u0026gt;\u0026gt;\u0026gt; Nodes configuration updated \u0026gt;\u0026gt;\u0026gt; Assign a different config epoch to each node \u0026gt;\u0026gt;\u0026gt; Sending CLUSTER MEET messages to join the cluster Waiting for the cluster to join . \u0026gt;\u0026gt;\u0026gt; Performing Cluster Check (using node 192.168.1.101:6379) M: 24ea7569f0a433eb9706d991f21ae49ec21e48cf 192.168.1.101:6379 slots:[0-5460] (5461 slots) master 1 additional replica(s) S: a0e986c4cfb914f34efc8f6ea07cb9b72b615593 192.168.1.102:6380 slots: (0 slots) slave replicates 24ea7569f0a433eb9706d991f21ae49ec21e48cf S: 25e44d3ff2d94400b3c53d66993fc99332adffe4 192.168.1.103:6380 slots: (0 slots) slave replicates 518fc32f556b10d4b8f83bda420d01aaeeb25f51 M: c021bdbaf1c3a476616781c25dbc2b3042ed6f10 192.168.1.103:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s) S: a6159c5dda95017ba5433f597ea4d18780868dfc 192.168.1.101:6380 slots: (0 slots) slave replicates c021bdbaf1c3a476616781c25dbc2b3042ed6f10 M: 518fc32f556b10d4b8f83bda420d01aaeeb25f51 192.168.1.102:6379 slots:[5461-10922] (5462 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. \u0026gt;\u0026gt;\u0026gt; Check for open slots... \u0026gt;\u0026gt;\u0026gt; Check slots coverage... [OK] All 16384 slots covered. #所有槽位都分配成功 随便使用一个节点查询：\nredis-cli -h node1 -p 6380 cluster nodes 25e44d3ff2d94400b3c53d66993fc99332adffe4 192.168.1.103:6380@16380 slave 518fc32f556b10d4b8f83bda420d01aaeeb25f51 0 1681482908718 2 connected 518fc32f556b10d4b8f83bda420d01aaeeb25f51 192.168.1.102:6379@16379 master - 0 1681482906668 2 connected 5461-10922 24ea7569f0a433eb9706d991f21ae49ec21e48cf 192.168.1.101:6379@16379 master - 0 1681482908000 1 connected 0-5460 c021bdbaf1c3a476616781c25dbc2b3042ed6f10 192.168.1.103:6379@16379 master - 0 1681482907000 3 connected 10923-16383 a0e986c4cfb914f34efc8f6ea07cb9b72b615593 192.168.1.102:6380@16380 slave 24ea7569f0a433eb9706d991f21ae49ec21e48cf 0 1681482909743 1 connected a6159c5dda95017ba5433f597ea4d18780868dfc 192.168.1.101:6380@16380 myself,slave c021bdbaf1c3a476616781c25dbc2b3042ed6f10 0 1681482908000 3 connected 如上，槽位都已经平均分配完，且主从关系也配置好了\n弊端：通过该方式创建的带有从节点的机器不能够自己手动指定主节点，所以如果需要指定的话，需要自己手动指定\na: 先使用redis-cli --cluster create 192.168.163.132:6379 192.168.163.132:6380 192.168.163.132:6381\nb:或```redis-cli \u0026ndash;cluster add-node 192.168.163.132:6382 192.168.163.132:6379 说明：b:为一个指定集群添加节点，需要先连到该集群的任意一个节点IP（192.168.163.132:6379），再把新节点加入。该2个参数的顺序有要求：新加入的节点放前面 通过redis-cli --cluster add-node 192.168.163.132:6382 192.168.163.132:6379 --cluster-slave --cluster-master-id 117457eab5071954faab5e81c3170600d5192270来处理。 说明：把6382节点加入到6379节点的集群中，并且当做node_id为 117457eab5071954faab5e81c3170600d5192270 的从节点。如果不指定 \u0026ndash;cluster-master-id 会随机分配到任意一个主节点。 总结：也就是先创建主节点，再创建从节点就是了 MOVED重定向 # redis-cli -h node1 -p 6379 node1:6379\u0026gt; set k1 \u0026#34;v1\u0026#34; (error) MOVED 12706 192.168.1.103:6379 node1:6379\u0026gt; get k1 (error) MOVED 12706 192.168.1.103:6379 //上面没有设置成功，(连接时)使用下面的命令，Redis集群会自动进行MOVED重定向\nredis-cli -c -h node1 -p 6379 node1:6379\u0026gt; get k1 -\u0026gt; Redirected to slot [12706] located at 192.168.1.103:6379 (nil) 192.168.1.103:6379\u0026gt; set k1 \u0026#34;v1\u0026#34; OK 192.168.1.103:6379\u0026gt; get k1 \u0026#34;v1\u0026#34; #如上，会自动给你切换到slot对应的机器上 //在master3的slave3上查找数据\nredis-cli -h node2 -p 6380 -c node2:6380\u0026gt; keys * 1) \u0026#34;k1\u0026#34; node2:6380\u0026gt; get k1 -\u0026gt; Redirected to slot [12706] located at 192.168.1.103:6379 \u0026#34;v1\u0026#34; ## 1 只有master分配了槽位，所以会重定向到master3去取数据 ## 2 同一个槽位不能同时分配给2个节点 ## 3 在redis的官方文档中，对redis-cluster架构上，有这样的说明：在cluster架构下，默认的，一般redis-master用于接收读写，而redis-slave则用于备份，当有请求是在向slave发起时，会直接重定向到对应key所在的master来处理。但如果不介意读取的是redis-cluster中有可能过期的数据并且对写请求不感兴趣时，则亦可通过readonly命令，将slave设置成可读，然后通过slave获取相关的key，达到读写分离。 # readOnly设置 redis-cli -h node2 -p 6380 node2:6380\u0026gt; get k1 (error) MOVED 12706 192.168.1.103:6379 node2:6380\u0026gt; keys * 1) \u0026#34;k1\u0026#34; node2:6380\u0026gt; readonly OK node2:6380\u0026gt; keys * 1) \u0026#34;k1\u0026#34; node2:6380\u0026gt; get k1 \u0026#34;v1\u0026#34; ## 重置Readonly node2:6380\u0026gt; readwrite OK node2:6380\u0026gt; get k1 (error) MOVED 12706 192.168.1.103:6379 故障转移 # 关闭前\nnode2:6379\u0026gt; cluster nodes f6cf3978d3397582c87480f8c335297675d4354a 192.168.1.103:6380@16380 slave fd6acb4af8afa5ddd31cf559ee2c80ffcbea456f 0 1681470413429 0 connected fd6acb4af8afa5ddd31cf559ee2c80ffcbea456f 192.168.1.101:6379@16379 master - 0 1681470414459 0 connected 0-5461 95b2dcd681674398d22817728af08c31d4bd4872 192.168.1.102:6379@16379 myself,master - 0 1681470413000 4 connected 5462-10922 f1151c2350820b35e117d3c32b59b64917688745 192.168.1.103:6379@16379 master - 0 1681470412000 5 connected 10923-16383 83bfb30e39e3040397d995a7b1f560e8fb53c6a9 192.168.1.102:6380@16380 slave f1151c2350820b35e117d3c32b59b64917688745 0 1681470412397 5 connected 4e9452afe7a8f53dc546b0436109bef570e03888 192.168.1.101:6380@16380 slave 95b2dcd681674398d22817728af08c31d4bd4872 0 1681470411371 4 connected 关闭node1 master\nredis-cli -h node1 -p 6379 shutdown 如下，node3的slave变成了master\nredis-cli -h node1 -p 6380 node1:6380\u0026gt; cluster nodes f6cf3978d3397582c87480f8c335297675d4354a 192.168.1.103:6380@16380 master - 0 1681470479000 6 connected 0-5461 ###这里升级成了master，槽位也转移了 83bfb30e39e3040397d995a7b1f560e8fb53c6a9 192.168.1.102:6380@16380 slave f1151c2350820b35e117d3c32b59b64917688745 0 1681470479664 5 connected 4e9452afe7a8f53dc546b0436109bef570e03888 192.168.1.101:6380@16380 myself,slave 95b2dcd681674398d22817728af08c31d4bd4872 0 1681470479000 4 connected fd6acb4af8afa5ddd31cf559ee2c80ffcbea456f 192.168.1.101:6379@16379 master,fail - 1681470463058 1681470459947 0 disconnected 95b2dcd681674398d22817728af08c31d4bd4872 192.168.1.102:6379@16379 master - 0 1681470478616 4 connected 5462-10922 f1151c2350820b35e117d3c32b59b64917688745 192.168.1.103:6379@16379 master - 0 1681470480698 5 connected 10923-16383 此时将6379再次上线\nredis-server /usr/local/redis_cluster/redis_6379/conf/redis.conf ## 此时node1的6379变成了node3的6380的从库 node1:6379\u0026gt; cluster nodes f6cf3978d3397582c87480f8c335297675d4354a 192.168.1.103:6380@16380 master - 0 1681470625000 6 connected 0-5461 f1151c2350820b35e117d3c32b59b64917688745 192.168.1.103:6379@16379 master - 0 1681470628003 5 connected 10923-16383 83bfb30e39e3040397d995a7b1f560e8fb53c6a9 192.168.1.102:6380@16380 slave f1151c2350820b35e117d3c32b59b64917688745 0 1681470626979 5 connected 4e9452afe7a8f53dc546b0436109bef570e03888 192.168.1.101:6380@16380 slave 95b2dcd681674398d22817728af08c31d4bd4872 0 1681470627000 4 connected 95b2dcd681674398d22817728af08c31d4bd4872 192.168.1.102:6379@16379 master - 0 1681470627000 4 connected 5462-10922 fd6acb4af8afa5ddd31cf559ee2c80ffcbea456f 192.168.1.101:6379@16379 myself,slave f6cf3978d3397582c87480f8c335297675d4354a 0 1681470625000 6 connected 集群扩容 # 当前集群状态 # ▶ redis-cli -h node1 -p 6379 cluster nodes f635a8cdaa48e04f2531d28c103bea9dc2d8f48d 192.168.1.102:6380@16380 slave f9d707317348314a7306fdaf91da2d153590140e 0 1681527313557 5 connected f49300c718a7e0baf6d3e8ba4bf7e9915e8051cc 192.168.1.101:6380@16380 slave 9e9613cec2fdd48000509e9c3723d157263edd87 0 1681527313000 4 connected 9ea59136c61207347657503fd7a78349f57e919e 192.168.1.103:6380@16380 slave fff7298fa77799434bc8ef6c74c974c21ebc47b4 0 1681527314000 0 connected fff7298fa77799434bc8ef6c74c974c21ebc47b4 192.168.1.101:6379@16379 myself,master - 0 1681527313000 0 connected 0-5461 9e9613cec2fdd48000509e9c3723d157263edd87 192.168.1.102:6379@16379 master - 0 1681527314579 4 connected 5462-10922 f9d707317348314a7306fdaf91da2d153590140e 192.168.1.103:6379@16379 master - 0 1681527312000 5 connected 10923-16383 新增节点配置并启动 # 准备 # 假设在node3新增两个端口{6390,6391}，作为新节点\n且 node3:6391 replicate node3:6390\n步骤：通过mkdir -p /usr/local/redis_cluster/redis_63{91,90}/{conf,pid,logs}创建文件夹，然后再conf目录下配置集群配置文件\n# 守护进行模式启动 daemonize yes # 设置数据库数量，默认数据库为0 databases 16 # 绑定地址，需要修改 bind node3 # 绑定端口，需要修改 port 6390 # pid文件存储位置，文件名需要修改 pidfile /usr/local/redis_cluster/redis_6390/pid/redis_6390.pid # log文件存储位置，文件名需要修改 logfile /usr/local/redis_cluster/redis_6390/logs/redis_6390.log # RDB快照备份文件名，文件名需要修改 dbfilename redis_6390.rdb # 本地数据库存储目录，需要修改 dir /usr/local/redis_cluster/redis_6390 # 集群相关配置 # 是否以集群模式启动 cluster-enabled yes # 集群节点回应最长时间，超过该时间被认为下线 cluster-node-timeout 15000 # 生成的集群节点配置文件名，文件名需要修改 cluster-config-file nodes_6390.conf 目录结构\nroot@centos7103:/usr/local/redis_cluster ▶ ls redis6 redis_6379 redis_6380 redis_6390 redis_6391 ▶ tree *90 redis_6390 ├── conf │ └── redis.conf ├── logs └── pid 3 directories, 1 file 启动节点\n# 两个孤儿节点 root@centos7103:/usr/local/redis_cluster ⍉ ▶ redis-server /usr/local/redis_cluster/redis_6390/conf/redis.conf root@centos7103:/usr/local/redis_cluster ▶ redis-server /usr/local/redis_cluster/redis_6391/conf/redis.conf root@centos7103:/usr/local/redis_cluster ▶ netstat -lntup |grep redis tcp 0 0 192.168.1.103:6379 0.0.0.0:* LISTEN 3484/redis-server n tcp 0 0 192.168.1.103:6380 0.0.0.0:* LISTEN 3507/redis-server n tcp 0 0 192.168.1.103:6390 0.0.0.0:* LISTEN 5590/redis-server n tcp 0 0 192.168.1.103:6391 0.0.0.0:* LISTEN 5616/redis-server n tcp 0 0 192.168.1.103:16379 0.0.0.0:* LISTEN 3484/redis-server n tcp 0 0 192.168.1.103:16380 0.0.0.0:* LISTEN 3507/redis-server n tcp 0 0 192.168.1.103:16390 0.0.0.0:* LISTEN 5590/redis-server n tcp 0 0 192.168.1.103:16391 0.0.0.0:* LISTEN 5616/redis-server n 添加主节点 # 将新节点加入到node1:6379 [0,5460]所在的集群中\n加入前\nredis-cli -h node3 -p 6390 node3:6390\u0026gt; cluster nodes b014cfbeff6f9668ec9592cbc8aa874bda2d8d6b :6390@16390 myself,master - 0 0 0 connected 加入\n# 在node1客户端操作，将103:6390添加到101:6379所在的集群中 redis-cli -h node1 -p 6379 --cluster add-node 192.168.1.103:6390 192.168.1.101:6379 \u0026gt;\u0026gt;\u0026gt; Adding node 192.168.1.103:6390 to cluster 192.168.1.101:6379 \u0026gt;\u0026gt;\u0026gt; Performing Cluster Check (using node 192.168.1.101:6379) M: fff7298fa77799434bc8ef6c74c974c21ebc47b4 192.168.1.101:6379 slots:[0-5461] (5462 slots) master 1 additional replica(s) S: f635a8cdaa48e04f2531d28c103bea9dc2d8f48d 192.168.1.102:6380 slots: (0 slots) slave replicates f9d707317348314a7306fdaf91da2d153590140e S: f49300c718a7e0baf6d3e8ba4bf7e9915e8051cc 192.168.1.101:6380 slots: (0 slots) slave replicates 9e9613cec2fdd48000509e9c3723d157263edd87 S: 9ea59136c61207347657503fd7a78349f57e919e 192.168.1.103:6380 slots: (0 slots) slave replicates fff7298fa77799434bc8ef6c74c974c21ebc47b4 M: 9e9613cec2fdd48000509e9c3723d157263edd87 192.168.1.102:6379 slots:[5462-10922] (5461 slots) master 1 additional replica(s) M: f9d707317348314a7306fdaf91da2d153590140e 192.168.1.103:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s) [OK] All nodes agree about slots configuration. \u0026gt;\u0026gt;\u0026gt; Check for open slots... \u0026gt;\u0026gt;\u0026gt; Check slots coverage... [OK] All 16384 slots covered. \u0026gt;\u0026gt;\u0026gt; Send CLUSTER MEET to node 192.168.1.103:6390 to make it join the cluster. [OK] New node added correctly. 加入后\n▶ redis-cli -h node1 -p 6379 cluster nodes b014cfbeff6f9668ec9592cbc8aa874bda2d8d6b 192.168.1.103:6390@16390 master - 0 1681527533967 6 connected f635a8cdaa48e04f2531d28c103bea9dc2d8f48d 192.168.1.102:6380@16380 slave f9d707317348314a7306fdaf91da2d153590140e 0 1681527534990 5 connected f49300c718a7e0baf6d3e8ba4bf7e9915e8051cc 192.168.1.101:6380@16380 slave 9e9613cec2fdd48000509e9c3723d157263edd87 0 1681527533000 4 connected 9ea59136c61207347657503fd7a78349f57e919e 192.168.1.103:6380@16380 slave fff7298fa77799434bc8ef6c74c974c21ebc47b4 0 1681527533000 0 connected fff7298fa77799434bc8ef6c74c974c21ebc47b4 192.168.1.101:6379@16379 myself,master - 0 1681527529000 0 connected 0-5461 9e9613cec2fdd48000509e9c3723d157263edd87 192.168.1.102:6379@16379 master - 0 1681527534000 4 connected 5462-10922 f9d707317348314a7306fdaf91da2d153590140e 192.168.1.103:6379@16379 master - 0 1681527533000 5 connected 10923-16383 为他分配槽位\n# 最后一个参数，表示原来集群中任意一个节点，这里会将源节点所在集群的一部分分给新增节点 redis-cli -h node1 -p 6379 --cluster reshard 192.168.1.101:6379 ##过程 #后面的2000表示分配2000个槽位给新增节点 How many slots do you want to move (from 1 to 16384)? 2000 #输入 #表示接受节点的NodeId,填新增节点6390的 What is the receiving node ID? b014cfbeff6f9668ec9592cbc8aa874bda2d8d6b #输入 #这里填槽的来源，要么填all，表示所有master节点都拿出一部分槽位分配给新增节点； #要么填某个原有NodeId，表示这个节点拿出一部分槽位给新增节点 Please enter all the source node IDs. Type \u0026#39;all\u0026#39; to use all the nodes as source nodes for the hash slots. Type \u0026#39;done\u0026#39; once you entered all the source nodes IDs. Source node #1: 7e900adc7f977cfcccef12d48c7a29b64c4344c2 Source node #2: done # 这里把node1:6379 拿出了2000个槽位给新节点 结果：\n? redis-cli -h node1 -p 6380 cluster nodes 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 192.168.1.103:6379@16379 master - 0 1681530641000 2 connected 10923-16383 429ed631dbf09ba846a5371b707defe17b9f8c8e 192.168.1.101:6380@16380 myself,slave 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 0 1681530643000 4 connected 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 192.168.1.102:6379@16379 master - 0 1681530641000 4 connected 5462-10922 81e1e03230ed7700028fa56155e9531b48791164 192.168.1.103:6390@16390 master - 0 1681530644122 6 connected 0-1999 a04347e1af8930324dab7ae85f912449475a487f 192.168.1.102:6380@16380 slave 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 0 1681530643093 2 connected 92a9d6b988dcf8a219de0247975d8e341072134d 192.168.1.103:6380@16380 slave 7e900adc7f977cfcccef12d48c7a29b64c4344c2 0 1681530643000 1 connected 7e900adc7f977cfcccef12d48c7a29b64c4344c2 192.168.1.101:6379@16379 master - 0 1681530642063 1 connected 2000-5461 添加从节点 # 将节点添加到集群中\n▶ redis-cli -h node1 -p 6379 --cluster add-node 192.168.1.103:6391 192.168.1.101:6379 \u0026gt;\u0026gt;\u0026gt; Adding node 192.168.1.103:6391 to cluster 192.168.1.101:6379 \u0026gt;\u0026gt;\u0026gt; Performing Cluster Check (using node 192.168.1.101:6379) M: 7e900adc7f977cfcccef12d48c7a29b64c4344c2 192.168.1.101:6379 slots:[2000-5461] (3462 slots) master 1 additional replica(s) M: 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 192.168.1.102:6379 slots:[5462-10922] (5461 slots) master 1 additional replica(s) S: a04347e1af8930324dab7ae85f912449475a487f 192.168.1.102:6380 slots: (0 slots) slave replicates 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 M: 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 192.168.1.103:6379 slots:[10923-16383] (5461 slots) master 1 additional replica(s) M: 81e1e03230ed7700028fa56155e9531b48791164 192.168.1.103:6390 slots:[0-1999] (2000 slots) master S: 429ed631dbf09ba846a5371b707defe17b9f8c8e 192.168.1.101:6380 slots: (0 slots) slave replicates 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 S: 92a9d6b988dcf8a219de0247975d8e341072134d 192.168.1.103:6380 slots: (0 slots) slave replicates 7e900adc7f977cfcccef12d48c7a29b64c4344c2 [OK] All nodes agree about slots configuration. \u0026gt;\u0026gt;\u0026gt; Check for open slots... \u0026gt;\u0026gt;\u0026gt; Check slots coverage... [OK] All 16384 slots covered. \u0026gt;\u0026gt;\u0026gt; Send CLUSTER MEET to node 192.168.1.103:6391 to make it join the cluster. [OK] New node added correctly. 建立主从关系\n▶ redis-cli -h node1 -p 6380 cluster nodes 9babc7adc86da25ba501bd5bc007300dc04743a9 192.168.1.103:6391@16391 master - 0 1681530812000 0 connected 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 192.168.1.103:6379@16379 master - 0 1681530808000 2 connected 10923-16383 429ed631dbf09ba846a5371b707defe17b9f8c8e 192.168.1.101:6380@16380 myself,slave 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 0 1681530811000 4 connected 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 192.168.1.102:6379@16379 master - 0 1681530810000 4 connected 5462-10922 81e1e03230ed7700028fa56155e9531b48791164 192.168.1.103:6390@16390 master - 0 1681530810000 6 connected 0-1999 a04347e1af8930324dab7ae85f912449475a487f 192.168.1.102:6380@16380 slave 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 0 1681530811246 2 connected 92a9d6b988dcf8a219de0247975d8e341072134d 192.168.1.103:6380@16380 slave 7e900adc7f977cfcccef12d48c7a29b64c4344c2 0 1681530812275 1 connected 7e900adc7f977cfcccef12d48c7a29b64c4344c2 192.168.1.101:6379@16379 master - 0 1681530809182 1 connected 2000-5461 root@centos7101:/usr/local/redis_cluster ▶ redis-cli -h node3 -p 6391 cluster replicate 81e1e03230ed7700028fa56155e9531b48791164 OK root@centos7101:/usr/local/redis_cluster # 验证 ▶ redis-cli -h node1 -p 6380 cluster nodes 9babc7adc86da25ba501bd5bc007300dc04743a9 192.168.1.103:6391@16391 slave 81e1e03230ed7700028fa56155e9531b48791164 0 1681530870000 6 connected 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 192.168.1.103:6379@16379 master - 0 1681530868642 2 connected 10923-16383 429ed631dbf09ba846a5371b707defe17b9f8c8e 192.168.1.101:6380@16380 myself,slave 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 0 1681530867000 4 connected 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 192.168.1.102:6379@16379 master - 0 1681530871715 4 connected 5462-10922 81e1e03230ed7700028fa56155e9531b48791164 192.168.1.103:6390@16390 master - 0 1681530868000 6 connected 0-1999 a04347e1af8930324dab7ae85f912449475a487f 192.168.1.102:6380@16380 slave 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 0 1681530869000 2 connected 92a9d6b988dcf8a219de0247975d8e341072134d 192.168.1.103:6380@16380 slave 7e900adc7f977cfcccef12d48c7a29b64c4344c2 0 1681530870000 1 connected 7e900adc7f977cfcccef12d48c7a29b64c4344c2 192.168.1.101:6379@16379 master - 0 1681530870693 1 connected 2000-5461 测试\nnode1:6380\u0026gt; set 18 a -\u0026gt; Redirected to slot [511] located at 192.168.1.103:6390 OK 192.168.1.103:6390\u0026gt; get 18 \u0026#34;a\u0026#34; #在node3:6391上尝试-\u0026gt;说明从机上是有数据的 ▶ redis-cli -h node3 -p 6391 node3:6391\u0026gt; get 18 (error) MOVED 511 192.168.1.103:6390 node3:6391\u0026gt; readonly OK node3:6391\u0026gt; get 18 \u0026#34;a\u0026#34; node3:6391\u0026gt; keys * 1) \u0026#34;18\u0026#34; 集群收缩 # 迁移待移除节点的槽位 # #当前节点信息 429ed631dbf09ba846a5371b707defe17b9f8c8e 192.168.1.101:6380@16380 slave 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 0 1681531264142 4 connected 7e900adc7f977cfcccef12d48c7a29b64c4344c2 192.168.1.101:6379@16379 master - 0 1681531260000 1 connected 2000-5461 9355d72df6e9dc2643ac1c819cd2e496fb1aed60 192.168.1.102:6379@16379 myself,master - 0 1681531261000 4 connected 5462-10922 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 192.168.1.103:6379@16379 master - 0 1681531262088 2 connected 10923-16383 9babc7adc86da25ba501bd5bc007300dc04743a9 192.168.1.103:6391@16391 slave 81e1e03230ed7700028fa56155e9531b48791164 0 1681531265170 6 connected 81e1e03230ed7700028fa56155e9531b48791164 192.168.1.103:6390@16390 master - 0 1681531264000 6 connected 0-1999 92a9d6b988dcf8a219de0247975d8e341072134d 192.168.1.103:6380@16380 slave 7e900adc7f977cfcccef12d48c7a29b64c4344c2 0 1681531263115 1 connected a04347e1af8930324dab7ae85f912449475a487f 192.168.1.102:6380@16380 slave 259b65d7f3d1eac2716f7ae00cc6c1db27a55b27 0 1681531260038 2 connected 移除并将槽位分配给其他节点\nredis-cli -p 6379 -h node1 --cluster reshard --cluster-from bee9c03b1c4592119695a17472847736128c8603 --cluster-to 644b722eb996aeb392a8190b29cfdbe95536af9a --cluster-slots 2000 192.168.1.101:6379 # 用哪个客户端，最后的ip:host-\u0026gt;对该ip host所在集群的from和to操作，进行转移 # 结果 redis-cli -h node1 -p 6380 cluster nodes bee9c03b1c4592119695a17472847736128c8603 192.168.1.103:6390@16390 master - 0 1681532501000 6 connected f525c38c1a78e997a96315ca982f969c51500e86 192.168.1.102:6379@16379 master - 0 1681532501000 0 connected 5462-10922 2b905b7e2480d80bb7c7aa47940e9636697a7d4c 192.168.1.103:6379@16379 master - 0 1681532503071 2 connected 10923-16383 644b722eb996aeb392a8190b29cfdbe95536af9a 192.168.1.101:6379@16379 master - 0 1681532502000 8 connected 0-5461 180113f8ceeba0b17b4a122caa62d36e99141225 192.168.1.103:6391@16391 slave 644b722eb996aeb392a8190b29cfdbe95536af9a 0 1681532503000 8 connected 576e15ed8ac1f4632e5f0917c43d41f7e26dc1e0 192.168.1.101:6380@16380 myself,slave f525c38c1a78e997a96315ca982f969c51500e86 0 1681532500000 0 connected 7ff6ce4b934027c1cdb8720169873f8e97474885 192.168.1.102:6380@16380 slave 2b905b7e2480d80bb7c7aa47940e9636697a7d4c 0 1681532504083 2 connected 75f8df2756a83c121b5637e3a381fa8ebfb9204d 192.168.1.103:6380@16380 slave 644b722eb996aeb392a8190b29cfdbe95536af9a 0 1681532501053 8 connected ## 查看 ▶ redis-cli -h node1 -p 6379 node1:6379\u0026gt; get 18 \u0026#34;a\u0026#34; node1:6379\u0026gt; keys * 1) \u0026#34;18\u0026#34; node1:6379\u0026gt; exit ## 看看还在不在103:6390上 redis-cli -h node3 -p 6390 node3:6390\u0026gt; keys * (empty array) node3:6390\u0026gt; get 18 (error) MOVED 511 192.168.1.101:6379 槽位调整成功\n注意，node3:6391原本replicate node3:6390，但是node3:6390没有槽位了，所以他就跟到槽位所在的node上了，即：\n644b722eb996aeb392a8190b29cfdbe95536af9a 192.168.1.101:6379@16379 master - 0 1681532502000 8 connected 0-5461 180113f8ceeba0b17b4a122caa62d36e99141225 192.168.1.103:6391@16391 slave 644b722eb996aeb392a8190b29cfdbe95536af9a 0 1681532503000 8 connected 移除待删除的主从节点 # 先移除从节点，再移除主节点，防止触发集群故障转移(如上，这里可能并不会，因为已经没有节点replicate node3:6390了)\nredis-cli -p 6379 -h node1 --cluster del-node 192.168.1.102:6380 180113f8ceeba0b17b4a122caa62d36e99141225 #ip+port :哪个节点所在的集群 #nodeId \u0026gt;\u0026gt;\u0026gt; Removing node 180113f8ceeba0b17b4a122caa62d36e99141225 from cluster 192.168.1.102:6380 \u0026gt;\u0026gt;\u0026gt; Sending CLUSTER FORGET messages to the cluster... \u0026gt;\u0026gt;\u0026gt; Sending CLUSTER RESET SOFT to the deleted node. 移除主节点\nredis-cli -p 6379 -h node1 --cluster del-node 192.168.1.102:6380 bee9c03b1c4592119695a17472847736128c8603 \u0026gt;\u0026gt;\u0026gt; Removing node bee9c03b1c4592119695a17472847736128c8603 from cluster 192.168.1.102:6380 \u0026gt;\u0026gt;\u0026gt; Sending CLUSTER FORGET messages to the cluster... \u0026gt;\u0026gt;\u0026gt; Sending CLUSTER RESET SOFT to the deleted node. 查看状态（移除成功）\n▶ redis-cli -h node1 -p 6379 cluster nodes 644b722eb996aeb392a8190b29cfdbe95536af9a 192.168.1.101:6379@16379 myself,master - 0 1681533116000 8 connected 0-5461 75f8df2756a83c121b5637e3a381fa8ebfb9204d 192.168.1.103:6380@16380 slave 644b722eb996aeb392a8190b29cfdbe95536af9a 0 1681533119000 8 connected f525c38c1a78e997a96315ca982f969c51500e86 192.168.1.102:6379@16379 master - 0 1681533120514 0 connected 5462-10922 2b905b7e2480d80bb7c7aa47940e9636697a7d4c 192.168.1.103:6379@16379 master - 0 1681533119492 2 connected 10923-16383 576e15ed8ac1f4632e5f0917c43d41f7e26dc1e0 192.168.1.101:6380@16380 slave f525c38c1a78e997a96315ca982f969c51500e86 0 1681533118463 0 connected 7ff6ce4b934027c1cdb8720169873f8e97474885 192.168.1.102:6380@16380 slave 2b905b7e2480d80bb7c7aa47940e9636697a7d4c 0 1681533118000 2 connected cluster命令 # 以下是集群中常用的可执行命令，命令执行格式为：\ncluster 下表命令 命令如下，未全，如果想了解更多请执行cluster help操作：\n命令 描述 INFO 返回当前集群信息 MEET \u0026lt;ip\u0026gt; \u0026lt;port\u0026gt; [\u0026lt;bus-port\u0026gt;] 添加一个节点至当前集群 MYID 返回当前节点集群ID NODES 返回当前节点的集群信息 REPLICATE \u0026lt;node-id\u0026gt; 将当前节点作为某一集群节点的从库 FAILOVER [FORCE|TAKEOVER] 将当前从库升级为主库 RESET [HARD|SOFT] 重置当前节点信息 ADDSLOTS \u0026lt;slot\u0026gt; [\u0026lt;slot\u0026gt; ...] 为当前集群节点增加一个或多个插槽位，推荐在bash shell中执行，可通过{int..int}指定多个插槽位 DELSLOTS \u0026lt;slot\u0026gt; [\u0026lt;slot\u0026gt; ...] 为当前集群节点删除一个或多个插槽位，推荐在bash shell中执行，可通过{int..int}指定多个插槽位 FLUSHSLOTS 删除当前节点中所有的插槽信息 FORGET \u0026lt;node-id\u0026gt; 从集群中删除某一节点 COUNT-FAILURE-REPORTS \u0026lt;node-id\u0026gt; 返回当前集群节点的故障报告数量 COUNTKEYSINSLOT \u0026lt;slot\u0026gt; 返回某一插槽中的键的数量 GETKEYSINSLOT \u0026lt;slot\u0026gt; \u0026lt;count\u0026gt; 返回当前节点存储在插槽中的key名称。 KEYSLOT \u0026lt;key\u0026gt; 返回该key的哈希槽位 SAVECONFIG 保存当前集群配置，进行落盘操作 SLOTS 返回该插槽的信息 SpringBoot+RedisCluster # 依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置文件yml\nspring: redis: # 如果是redis-cluster 不能用这种形式，否则会报错,只适合单机 #Error in execution; nested exception is io.lettuce.core. #RedisCommandExecutionException: MOVED 15307 192.168.1.103:6379 #host: 192.168.1.102 #port: 6380 # 下面的配置,nodes写一个或者多个都行 cluster: nodes: # - 192.168.1.101:6379 # - 192.168.1.102:6379 # - 192.168.1.101:6380 - 192.168.1.102:6380 # - 192.168.1.103:6379 # - 192.168.1.103:6380 序列化处理\n@Configuration public class RedisConfig { @Bean public RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate\u0026lt;String, Object\u0026gt; template = new RedisTemplate\u0026lt;\u0026gt;(); RedisSerializer\u0026lt;String\u0026gt; redisSerializer = new StringRedisSerializer(); template.setConnectionFactory(redisConnectionFactory); //key序列化方式 template.setKeySerializer(redisSerializer); //value序列化 template.setValueSerializer(redisSerializer); //value hashmap序列化 template.setHashValueSerializer(redisSerializer); //key haspmap序列化 template.setHashKeySerializer(redisSerializer); // return template; } } 使用\n@Autowired private RedisTemplate\u0026lt;String, Object\u0026gt; redisTemplate; @RequestMapping(\u0026#34;/redisTest\u0026#34;) public String redisTest(){ redisTemplate.opsForValue().set(\u0026#34;190\u0026#34;,\u0026#34;hello,world\u0026#34;+new Date().getTime()); Object hello = redisTemplate.opsForValue().get(\u0026#34;190\u0026#34;); return hello.toString(); } RedisCluster架构原理分析 # 基础架构（数据分片） # 集群分片原理 # 如果有任意1个槽位没有被分配，则集群创建不成功。\n启动集群原理 # 集群通信原理 # "},{"id":52,"href":"/zh/docs/technology/Other/kaoshi/","title":"科目","section":"其他","content":" 科目 # 1022/9:00-11:30\n00024 普通逻辑 2010 02197 概率论与数理统计（二）2018 02318 计算机组成原理 2016 02324 离散数学 2014 02331 数据结构 2012 03709 马克思主义基本原理概论 2018 04747 Java语言程序设计（一） 2019 1022/14:30-17:00\n00023 高等数学（工本） 2019 00342 高级语言程序设计（一）2017 02326 操作系统 2017 04730 电子技术基础（三） 2006 04735 数据库系统原理 2018 1023/09:00-11:30\n02325 计算机系统结构 2012 03708 中国近现代史纲要 2018 04737 C++程序设计 2019 1023/14:30-17:00\n0015 英语（二）2012 02333 软件工程 2011 04741 计算机网络原理 2018 "},{"id":53,"href":"/zh/docs/technology/Linux/basic/","title":"基本操作","section":"Linux","content":" yum源替换成阿里云 # yum install -y wget ## 备份 mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak ## 下载 wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo ## 重建缓存 yum clean all yum makecache Java环境搭建 # yum search java | grep jdk yum install -y java-1.8.0-openjdk-devel.x86_64 # java -version 正常 # javac -version 正常 解压相关 # -zxvf\ntar -zxvf redis* -C /usr/local/redis* # z ：表示 tar 包是被 gzip 压缩过的 (后缀是.tar.gz)，所以解压时需要用 gunzip 解压 (.tar不需要) # x ：表示 从 tar 包中把文件提取出来 # v ：表示 显示打包过程详细信息 # f ：指定被处理的文件是什么 # 适用于参数分开使用的情况，连续无分隔参数不应该再使用（所以上面的命令不标准）， # 应该是 tar zxvf redis* -C /usr/local/redis* 主题修改 # oh my zsh\n在线 # Method Command curl sh -c \u0026quot;$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026quot; wget sh -c \u0026quot;$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026quot; fetch sh -c \u0026quot;$(fetch -o - https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\u0026quot; 手动安装 # yum install -y zsh #一定要先装 sh -c \u0026#34;$(wget https://gitee.com/liu_yi_er/ohmyzsh/raw/master/tools/install.sh -O -)\u0026#34; #自己的gitee目录，从官网下载 sh install.sh 修改主题 # //该主题样式如下\n$ vi ~/.zshrc # 找到这一行，修改为自己喜欢的主题名称 # ZSH_THEME=\u0026#34;ys\u0026#34; ZSH_THEME=\u0026#34;avit\u0026#34; # 修改保存后，使配置生效 $ source ~/.zshrc zsh home和end失效，可以改用ctrl+a / ctrl+e 代替\nVim的使用快捷使用 # # 清空文件--命令模式下输入 :%d 回车 # 处理粘贴时多出的行带#的问题-- 命令模式下输入 :set paste 再输入i进行粘贴 #快速修改-- 命令模式下输入 :%s/6379/6380/g （将文件中所有6379替换成6380） 基本网络工具安装 # yum install -y net-tools 查看端口监听情况\nnetstat -lntup | grep redis 解释\n-a (all)显示所有选项，默认不显示LISTEN相关 -t (tcp)仅显示tcp相关选项 -u (udp)仅显示udp相关选项 -n 拒绝显示别名，能显示数字的全部转化成数字。 -l 仅列出有在 Listen (监听) 的服務状态\n-p 显示建立相关链接的程序名 -r 显示路由信息，路由表 -e 显示扩展信息，例如uid等 -s 按各个协议进行统计 -c 每隔一个固定时间，执行该netstat命令。\n提示：LISTEN和LISTENING的状态只有用-a或者-l才能看到\nps命令 # ps -ef //-e表示全部进程 ，-f表示全部的列\n树形结构查看文件夹 # yum install -y tree\n快捷键 # ctrl+w 快速删除光标前的整个单词\nctrl+a 光标移到行首 [xshell]\nctrl+e 光标移到行尾 [xshell]\n创建多级目录 # mkdir -p /usr/local/redis_cluster/redis_63{79,80}/{conf,pid,logs}\n"},{"id":54,"href":"/zh/docs/technology/Linux/create_clone/","title":"vmware上linux主机的安装和克隆","section":"Linux","content":" 安装 # 虚拟机向导 # 典型\u0026mdash;稍后安装\u0026ndash;linux\u0026ndash;RedhatEnterpriseLinux7 64 虚拟机名称rheCentos700 接下来都默认即可(20G硬盘，2G内存，网络适配器(桥接模式)) 安装界面 # 日期\u0026ndash;亚洲上海，键盘\u0026ndash;汉语，语言支持\u0026ndash;简体中文(中国)\n软件安装\n最小安装\u0026mdash;\u0026gt; 兼容性程序库+开发工具\n其他存储选项\u0026ndash;配置分区\n/boot 1G 标准分区，文件系统ext4 swap 2G 标准分区 ，文件系统swap / 17G 标准分区，文件系统ext4 网络和主机名\n打开网络+设置主机名(rheCentos700)\n完成\u0026mdash;过程中配置密码 默认用户root+其他用户ly\n安装完成后修改ip及网关 # Centos # vi /etc/sysconfig/network-scripts/ifcfg-ens**\n修改部分键值对\nBOOTPROTO=\u0026#34;static\u0026#34; IPADDR=192.168.1.100 NETMASK=255.255.255.0 GATEWAY=192.168.1.1 DNS1=223.5.5.5 DNS2=223.6.6.6 systemctl restart network\nDebian # 查看当前网卡\nip link #1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000 # link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 #2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000 # link/ether 00:0c:29:ed:95:f5 brd ff:ff:ff:ff:ff:ff # altname enp2s1 得知网卡名为ens33\nvim /etc/network/interfaces 添加内容，为网卡（ens33）设置静态ip\n#ly-update auto ens33 iface ens33 inet static address 192.168.1.206 netmask 255.255.255.0 gateway 192.168.1.1 dns-nameservers 223.5.5.5 223.6.6.6 重启网络\nsudo service networking restart 查看ip\nip a #---------------------结果显示 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:xx:xx:23:f5 brd ff:ff:ff:ff:ff:ff altname enp2s1 inet 192.168.1.206/24 brd 192.168.1.255 scope global ens33 valid_lft forever preferred_lft forever inet6 xxxx::20c:29ff:feed:xxxx/64 scope link valid_lft forever preferred_lft forever 克隆虚拟机 # 右键\u0026ndash;管理\u0026ndash;克隆\u0026ndash;创建完整克隆\n修改MAC、主机名、ip、uuid\n右键\u0026ndash;设置\u0026ndash;网络适配器\u0026ndash;高级\u0026ndash;MAC地址-\u0026gt;生成\nvi /etc/hostname修改主机名\nreboot\nvi /etc/sysconfig/network-scripts/ifcfg-ens**修改ip及uuid\nuuid自动生成\n常用操作 # 常用命令安装\nyum install -y wget yum阿里云源切换\n备份mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak 下载并切换wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo 清理yum clean all 缓存处理yum makecache "},{"id":55,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0606lymysql-query-execution-plan/","title":"mysql执行计划","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n本文来自公号 MySQL 技术，JavaGuide 对其做了补充完善。原文地址：https://mp.weixin.qq.com/s/d5OowNLtXBGEAbT31sSH4g\n优化 SQL 的第一步应该是读懂 SQL 的执行计划。本篇文章，我们一起来学习下 MySQL EXPLAIN 执行计划相关知识。\n什么是执行计划？ # 执行计划 是指一条 SQL 语句在经过 MySQL 查询优化器 的优化会后，具体的执行方式。\n执行计划通常用于 SQL 性能分析、优化等场景。通过 EXPLAIN 的结果，可以了解到如数据表的查询顺序、数据查询操作的操作类型、哪些索引可以被命中、哪些索引实际会命中、每个数据表有多少行记录被查询等信息。\n如何获取执行计划？ # -- 提交准备数据 SET NAMES utf8mb4; SET FOREIGN_KEY_CHECKS = 0; -- ---------------------------- -- Table structure for dept_emp -- ---------------------------- DROP TABLE IF EXISTS `dept_emp`; CREATE TABLE `dept_emp` ( `id` int(0) NOT NULL, `emp_no` varchar(128) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL, `other1` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL, `other2` varchar(255) CHARACTER SET utf8mb4 COLLATE utf8mb4_0900_ai_ci NULL DEFAULT NULL, PRIMARY KEY (`id`) USING BTREE, INDEX `index_emp_no`(`emp_no`) USING BTREE ) ENGINE = InnoDB CHARACTER SET = utf8mb4 COLLATE = utf8mb4_0900_ai_ci ROW_FORMAT = Dynamic; -- ---------------------------- -- Records of dept_emp -- ---------------------------- INSERT INTO `dept_emp` VALUES (1, \u0026#39;a1\u0026#39;, \u0026#39;o11\u0026#39;, \u0026#39;012\u0026#39;); INSERT INTO `dept_emp` VALUES (2, \u0026#39;a2\u0026#39;, \u0026#39;o21\u0026#39;, \u0026#39;o22\u0026#39;); INSERT INTO `dept_emp` VALUES (3, \u0026#39;a3\u0026#39;, \u0026#39;o31\u0026#39;, \u0026#39;o32\u0026#39;); INSERT INTO `dept_emp` VALUES (4, \u0026#39;a4\u0026#39;, \u0026#39;o41\u0026#39;, \u0026#39;o42\u0026#39;); INSERT INTO `dept_emp` VALUES (5, \u0026#39;a5\u0026#39;, \u0026#39;o51\u0026#39;, \u0026#39;o52\u0026#39;); SET FOREIGN_KEY_CHECKS = 1; MySQL 为我们提供了 EXPLAIN 命令，来获取执行计划的相关信息。\n需要注意的是，EXPLAIN 语句并不会真的去执行相关的语句，而是通过查询优化器对语句进行分析，找出最优的查询方案，并显示对应的信息。\nEXPLAIN 执行计划支持 SELECT、DELETE、INSERT、REPLACE 以及 UPDATE 语句。我们一般多用于分析 SELECT 查询语句，使用起来非常简单，语法如下：\nEXPLAIN + SELECT 查询语句； 我们简单来看下一条查询语句的执行计划：\nmysql\u0026gt; explain SELECT * FROM dept_emp WHERE emp_no IN (SELECT emp_no FROM dept_emp GROUP BY emp_no HAVING COUNT(emp_no)\u0026gt;1); +----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+ | 1 | PRIMARY | dept_emp | NULL | ALL | NULL | NULL | NULL | NULL | 331143 | 100.00 | Using where | | 2 | SUBQUERY | dept_emp | NULL | index | PRIMARY,dept_no | PRIMARY | 16 | NULL | 331143 | 100.00 | Using index | +----+-------------+----------+------------+-------+-----------------+---------+---------+------+--------+----------+-------------+ 可以看到，执行计划结果中共有 12 列，各列代表的含义总结如下表：\n列名 含义 id SELECT查询的序列标识符 select_type SELECT关键字对应的查询类型 table 用到的表名 partitions 匹配的分区，对于未分区的表，值为 NULL type 表的访问方法 possible_keys 可能用到的索引 key 实际用到的索引 key_len 所选索引的长度 ref 当使用索引等值查询时，与索引作比较的列或常量 rows 预计要读取的行数 filtered 按表条件过滤后，留存的记录数的百分比 Extra 附加信息 如何分析 EXPLAIN 结果？ # 为了分析 EXPLAIN 语句的执行结果，我们需要搞懂执行计划中的重要字段。\nid # SELECT 标识符，是查询中 SELECT 的序号，用来标识整个查询中 SELELCT 语句的顺序。\nid 如果相同，从上往下依次执行。id 不同，id 值越大，执行优先级越高，如果行引用其他行的并集结果，则该值可以为 NULL。\nselect_type # 查询的类型，主要用于区分普通查询、联合查询、子查询等复杂的查询，常见的值有：\nSIMPLE：简单查询，不包含 UNION 或者子查询。 PRIMARY：查询中如果包含子查询或其他部分，外层的 SELECT 将被标记为 PRIMARY。 SUBQUERY：子查询中的第一个 SELECT。 UNION：在 UNION 语句中，UNION 之后出现的 SELECT。 DERIVED：在 FROM 中出现的子查询将被标记为 DERIVED。 UNION RESULT：UNION 查询的结果。 table # 查询用到的表名，每行都有对应的表名，表名除了正常的表之外，也可能是以下列出的值：\n\u0026lt;unionM,N\u0026gt; : 本行引用了 id 为 M 和 N 的行的 UNION 结果； \u0026lt;derivedN\u0026gt; : 本行引用了 id 为 N 的表所产生的的派生表结果。派生表有可能产生自 FROM 语句中的子查询。 -\u0026lt;subqueryN\u0026gt; : 本行引用了 id 为 N 的表所产生的的物化子查询结果。 type（重要） # 查询执行的类型，描述了查询是如何执行的。所有值的顺序从最优到最差排序为：system \u0026gt; const \u0026gt; eq_ref \u0026gt; ref \u0026gt; fulltext \u0026gt; ref_or_null \u0026gt; index_merge \u0026gt; unique_subquery \u0026gt; index_subquery \u0026gt; range \u0026gt; index \u0026gt; ALL\n常见的几种类型具体含义如下：\nsystem：如果表使用的引擎对于表行数统计是精确的（如：MyISAM），且表中只有一行记录的情况下，访问方法是 system ，是 const 的一种特例。 const：表中最多只有一行匹配的记录，一次查询就可以找到，常用于使用主键或唯一索引的所有字段作为查询条件。 eq_ref：当连表查询时，前一张表的行在当前这张表中只有一行与之对应。是除了 system 与 const 之外最好的 join 方式，常用于使用主键或唯一索引的所有字段作为连表条件。 ref：使用普通索引作为查询条件，查询结果可能找到多个符合条件的行。 index_merge：当查询条件使用了多个索引时，表示开启了 Index Merge 优化，此时执行计划中的 key 列列出了使用到的索引。 range：对索引列进行范围查询，执行计划中的 key 列表示哪个索引被使用了。 index：查询遍历了整棵索引树，与 ALL 类似，只不过扫描的是索引，而索引一般在内存中，速度更快。 ALL：全表扫描。 possible_keys # possible_keys 列表示 MySQL 执行查询时可能用到的索引。如果这一列为 NULL ，则表示没有可能用到的索引；这种情况下，需要检查 WHERE 语句中所使用的的列，看是否可以通过给这些列中某个或多个添加索引的方法来提高查询性能。\nkey（重要） # key 列表示 MySQL 实际使用到的索引。如果为 NULL，则表示未用到索引。\nkey_len # key_len 列表示 MySQL 实际使用的索引的最大长度；当使用到联合索引时，有可能是多个列的长度和。在满足需求的前提下越短越好。如果 key 列显示 NULL ，则 key_len 列也显示 NULL 。\nrows # rows 列表示根据表统计信息及选用情况，大致估算出找到所需的记录或所需读取的行数，数值越小越好。\nExtra（重要） # 这列包含了 MySQL 解析查询的额外信息，通过这些信息，可以更准确的理解 MySQL 到底是如何执行查询的。常见的值如下：\nUsing filesort：在排序时使用了外部的索引排序，没有用到表内索引进行排序。 Using temporary：MySQL 需要创建临时表来存储查询的结果，常见于 ORDER BY 和 GROUP BY。 Using index：表明查询使用了覆盖索引，不用回表，查询效率非常高。 Using index condition：表示查询优化器选择使用了索引条件下推这个特性。 Using where：表明查询使用了 WHERE 子句进行条件过滤。一般在没有使用到索引的时候会出现。 Using join buffer (Block Nested Loop)：连表查询的方式，表示当被驱动表的没有使用索引的时候，MySQL 会先将驱动表读出来放到 join buffer 中，再遍历被驱动表与驱动表进行查询。 这里提醒下，当 Extra 列包含 Using filesort 或 Using temporary 时，MySQL 的性能可能会存在问题，需要尽可能避免。\n参考 # https://dev.mysql.com/doc/refman/5.7/en/explain-output.html https://juejin.cn/post/6953444668973514789 "},{"id":56,"href":"/zh/docs/technology/Review/java_guide/database/ly0503lysql-question-01/","title":"sql常见面试题总结01","section":"数据库","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n题目来源于：牛客题霸 - SQL 必知必会\n检索数据 # select 用于从数据库中查询数据。\n从 Customers 表中检索所有的 ID # 现有表 Customers 如下：\ncust_id A B C 编写 SQL 语句，从 Customers 表中检索所有的 cust_id。\n答案：\nselect cust_id from Customers; 检索并列出已订购产品的清单 # 表 OrderItems 含有非空的列 prod_id 代表商品 id，包含了所有已订购的商品（有些已被订购多次）。\nprod_id a1 a2 a3 a4 a5 a6 a7 编写 SQL 语句，检索并列出所有已订购商品（prod_id）的去重后的清单。\n答案：\nselect distinct prod_id from OrderItems; 知识点：distinct 用于返回列中的唯一不同值。\n检索所有列 # 现在有 Customers 表（表中含有列 cust_id 代表客户 id，cust_name 代表客户姓名）\ncust_id cust_name a1 andy a2 ben a3 tony a4 tom a5 an a6 lee a7 hex 需要编写 SQL 语句，检索所有列。\n答案：\nselect cust_id, cust_name from Customers; 排序检索数据 # order by 用于对结果集按照一个列或者多个列进行排序。默认按照升序对记录进行排序，如果需要按照降序对记录进行排序，可以使用 desc 关键字。\n检索顾客名称并且排序 # 有表 Customers，cust_id 代表客户 id，cust_name 代表客户姓名。\ncust_id cust_name a1 andy a2 ben a3 tony a4 tom a5 an a6 lee a7 hex 从 Customers 中检索所有的顾客名称（cust_name），并按从 Z 到 A 的顺序显示结果。\n答案：\nselect cust_name from Customers order by cust_name desc 对顾客 ID 和日期排序 # 有 Orders 表：\ncust_id order_num order_date andy aaaa 2021-01-01 00:00:00 andy bbbb 2021-01-01 12:00:00 bob cccc 2021-01-10 12:00:00 dick dddd 2021-01-11 00:00:00 编写 SQL 语句，从 Orders 表中检索顾客 ID（cust_id）和订单号（order_num），并先按顾客 ID 对结果进行排序，再按订单日期倒序排列。\n答案：\n# 根据列名排序 # 注意：是 order_date 降序，而不是 order_num select cust_id, order_num from Orders order by cust_id, order_date desc; 知识点：order by 对多列排序的时候，先排序的列放前面，后排序的列放后面。并且，不同的列可以有不同的排序规则。\n按照数量和价格排序 # 假设有一个 OrderItems 表：\nquantity item_price 1 100 10 1003 2 500 编写 SQL 语句，显示 OrderItems 表中的数量（quantity）和价格（item_price），并按数量由多到少、价格由高到低排序。\n答案：\nselect quantity, item_price from OrderItems order by quantity desc, item_price desc; 检查 SQL 语句 # 有 Vendors 表：\nvend_name 海底捞 小龙坎 大龙燚 下面的 SQL 语句有问题吗？尝试将它改正确，使之能够正确运行，并且返回结果根据vend_name 逆序排列。\nSELECT vend_name, FROM Vendors ORDER vend_name DESC; 改正后：\nselect vend_name from Vendors order by vend_name desc; 知识点：\n逗号作用是用来隔开列与列之间的。 order by 是有 by 的，需要撰写完整，且位置正确。 过滤数据 # where 可以过滤返回的数据。\n下面的运算符可以在 where 子句中使用：\n运算符 描述 = 等于 \u0026lt;\u0026gt; 不等于。**注释：**在 SQL 的一些版本中，该操作符可被写成 != \u0026gt; 大于 \u0026lt; 小于 \u0026gt;= 大于等于 \u0026lt;= 小于等于 BETWEEN 在某个范围内 LIKE 搜索某种模式 IN 指定针对某个列的多个可能值 返回固定价格的产品 # 有表 Products ：\nprod_id prod_name prod_price a0018 sockets 9.49 a0019 iphone13 600 b0018 gucci t-shirts 1000 【问题】从 Products 表中检索产品 ID（prod_id）和产品名称（prod_name），只返回价格为 9.49 美元的产品。\n答案：\nselect prod_id, prod_name from Products where prod_price = 9.49; 返回更高价格的产品 # 有表 Products ：\nprod_id prod_name prod_price a0018 sockets 9.49 a0019 iphone13 600 b0019 gucci t-shirts 1000 【问题】编写 SQL 语句，从 Products 表中检索产品 ID（prod_id）和产品名称（prod_name），只返回价格为 9 美元或更高的产品。\n答案：\nselect prod_id, prod_name from Products where prod_price \u0026gt;= 9; 返回产品并且按照价格排序 # 有表 Products ：\nprod_id prod_name prod_price a0011 egg 3 a0019 sockets 4 b0019 coffee 15 【问题】编写 SQL 语句，返回 Products 表中所有价格在 3 美元到 6 美元之间的产品的名称（prod_name）和价格（prod_price），然后按价格对结果进行排序。\n答案：\nselect prod_name, prod_price from Products where prod_price between 3 and 6 order by prod_price; # 或者 select prod_name, prod_price from Products where prod_price \u0026gt;= 3 and prod_price \u0026lt;= 6 order by prod_price; 返回更多的产品 # OrderItems 表含有：订单号 order_num，quantity产品数量\norder_num quantity a1 105 a2 1100 a2 200 a4 1121 a5 10 a2 19 a7 5 【问题】从 OrderItems 表中检索出所有不同且不重复的订单号（order_num），其中每个订单都要包含 100 个或更多的产品。\n答案：\nselect distinct order_num from OrderItems where quantity \u0026gt;= 100; 高级数据过滤 # and 和 or 运算符用于基于一个以上的条件对记录进行过滤，两者可以结合使用。and 必须 2 个条件都成立，or只要 2 个条件中的一个成立即可。\n检索供应商名称 # Vendors 表有字段供应商名称（vend_name）、供应商国家（vend_country）、供应商州（vend_state）\nvend_name vend_country vend_state apple USA CA vivo CNA shenzhen huawei CNA xian 【问题】编写 SQL 语句，从 Vendors 表中检索供应商名称（vend_name），仅返回加利福尼亚州的供应商（这需要按国家[USA]和州[CA]进行过滤，没准其他国家也存在一个 CA）\n答案：\nselect vend_name from Vendors where vend_country = \u0026#39;USA\u0026#39; and vend_state = \u0026#39;CA\u0026#39;; 检索并列出已订购产品的清单 # OrderItems 表包含了所有已订购的产品（有些已被订购多次）。\nprod_id order_num quantity BR01 a1 105 BR02 a2 1100 BR02 a2 200 BR03 a4 1121 BR017 a5 10 BR02 a2 19 BR017 a7 5 【问题】编写 SQL 语句，查找所有订购了数量至少 100 个的 BR01、BR02 或 BR03 的订单。你需要返回 OrderItems 表的订单号（order_num）、产品 ID（prod_id）和数量（quantity），并按产品 ID 和数量进行过滤。\n答案：\nselect order_num, prod_id, quantity from OrderItems where quantity \u0026gt;= 100 and prod_id in(\u0026#39;BR01\u0026#39;, \u0026#39;BR02\u0026#39;, \u0026#39;BR03\u0026#39;); 返回所有价格在 3 美元到 6 美元之间的产品的名称和价格 # 有表 Products：\nprod_id prod_name prod_price a0011 egg 3 a0019 sockets 4 b0019 coffee 15 【问题】编写 SQL 语句，返回所有价格在 3 美元到 6 美元之间的产品的名称（prod_name）和价格（prod_price），使用 AND 操作符，然后按价格对结果进行升序排序。\n答案：\nselect prod_name, prod_price from Products where prod_price between 3 and 6 order by prod_price; 检查 SQL 语句 # 供应商表 Vendors 有字段供应商名称 vend_name、供应商国家 vend_country、供应商省份 vend_state\nvend_name vend_country vend_state apple USA CA vivo CNA shenzhen huawei CNA xian 【问题】修改正确下面 sql，使之正确返回。\nSELECT vend_name FROM Vendors ORDER BY vend_name WHERE vend_country = \u0026#39;USA\u0026#39; AND vend_state = \u0026#39;CA\u0026#39;; 修改后：\nselect vend_name from Vendors where vend_country = \u0026#39;USA\u0026#39; and vend_state = \u0026#39;CA\u0026#39; order by vend_name; order by 语句必须放在 where 之后。\n用通配符进行过滤 # SQL 通配符必须与 LIKE 运算符一起使用\n在 SQL 中，可使用以下通配符：\n通配符 描述 % 代表零个或多个字符 _ 仅替代一个字符 [charlist] 字符列中的任何单一字符 [^charlist] 或者 [!charlist] 不在字符列中的任何单一字符 检索产品名称和描述（一） # Products 表如下：\nprod_name prod_desc a0011 usb a0019 iphone13 b0019 gucci t-shirts c0019 gucci toy d0019 lego toy 【问题】编写 SQL 语句，从 Products 表中检索产品名称（prod_name）和描述（prod_desc），仅返回描述中包含 toy 一词的产品名称。\n答案：\nselect prod_name, prod_desc from Products where prod_desc like \u0026#39;%toy%\u0026#39;; 检索产品名称和描述（二） # Products 表如下：\nprod_name prod_desc a0011 usb a0019 iphone13 b0019 gucci t-shirts c0019 gucci toy d0019 lego toy 【问题】编写 SQL 语句，从 Products 表中检索产品名称（prod_name）和描述（prod_desc），仅返回描述中未出现 toy 一词的产品，最后按”产品名称“对结果进行排序。\n答案：\nselect prod_name, prod_desc from Products where prod_desc not like \u0026#39;%toy%\u0026#39; order by prod_name; 检索产品名称和描述（三） # Products 表如下：\nprod_name prod_desc a0011 usb a0019 iphone13 b0019 gucci t-shirts c0019 gucci toy d0019 lego carrots toy 【问题】编写 SQL 语句，从 Products 表中检索产品名称（prod_name）和描述（prod_desc），仅返回描述中同时出现 toy 和 carrots 的产品。有好几种方法可以执行此操作，但对于这个挑战题，请使用 AND 和两个 LIKE 比较。\n答案：\nselect prod_name, prod_desc from Products where prod_desc like \u0026#39;%toy%\u0026#39; and prod_desc like \u0026#34;%carrots%\u0026#34;; 检索产品名称和描述（四） # Products 表如下：\nprod_name prod_desc a0011 usb a0019 iphone13 b0019 gucci t-shirts c0019 gucci toy d0019 lego toy carrots 【问题】编写 SQL 语句，从 Products 表中检索产品名称（prod_name）和描述（prod_desc），仅返回在描述中以先后顺序同时出现 toy 和 carrots 的产品。提示：只需要用带有三个 % 符号的 LIKE 即可。\n答案：\nselect prod_name, prod_desc from Products where prod_desc like \u0026#39;%toy%carrots%\u0026#39;; 创建计算字段 # 别名 # 别名的常见用法是在检索出的结果中重命名表的列字段（为了符合特定的报表要求或客户需求）。有表 Vendors 代表供应商信息，vend_id 供应商 id、vend_name 供应商名称、vend_address 供应商地址、vend_city 供应商城市。\nvend_id vend_name vend_address vend_city a001 tencent cloud address1 shenzhen a002 huawei cloud address2 dongguan a003 aliyun cloud address3 hangzhou a003 netease cloud address4 guangzhou 【问题】编写 SQL 语句，从 Vendors 表中检索 vend_id、vend_name、vend_address 和 vend_city，将 vend_name 重命名为 vname，将 vend_city 重命名为 vcity，将 vend_address 重命名为 vaddress，按供应商名称对结果进行升序排序。\n答案：\nselect vend_id, vend_name as vname, vend_address as vaddress, vend_city as vcity from Vendors order by vname; # as 可以省略 select vend_id, vend_name vname, vend_address vaddress, vend_city vcity from Vendors order by vname; 打折 # 我们的示例商店正在进行打折促销，所有产品均降价 10%。Products 表包含 prod_id 产品 id、prod_price 产品价格。\n【问题】编写 SQL 语句，从 Products 表中返回 prod_id、prod_price 和 sale_price。sale_price 是一个包含促销价格的计算字段。提示：可以乘以 0.9，得到原价的 90%（即 10%的折扣）。\n答案：\nselect prod_id, prod_price, prod_price * 0.9 as sale_price from Products; 注意：sale_price 是对计算结果的命名，而不是原有的列名。\n使用函数处理数据 # 顾客登录名 # 我们的商店已经上线了，正在创建顾客账户。所有用户都需要登录名，默认登录名是其名称和所在城市的组合。\n给出 Customers 表 如下：\ncust_id cust_name cust_contact cust_city a1 Andy Li Andy Li Oak Park a2 Ben Liu Ben Liu Oak Park a3 Tony Dai Tony Dai Oak Park a4 Tom Chen Tom Chen Oak Park a5 An Li An Li Oak Park a6 Lee Chen Lee Chen Oak Park a7 Hex Liu Hex Liu Oak Park 【问题】编写 SQL 语句，返回顾客 ID（cust_id）、顾客名称（cust_name）和登录名（user_login），其中登录名全部为大写字母，并由顾客联系人的前两个字符（cust_contact）和其所在城市的前三个字符（cust_city）组成。提示：需要使用函数、拼接和别名。\n答案：\nselect cust_id, cust_name, upper(concat(substring(cust_contact, 1, 2), substring(cust_city, 1, 3))) as user_login from Customers; 知识点：\n截取函数substring()：截取字符串，substring(str ,n ,m)：返回字符串 str 从第 n 个字符截取到第 m 个字符（左闭右闭）；\n返回字符串 str 从第 n 个字符截取 m 个字符（左闭右闭）\n拼接函数concat()：将两个或多个字符串连接成一个字符串，select concat(A,B) ：连接字符串 A 和 B。\n大写函数 upper()：将指定字符串转换为大写。\n返回 2020 年 1 月的所有订单的订单号和订单日期 # Orders 订单表如下：\norder_num order_date a0001 2020-01-01 00:00:00 a0002 2020-01-02 00:00:00 a0003 2020-01-01 12:00:00 a0004 2020-02-01 00:00:00 a0005 2020-03-01 00:00:00 【问题】编写 SQL 语句，返回 2020 年 1 月的所有订单的订单号（order_num）和订单日期（order_date），并按订单日期升序排序\n答案：\nselect order_num, order_date from Orders where month(order_date) = \u0026#39;01\u0026#39; and year(order_date) = \u0026#39;2020\u0026#39; order by order_date; 也可以用通配符来做：\nselect order_num, order_date from Orders where order_date like \u0026#39;2020-01%\u0026#39; order by order_date; 知识点：\n日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 日期和时间处理相关的常用函数：\n函 数 说 明 adddate() 增加一个日期（天、周等） addtime() 增加一个时间（时、分等） curdate() 返回当前日期 curtime() 返回当前时间 date() 返回日期时间的日期部分 datediff() 计算两个日期之差 date_format() 返回一个格式化的日期或时间串 day() 返回一个日期的天数部分 dayofweek() 对于一个日期，返回对应的星期几 hour() 返回一个时间的小时部分 minute() 返回一个时间的分钟部分 month() 返回一个日期的月份部分 now() 返回当前日期和时间 second() 返回一个时间的秒部分 time() 返回一个日期时间的时间部分 year() 返回一个日期的年份部分 汇总数据 # 汇总数据相关的函数：\n函 数 说 明 avg() 返回某列的平均值 count() 返回某列的行数 max() 返回某列的最大值 min() 返回某列的最小值 sum() 返回某列值之和 确定已售出产品的总数 # OrderItems 表代表售出的产品，quantity 代表售出商品数量。\nquantity 10 100 1000 10001 2 15 【问题】编写 SQL 语句，确定已售出产品的总数。\n答案：\nselect sum(quantity) as items_ordered from OrderItems; 确定已售出产品项 BR01 的总数 # OrderItems 表代表售出的产品，quantity 代表售出商品数量，产品项为 prod_id。\nquantity prod_id 10 AR01 100 AR10 1000 BR01 10001 BR010 【问题】修改创建的语句，确定已售出产品项（prod_id）为\u0026quot;BR01\u0026quot;的总数。\n答案：\nselect sum(quantity) as items_ordered from OrderItems where prod_id = \u0026#39;BR01\u0026#39;; 确定 Products 表中价格不超过 10 美元的最贵产品的价格 # Products 表如下，prod_price 代表商品的价格。\nprod_price 9.49 600 1000 【问题】编写 SQL 语句，确定 Products 表中价格不超过 10 美元的最贵产品的价格（prod_price）。将计算所得的字段命名为 max_price。\n答案：\nselect max(prod_price) as max_price from Products where prod_price \u0026lt;= 10; 分组数据 # group by ：\ngroup by 子句将记录分组到汇总行中。 group by 为每个组返回一个记录。 group by 通常还涉及聚合count，max，sum，avg 等。 group by 可以按一列或多列进行分组。 group by 按分组字段进行排序后，order by 可以以汇总字段来进行排序。 having：\nhaving 用于对汇总的 group by 结果进行过滤。 having 必须要与 group by 连用。 where 和 having 可以在相同的查询中。 having vs where：\nwhere：过滤过滤指定的行，后面不能加聚合函数（分组函数）。 having：过滤分组，必须要与 group by 连用，不能单独使用。 返回每个订单号各有多少行数 # OrderItems 表包含每个订单的每个产品\norder_num a002 a002 a002 a004 a007 【问题】编写 SQL 语句，返回每个订单号（order_num）各有多少行数（order_lines），并按 order_lines 对结果进行升序排序。\n答案：\nselect order_num, count(order_num) as order_lines from OrderItems group by order_num order by order_lines; 知识点：\ncount(*),count(列名)都可以，区别在于，count(列名)是统计非 NULL 的行数； order by 最后执行，所以可以使用列别名； 分组聚合一定不要忘记加上 group by ,不然只会有一行结果。 每个供应商成本最低的产品 # 有 Products 表，含有字段 prod_price 代表产品价格，vend_id 代表供应商 id\nvend_id prod_price a0011 100 a0019 0.1 b0019 1000 b0019 6980 b0019 20 【问题】编写 SQL 语句，返回名为 cheapest_item 的字段，该字段包含每个供应商成本最低的产品（使用 Products 表中的 prod_price），然后从最低成本到最高成本对结果进行升序排序。\n答案：\nselect vend_id, min(prod_price) as cheapest_item from Products group by vend_id order by cheapest_item; 返回订单数量总和不小于 100 的所有订单的订单号 # OrderItems 代表订单商品表，包括：订单号 order_num 和订单数量 quantity。\norder_num quantity a1 105 a2 1100 a2 200 a4 1121 a5 10 a2 19 a7 5 【问题】请编写 SQL 语句，返回订单数量总和不小于 100 的所有订单号，最后结果按照订单号升序排序。\n答案：\n# 直接聚合 select order_num from OrderItems group by order_num having sum(quantity) \u0026gt;= 100 order by order_num; # 子查询 select order_num from (select order_num, sum(quantity) as sum_num from OrderItems group by order_num having sum_num \u0026gt;= 100 ) a order by order_num; 知识点：\nwhere：过滤过滤指定的行，后面不能加聚合函数（分组函数）。 having：过滤分组，与 group by 连用，不能单独使用。 计算总和 # OrderItems 表代表订单信息，包括字段：订单号 order_num 和 item_price 商品售出价格、quantity 商品数量。\norder_num item_price quantity a1 10 105 a2 1 1100 a2 1 200 a4 2 1121 a5 5 10 a2 1 19 a7 7 5 【问题】编写 SQL 语句，根据订单号聚合，返回订单总价不小于 1000 的所有订单号，最后的结果按订单号进行升序排序。\n提示：总价 = item_price 乘以 quantity\n答案：\nselect order_num, sum(item_price * quantity) as total_price from OrderItems group by order_num having total_price \u0026gt;= 1000 order by order_num; 检查 SQL 语句 # OrderItems 表含有 order_num 订单号\norder_num a002 a002 a002 a004 a007 【问题】将下面代码修改正确后执行\nSELECT order_num, COUNT(*) AS items FROM OrderItems GROUP BY items HAVING COUNT(*) \u0026gt;= 3 ORDER BY items, order_num; 修改后：\nSELECT order_num, COUNT(*) AS items FROM OrderItems GROUP BY order_num HAVING items \u0026gt;= 3 ORDER BY items, order_num; 使用子查询 # 子查询是嵌套在较大查询中的 SQL 查询，也称内部查询或内部选择，包含子查询的语句也称为外部查询或外部选择。简单来说，子查询就是指将一个 select 查询（子查询）的结果作为另一个 SQL 语句（主查询）的数据来源或者判断条件。\n子查询可以嵌入 select、insert、update 和 delete 语句中，也可以和 =、\u0026lt;、\u0026gt;、in、between、exists 等运算符一起使用。\n子查询常用在 where 子句和 from 子句后边：\n当用于 where 子句时，根据不同的运算符，子查询可以返回单行单列、多行单列、单行多列数据。子查询就是要返回能够作为 WHERE 子句查询条件的值。 当用于 from 子句时，一般返回多行多列数据，相当于返回一张临时表，这样才符合 from 后面是表的规则。这种做法能够实现多表联合查询。 注意：MySQL 数据库从 4.1 版本才开始支持子查询，早期版本是不支持的。\n用于 where 子句的子查询的基本语法如下：\nselect column_name [, column_name ] from table1 [, table2 ] where column_name operator (select column_name [, column_name ] from table1 [, table2 ] [where]) 子查询需要放在括号( )内。 operator 表示用于 where 子句的运算符。 用于 from 子句的子查询的基本语法如下：\nselect column_name [, column_name ] from (select column_name [, column_name ] from table1 [, table2 ] [where]) as temp_table_name where condition 用于 from 的子查询返回的结果相当于一张临时表，所以需要使用 AS 关键字为该临时表起一个名字。\n返回购买价格为 10 美元或以上产品的顾客列表 # OrderItems` 表示订单商品表，含有字段 订单号：`order_num`、 订单价格：`item_price`； `Orders` 表代表订单信息表，含有 顾客 `id：cust_id` 和 订单号：`order_num OrderItems 表:\norder_num item_price a1 10 a2 1 a2 1 a4 2 a5 5 a2 1 a7 7 Orders 表：\norder_num cust_id a1 cust10 a2 cust1 a2 cust1 a4 cust2 a5 cust5 a2 cust1 a7 cust7 【问题】使用子查询，返回购买价格为 10 美元或以上产品的顾客列表，结果无需排序。\n答案：\nselect cust_id from Orders where order_num in ( select order_num from OrderItems group by order_num having sum(item_price) \u0026gt;= 10 ); 确定哪些订单购买了 prod_id 为 BR01 的产品（一） # 表 OrderItems 代表订单商品信息表，prod_id 为产品 id；Orders 表代表订单表有 cust_id 代表顾客 id 和订单日期 order_date\nOrderItems 表：\nprod_id order_num BR01 a0001 BR01 a0002 BR02 a0003 BR02 a0013 Orders 表：\norder_num cust_id order_date a0001 cust10 2022-01-01 00:00:00 a0002 cust1 2022-01-01 00:01:00 a0003 cust1 2022-01-02 00:00:00 a0013 cust2 2022-01-01 00:20:00 【问题】\n编写 SQL 语句，使用子查询来确定哪些订单（在 OrderItems 中）购买了 prod_id 为 \u0026ldquo;BR01\u0026rdquo; 的产品，然后从 Orders 表中返回每个产品对应的顾客 ID（cust_id）和订单日期（order_date），按订购日期对结果进行升序排序。\n答案：\n# 写法 1：子查询 select cust_id, order_date from Orders where order_num in ( select order_num from OrderItems where prod_id = \u0026#39;BR01\u0026#39; ) order by order_date; # 写法 2: 连接表 select b.cust_id, b.order_date from OrderItems a, Orders b where a.order_num = b.order_num and a.prod_id = \u0026#39;BR01\u0026#39; order by order_date; 返回购买 prod_id 为 BR01 的产品的所有顾客的电子邮件（一） # 你想知道订购 BR01 产品的日期，有表 OrderItems 代表订单商品信息表，prod_id 为产品 id；Orders 表代表订单表有 cust_id 代表顾客 id 和订单日期 order_date；Customers 表含有 cust_email 顾客邮件和 cust_id 顾客 id\nOrderItems 表：\nprod_id order_num BR01 a0001 BR01 a0002 BR02 a0003 BR02 a0013 Orders 表：\norder_num cust_id order_date a0001 cust10 2022-01-01 00:00:00 a0002 cust1 2022-01-01 00:01:00 a0003 cust1 2022-01-02 00:00:00 a0013 cust2 2022-01-01 00:20:00 Customers 表代表顾客信息，cust_id 为顾客 id，cust_email 为顾客 email\ncust_id cust_email cust10 cust10@cust.com cust1 cust1@cust.com cust2 cust2@cust.com 【问题】返回购买 prod_id 为 BR01 的产品的所有顾客的电子邮件（Customers 表中的 cust_email），结果无需排序。\n提示：这涉及 SELECT 语句，最内层的从 OrderItems 表返回 order_num，中间的从 Customers 表返回 cust_id。\n答案：\n# 写法 1：子查询 select cust_email from Customers where cust_id in ( select cust_id from Orders where order_num in ( select order_num from OrderItems where prod_id = \u0026#39;BR01\u0026#39; ) ); # 写法 2: 连接表（inner join） select c.cust_email from OrderItems a, Orders b, Customers c where a.order_num = b.order_num and b.cust_id = c.cust_id and a.prod_id = \u0026#39;BR01\u0026#39;; # 写法 3：连接表（left join） select c.cust_email from Orders a left join OrderItems b on a.order_num = b.order_num left join Customers c on a.cust_id = c.cust_id where b.prod_id = \u0026#39;BR01\u0026#39;; 返回每个顾客不同订单的总金额 # 我们需要一个顾客 ID 列表，其中包含他们已订购的总金额。\nOrderItems 表代表订单信息，OrderItems 表有订单号：order_num 和商品售出价格：item_price、商品数量：quantity。\norder_num item_price quantity a0001 10 105 a0002 1 1100 a0002 1 200 a0013 2 1121 a0003 5 10 a0003 1 19 a0003 7 5 Orders` 表订单号：`order_num`、顾客 id：`cust_id order_num cust_id a0001 cust10 a0002 cust1 a0003 cust1 a0013 cust2 【问题】\n编写 SQL 语句，返回顾客 ID（Orders 表中的 cust_id），并使用子查询返回 total_ordered 以便返回每个顾客的订单总数，将结果按金额从大到小排序。\n答案：\n# 写法 1：子查询 SELECT o.cust_id cust_id, tb.total_ordered total_ordered FROM ( SELECT order_num, SUM(item_price * quantity) total_ordered FROM OrderItems GROUP BY order_num ) as tb, Orders o WHERE tb.order_num = o.order_num ORDER BY total_ordered DESC; # 写法 2：连接表 select b.cust_id, sum(a.quantity * a.item_price) as total_ordered from OrderItems a, Orders b where a.order_num = b.order_num group by cust_id order by total_ordered desc; 从 Products 表中检索所有的产品名称以及对应的销售总数 # Products` 表中检索所有的产品名称：`prod_name`、产品 id：`prod_id prod_id prod_name a0001 egg a0002 sockets a0013 coffee a0003 cola OrderItems` 代表订单商品表，订单产品：`prod_id`、售出数量：`quantity prod_id quantity a0001 105 a0002 1100 a0002 200 a0013 1121 a0003 10 a0003 19 a0003 5 【问题】\n编写 SQL 语句，从 Products 表中检索所有的产品名称（prod_name），以及名为 quant_sold 的计算列，其中包含所售产品的总数（在 OrderItems 表上使用子查询和 SUM(quantity) 检索）。\n答案：\n# 写法 1：子查询 select p.prod_name, tb.quant_sold from ( select prod_id, sum(quantity) as quant_sold from OrderItems group by prod_id ) as tb, Products p where tb.prod_id = p.prod_id; # 写法 2：连接表 select p.prod_name, sum(o.quantity) as quant_sold from Products p, OrderItems o where p.prod_id = o.prod_id group by p.prod_name;（这里不能用 p.prod_id，会报错） 连接表 # JOIN 是“连接”的意思，顾名思义，SQL JOIN 子句用于将两个或者多个表联合起来进行查询。\n连接表时需要在每个表中选择一个字段，并对这些字段的值进行比较，值相同的两条记录将合并为一条。连接表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。\n使用 JOIN 连接两个表的基本语法如下：\nselect table1.column1, table2.column2... from table1 join table2 on table1.common_column1 = table2.common_column2; table1.common_column1 = table2.common_column2 是连接条件，只有满足此条件的记录才会合并为一行。您可以使用多个运算符来连接表，例如 =、\u0026gt;、\u0026lt;、\u0026lt;\u0026gt;、\u0026lt;=、\u0026gt;=、!=、between、like 或者 not，但是最常见的是使用 =。\n当两个表中有同名的字段时，为了帮助数据库引擎区分是哪个表的字段，在书写同名字段名时需要加上表名。当然，如果书写的字段名在两个表中是唯一的，也可以不使用以上格式，只写字段名即可。\n另外，如果两张表的关联字段名相同，也可以使用 USING子句来代替 ON，举个例子：\n# join....on select c.cust_name, o.order_num from Customers c inner join Orders o on c.cust_id = o.cust_id order by c.cust_name; # 如果两张表的关联字段名相同，也可以使用USING子句：join....using() select c.cust_name, o.order_num from Customers c inner join Orders o using(cust_id) order by c.cust_name; ON 和 WHERE 的区别：\n连接表时，SQL 会根据连接条件生成一张新的临时表。ON 就是连接条件，它决定临时表的生成。 WHERE 是在临时表生成以后，再对临时表中的数据进行过滤，生成最终的结果集，这个时候已经没有 JOIN-ON 了。 所以总结来说就是：SQL 先根据 ON 生成一张临时表，然后再根据 WHERE 对临时表进行筛选。\nSQL 允许在 JOIN 左边加上一些修饰性的关键词，从而形成不同类型的连接，如下表所示：\n连接类型 说明 INNER JOIN 内连接 （默认连接方式）只有当两个表都存在满足条件的记录时才会返回行。 LEFT JOIN / LEFT OUTER JOIN 左(外)连接 返回左表中的所有行，即使右表中没有满足条件的行也是如此。 RIGHT JOIN / RIGHT OUTER JOIN 右(外)连接 返回右表中的所有行，即使左表中没有满足条件的行也是如此。 FULL JOIN / FULL OUTER JOIN 全(外)连接 只要其中有一个表存在满足条件的记录，就返回行。 SELF JOIN 将一个表连接到自身，就像该表是两个表一样。为了区分两个表，在 SQL 语句中需要至少重命名一个表。 CROSS JOIN 交叉连接，从两个或者多个连接表中返回记录集的笛卡尔积。 下图展示了 LEFT JOIN、RIGHT JOIN、INNER JOIN、OUTER JOIN 相关的 7 种用法。\n如果不加任何修饰词，只写 JOIN，那么默认为 INNER JOIN\n对于 INNER JOIN 来说，还有一种隐式的写法，称为 “隐式内连接”，也就是没有 INNER JOIN 关键字，使用 WHERE 语句实现内连接的功能\n# 隐式内连接 select c.cust_name, o.order_num from Customers c, Orders o where c.cust_id = o.cust_id order by c.cust_name; # 显式内连接 select c.cust_name, o.order_num from Customers c inner join Orders o using(cust_id) order by c.cust_name; 返回顾客名称和相关订单号 # Customers` 表有字段顾客名称 `cust_name`、顾客 id `cust_id cust_id cust_name cust10 andy cust1 ben cust2 tony cust22 tom cust221 an cust2217 hex Orders 订单信息表，含有字段 order_num 订单号、cust_id 顾客 id\norder_num cust_id a1 cust10 a2 cust1 a3 cust2 a4 cust22 a5 cust221 a7 cust2217 【问题】编写 SQL 语句，返回 Customers 表中的顾客名称（cust_name）和 Orders 表中的相关订单号（order_num），并按顾客名称再按订单号对结果进行升序排序。你可以尝试用两个不同的写法，一个使用简单的等连接语法，另外一个使用 INNER JOIN。\n答案：\n# 隐式内连接 select c.cust_name, o.order_num from Customers c, Orders o where c.cust_id = o.cust_id order by c.cust_name; # 显式内连接 select c.cust_name, o.order_num from Customers c inner join Orders o using(cust_id) order by c.cust_name; 返回顾客名称和相关订单号以及每个订单的总价 # Customers` 表有字段，顾客名称：`cust_name`、顾客 id：`cust_id cust_id cust_name cust10 andy cust1 ben cust2 tony cust22 tom cust221 an cust2217 hex Orders` 订单信息表，含有字段，订单号：`order_num`、顾客 id：`cust_id order_num cust_id a1 cust10 a2 cust1 a3 cust2 a4 cust22 a5 cust221 a7 cust2217 OrderItems` 表有字段，商品订单号：`order_num`、商品数量：`quantity`、商品价格：`item_price order_num quantity item_price a1 1000 10 a2 200 10 a3 10 15 a4 25 50 a5 15 25 a7 7 7 【问题】除了返回顾客名称和订单号，返回 Customers 表中的顾客名称（cust_name）和 Orders 表中的相关订单号（order_num），添加第三列 OrderTotal，其中包含每个订单的总价，并按顾客名称再按订单号对结果进行升序排序。\n# 简单的等连接语法 select c.cust_name, o.order_num, sum(quantity * item_price) as OrderTotal from Customers c, Orders o, OrderItems oi where c.cust_id = o.cust_id and o.order_num = oi.order_num group by c.cust_name, o.order_num order by c.cust_name, o.order_num; 注意，可能有小伙伴会这样写：\nselect c.cust_name, o.order_num, sum(quantity * item_price) as OrderTotal from Customers c, Orders o, OrderItems oi where c.cust_id = o.cust_id and o.order_num = oi.order_num group by c.cust_name order by c.cust_name, o.order_num; 这是错误的！只对 cust_name 进行聚类确实符合题意，但是不符合 group by 的语法。\nselect 语句中，如果没有 group by 语句，那么 cust_name、order_num 会返回若干个值，而 sum(quantity _ item_price) 只返回一个值，通过 group by cust_name 可以让 cust_name 和 sum(quantity _ item_price) 一一对应起来，或者说聚类，所以同样的，也要对 order_num 进行聚类。\n一句话，select 中的字段要么都聚类，要么都不聚类\n确定哪些订单购买了 prod_id 为 BR01 的产品（二） # 表 OrderItems 代表订单商品信息表，prod_id 为产品 id；Orders 表代表订单表有 cust_id 代表顾客 id 和订单日期 order_date\nOrderItems 表：\nprod_id order_num BR01 a0001 BR01 a0002 BR02 a0003 BR02 a0013 Orders 表：\norder_num cust_id order_date a0001 cust10 2022-01-01 00:00:00 a0002 cust1 2022-01-01 00:01:00 a0003 cust1 2022-01-02 00:00:00 a0013 cust2 2022-01-01 00:20:00 【问题】\n编写 SQL 语句，使用子查询来确定哪些订单（在 OrderItems 中）购买了 prod_id 为 \u0026ldquo;BR01\u0026rdquo; 的产品，然后从 Orders 表中返回每个产品对应的顾客 ID（cust_id）和订单日期（order_date），按订购日期对结果进行升序排序。\n提示：这一次使用连接和简单的等连接语法。\n# 写法 1：子查询 select cust_id, order_date from Orders where order_num in ( select order_num from OrderItems where prod_id = \u0026#39;BR01\u0026#39; ) order by order_date; # 写法 2：连接表 inner join select cust_id, order_date from Orders o inner join ( select order_num from OrderItems where prod_id = \u0026#39;BR01\u0026#39; ) tb on o.order_num = tb.order_num order by order_date; # 写法 3：写法 2 的简化版 select cust_id, order_date from Orders inner join OrderItems using(order_num) where OrderItems.prod_id = \u0026#39;BR01\u0026#39; order by order_date; 返回购买 prod_id 为 BR01 的产品的所有顾客的电子邮件（二） # 有表 OrderItems 代表订单商品信息表，prod_id 为产品 id；Orders 表代表订单表有 cust_id 代表顾客 id 和订单日期 order_date；Customers 表含有 cust_email 顾客邮件和 cust_id 顾客 id\nOrderItems 表：\nprod_id order_num BR01 a0001 BR01 a0002 BR02 a0003 BR02 a0013 Orders 表：\norder_num cust_id order_date a0001 cust10 2022-01-01 00:00:00 a0002 cust1 2022-01-01 00:01:00 a0003 cust1 2022-01-02 00:00:00 a0013 cust2 2022-01-01 00:20:00 Customers 表代表顾客信息，cust_id 为顾客 id，cust_email 为顾客 email\ncust_id cust_email cust10 cust10@cust.com cust1 cust1@cust.com cust2 cust2@cust.com 【问题】返回购买 prod_id 为 BR01 的产品的所有顾客的电子邮件（Customers 表中的 cust_email），结果无需排序。\n提示：涉及到 SELECT 语句，最内层的从 OrderItems 表返回 order_num，中间的从 Customers 表返回 cust_id，但是必须使用 INNER JOIN 语法。\nselect cust_email from Customers inner join Orders using(cust_id) inner join OrderItems using(order_num) where OrderItems.prod_id = \u0026#39;BR01\u0026#39;; 确定最佳顾客的另一种方式（二） # OrderItems 表代表订单信息，确定最佳顾客的另一种方式是看他们花了多少钱，OrderItems 表有订单号 order_num 和 item_price 商品售出价格、quantity 商品数量\norder_num item_price quantity a1 10 105 a2 1 1100 a2 1 200 a4 2 1121 a5 5 10 a2 1 19 a7 7 5 Orders 表含有字段 order_num 订单号、cust_id 顾客 id\norder_num cust_id a1 cust10 a2 cust1 a3 cust2 a4 cust22 a5 cust221 a7 cust2217 顾客表 Customers 有字段 cust_id 客户 id、cust_name 客户姓名\ncust_id cust_name cust10 andy cust1 ben cust2 tony cust22 tom cust221 an cust2217 hex 【问题】编写 SQL 语句，返回订单总价不小于 1000 的客户名称和总额（OrderItems 表中的 order_num）。\n提示：需要计算总和（item_price 乘以 quantity）。按总额对结果进行排序，请使用 INNER JOIN 语法。\nselect cust_name, sum(item_price * quantity) as total_price from Customers inner join Orders using(cust_id) inner join OrderItems using(order_num) group by cust_name having total_price \u0026gt;= 1000 order by total_price; 创建高级连接 # 检索每个顾客的名称和所有的订单号（一） # Customers` 表代表顾客信息含有顾客 id `cust_id` 和 顾客名称 `cust_name cust_id cust_name cust10 andy cust1 ben cust2 tony cust22 tom cust221 an cust2217 hex Orders` 表代表订单信息含有订单号 `order_num` 和顾客 id `cust_id order_num cust_id a1 cust10 a2 cust1 a3 cust2 a4 cust22 a5 cust221 a7 cust2217 【问题】使用 INNER JOIN 编写 SQL 语句，检索每个顾客的名称（Customers 表中的 cust_name）和所有的订单号（Orders 表中的 order_num），最后根据顾客姓名 cust_name 升序返回。\nselect cust_name, order_num from Customers inner join Orders using(cust_id) order by cust_name; 检索每个顾客的名称和所有的订单号（二） # Orders` 表代表订单信息含有订单号 `order_num` 和顾客 id `cust_id order_num cust_id a1 cust10 a2 cust1 a3 cust2 a4 cust22 a5 cust221 a7 cust2217 Customers` 表代表顾客信息含有顾客 id `cust_id` 和 顾客名称 `cust_name cust_id cust_name cust10 andy cust1 ben cust2 tony cust22 tom cust221 an cust2217 hex cust40 ace 【问题】检索每个顾客的名称（Customers 表中的 cust_name）和所有的订单号（Orders 表中的 order_num），列出所有的顾客，即使他们没有下过订单。最后根据顾客姓名 cust_name 升序返回。\nselect cust_name, order_num from Customers left join Orders using(cust_id) order by cust_name; 返回产品名称和与之相关的订单号 # Products 表为产品信息表含有字段 prod_id 产品 id、prod_name 产品名称\nprod_id prod_name a0001 egg a0002 sockets a0013 coffee a0003 cola a0023 soda OrderItems` 表为订单信息表含有字段 `order_num` 订单号和产品 id `prod_id prod_id order_num a0001 a105 a0002 a1100 a0002 a200 a0013 a1121 a0003 a10 a0003 a19 a0003 a5 【问题】使用外连接（left join、 right join、full join）联结 Products 表和 OrderItems 表，返回产品名称（prod_name）和与之相关的订单号（order_num）的列表，并按照产品名称升序排序。\nselect prod_name, order_num from Products left join OrderItems using(prod_id) order by prod_name; 返回产品名称和每一项产品的总订单数 # Products 表为产品信息表含有字段 prod_id 产品 id、prod_name 产品名称\nprod_id prod_name a0001 egg a0002 sockets a0013 coffee a0003 cola a0023 soda OrderItems` 表为订单信息表含有字段 `order_num` 订单号和产品 id `prod_id prod_id order_num a0001 a105 a0002 a1100 a0002 a200 a0013 a1121 a0003 a10 a0003 a19 a0003 a5 【问题】\n使用 OUTER JOIN 联结 Products 表和 OrderItems 表，返回产品名称（prod_name）和每一项产品的总订单数（不是订单号），并按产品名称升序排序。\nselect prod_name, count(order_num) as orders from Products left join OrderItems using(prod_id) group by prod_name order by prod_name; 列出供应商及其可供产品的数量 # 有 Vendors 表含有 vend_id （供应商 id）\nvend_id a0002 a0013 a0003 a0010 有 Products 表含有 vend_id（供应商 id）和 prod_id（供应产品 id）\nvend_id prod_id a0001 egg a0002 prod_id_iphone a00113 prod_id_tea a0003 prod_id_vivo phone a0010 prod_id_huawei phone 【问题】列出供应商（Vendors 表中的 vend_id）及其可供产品的数量，包括没有产品的供应商。你需要使用 OUTER JOIN 和 COUNT()聚合函数来计算 Products 表中每种产品的数量，最后根据 vend_id 升序排序。\n注意：vend_id 列会显示在多个表中，因此在每次引用它时都需要完全限定它。\nselect vend_id, count(prod_id) as prod_id from Vendors left join Products using(vend_id) group by vend_id order by vend_id; 组合查询 # UNION 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 UNION 中参与查询的提取行。\nUNION 基本规则：\n所有查询的列数和列顺序必须相同。 每个查询中涉及表的列的数据类型必须相同或兼容。 通常返回的列名取自第一个查询。 默认地，UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。\nSELECT column_name(s) FROM table1 UNION ALL SELECT column_name(s) FROM table2; UNION 结果集中的列名总是等于 UNION 中第一个 SELECT 语句中的列名。\nJOIN vs UNION：\nJOIN 中连接表的列可能不同，但在 UNION 中，所有查询的列数和列顺序必须相同。 UNION 将查询之后的行放在一起（垂直放置），但 JOIN 将查询之后的列放在一起（水平放置），即它构成一个笛卡尔积。 将两个 SELECT 语句结合起来（一） # 表 OrderItems 包含订单产品信息，字段 prod_id 代表产品 id、quantity 代表产品数量\nprod_id quantity a0001 105 a0002 100 a0002 200 a0013 1121 a0003 10 a0003 19 a0003 5 BNBG 10002 【问题】将两个 SELECT 语句结合起来，以便从 OrderItems 表中检索产品 id（prod_id）和 quantity。其中，一个 SELECT 语句过滤数量为 100 的行，另一个 SELECT 语句过滤 id 以 BNBG 开头的产品，最后按产品 id 对结果进行升序排序。\nselect prod_id, quantity from OrderItems where quantity = 100 union select prod_id, quantity from OrderItems where prod_id like \u0026#39;BNBG%\u0026#39;; 将两个 SELECT 语句结合起来（二） # 表 OrderItems 包含订单产品信息，字段 prod_id 代表产品 id、quantity 代表产品数量。\nprod_id quantity a0001 105 a0002 100 a0002 200 a0013 1121 a0003 10 a0003 19 a0003 5 BNBG 10002 【问题】将两个 SELECT 语句结合起来，以便从 OrderItems 表中检索产品 id（prod_id）和 quantity。其中，一个 SELECT 语句过滤数量为 100 的行，另一个 SELECT 语句过滤 id 以 BNBG 开头的产品，最后按产品 id 对结果进行升序排序。 注意：这次仅使用单个 SELECT 语句。\n答案：\n要求只用一条 select 语句，那就用 or 不用 union 了。\nselect prod_id, quantity from OrderItems where quantity = 100 or prod_id like \u0026#39;BNBG%\u0026#39;; 组合 Products 表中的产品名称和 Customers 表中的顾客名称 # Products 表含有字段 prod_name 代表产品名称\nprod_name flower rice ring umbrella Customers 表代表顾客信息，cust_name 代表顾客名称\ncust_name andy ben tony tom an lee hex 【问题】编写 SQL 语句，组合 Products 表中的产品名称（prod_name）和 Customers 表中的顾客名称（cust_name）并返回，然后按产品名称对结果进行升序排序。\n# UNION 结果集中的列名总是等于 UNION 中第一个 SELECT 语句中的列名。 select prod_name from Products union select cust_name from Customers order by prod_name; 检查 SQL 语句 # 表 Customers 含有字段 cust_name 顾客名、cust_contact 顾客联系方式、cust_state 顾客州、cust_email 顾客 email\ncust_name cust_contact cust_state cust_email cust10 8695192 MI cust10@cust.com cust1 8695193 MI cust1@cust.com cust2 8695194 IL cust2@cust.com 【问题】修正下面错误的 SQL\nSELECT cust_name, cust_contact, cust_email FROM Customers WHERE cust_state = \u0026#39;MI\u0026#39; ORDER BY cust_name; UNION SELECT cust_name, cust_contact, cust_email FROM Customers WHERE cust_state = \u0026#39;IL\u0026#39;ORDER BY cust_name; 修正后：\nSELECT cust_name, cust_contact, cust_email FROM Customers WHERE cust_state = \u0026#39;MI\u0026#39; UNION SELECT cust_name, cust_contact, cust_email FROM Customers WHERE cust_state = \u0026#39;IL\u0026#39; ORDER BY cust_name; 使用 union 组合查询时，只能使用一条 order by 字句，他必须位于最后一条 select 语句之后\n或者直接用 or 来做：\nSELECT cust_name, cust_contact, cust_email FROM Customers WHERE cust_state = \u0026#39;MI\u0026#39; or cust_state = \u0026#39;IL\u0026#39; ORDER BY cust_name; "},{"id":57,"href":"/zh/docs/technology/Review/java_guide/database/ly0504lysql-syntax-summary/","title":"sql语法基础知识总结","section":"数据库","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n本文整理完善自下面这两份资料：\nSQL 语法速成手册 MySQL 超全教程 基本概念 # 数据库术语 # 数据库（database） - 保存有组织的数据的容器（通常是一个文件或一组文件）。 数据表（table） - 某种特定类型数据的结构化清单。 模式（schema） - 关于数据库和表的布局及特性的信息。模式定义了数据在表中如何存储，包含存储什么样的数据，数据如何分解，各部分信息如何命名等信息。数据库和表都有模式。 列（column） - 表中的一个字段。所有表都是由一个或多个列组成的。 行（row） - 表中的一个记录。 主键（primary key） - 一列（或一组列），其值能够唯一标识表中每一行。 SQL 语法 # SQL（Structured Query Language)，标准 SQL 由 ANSI 标准委员会管理，从而称为 ANSI SQL。各个 DBMS 都有自己的实现，如 PL/SQL、Transact-SQL 等。\nSQL 语法结构 # SQL 语法结构包括：\n子句 - 是语句和查询的组成成分。（在某些情况下，这些都是可选的。） 表达式 - 可以产生任何标量值，或由列和行的数据库表 谓词 - 给需要评估的 SQL 三值逻辑（3VL）（true/false/unknown）或布尔真值指定条件，并限制语句和查询的效果，或改变程序流程。 查询 - 基于特定条件检索数据。这是 SQL 的一个重要组成部分。 语句 - 可以持久地影响纲要和数据，也可以控制数据库事务、程序流程、连接、会话或诊断。 SQL 语法要点 # SQL 语句不区分大小写，但是数据库表名、列名和值是否区分，依赖于具体的 DBMS 以及配置。例如：SELECT 与 select 、Select 是相同的。 多条 SQL 语句必须以分号（;）分隔。 处理 SQL 语句时，所有空格都被忽略。 SQL 语句可以写成一行，也可以分写为多行。\n-- 一行 SQL 语句 UPDATE user SET username=\u0026#39;robot\u0026#39;, password=\u0026#39;robot\u0026#39; WHERE username = \u0026#39;root\u0026#39;; -- 多行 SQL 语句 UPDATE user SET username=\u0026#39;robot\u0026#39;, password=\u0026#39;robot\u0026#39; WHERE username = \u0026#39;root\u0026#39;; SQL 支持三种注释：\n## 注释1 -- 注释2 /* 注释3 */ SQL 分类 # 数据定义语言（DDL） # 数据定义语言（Data Definition Language，DDL）是 SQL 语言集中负责数据结构定义与数据库对象定义的语言。\nDDL 的主要功能是定义数据库对象。\nDDL 的核心指令是 CREATE、ALTER、DROP。\n数据操纵语言（DML） # 数据操纵语言（Data Manipulation Language, DML）是用于数据库操作，对数据库其中的对象和数据运行访问工作的编程语句。\nDML 的主要功能是 访问数据，因此其语法都是以读写数据库为主。\nDML 的核心指令是 INSERT、UPDATE、DELETE、SELECT。这四个指令合称 CRUD(Create, Read, Update, Delete)，即增删改查。\n事务控制语言（TCL） # 事务控制语言 (Transaction Control Language, TCL) 用于管理数据库中的事务。这些用于管理由 DML 语句所做的更改。它还允许将语句分组为逻辑事务。\nTCL 的核心指令是 COMMIT、ROLLBACK。\n数据控制语言（DCL） # 数据控制语言 (Data Control Language, DCL) 是一种可对数据访问权进行控制的指令，它可以控制特定用户账户对数据表、查看表、预存程序、用户自定义函数等数据库对象的控制权。\nDCL 的核心指令是 GRANT、REVOKE。\nDCL 以控制用户的访问权限为主，因此其指令作法并不复杂，可利用 DCL 控制的权限有：CONNECT、SELECT、INSERT、UPDATE、DELETE、EXECUTE、USAGE、REFERENCES。\n根据不同的 DBMS 以及不同的安全性实体，其支持的权限控制也有所不同。\n我们先来介绍 DML 语句用法。 DML 的主要功能是读写数据库实现增删改查。\n增删改查 # 增删改查，又称为 CRUD，数据库基本操作中的基本操作。\n插入数据 # INSERT INTO 语句用于向表中插入新记录。\n插入完整的行\n# 插入一行 INSERT INTO user VALUES (10, \u0026#39;root\u0026#39;, \u0026#39;root\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); # 插入多行 INSERT INTO user VALUES (10, \u0026#39;root\u0026#39;, \u0026#39;root\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;), (12, \u0026#39;user1\u0026#39;, \u0026#39;user1\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;), (18, \u0026#39;user2\u0026#39;, \u0026#39;user2\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); 插入行的一部分\nINSERT INTO user(username, password, email) VALUES (\u0026#39;admin\u0026#39;, \u0026#39;admin\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); 插入查询出来的数据\nINSERT INTO user(username) SELECT name FROM account; 更新数据 # UPDATE 语句用于更新表中的记录。\nUPDATE user SET username=\u0026#39;robot\u0026#39;, password=\u0026#39;robot\u0026#39; WHERE username = \u0026#39;root\u0026#39;; 删除数据 # DELETE 语句用于删除表中的记录。 TRUNCATE TABLE 可以清空表，也就是删除所有行。 删除表中的指定数据\nDELETE FROM user WHERE username = \u0026#39;robot\u0026#39;; 清空表中的数据\nTRUNCATE TABLE user; 查询数据 # SELECT 语句用于从数据库中查询数据。\nDISTINCT 用于返回唯一不同的值。它作用于所有列，也就是说所有列的值都相同才算相同。\nLIMIT 限制返回的行数。可以有两个参数，第一个参数为起始行，从 0 开始；第二个参数为返回的总行数。\nASC ：升序（默认） DESC ：降序 查询单列\nSELECT prod_name FROM products; 查询多列\nSELECT prod_id, prod_name, prod_price FROM products; 查询所有列\nELECT * FROM products; 查询不同的值\nSELECT DISTINCT vend_id FROM products; 限制查询结果\n-- 返回前 5 行 SELECT * FROM mytable LIMIT 5; SELECT * FROM mytable LIMIT 0, 5; -- 返回第 3 ~ 5 行 SELECT * FROM mytable LIMIT 2, 3; 排序 # order by 用于对结果集按照一个列或者多个列进行排序。默认按照升序对记录进行排序，如果需要按照降序对记录进行排序，可以使用 desc 关键字。\norder by 对多列排序的时候，先排序的列放前面，后排序的列放后面。并且，不同的列可以有不同的排序规则。\nSELECT * FROM products ORDER BY prod_price DESC, prod_name ASC; 分组 # group by ：\ngroup by 子句将记录分组到汇总行中。 group by 为每个组返回一个记录。 group by 通常还涉及聚合count，max，sum，avg 等。 group by 可以按一列或多列进行分组。 group by 按分组字段进行排序后，order by 可以以汇总字段来进行排序。 分组\nSELECT cust_name, COUNT(cust_address) AS addr_num FROM Customers GROUP BY cust_name; 分组后排序\nSELECT cust_name, COUNT(cust_address) AS addr_num FROM Customers GROUP BY cust_name ORDER BY cust_name DESC; having：\nhaving 用于对汇总的 group by 结果进行过滤。 having 一般都是和 group by 连用。 where 和 having 可以在相同的查询中。 使用 WHERE 和 HAVING 过滤数据\nSELECT cust_name, COUNT(*) AS num FROM Customers WHERE cust_email IS NOT NULL GROUP BY cust_name HAVING COUNT(*) \u0026gt;= 1; having vs where ：\nwhere：过滤过滤指定的行，后面不能加聚合函数（分组函数）。where 在group by 前。 having：过滤分组，一般都是和 group by 连用，不能单独使用。having 在 group by 之后。 子查询 # 子查询是嵌套在较大查询中的 SQL 查询，也称内部查询或内部选择，包含子查询的语句也称为外部查询或外部选择。简单来说，子查询就是指将一个 select 查询（子查询）的结果作为另一个 SQL 语句（主查询）的数据来源或者判断条件。\n子查询可以嵌入 SELECT、INSERT、UPDATE 和 DELETE 语句中，也可以和 =、\u0026lt;、\u0026gt;、IN、BETWEEN、EXISTS 等运算符一起使用。\n子查询常用在 WHERE 子句和 FROM 子句后边：\n当用于 WHERE 子句时，根据不同的运算符，子查询可以返回单行单列、多行单列、单行多列数据。子查询就是要返回能够作为 WHERE 子句查询条件的值。 当用于 FROM 子句时，一般返回多行多列数据，相当于返回一张临时表，这样才符合 FROM 后面是表的规则。这种做法能够实现多表联合查询。 注意：MYSQL 数据库从 4.1 版本才开始支持子查询，早期版本是不支持的。\n用于 WHERE 子句的子查询的基本语法如下：\nselect column_name [, column_name ] from table1 [, table2 ] where column_name operator (select column_name [, column_name ] from table1 [, table2 ] [where]) 子查询需要放在括号( )内。 operator 表示用于 where 子句的运算符。 用于 FROM 子句的子查询的基本语法如下：\nselect column_name [, column_name ] from (select column_name [, column_name ] from table1 [, table2 ] [where]) as temp_table_name where condition 用于 FROM 的子查询返回的结果相当于一张临时表，所以需要使用 AS 关键字为该临时表起一个名字。\n子查询的子查询\nSELECT cust_name, cust_contact FROM customers WHERE cust_id IN (SELECT cust_id FROM orders WHERE order_num IN (SELECT order_num FROM orderitems WHERE prod_id = \u0026#39;RGAN01\u0026#39;)); 内部查询首先在其父查询之前执行，以便可以将内部查询的结果传递给外部查询。执行过程可以参考下图：\nWHERE # WHERE 子句用于过滤记录，即缩小访问数据的范围。 WHERE 后跟一个返回 true 或 false 的条件。 WHERE 可以与 SELECT，UPDATE 和 DELETE 一起使用。 可以在 WHERE 子句中使用的操作符。 运算符 描述 = 等于 \u0026lt;\u0026gt; 不等于。注释：在 SQL 的一些版本中，该操作符可被写成 != \u0026gt; 大于 \u0026lt; 小于 \u0026gt;= 大于等于 \u0026lt;= 小于等于 BETWEEN 在某个范围内 LIKE 搜索某种模式 IN 指定针对某个列的多个可能值 SELECT 语句中的 WHERE 子句\nSELECT * FROM Customers WHERE cust_name = \u0026#39;Kids Place\u0026#39;; UPDATE 语句中的 WHERE 子句\nUPDATE Customers SET cust_name = \u0026#39;Jack Jones\u0026#39; WHERE cust_name = \u0026#39;Kids Place\u0026#39;; DELETE 语句中的 WHERE 子句\nDELETE FROM Customers WHERE cust_name = \u0026#39;Kids Place\u0026#39;; IN 和 BETWEEN # IN 操作符在 WHERE 子句中使用，作用是在指定的几个特定值中任选一个值。 BETWEEN 操作符在 WHERE 子句中使用，作用是选取介于某个范围内的值。 IN 示例\nSELECT * FROM products WHERE vend_id IN (\u0026#39;DLL01\u0026#39;, \u0026#39;BRS01\u0026#39;); BETWEEN 示例\nSELECT * FROM products WHERE prod_price BETWEEN 3 AND 5; AND、OR、NOT # AND、OR、NOT 是用于对过滤条件的逻辑处理指令。 AND 优先级高于 OR，为了明确处理顺序，可以使用 ()。 AND 操作符表示左右条件都要满足。 OR 操作符表示左右条件满足任意一个即可。 NOT 操作符用于否定一个条件。 AND 示例\nSELECT prod_id, prod_name, prod_price FROM products WHERE vend_id = \u0026#39;DLL01\u0026#39; AND prod_price \u0026lt;= 4; OR 示例\nSELECT prod_id, prod_name, prod_price FROM products WHERE vend_id = \u0026#39;DLL01\u0026#39; OR vend_id = \u0026#39;BRS01\u0026#39;; NOT 示例\nSELECT * FROM products WHERE prod_price NOT BETWEEN 3 AND 5; LIKE # LIKE 操作符在 WHERE 子句中使用，作用是确定字符串是否匹配模式。 只有字段是文本值时才使用 LIKE。 LIKE 支持两个通配符匹配选项：% 和 _。 不要滥用通配符，通配符位于开头处匹配会非常慢。 % 表示任何字符出现任意次数。 _ 表示任何字符出现一次。 % 示例\nSELECT prod_id, prod_name, prod_price FROM products WHERE prod_name LIKE \u0026#39;%bean bag%\u0026#39;; _ 示例\nSELECT prod_id, prod_name, prod_price FROM products WHERE prod_name LIKE \u0026#39;__ inch teddy bear\u0026#39;; 连接 # JOIN 是“连接”的意思，顾名思义，SQL JOIN 子句用于将两个或者多个表联合起来进行查询。\n连接表时需要在每个表中选择一个字段，并对这些字段的值进行比较，值相同的两条记录将合并为一条。连接表的本质就是将不同表的记录合并起来，形成一张新表。当然，这张新表只是临时的，它仅存在于本次查询期间。\n使用 JOIN 连接两个表的基本语法如下：\nselect table1.column1, table2.column2... from table1 join table2 on table1.common_column1 = table2.common_column2; table1.common_column1 = table2.common_column2 是连接条件，只有满足此条件的记录才会合并为一行。您可以使用多个运算符来连接表，例如 =、\u0026gt;、\u0026lt;、\u0026lt;\u0026gt;、\u0026lt;=、\u0026gt;=、!=、between、like 或者 not，但是最常见的是使用 =。\n当两个表中有同名的字段时，为了帮助数据库引擎区分是哪个表的字段，在书写同名字段名时需要加上表名。当然，如果书写的字段名在两个表中是唯一的，也可以不使用以上格式，只写字段名即可。\n另外，如果两张表的关联字段名相同，也可以使用 USING子句来代替 ON，举个例子：\n# join....on select c.cust_name, o.order_num from Customers c inner join Orders o on c.cust_id = o.cust_id order by c.cust_name; # 如果两张表的关联字段名相同，也可以使用USING子句：join....using() select c.cust_name, o.order_num from Customers c inner join Orders o using(cust_id) order by c.cust_name; ON 和 WHERE 的区别：\n连接表时，SQL 会根据连接条件生成一张新的临时表。ON 就是连接条件，它决定临时表的生成。 WHERE 是在临时表生成以后，再对临时表中的数据进行过滤，生成最终的结果集，这个时候已经没有 JOIN-ON 了。 所以总结来说就是：SQL 先根据 ON 生成一张临时表，然后再根据 WHERE 对临时表进行筛选。\nSQL 允许在 JOIN 左边加上一些修饰性的关键词，从而形成不同类型的连接，如下表所示：\n连接类型 说明 INNER JOIN 内连接 （默认连接方式）只有当两个表都存在满足条件的记录时才会返回行。 LEFT JOIN / LEFT OUTER JOIN 左(外)连接 返回左表中的所有行，即使右表中没有满足条件的行也是如此。 RIGHT JOIN / RIGHT OUTER JOIN 右(外)连接 返回右表中的所有行，即使左表中没有满足条件的行也是如此。 FULL JOIN / FULL OUTER JOIN 全(外)连接 只要其中有一个表存在满足条件的记录，就返回行。 SELF JOIN 将一个表连接到自身，就像该表是两个表一样。为了区分两个表，在 SQL 语句中需要至少重命名一个表。 CROSS JOIN 交叉连接，从两个或者多个连接表中返回记录集的笛卡尔积。 下图展示了 LEFT JOIN、RIGHT JOIN、INNER JOIN、OUTER JOIN 相关的 7 种用法。\n如果不加任何修饰词，只写 JOIN，那么默认为 INNER JOIIN\n对于 INNER JOIIN 来说，还有一种隐式的写法，称为 “隐式内连接”，也就是没有 INNER JOIIN 关键字，使用 WHERE 语句实现内连接的功能\n# 隐式内连接 select c.cust_name, o.order_num from Customers c, Orders o where c.cust_id = o.cust_id order by c.cust_name; # 显式内连接 select c.cust_name, o.order_num from Customers c inner join Orders o using(cust_id) order by c.cust_name; 组合 # UNION 运算符将两个或更多查询的结果组合起来，并生成一个结果集，其中包含来自 UNION 中参与查询的提取行。\nUNION 基本规则：\n所有查询的列数和列顺序必须相同。 每个查询中涉及表的列的数据类型必须相同或兼容。 通常返回的列名取自第一个查询。 默认地，UNION 操作符选取不同的值。如果允许重复的值，请使用 UNION ALL。\nSELECT column_name(s) FROM table1 UNION ALL SELECT column_name(s) FROM table2; UNION 结果集中的列名总是等于 UNION 中第一个 SELECT 语句中的列名。\nJOIN vs UNION：\nJOIN 中连接表的列可能不同，但在 UNION 中，所有查询的列数和列顺序必须相同。 UNION 将查询之后的行放在一起（垂直放置），但 JOIN 将查询之后的列放在一起（水平放置），即它构成一个笛卡尔积。 函数 # 不同数据库的函数往往各不相同，因此不可移植。本节主要以 MysSQL 的函数为例。\n文本处理 # 函数 说明 LEFT()、RIGHT() 左边或者右边的字符 LOWER()、UPPER() 转换为小写或者大写 LTRIM()、RTIM() 去除左边或者右边的空格 LENGTH() 长度 SOUNDEX() 转换为语音值 其中， SOUNDEX() 可以将一个字符串转换为描述其语音表示的字母数字模式。\nSELECT * FROM mytable WHERE SOUNDEX(col1) = SOUNDEX(\u0026#39;apple\u0026#39;) 日期和时间处理 # 日期格式：YYYY-MM-DD 时间格式：HH:MM:SS 函 数 说 明 AddDate() 增加一个日期（天、周等） AddTime() 增加一个时间（时、分等） CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分 DateDiff() 计算两个日期之差 Date_Add() 高度灵活的日期运算函数 Date_Format() 返回一个格式化的日期或时间串 Day() 返回一个日期的天数部分 DayOfWeek() 对于一个日期，返回对应的星期几 Hour() 返回一个时间的小时部分 Minute() 返回一个时间的分钟部分 Month() 返回一个日期的月份部分 Now() 返回当前日期和时间 Second() 返回一个时间的秒部分 Time() 返回一个日期时间的时间部分 Year() 返回一个日期的年份部分 数值处理 # 函数 说明 SIN() 正弦 COS() 余弦 TAN() 正切 ABS() 绝对值 SQRT() 平方根 MOD() 余数 EXP() 指数 PI() 圆周率 RAND() 随机数 汇总 # 函 数 说 明 AVG() 返回某列的平均值 COUNT() 返回某列的行数 MAX() 返回某列的最大值 MIN() 返回某列的最小值 SUM() 返回某列值之和 AVG() 会忽略 NULL 行。\n使用 DISTINCT 可以让汇总函数值汇总不同的值。\nSELECT AVG(DISTINCT col1) AS avg_col FROM mytable 接下来，我们来介绍 DDL 语句用法。DDL 的主要功能是定义数据库对象（如：数据库、数据表、视图、索引等）\n数据定义 # 数据库（DATABASE） # 创建数据库 # CREATE DATABASE test; 删除数据库 # DROP DATABASE test; 选择数据库 # USE test; 数据表（TABLE） # 创建数据表 # 普通创建\nCREATE TABLE user ( id int(10) unsigned NOT NULL COMMENT \u0026#39;Id\u0026#39;, username varchar(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;用户名\u0026#39;, password varchar(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;密码\u0026#39;, email varchar(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;邮箱\u0026#39; ) COMMENT=\u0026#39;用户表\u0026#39;; 根据已有的表创建新表\nCREATE TABLE vip_user AS SELECT * FROM user; 删除数据表 # DROP TABLE user; 修改数据表 # 添加列\nALTER TABLE user ADD age int(3); 删除列\nALTER TABLE user DROP COLUMN age; 修改列\nALTER TABLE `user` MODIFY COLUMN age tinyint; 添加主键\nALTER TABLE user ADD PRIMARY KEY (id); 删除主键\nALTER TABLE user DROP PRIMARY KEY; 视图（VIEW） # 定义：\n视图是基于 SQL 语句的结果集的可视化的表。 视图是虚拟的表，本身不包含数据，也就不能对其进行索引操作。对视图的操作和对普通表的操作一样。 作用：\n简化复杂的 SQL 操作，比如复杂的联结； 只使用实际表的一部分数据； 通过只给用户访问视图的权限，保证数据的安全性； 更改数据格式和表示。 创建视图 # CREATE VIEW top_10_user_view AS SELECT id, username FROM user WHERE id \u0026lt; 10; 删除视图 # DROP VIEW top_10_user_view; 索引（INDEX） # 索引是一种用于快速查询和检索数据的数据结构，其本质可以看成是一种排序好的数据结构。\n索引的作用就相当于书的目录。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了。\n优点 ：\n使用索引可以大大加快 数据的检索速度（大大减少检索的数据量）, 这也是创建索引的最主要的原因。 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 缺点 ：\n创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。 索引需要使用物理文件存储，也会耗费一定空间。 但是，使用索引一定能提高查询性能吗?\n大多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升。\n关于索引的详细介绍，请看我写的 MySQL 索引详解 这篇文章。\n创建索引 # CREATE INDEX user_index ON user (id); 添加索引 # ALTER table user ADD INDEX user_index(id) 创建唯一索引 # CREATE UNIQUE INDEX user_index ON user (id); 删除索引 # ALTER TABLE user DROP INDEX user_index; 约束 # SQL 约束用于规定表中的数据规则。\n如果存在违反约束的数据行为，行为会被约束终止。\n约束可以在创建表时规定（通过 CREATE TABLE 语句），或者在表创建之后规定（通过 ALTER TABLE 语句）。\n约束类型：\nNOT NULL - 指示某列不能存储 NULL 值。 UNIQUE - 保证某列的每行必须有唯一的值。 PRIMARY KEY - NOT NULL 和 UNIQUE 的结合。确保某列（或两个列多个列的结合）有唯一标识，有助于更容易更快速地找到表中的一个特定的记录。 FOREIGN KEY - 保证一个表中的数据匹配另一个表中的值的参照完整性。 CHECK - 保证列中的值符合指定的条件。 DEFAULT - 规定没有给列赋值时的默认值。 创建表时使用约束条件：\nCREATE TABLE Users ( Id INT(10) UNSIGNED NOT NULL AUTO_INCREMENT COMMENT \u0026#39;自增Id\u0026#39;, Username VARCHAR(64) NOT NULL UNIQUE DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;用户名\u0026#39;, Password VARCHAR(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;密码\u0026#39;, Email VARCHAR(64) NOT NULL DEFAULT \u0026#39;default\u0026#39; COMMENT \u0026#39;邮箱地址\u0026#39;, Enabled TINYINT(4) DEFAULT NULL COMMENT \u0026#39;是否有效\u0026#39;, PRIMARY KEY (Id) ) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8mb4 COMMENT=\u0026#39;用户表\u0026#39;; 接下来，我们来介绍 TCL 语句用法。TCL 的主要功能是管理数据库中的事务。\n事务处理 # 不能回退 SELECT 语句，回退 SELECT 语句也没意义；也不能回退 CREATE 和 DROP 语句。\nMySQL 默认是隐式提交，每执行一条语句就把这条语句当成一个事务然后进行提交。当出现 START TRANSACTION 语句时，会关闭隐式提交；当 COMMIT 或 ROLLBACK 语句执行后，事务会自动关闭，重新恢复隐式提交。\n通过 set autocommit=0 可以取消自动提交，直到 set autocommit=1 才会提交；autocommit 标记是针对每个连接而不是针对服务器的。\n指令：\nSTART TRANSACTION - 指令用于标记事务的起始点。 SAVEPOINT - 指令用于创建保留点。 ROLLBACK TO - 指令用于回滚到指定的保留点；如果没有设置保留点，则回退到 START TRANSACTION 语句处。 COMMIT - 提交事务。 -- 开始事务 START TRANSACTION; -- 插入操作 A INSERT INTO `user` VALUES (1, \u0026#39;root1\u0026#39;, \u0026#39;root1\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); -- 创建保留点 updateA SAVEPOINT updateA; -- 插入操作 B INSERT INTO `user` VALUES (2, \u0026#39;root2\u0026#39;, \u0026#39;root2\u0026#39;, \u0026#39;xxxx@163.com\u0026#39;); -- 回滚到保留点 updateA ROLLBACK TO updateA; -- 提交事务，只有操作 A 生效 COMMIT; 接下来，我们来介绍 DCL 语句用法。DCL 的主要功能是控制用户的访问权限。\n权限控制 # 要授予用户帐户权限，可以用GRANT命令。有撤销用户的权限，可以用REVOKE命令。这里以 MySQl 为例，介绍权限控制实际应用。\nGRANT授予权限语法：\nGRANT privilege,[privilege],.. ON privilege_level TO user [IDENTIFIED BY password] [REQUIRE tsl_option] [WITH [GRANT_OPTION | resource_option]]; 简单解释一下：\n在GRANT关键字后指定一个或多个权限。如果授予用户多个权限，则每个权限由逗号分隔。 ON privilege_level 确定权限应用级别。MySQL 支持 global（*.*），database（database.*），table（database.table）和列级别。如果使用列权限级别，则必须在每个权限之后指定一个或逗号分隔列的列表。 user 是要授予权限的用户。如果用户已存在，则GRANT语句将修改其权限。否则，GRANT语句将创建一个新用户。可选子句IDENTIFIED BY允许您为用户设置新的密码。 REQUIRE tsl_option指定用户是否必须通过 SSL，X059 等安全连接连接到数据库服务器。 可选 WITH GRANT OPTION 子句允许您授予其他用户或从其他用户中删除您拥有的权限。此外，您可以使用WITH子句分配 MySQL 数据库服务器的资源，例如，设置用户每小时可以使用的连接数或语句数。这在 MySQL 共享托管等共享环境中非常有用。 REVOKE 撤销权限语法：\nREVOKE privilege_type [(column_list)] [, priv_type [(column_list)]]... ON [object_type] privilege_level FROM user [, user]... 简单解释一下：\n在 REVOKE 关键字后面指定要从用户撤消的权限列表。您需要用逗号分隔权限。 指定在 ON 子句中撤销特权的特权级别。 指定要撤消 FROM 子句中的权限的用户帐户。 GRANT 和 REVOKE 可在几个层次上控制访问权限：\n整个服务器，使用 GRANT ALL 和 REVOKE ALL； 整个数据库，使用 ON database.*； 特定的表，使用 ON database.table； 特定的列； 特定的存储过程。 新创建的账户没有任何权限。账户用 username@host 的形式定义，username@% 使用的是默认主机名。MySQL 的账户信息保存在 mysql 这个数据库中。\nUSE mysql; SELECT user FROM user; 下表说明了可用于GRANT和REVOKE语句的所有允许权限：\n特权 说明 级别 全局 数据库 表 列 程序 代理 ALL [PRIVILEGES] 授予除 GRANT OPTION 之外的指定访问级别的所有权限 ALTER 允许用户使用 ALTER TABLE 语句 X X X ALTER ROUTINE 允许用户更改或删除存储的例程 X X X CREATE 允许用户创建数据库和表 X X X CREATE ROUTINE 允许用户创建存储的例程 X X CREATE TABLESPACE 允许用户创建，更改或删除表空间和日志文件组 X CREATE TEMPORARY TABLES 允许用户使用 CREATE TEMPORARY TABLE 创建临时表 X X CREATE USER 允许用户使用 CREATE USER，DROP USER，RENAME USER 和 REVOKE ALL PRIVILEGES 语句。 X CREATE VIEW 允许用户创建或修改视图。 X X X DELETE 允许用户使用 DELETE X X X DROP 允许用户删除数据库，表和视图 X X X EVENT 启用事件计划程序的事件使用。 X X EXECUTE 允许用户执行存储的例程 X X X FILE 允许用户读取数据库目录中的任何文件。 X GRANT OPTION 允许用户拥有授予或撤消其他帐户权限的权限。 X X X X X INDEX 允许用户创建或删除索引。 X X X INSERT 允许用户使用 INSERT 语句 X X X X LOCK TABLES 允许用户对具有 SELECT 权限的表使用 LOCK TABLES X X PROCESS 允许用户使用 SHOW PROCESSLIST 语句查看所有进程。 X PROXY 启用用户代理。 REFERENCES 允许用户创建外键 X X X X RELOAD 允许用户使用 FLUSH 操作 X REPLICATION CLIENT 允许用户查询以查看主服务器或从属服务器的位置 X REPLICATION SLAVE 允许用户使用复制从属从主服务器读取二进制日志事件。 X SELECT 允许用户使用 SELECT 语句 X X X X SHOW DATABASES 允许用户显示所有数据库 X SHOW VIEW 允许用户使用 SHOW CREATE VIEW 语句 X X X SHUTDOWN 允许用户使用 mysqladmin shutdown 命令 X SUPER 允许用户使用其他管理操作，例如 CHANGE MASTER TO，KILL，PURGE BINARY LOGS，SET GLOBAL 和 mysqladmin 命令 X TRIGGER 允许用户使用 TRIGGER 操作。 X X X UPDATE 允许用户使用 UPDATE 语句 X X X X USAGE 相当于“没有特权” 创建账户 # CREATE USER myuser IDENTIFIED BY \u0026#39;mypassword\u0026#39;; 修改账户名 # UPDATE user SET user=\u0026#39;newuser\u0026#39; WHERE user=\u0026#39;myuser\u0026#39;; FLUSH PRIVILEGES; 删除账户 # DROP USER myuser; 查看权限 # SHOW GRANTS FOR myuser; 授予权限 # GRANT SELECT, INSERT ON *.* TO myuser; 删除权限 # REVOKE SELECT, INSERT ON *.* FROM myuser; 更改密码 # SET PASSWORD FOR myuser = \u0026#39;mypass\u0026#39;; 存储过程 # 存储过程可以看成是对一系列 SQL 操作的批处理。存储过程可以由触发器，其他存储过程以及 Java， Python，PHP 等应用程序调用。\n使用存储过程的好处：\n代码封装，保证了一定的安全性； 代码复用； 由于是预先编译，因此具有很高的性能。 创建存储过程：\n命令行中创建存储过程需要自定义分隔符，因为命令行是以 ; 为结束符，而存储过程中也包含了分号，因此会错误把这部分分号当成是结束符，造成语法错误。 包含 in、out 和 inout 三种参数。 给变量赋值都需要用 select into 语句。 每次只能给一个变量赋值，不支持集合的操作。 需要注意的是：阿里巴巴《Java 开发手册》强制禁止使用存储过程。因为存储过程难以调试和扩展，更没有移植性。\n至于到底要不要在项目中使用，还是要看项目实际需求，权衡好利弊即可！\n创建存储过程 # DROP PROCEDURE IF EXISTS `proc_adder`; DELIMITER ;; CREATE DEFINER=`root`@`localhost` PROCEDURE `proc_adder`(IN a int, IN b int, OUT sum int) BEGIN DECLARE c int; if a is null then set a = 0; end if; if b is null then set b = 0; end if; set sum = a + b; END ;; DELIMITER ; 使用存储过程 # set @b=5; call proc_adder(2,@b,@s); select @s as sum; 游标 # 游标（cursor）是一个存储在 DBMS 服务器上的数据库查询，它不是一条 SELECT 语句，而是被该语句检索出来的结果集。\n在存储过程中使用游标可以对一个结果集进行移动遍历。\n游标主要用于交互式应用，其中用户需要滚动屏幕上的数据，并对数据进行浏览或做出更改。\n使用游标的几个明确步骤：\n在使用游标前，必须声明(定义)它。这个过程实际上没有检索数据， 它只是定义要使用的 SELECT 语句和游标选项。\n一旦声明，就必须打开游标以供使用。这个过程用前面定义的 SELECT 语句把数据实际检索出来。\n对于填有数据的游标，根据需要取出(检索)各行。\n在结束游标使用时，必须关闭游标，可能的话，释放游标(有赖于具\n体的 DBMS)。\nDELIMITER $ CREATE PROCEDURE getTotal() BEGIN DECLARE total INT; -- 创建接收游标数据的变量 DECLARE sid INT; DECLARE sname VARCHAR(10); -- 创建总数变量 DECLARE sage INT; -- 创建结束标志变量 DECLARE done INT DEFAULT false; -- 创建游标 DECLARE cur CURSOR FOR SELECT id,name,age from cursor_table where age\u0026gt;30; -- 指定游标循环结束时的返回值 DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = true; SET total = 0; OPEN cur; FETCH cur INTO sid, sname, sage; WHILE(NOT done) DO SET total = total + 1; FETCH cur INTO sid, sname, sage; END WHILE; CLOSE cur; SELECT total; END $ DELIMITER ; -- 调用存储过程 call getTotal(); 触发器 # 触发器是一种与表操作有关的数据库对象，当触发器所在表上出现指定事件时，将调用该对象，即表的操作事件触发表上的触发器的执行。\n我们可以使用触发器来进行审计跟踪，把修改记录到另外一张表中。\n使用触发器的优点：\nSQL 触发器提供了另一种检查数据完整性的方法。 SQL 触发器可以捕获数据库层中业务逻辑中的错误。 SQL 触发器提供了另一种运行计划任务的方法。通过使用 SQL 触发器，您不必等待运行计划任务，因为在对表中的数据进行更改之前或之后会自动调用触发器。 SQL 触发器对于审计表中数据的更改非常有用。 使用触发器的缺点：\nSQL 触发器只能提供扩展验证，并且不能替换所有验证。必须在应用程序层中完成一些简单的验证。例如，您可以使用 JavaScript 在客户端验证用户的输入，或者使用服务器端脚本语言（如 JSP，PHP，ASP.NET，Perl）在服务器端验证用户的输入。 从客户端应用程序调用和执行 SQL 触发器是不可见的，因此很难弄清楚数据库层中发生了什么。 SQL 触发器可能会增加数据库服务器的开销。 MySQL 不允许在触发器中使用 CALL 语句 ，也就是不能调用存储过程。\n注意：在 MySQL 中，分号 ; 是语句结束的标识符，遇到分号表示该段语句已经结束，MySQL 可以开始执行了。因此，解释器遇到触发器执行动作中的分号后就开始执行，然后会报错，因为没有找到和 BEGIN 匹配的 END。\n这时就会用到 DELIMITER 命令（DELIMITER 是定界符，分隔符的意思）。它是一条命令，不需要语句结束标识，语法为：DELIMITER new_delemiter。new_delemiter 可以设为 1 个或多个长度的符号，默认的是分号 ;，我们可以把它修改为其他符号，如 $ - DELIMITER $ 。在这之后的语句，以分号结束，解释器不会有什么反应，只有遇到了 $，才认为是语句结束。注意，使用完之后，我们还应该记得把它给修改回来。\n在 MySQL 5.7.2 版之前，可以为每个表定义最多六个触发器。\nBEFORE INSERT - 在将数据插入表格之前激活。 AFTER INSERT - 将数据插入表格后激活。 BEFORE UPDATE - 在更新表中的数据之前激活。 AFTER UPDATE - 更新表中的数据后激活。 BEFORE DELETE - 在从表中删除数据之前激活。 AFTER DELETE - 从表中删除数据后激活。 但是，从 MySQL 版本 5.7.2+开始，可以为同一触发事件和操作时间定义多个触发器。\nNEW 和 OLD ：\nMySQL 中定义了 NEW 和 OLD 关键字，用来表示触发器的所在表中，触发了触发器的那一行数据。 在 INSERT 型触发器中，NEW 用来表示将要（BEFORE）或已经（AFTER）插入的新数据； 在 UPDATE 型触发器中，OLD 用来表示将要或已经被修改的原数据，NEW 用来表示将要或已经修改为的新数据； 在 DELETE 型触发器中，OLD 用来表示将要或已经被删除的原数据； 使用方法： NEW.columnName （columnName 为相应数据表某一列名） 创建触发器 # 提示：为了理解触发器的要点，有必要先了解一下创建触发器的指令。\nCREATE TRIGGER 指令用于创建触发器。\n语法：\nCREATE TRIGGER trigger_name trigger_time trigger_event ON table_name FOR EACH ROW BEGIN trigger_statements END; 说明：\ntrigger_name ：触发器名 trigger_time : 触发器的触发时机。取值为 BEFORE 或 AFTER。 trigger_event : 触发器的监听事件。取值为 INSERT、UPDATE 或 DELETE。 table_name : 触发器的监听目标。指定在哪张表上建立触发器。 FOR EACH ROW: 行级监视，Mysql 固定写法，其他 DBMS 不同。 trigger_statements: 触发器执行动作。是一条或多条 SQL 语句的列表，列表内的每条语句都必须用分号 ; 来结尾。 当触发器的触发条件满足时，将会执行 BEGIN 和 END 之间的触发器执行动作。\n示例：\nDELIMITER $ CREATE TRIGGER `trigger_insert_user` AFTER INSERT ON `user` FOR EACH ROW BEGIN INSERT INTO `user_history`(user_id, operate_type, operate_time) VALUES (NEW.id, \u0026#39;add a user\u0026#39;, now()); END $ DELIMITER ; 查看触发器 # SHOW TRIGGERS; 删除触发器 # DROP TRIGGER IF EXISTS trigger_insert_user; 文章推荐 # 后端程序员必备：SQL高性能优化指南！35+条优化建议立马GET! 后端程序员必备：书写高质量SQL的30条建议 "},{"id":58,"href":"/zh/docs/technology/Review/java_guide/database/Redis/diagram/","title":"redis问题图解","section":"Redis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n主从复制原理\n哨兵模式(简单)\n哨兵模式详解\n先配置主从模式，再配置哨兵模式\n所有的哨兵 sentinel.conf 都是配置为监听master\u0026ndash;\u0026gt; 192.168.14.101，如果主机宕机，sentinel.conf 中的配置也会自动更改为选举后的\nJava客户端连接原理\n客户端是和Sentinel来进行交互的,通过Sentinel来获取真正的Redis节点信息,然后来操作.实际工作时,Sentinel 内部维护了一个主题队列,用来保存Redis的节点信息,并实时更新,客户端订阅了这个主题,然后实时的去获取这个队列的Redis节点信息.\n/** 代码相对比较简单 **/ //1.设置sentinel 各个节点集合 Set\u0026lt;String\u0026gt; sentinelSet = new HashSet\u0026lt;\u0026gt;(); sentinelSet.add(\u0026#34;192.168.14.101:26379\u0026#34;); sentinelSet.add(\u0026#34;192.168.14.102:26380\u0026#34;); sentinelSet.add(\u0026#34;192.168.14.103:26381\u0026#34;); //2.设置jedispool 连接池配置文件 JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(10); config.setMaxWaitMillis(1000); //3.设置mastername,sentinelNode集合,配置文件,Redis登录密码 JedisSentinelPool jedisSentinelPool = new JedisSentinelPool(\u0026#34;mymaster\u0026#34;,sentinelSet,config,\u0026#34;123\u0026#34;); Jedis jedis = null; try { jedis = jedisSentinelPool.getResource(); //获取Redis中key=hello的值 String value = jedis.get(\u0026#34;hello\u0026#34;); System.out.println(value); } catch (Exception e) { e.printStackTrace(); } finally { if(jedis != null){ jedis.close(); } } 哨兵工作原理\n主观宕机：sentinel自认为redis不可用\n客观宕机：sentinel集群认为redis不可用\n故障转移\n"},{"id":59,"href":"/zh/docs/technology/Review/java_guide/database/Redis/ly0703ly3-commonly-used-cache-read-and-write-strategies/","title":"3种常用的缓存读写策略详解","section":"Redis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n看到很多小伙伴简历上写了“熟练使用缓存”，但是被我问到“缓存常用的3种读写策略”的时候却一脸懵逼。\n在我看来，造成这个问题的原因是我们在学习 Redis 的时候，可能只是简单了写一些 Demo，并没有去关注缓存的读写策略，或者说压根不知道这回事。\n但是，搞懂3种常见的缓存读写策略对于实际工作中使用缓存以及面试中被问到缓存都是非常有帮助的！\n下面介绍到的三种模式各有优劣，不存在最佳模式，根据具体的业务场景选择适合自己的缓存读写模式。\nCache Aside Pattern（旁路缓存模式） # Cache Aside Pattern 是我们平时使用比较多的一个缓存读写模式，比较适合读请求比较多的场景。\nCache Aside Pattern 中服务端需要同时维系 db 和 cache，并且是以 db 的结果为准。\n下面我们来看一下这个策略模式下的缓存读写步骤。\n写 ：\n先更新 db 然后直接删除 cache 。 简单画了一张图帮助大家理解写的步骤。\n读 :\n从 cache 中读取数据，读取到就直接返回 cache 中读取不到的话，就从 db 中读取数据返回 再把数据放到 cache 中。 简单画了一张图帮助大家理解读的步骤。\n你仅仅了解了上面这些内容的话是远远不够的，我们还要搞懂其中的原理。\n比如说面试官很可能会追问：“在写数据的过程中，可以先删除 cache ，后更新 db 么？”\n答案： 那肯定是不行的！因为这样可能会造成 数据库（db）和缓存（Cache）数据不一致的问题。\n举例：请求 1 先写数据 A，请求 2 随后读数据 A 的话，就很有可能产生数据不一致性的问题。\n这个过程可以简单描述为：\n请求 1 先把 cache 中的 A 数据删除 -\u0026gt; 请求 2 从 db 中读取数据【此时请求2把脏数据(对于请求1来说是)更新到缓存去了】-\u0026gt;请求 1 再把 db 中的 A 数据更新，即请求1的操作非原子\n当你这样回答之后，面试官可能会紧接着就追问：“在写数据的过程中，先更新 db，后删除 cache 就没有问题了么？”\n答案： 理论上来说还是可能会出现数据不一致性的问题，不过概率非常小，因为缓存的写入速度是比数据库的写入速度快很多。\n举例：请求 1 先读数据 A，请求 2 随后写数据 A，并且数据 A 在请求 1 请求之前不在缓存中的话，也有可能产生数据不一致性的问题。\n这个过程可以简单描述为：\n请求 1 从 db 读数据 A-\u0026gt; 请求 2 更新 db 中的数据 A（此时缓存中无数据 A ，故不用执行删除缓存操作 ） -\u0026gt; 请求 1 将数据 A 写入 cache\n现在我们再来分析一下 Cache Aside Pattern 的缺陷。\n缺陷 1：首次请求数据一定不在 cache 的问题\n解决办法：可以将热点数据可以提前放入 cache 中。\n缺陷 2：写操作比较频繁的话导致 cache 中的数据会被频繁被删除，这样会影响缓存命中率 。\n解决办法：\n数据库和缓存数据强一致场景 ：更新 db 的时候同样更新 cache，不过我们需要加一个锁/分布式锁来保证更新 cache 的时候不存在线程安全问题。 可以短暂地允许数据库和缓存数据不一致的场景 ：更新 db 的时候同样更新 cache，但是给缓存加一个比较短的过期时间，这样的话就可以保证即使数据不一致的话影响也比较小。 Read/Write Through Pattern（读写穿透） # Read/Write Through Pattern 中服务端把 cache 视为主要数据存储，从中读取数据并将数据写入其中。cache 服务负责将此数据读取和写入 db，从而减轻了应用程序的职责。\n这种缓存读写策略小伙伴们应该也发现了在平时在开发过程中非常少见。抛去性能方面的影响，大概率是因为我们经常使用的分布式缓存 Redis 并没有提供 cache 将数据写入 db 的功能。\n写（Write Through）：\n先查 cache，cache 中不存在，直接更新 db。 cache 中存在，则先更新 cache，然后 cache 服务自己更新 db（同步更新 cache 和 db）。 简单画了一张图帮助大家理解写的步骤。\n读(Read Through)：\n从 cache 中读取数据，读取到就直接返回 。 读取不到的话，先从 db 加载，写入到 cache 后返回响应。 简单画了一张图帮助大家理解读的步骤。\nRead-Through Pattern 实际只是在 Cache-Aside Pattern 之上进行了封装。在 Cache-Aside Pattern 下，发生读请求的时候，如果 cache 中不存在对应的数据，是由客户端自己负责把数据写入 cache，而 Read Through Pattern 则是 cache 服务自己来写入缓存的，这对客户端是透明的。\n和 Cache Aside Pattern 一样， Read-Through Pattern 也有首次请求数据一定不再 cache 的问题，对于热点数据可以提前放入缓存中。\nWrite Behind Pattern（异步缓存写入） # Write Behind Pattern 和 Read/Write Through Pattern 很相似，两者都是由 cache 服务来负责 cache 和 db 的读写。\n但是，两个又有很大的不同：Read/Write Through 是同步更新 cache 和 db，而 Write Behind 则是只更新缓存，不直接更新 db，而是改为异步批量的方式来更新 db。\n很明显，这种方式对数据一致性带来了更大的挑战，比如 cache 数据可能还没异步更新 db 的话，cache 服务可能就就挂掉了。\n这种策略在我们平时开发过程中也非常非常少见，但是不代表它的应用场景少，比如消息队列中消息的异步写入磁盘、MySQL 的 Innodb Buffer Pool 机制都用到了这种策略。\nWrite Behind Pattern 下 db 的写性能非常高，非常适合一些数据经常变化又对数据一致性要求没那么高的场景，比如浏览量、点赞量。\n"},{"id":60,"href":"/zh/docs/technology/Review/java_guide/database/Redis/ly0704lyredis-memory-fragmentation/","title":"redis内存碎片","section":"Redis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n什么是内存碎片? # 你可以将内存碎片简单地理解为那些不可用的空闲内存。\n举个例子：操作系统为你分配了 32 字节的连续内存空间，而你存储数据实际只需要使用 24 字节内存空间，那这多余出来的 8 字节内存空间如果后续没办法再被分配存储其他数据的话，就可以被称为内存碎片。\nRedis 内存碎片虽然不会影响 Redis 性能，但是会增加内存消耗。\n为什么会有 Redis 内存碎片? # Redis 内存碎片产生比较常见的 2 个原因：\n1、Redis 存储存储数据的时候向操作系统申请的内存空间可能会大于数据实际需要的存储空间。\n以下是这段 Redis 官方的原话：\nTo store user keys, Redis allocates at most as much memory as the maxmemory setting enables (however there are small extra allocations possible).\nRedis 使用 zmalloc 方法(Redis 自己实现的内存分配方法)进行内存分配的时候，除了要分配 size 大小的内存之外，还会多分配 PREFIX_SIZE 大小的内存。\nzmalloc 方法源码如下（源码地址：https://github.com/antirez/redis-tools/blob/master/zmalloc.c）：\nvoid *zmalloc(size_t size) { // 分配指定大小的内存 void *ptr = malloc(size+PREFIX_SIZE); if (!ptr) zmalloc_oom_handler(size); #ifdef HAVE_MALLOC_SIZE update_zmalloc_stat_alloc(zmalloc_size(ptr)); return ptr; #else *((size_t*)ptr) = size; update_zmalloc_stat_alloc(size+PREFIX_SIZE); return (char*)ptr+PREFIX_SIZE; #endif } 另外，Redis 可以使用多种内存分配器来分配内存（ libc、jemalloc、tcmalloc），默认使用 jemalloc，而 jemalloc 按照一系列固定的大小（8 字节、16 字节、32 字节\u0026hellip;\u0026hellip;）来分配内存的。jemalloc 划分的内存单元如下图所示：\n当程序申请的内存最接近某个固定值时，jemalloc 会给它分配相应大小的空间，就比如说程序需要申请 17 字节的内存，jemalloc 会直接给它分配 32 字节的内存，这样会导致有 15 字节内存的浪费。不过，jemalloc 专门针对内存碎片问题做了优化，一般不会存在过度碎片化的问题。\n2、频繁修改 Redis 中的数据也会产生内存碎片。\n当 Redis 中的某个数据删除时，Redis 通常不会轻易释放内存给操作系统。\n这个在 Redis 官方文档中也有对应的原话:\n文档地址：https://redis.io/topics/memory-optimization 。\n如何查看 Redis 内存碎片的信息？ # 使用 info memory 命令即可查看 Redis 内存相关的信息。下图中每个参数具体的含义，Redis 官方文档有详细的介绍：https://redis.io/commands/INFO 。\n]\nRedis 内存碎片率的计算公式：mem_fragmentation_ratio （内存碎片率）= used_memory_rss (操作系统实际分配给 Redis 的物理内存空间大小)/ used_memory(Redis 内存分配器为了存储数据实际申请使用的内存空间大小)\n也就是说，mem_fragmentation_ratio （内存碎片率）的值越大代表内存碎片率越严重。\n一定不要误认为used_memory_rss 减去 used_memory值就是内存碎片的大小！！！这不仅包括内存碎片，还包括其他进程开销，以及共享库、堆栈等的开销。\n很多小伙伴可能要问了：“多大的内存碎片率才是需要清理呢？”。\n通常情况下，我们认为 mem_fragmentation_ratio \u0026gt; 1.5 的话才需要清理内存碎片。 mem_fragmentation_ratio \u0026gt; 1.5 意味着你使用 Redis 存储实际大小 2G 的数据需要使用大于 3G 的内存。\n如果想要快速查看内存碎片率的话，你还可以通过下面这个命令：\n\u0026gt; redis-cli -p 6379 info | grep mem_fragmentation_ratio 另外，内存碎片率可能存在小于 1 的情况。这种情况我在日常使用中还没有遇到过，感兴趣的小伙伴可以看看这篇文章 故障分析 | Redis 内存碎片率太低该怎么办？- 爱可生开源社区 。\n如何清理 Redis 内存碎片？ # Redis4.0-RC3 版本以后自带了内存整理，可以避免内存碎片率过大的问题。\n直接通过 config set 命令将 activedefrag 配置项设置为 yes 即可。\nconfig set activedefrag yes 具体什么时候清理需要通过下面两个参数控制：\n# 内存碎片占用空间达到 500mb 的时候开始清理 config set active-defrag-ignore-bytes 500mb # 内存碎片率大于 1.5 的时候开始清理 config set active-defrag-threshold-lower 50 通过 Redis 自动内存碎片清理机制可能会对 Redis 的性能产生影响，我们可以通过下面两个参数来减少对 Redis 性能的影响：\n# 内存碎片清理所占用 CPU 时间的比例不低于 20% config set active-defrag-cycle-min 20 # 内存碎片清理所占用 CPU 时间的比例不高于 50% config set active-defrag-cycle-max 50 另外，重启节点可以做到内存碎片重新整理。如果你采用的是高可用架构的 Redis 集群的话，你可以将碎片率过高的主节点转换为从节点，以便进行安全重启。\n参考 # Redis 官方文档：https://redis.io/topics/memory-optimization Redis 核心技术与实战 - 极客时间 - 删除数据后，为什么内存占用率还是很高？：https://time.geekbang.org/column/article/289140 Redis 源码解析——内存分配：https://shinerio.cc/2020/05/17/redis/Redis%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%E2%80%94%E2%80%94%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/ "},{"id":61,"href":"/zh/docs/technology/Review/java_guide/database/Redis/ly0702lyredis-spec-data-structure/","title":"redis特殊数据结构","section":"Redis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n除了 5 种基本的数据结构之外，Redis 还支持 3 种特殊的数据结构 ：Bitmap、HyperLogLog、GEO。\nBitmap # 介绍 # Bitmap 存储的是连续的二进制数字（0 和 1），通过 Bitmap, 只需要一个 bit 位来表示某个元素对应的值或者状态，key 就是对应元素本身 。我们知道 8 个 bit 可以组成一个 byte，所以 Bitmap 本身会极大的节省储存空间。\n你可以将 Bitmap 看作是一个存储二进制数字（0 和 1）的数组，数组中每个元素的下标叫做 offset（偏移量）。\n常用命令 # 命令 介绍 SETBIT key offset value 设置指定 offset 位置的值 GETBIT key offset 获取指定 offset 位置的值 BITCOUNT key start end 获取 start 和 end 之前值为 1 的元素个数 BITOP operation destkey key1 key2 \u0026hellip; 对一个或多个 Bitmap 进行运算，可用运算符有 AND, OR, XOR 以及 NOT Bitmap 基本操作演示 ：\n# SETBIT 会返回之前位的值（默认是 0）这里会生成 7 个位 \u0026gt; SETBIT mykey 7 1 (integer) 0 \u0026gt; SETBIT mykey 7 0 (integer) 1 \u0026gt; GETBIT mykey 7 (integer) 0 \u0026gt; SETBIT mykey 6 1 (integer) 0 \u0026gt; SETBIT mykey 8 1 (integer) 0 # 通过 bitcount 统计被被设置为 1 的位的数量。 \u0026gt; BITCOUNT mykey (integer) 2 应用场景 # 需要保存状态信息（0/1 即可表示）的场景\n举例 ：用户签到情况、活跃用户情况、用户行为统计（比如是否点赞过某个视频）。 相关命令 ：SETBIT、GETBIT、BITCOUNT、BITOP。 HyperLogLog # 介绍 # HyperLogLog 是一种有名的基数计数概率算法 ，基于 LogLog Counting(LLC)优化改进得来，并不是 Redis 特有的，Redis 只是实现了这个算法并提供了一些开箱即用的 API。\nRedis 提供的 HyperLogLog 占用空间非常非常小，只需要 12k 的空间就能存储接近2^64个不同元素。这是真的厉害，这就是数学的魅力么！并且，Redis 对 HyperLogLog 的存储结构做了优化，采用两种方式计数：\n稀疏矩阵 ：计数较少的时候，占用空间很小。 稠密矩阵 ：计数达到某个阈值的时候，占用 12k 的空间。 Redis 官方文档中有对应的详细说明：\n基数计数概率算法为了节省内存并不会直接存储元数据，而是通过一定的概率统计方法预估基数值（集合中包含元素的个数）。因此， HyperLogLog 的计数结果并不是一个精确值，存在一定的误差（标准误差为 0.81% 。）。\nHyperLogLog 的使用非常简单，但原理非常复杂。HyperLogLog 的原理以及在 Redis 中的实现可以看这篇文章：HyperLogLog 算法的原理讲解以及 Redis 是如何应用它的 。\n再推荐一个可以帮助理解 HyperLogLog 原理的工具：Sketch of the Day: HyperLogLog — Cornerstone of a Big Data Infrastructure 。\n常用命令 # HyperLogLog 相关的命令非常少，最常用的也就 3 个。\n命令 介绍 PFADD key element1 element2 \u0026hellip; 添加一个或多个元素到 HyperLogLog 中 PFCOUNT key1 key2 获取一个或者多个 HyperLogLog 的唯一计数。 PFMERGE destkey sourcekey1 sourcekey2 \u0026hellip; 将多个 HyperLogLog 合并到 destkey 中，destkey 会结合多个源，算出对应的唯一计数。 HyperLogLog 基本操作演示 ：\n\u0026gt; PFADD hll foo bar zap (integer) 1 \u0026gt; PFADD hll zap zap zap (integer) 0 \u0026gt; PFADD hll foo bar (integer) 0 \u0026gt; PFCOUNT hll (integer) 3 \u0026gt; PFADD some-other-hll 1 2 3 (integer) 1 \u0026gt; PFCOUNT hll some-other-hll (integer) 6 \u0026gt; PFMERGE desthll hll some-other-hll \u0026#34;OK\u0026#34; \u0026gt; PFCOUNT desthll (integer) 6 应用场景 # 数量量巨大（百万、千万级别以上）的计数场景\n举例 ：热门网站每日/每周/每月访问 ip 数统计、热门帖子 uv 统计、 相关命令 ：PFADD、PFCOUNT 。 Geospatial index # 介绍 # Geospatial index（地理空间索引，简称 GEO） 主要用于存储地理位置信息，基于 Sorted Set 实现。\n通过 GEO 我们可以轻松实现两个位置距离的计算、获取指定位置附近的元素等功能。\n常用命令 # 命令 介绍 GEOADD key longitude1 latitude1 member1 \u0026hellip; 添加一个或多个元素对应的经纬度信息到 GEO 中 GEOPOS key member1 member2 \u0026hellip; 返回给定元素的经纬度信息 GEODIST key member1 member2 M/KM/FT/MI 返回两个给定元素之间的距离 GEORADIUS key longitude latitude radius distance 获取指定位置附近 distance 范围内的其他元素，支持 ASC(由近到远)、DESC（由远到近）、Count(数量) 等参数 GEORADIUSBYMEMBER key member radius distance 类似于 GEORADIUS 命令，只是参照的中心点是 GEO 中的元素 基本操作 ：\n\u0026gt; GEOADD personLocation 116.33 39.89 user1 116.34 39.90 user2 116.35 39.88 user3 3 \u0026gt; GEOPOS personLocation user1 116.3299986720085144 39.89000061669732844 \u0026gt; GEODIST personLocation user1 user2 km 1.4018 通过 Redis 可视化工具查看 personLocation ，果不其然，底层就是 Sorted Set。\nGEO 中存储的地理位置信息的经纬度数据通过 GeoHash 算法转换成了一个整数，这个整数作为 Sorted Set 的 score(权重参数)使用。\n获取指定位置范围内的其他元素 ：\n\u0026gt; GEORADIUS personLocation 116.33 39.87 3 km user3 user1 \u0026gt; GEORADIUS personLocation 116.33 39.87 2 km \u0026gt; GEORADIUS personLocation 116.33 39.87 5 km user3 user1 user2 \u0026gt; GEORADIUSBYMEMBER personLocation user1 5 km user3 user1 user2 \u0026gt; GEORADIUSBYMEMBER personLocation user1 2 km user1 user2 GEORADIUS 命令的底层原理解析可以看看阿里的这篇文章：Redis 到底是怎么实现“附近的人”这个功能的呢？ 。\n移除元素 ：\nGEO 底层是 Sorted Set ，你可以对 GEO 使用 Sorted Set 相关的命令。\n\u0026gt; ZREM personLocation user1 1 \u0026gt; ZRANGE personLocation 0 -1 user3 user2 \u0026gt; ZSCORE personLocation user2 4069879562983946 应用场景 # 需要管理使用地理空间数据的场景\n举例：附近的人。 相关命令: GEOADD、GEORADIUS、GEORADIUSBYMEMBER 。 参考 # Redis Data Structures ：https://redis.com/redis-enterprise/data-structures/ 。 《Redis 深度历险：核心原理与应用实践》1.6 四两拨千斤——HyperLogLog 布隆过滤器,位图,HyperLogLog：https://hogwartsrico.github.io/2020/06/08/BloomFilter-HyperLogLog-BitMap/index.html "},{"id":62,"href":"/zh/docs/technology/Review/java_guide/database/Redis/ly0701lyredis-base-data-structures/","title":"redis基本数据结构","section":"Redis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\nRedis 共有 5 种基本数据结构：String（字符串）、List（列表）、Set（集合）、Hash（散列）、Zset（有序集合）。\n这 5 种数据结构是直接提供给用户使用的，是数据的保存形式，其底层实现主要依赖这 8 种数据结构：简单动态字符串（SDS）、LinkedList（双向链表）、Hash Table（哈希表）、SkipList（跳跃表）、Intset（整数集合）、ZipList（压缩列表）、QuickList（快速列表）。\nRedis 基本数据结构的底层数据结构实现如下：\nString List Hash Set Zset SDS LinkedList/ZipList/QuickList Hash Table、ZipList ZipList、Intset ZipList、SkipList Redis 3.2 之前，List 底层实现是 LinkedList 或者 ZipList。 Redis 3.2 之后，引入了 LinkedList 和 ZipList 的结合 QuickList，List 的底层实现变为 QuickList。\n你可以在 Redis 官网上找到 Redis 数据结构非常详细的介绍：\nRedis Data Structures Redis Data types tutorial 未来随着 Redis 新版本的发布，可能会有新的数据结构出现，通过查阅 Redis 官网对应的介绍，你总能获取到最靠谱的信息。\nString（字符串） # 介绍 # String 是 Redis 中最简单同时也是最常用的一个数据结构。\nString 是一种二进制安全的数据结构，可以用来存储任何类型的数据比如字符串、整数、浮点数、图片（图片的 base64 编码或者解码或者图片的路径）、序列化后的对象。\n虽然 Redis 是用 C 语言写的，但是 Redis 并没有使用 C 的字符串表示，而是自己构建了一种 简单动态字符串（Simple Dynamic String，SDS）。相比于 C 的原生字符串，Redis 的 SDS 不光可以保存文本数据还可以保存二进制数据，并且获取字符串长度复杂度为 O(1)（C 字符串为 O(N)）,除此之外，Redis 的 SDS API 是安全的，不会造成缓冲区溢出。\n常用命令 # 命令 介绍 SET key value 设置指定 key 的值 SETNX key value 只有在 key 不存在时设置 key 的值 GET key 获取指定 key 的值 MSET key1 value1 key2 value2 … 设置一个或多个指定 key 的值 MGET key1 key2 \u0026hellip; 获取一个或多个指定 key 的值 STRLEN key 返回 key 所储存的字符串值的长度 INCR key 将 key 中储存的数字值增一 DECR key 将 key 中储存的数字值减一 EXISTS key 判断指定 key 是否存在 DEL key（通用） 删除指定的 key EXPIRE key seconds（通用） 给指定 key 设置过期时间 更多 Redis String 命令以及详细使用指南，请查看 Redis 官网对应的介绍：https://redis.io/commands/?group=string 。\n基本操作 ：\n\u0026gt; SET key value OK \u0026gt; GET key \u0026#34;value\u0026#34; \u0026gt; EXISTS key (integer) 1 \u0026gt; STRLEN key (integer) 5 \u0026gt; DEL key (integer) 1 \u0026gt; GET key (nil) 批量设置 ：\n\u0026gt; MSET key1 value1 key2 value2 OK \u0026gt; MGET key1 key2 # 批量获取多个 key 对应的 value 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; 计数器（字符串的内容为整数的时候可以使用）：\n\u0026gt; SET number 1 OK \u0026gt; INCR number # 将 key 中储存的数字值增一 (integer) 2 \u0026gt; GET number \u0026#34;2\u0026#34; \u0026gt; DECR number # 将 key 中储存的数字值减一 (integer) 1 \u0026gt; GET number \u0026#34;1\u0026#34; 设置过期时间（默认为永不过期）：\n\u0026gt; EXPIRE key 60 (integer) 1 \u0026gt; SETNX key 60 value # 设置值并设置过期时间 OK \u0026gt; TTL key (integer) 56 应用场景 # 需要存储常规数据的场景\n举例 ：缓存 session、token、图片地址、序列化后的对象(相比较于 Hash 存储更节省内存)。 相关命令 ： SET、GET。 需要计数的场景\n举例 ：用户单位时间的请求数（简单限流可以用到）、页面单位时间的访问数。 相关命令 ：SET、GET、 INCR、DECR 。 分布式锁\n利用 SETNX key value 命令可以实现一个最简易的分布式锁（存在一些缺陷，通常不建议这样实现分布式锁）。\nList（列表） # 介绍 # Redis 中的 List 其实就是链表数据结构的实现。我在 线性数据结构 :数组、链表、栈、队列 这篇文章中详细介绍了链表这种数据结构，我这里就不多做介绍了。\n许多高级编程语言都内置了链表的实现比如 Java 中的 LinkedList，但是 C 语言并没有实现链表，所以 Redis 实现了自己的链表数据结构。Redis 的 List 的实现为一个 双向链表，即可以支持反向查找和遍历，更方便操作，不过带来了部分额外的内存开销。\n常用命令 # 命令 介绍 RPUSH key value1 value2 \u0026hellip; 在指定列表的尾部（右边）添加一个或多个元素 LPUSH key value1 value2 \u0026hellip; 在指定列表的头部（左边）添加一个或多个元素 LSET key index value 将指定列表索引 index 位置的值设置为 value LPOP key 移除并获取指定列表的第一个元素(最左边) RPOP key 移除并获取指定列表的最后一个元素(最右边) LLEN key 获取列表元素数量 LRANGE key start end 获取列表 start 和 end 之间 的元素 更多 Redis List 命令以及详细使用指南，请查看 Redis 官网对应的介绍：https://redis.io/commands/?group=list 。\n通过 RPUSH/LPOP 或者 LPUSH/RPOP实现队列 ：\n\u0026gt; RPUSH myList value1 (integer) 1 \u0026gt; RPUSH myList value2 value3 (integer) 3 \u0026gt; LPOP myList \u0026#34;value1\u0026#34; \u0026gt; LRANGE myList 0 1 1) \u0026#34;value2\u0026#34; 2) \u0026#34;value3\u0026#34; \u0026gt; LRANGE myList 0 -1 1) \u0026#34;value2\u0026#34; 2) \u0026#34;value3\u0026#34; 通过 RPUSH/RPOP或者LPUSH/LPOP 实现栈 ：\n\u0026gt; RPUSH myList2 value1 value2 value3 (integer) 3 \u0026gt; RPOP myList2 # 将 list的头部(最右边)元素取出 \u0026#34;value3\u0026#34; 我专门画了一个图方便大家理解 RPUSH , LPOP , lpush , RPOP 命令：\n通过 LRANGE 查看对应下标范围的列表元素 ：\n\u0026gt; RPUSH myList value1 value2 value3 (integer) 3 \u0026gt; LRANGE myList 0 1 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; \u0026gt; LRANGE myList 0 -1 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; 3) \u0026#34;value3\u0026#34; 通过 LRANGE 命令，你可以基于 List 实现分页查询，性能非常高！\n通过 LLEN 查看链表长度 ：\n\u0026gt; LLEN myList (integer) 3 应用场景 # 信息流展示\n举例 ：最新文章、最新动态。 相关命令 ： LPUSH、LRANGE。 消息队列\nRedis List 数据结构可以用来做消息队列，只是功能过于简单且存在很多缺陷，不建议这样做。\n相对来说，Redis 5.0 新增加的一个数据结构 Stream 更适合做消息队列一些，只是功能依然非常简陋。和专业的消息队列相比，还是有很多欠缺的地方比如消息丢失和堆积问题不好解决。\nHash（哈希） # 介绍 # Redis 中的 Hash 是一个 String 类型的 field-value（键值对） 的映射表，特别适合用于存储对象，后续操作的时候，你可以直接修改这个对象中的某些字段的值。\nHash 类似于 JDK1.8 前的 HashMap，内部实现也差不多(数组 + 链表)。不过，Redis 的 Hash 做了更多优化。\n常用命令 # 命令 介绍 HSET key field value 设置指定哈希表中指定字段的值 HSETNX key field value 只有指定字段不存在时设置指定字段的值 HMSET key field1 value1 field2 value2 \u0026hellip; 同时将一个或多个 field-value (域-值)对设置到指定哈希表中 HGET key field 获取指定哈希表中指定字段的值 HMGET key field1 field2 \u0026hellip; 获取指定哈希表中一个或者多个指定字段的值 HGETALL key 获取指定哈希表中所有的键值对 HEXISTS key field 查看指定哈希表中指定的字段是否存在 HDEL key field1 field2 \u0026hellip; 删除一个或多个哈希表字段 HLEN key 获取指定哈希表中字段的数量 HINCRBY key field increment 对指定哈希中的指定字段做运算操作（正数为加，负数为减） 更多 Redis Hash 命令以及详细使用指南，请查看 Redis 官网对应的介绍：https://redis.io/commands/?group=hash 。\n模拟对象数据存储 ：\n\u0026gt; HMSET userInfoKey name \u0026#34;guide\u0026#34; description \u0026#34;dev\u0026#34; age 24 OK \u0026gt; HEXISTS userInfoKey name # 查看 key 对应的 value中指定的字段是否存在。 (integer) 1 \u0026gt; HGET userInfoKey name # 获取存储在哈希表中指定字段的值。 \u0026#34;guide\u0026#34; \u0026gt; HGET userInfoKey age \u0026#34;24\u0026#34; \u0026gt; HGETALL userInfoKey # 获取在哈希表中指定 key 的所有字段和值 1) \u0026#34;name\u0026#34; 2) \u0026#34;guide\u0026#34; 3) \u0026#34;description\u0026#34; 4) \u0026#34;dev\u0026#34; 5) \u0026#34;age\u0026#34; 6) \u0026#34;24\u0026#34; \u0026gt; HSET userInfoKey name \u0026#34;GuideGeGe\u0026#34; \u0026gt; HGET userInfoKey name \u0026#34;GuideGeGe\u0026#34; \u0026gt; HINCRBY userInfoKey age 2 (integer) 26 应用场景 # 对象数据存储场景\n举例 ：用户信息、商品信息、文章信息、购物车信息。 相关命令 ：HSET （设置单个字段的值）、HMSET（设置多个字段的值）、HGET（获取单个字段的值）、HMGET（获取多个字段的值）。 Set（集合） # 介绍 # Redis 中的 Set 类型是一种无序集合，集合中的元素没有先后顺序但都唯一，有点类似于 Java 中的 HashSet 。当你需要存储一个列表数据，又不希望出现重复数据时，Set 是一个很好的选择，并且 Set 提供了判断某个元素是否在一个 Set 集合内的重要接口，这个也是 List 所不能提供的。\n你可以基于 Set 轻易实现交集、并集、差集的操作，比如你可以将一个用户所有的关注人存在一个集合中，将其所有粉丝存在一个集合。这样的话，Set 可以非常方便的实现如共同关注、共同粉丝、共同喜好等功能。这个过程也就是求交集的过程。\n常用命令 # 命令 介绍 SADD key member1 member2 \u0026hellip; 向指定集合添加一个或多个元素 SMEMBERS key 获取指定集合中的所有元素 SCARD key 获取指定集合的元素数量 SISMEMBER key member 判断指定元素是否在指定集合中 SINTER key1 key2 \u0026hellip; 获取给定所有集合的交集 SINTERSTORE destination key1 key2 \u0026hellip; 将给定所有集合的交集存储在 destination 中 SUNION key1 key2 \u0026hellip; 获取给定所有集合的并集 SUNIONSTORE destination key1 key2 \u0026hellip; 将给定所有集合的并集存储在 destination 中 SDIFF key1 key2 \u0026hellip; 获取给定所有集合的差集 SDIFFSTORE destination key1 key2 \u0026hellip; 将给定所有集合的差集存储在 destination 中 SPOP key count 随机移除并获取指定集合中一个或多个元素 SRANDMEMBER key count 随机获取指定集合中指定数量的元素 更多 Redis Set 命令以及详细使用指南，请查看 Redis 官网对应的介绍：https://redis.io/commands/?group=set 。\n基本操作 ：\n\u0026gt; SADD mySet value1 value2 (integer) 2 \u0026gt; SADD mySet value1 # 不允许有重复元素，因此添加失败 (integer) 0 \u0026gt; SMEMBERS mySet 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; \u0026gt; SCARD mySet (integer) 2 \u0026gt; SISMEMBER mySet value1 (integer) 1 \u0026gt; SADD mySet2 value2 value3 (integer) 2 mySet : value1、value2 。 mySet2 ： value2 、value3 。 求交集 ：\n\u0026gt; SINTERSTORE mySet3 mySet mySet2 (integer) 1 \u0026gt; SMEMBERS mySet3 1) \u0026#34;value2\u0026#34; 求并集 ：\n\u0026gt; SUNION mySet mySet2 1) \u0026#34;value3\u0026#34; 2) \u0026#34;value2\u0026#34; 3) \u0026#34;value1\u0026#34; 求差集 ：\n\u0026gt; SDIFF mySet mySet2 # 差集是由所有属于 mySet 但不属于 A 的元素组成的集合 1) \u0026#34;value1\u0026#34; 应用场景 # 需要存放的数据不能重复的场景\n举例：网站 UV 统计（数据量巨大的场景还是 HyperLogLog更适合一些）、文章点赞、动态点赞等场景。 相关命令：SCARD（获取集合数量） 。 需要获取多个数据源交集、并集和差集的场景\n举例 ：**共同好友(**交集)、共同粉丝(交集)、共同关注(交集)、好友推荐（差集）、音乐推荐（差集） 、订阅号推荐（差集+交集） 等场景。 相关命令：SINTER（交集）、SINTERSTORE （交集）、SUNION （并集）、SUNIONSTORE（并集）、SDIFF（差集）、SDIFFSTORE （差集）。 需要随机获取数据源中的元素的场景\n举例 ：抽奖系统、随机。 相关命令：SPOP（随机获取集合中的元素并移除，适合不允许重复中奖的场景）、SRANDMEMBER（随机获取集合中的元素，适合允许重复中奖的场景）。 Sorted Set（有序集合） # 介绍 # Sorted Set 类似于 Set，但和 Set 相比，Sorted Set 增加了一个权重参数 score，使得集合中的元素能够按 score 进行有序排列，还可以通过 score 的范围来获取元素的列表。有点像是 Java 中 HashMap 和 TreeSet 的结合体。\n常用命令 # 命令 介绍 ZADD key score1 member1 score2 member2 \u0026hellip; 向指定有序集合添加一个或多个元素 ZCARD KEY 获取指定有序集合的元素数量 ZSCORE key member 获取指定有序集合中指定元素的 score 值 ZINTERSTORE destination numkeys key1 key2 \u0026hellip; 将给定所有有序集合的交集存储在 destination 中，对相同元素对应的 score 值进行 SUM 聚合操作，numkeys 为集合数量 ZUNIONSTORE destination numkeys key1 key2 \u0026hellip; 求并集，其它和 ZINTERSTORE 类似 ZDIFF destination numkeys key1 key2 \u0026hellip; 求差集，其它和 ZINTERSTORE 类似 ZRANGE key start end 获取指定有序集合 start 和 end 之间的元素（score 从低到高） ZREVRANGE key start end 获取指定有序集合 start 和 end 之间的元素（score 从高到底） ZREVRANK key member 获取指定有序集合中指定元素的排名(score 从大到小排序) 更多 Redis Sorted Set 命令以及详细使用指南，请查看 Redis 官网对应的介绍：https://redis.io/commands/?group=sorted-set 。\n基本操作 ：\n\u0026gt; ZADD myZset 2.0 value1 1.0 value2 (integer) 2 \u0026gt; ZCARD myZset 2 \u0026gt; ZSCORE myZset value1 2.0 \u0026gt; ZRANGE myZset 0 1 1) \u0026#34;value2\u0026#34; 2) \u0026#34;value1\u0026#34; \u0026gt; ZREVRANGE myZset 0 1 1) \u0026#34;value1\u0026#34; 2) \u0026#34;value2\u0026#34; \u0026gt; ZADD myZset2 4.0 value2 3.0 value3 (integer) 2 myZset : value1(2.0)、value2(1.0) 。 myZset2 ： value2 （4.0）、value3(3.0) 。 获取指定元素的排名 ：\n\u0026gt; ZREVRANK myZset value1 0 \u0026gt; ZREVRANK myZset value2 1 求交集 ：\n\u0026gt; ZINTERSTORE myZset3 2 myZset myZset2 1 \u0026gt; ZRANGE myZset3 0 1 WITHSCORES value2 5 求并集 ：\n\u0026gt; ZUNIONSTORE myZset4 2 myZset myZset2 3 \u0026gt; ZRANGE myZset4 0 2 WITHSCORES value1 2 value3 3 value2 5 求差集 ：\n\u0026gt; ZDIFF 2 myZset myZset2 WITHSCORES value1 2 应用场景 # 需要随机获取数据源中的元素根据某个权重进行排序的场景\n举例 ：各种排行榜比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等。 相关命令 ：ZRANGE (从小到大排序) 、 ZREVRANGE （从大到小排序）、ZREVRANK (指定元素排名)。 《Java 面试指北》 的「技术面试题篇」就有一篇文章详细介绍如何使用 Sorted Set 来设计制作一个排行榜。\n需要存储的数据有优先级或者重要程度的场景 比如优先级任务队列。\n举例 ：优先级任务队列。 相关命令 ：ZRANGE (从小到大排序) 、 ZREVRANGE （从大到小排序）、ZREVRANK (指定元素排名)。 参考 # Redis Data Structures ：https://redis.com/redis-enterprise/data-structures/ 。 Redis Commands ： https://redis.io/commands/ 。 Redis Data types tutorial：https://redis.io/docs/manual/data-types/data-types-tutorial/ 。 Redis 存储对象信息是用 Hash 还是 String : https://segmentfault.com/a/1190000040032006 "},{"id":63,"href":"/zh/docs/technology/Review/java_guide/database/Redis/ly0706lyredis-questions-02/","title":"redis面试题02","section":"Redis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\nRedis 事务 # 如何使用 Redis 事务？ # Redis 可以通过 MULTI，EXEC，DISCARD 和 WATCH 等命令来实现事务(transaction)功能。\n\u0026gt; MULTI OK \u0026gt; SET PROJECT \u0026#34;JavaGuide\u0026#34; QUEUED \u0026gt; GET PROJECT QUEUED \u0026gt; EXEC 1) OK 2) \u0026#34;JavaGuide\u0026#34; MULTI 命令后可以输入多个命令，Redis 不会立即执行这些命令，而是将它们放到队列，当调用了 EXEC 命令后，再执行所有的命令。\n这个过程是这样的：\n开始事务（MULTI）； 命令入队(批量操作 Redis 的命令，先进先出（FIFO）的顺序执行)； 执行事务(EXEC)。 你也可以通过 DISCARD 命令取消一个事务，它会清空事务队列中保存的所有命令。\n\u0026gt; MULTI OK \u0026gt; SET PROJECT \u0026#34;JavaGuide\u0026#34; QUEUED \u0026gt; GET PROJECT QUEUED \u0026gt; DISCARD OK 你可以通过WATCH 命令监听指定的 Key，当调用 EXEC 命令执行事务时，如果一个被 WATCH 命令监视的 Key 被 其他客户端/Session 修改的话，整个事务都不会被执行。\n# 客户端 1 \u0026gt; SET PROJECT \u0026#34;RustGuide\u0026#34; OK \u0026gt; WATCH PROJECT OK \u0026gt; MULTI OK \u0026gt; SET PROJECT \u0026#34;JavaGuide\u0026#34; QUEUED # 客户端 2 # 在客户端 1 执行 EXEC 命令提交事务之前修改 PROJECT 的值 \u0026gt; SET PROJECT \u0026#34;GoGuide\u0026#34; # 客户端 1 # 修改失败，因为 PROJECT 的值被客户端2修改了 \u0026gt; EXEC (nil) \u0026gt; GET PROJECT \u0026#34;GoGuide\u0026#34; 不过，如果 WATCH 与 事务 在同一个 Session 里，并且被 WATCH 监视的 Key 被修改的操作发生在事务内部，这个事务是可以被执行成功的（相关 issue ：WATCH 命令碰到 MULTI 命令时的不同效果）。\n事务内部修改 WATCH 监视的 Key：\n\u0026gt; SET PROJECT \u0026#34;JavaGuide\u0026#34; OK \u0026gt; WATCH PROJECT OK \u0026gt; MULTI OK \u0026gt; SET PROJECT \u0026#34;JavaGuide1\u0026#34; QUEUED \u0026gt; SET PROJECT \u0026#34;JavaGuide2\u0026#34; QUEUED \u0026gt; SET PROJECT \u0026#34;JavaGuide3\u0026#34; QUEUED \u0026gt; EXEC 1) OK 2) OK 3) OK 127.0.0.1:6379\u0026gt; GET PROJECT \u0026#34;JavaGuide3\u0026#34; 事务外部修改 WATCH 监视的 Key：\n\u0026gt; SET PROJECT \u0026#34;JavaGuide\u0026#34; OK \u0026gt; WATCH PROJECT OK \u0026gt; SET PROJECT \u0026#34;JavaGuide2\u0026#34; OK \u0026gt; MULTI OK \u0026gt; GET USER QUEUED \u0026gt; EXEC (nil) Redis 官网相关介绍 https://redis.io/topics/transactions 如下：\nRedis 支持原子性吗？ # Redis 的事务和我们平时理解的关系型数据库的事务不同。我们知道事务具有四大特性： 1. 原子性，2. 隔离性，3. 持久性，4. 一致性。\n原子性（Atomicity）： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 一致性（Consistency）： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； Redis 事务在运行错误的情况下，除了执行过程中出现错误的命令外，其他命令都能正常执行。并且，Redis 是不支持回滚（roll back）操作的。因此，Redis 事务其实是不满足原子性的（而且不满足持久性）。\nRedis 官网也解释了自己为啥不支持回滚。简单来说就是 Redis 开发者们觉得没必要支持回滚，这样更简单便捷并且性能更好。Redis 开发者觉得即使命令执行错误也应该在开发过程中就被发现而不是生产过程中。\n你可以将 Redis 中的事务就理解为 ：Redis 事务提供了一种将多个命令请求打包的功能。然后，再按顺序执行打包的所有命令，并且不会被中途打断。\n除了不满足原子性之外，事务中的每条命令都会与 Redis 服务器进行网络交互，这是比较浪费资源的行为。明明一次批量执行多个命令就可以了，这种操作实在是看不懂。\n因此，Redis 事务是不建议在日常开发中使用的。\n相关 issue :\nissue452: 关于 Redis 事务不满足原子性的问题 。 Issue491:关于 redis 没有事务回滚？ 如何解决 Redis 事务的缺陷？ # Redis 从 2.6 版本开始支持执行 Lua 脚本，它的功能和事务非常类似。我们可以利用 Lua 脚本来批量执行多条 Redis 命令，这些 Redis 命令会被提交到 Redis 服务器一次性执行完成，大幅减小了网络开销。\n一段 Lua 脚本可以视作一条命令执行，一段 Lua 脚本执行过程中不会有其他脚本或 Redis 命令同时执行，保证了操作不会被其他指令插入或打扰。\n如果 Lua 脚本运行时出错并中途结束，出错之后的命令是不会被执行的。并且，出错之前执行的命令是无法被撤销的。因此，严格来说，通过 Lua 脚本来批量执行 Redis 命令也是不满足原子性的。\n另外，Redis 7.0 新增了 Redis functions 特性，你可以将 Redis functions 看作是比 Lua 更强大的脚本。\nRedis 性能优化 # Redis bigkey # 什么是 bigkey？ # 简单来说，如果一个 key 对应的 value 所占用的内存比较大，那这个 key 就可以看作是 bigkey。具体多大才算大呢？有一个不是特别精确的参考标准：string 类型的 value 超过 10 kb，复合类型的 value 包含的元素超过 5000 个（对于复合类型的 value 来说，不一定包含的元素越多，占用的内存就越多）。\nbigkey 有什么危害？ # 除了会消耗更多的内存空间，bigkey 对性能也会有比较大的影响。\n因此，我们应该尽量避免写入 bigkey！\n如何发现 bigkey？ # 1、使用 Redis 自带的 --bigkeys 参数来查找。\n# redis-cli -p 6379 --bigkeys # Scanning the entire keyspace to find biggest keys as well as # average sizes per key type. You can use -i 0.1 to sleep 0.1 sec # per 100 SCAN commands (not usually needed). [00.00%] Biggest string found so far \u0026#39;\u0026#34;ballcat:oauth:refresh_auth:f6cdb384-9a9d-4f2f-af01-dc3f28057c20\u0026#34;\u0026#39; with 4437 bytes [00.00%] Biggest list found so far \u0026#39;\u0026#34;my-list\u0026#34;\u0026#39; with 17 items -------- summary ------- Sampled 5 keys in the keyspace! Total key length in bytes is 264 (avg len 52.80) Biggest list found \u0026#39;\u0026#34;my-list\u0026#34;\u0026#39; has 17 items Biggest string found \u0026#39;\u0026#34;ballcat:oauth:refresh_auth:f6cdb384-9a9d-4f2f-af01-dc3f28057c20\u0026#34;\u0026#39; has 4437 bytes 1 lists with 17 items (20.00% of keys, avg size 17.00) 0 hashs with 0 fields (00.00% of keys, avg size 0.00) 4 strings with 4831 bytes (80.00% of keys, avg size 1207.75) 0 streams with 0 entries (00.00% of keys, avg size 0.00) 0 sets with 0 members (00.00% of keys, avg size 0.00) 0 zsets with 0 members (00.00% of keys, avg size 0.00 从这个命令的运行结果，我们可以看出：这个命令会扫描(Scan) Redis 中的所有 key ，会对 Redis 的性能有一点影响。并且，这种方式只能找出每种数据结构 top 1 bigkey（占用内存最大的 string 数据类型，包含元素最多的复合数据类型）。\n2、分析 RDB 文件\n通过分析 RDB 文件来找出 big key。这种方案的前提是你的 Redis 采用的是 RDB 持久化。\n网上有现成的代码/工具可以直接拿来使用：\nredis-rdb-tools ：Python 语言写的用来分析 Redis 的 RDB 快照文件用的工具 rdb_bigkeys : Go 语言写的用来分析 Redis 的 RDB 快照文件用的工具，性能更好。 大量 key 集中过期问题 # 我在上面提到过：对于过期 key，Redis 采用的是 定期删除+惰性/懒汉式删除 策略。\n定期删除执行过程中，如果突然遇到大量过期 key 的话，客户端请求必须等待定期清理过期 key 任务线程执行完成，因为这个这个定期任务线程是在 Redis 主线程中执行的。这就导致客户端请求没办法被及时处理，响应速度会比较慢。\n如何解决呢？下面是两种常见的方法：\n给 key 设置随机过期时间。 开启 lazy-free（惰性删除/延迟释放） 。lazy-free 特性是 Redis 4.0 开始引入的，指的是让 Redis 采用异步方式延迟释放 key 使用的内存，将该操作交给单独的子线程处理，避免阻塞主线程。 个人建议不管是否开启 lazy-free，我们都尽量给 key 设置随机过期时间。\nRedis 生产问题 # 缓存穿透 # 什么是缓存穿透？ # 缓存穿透说简单点就是大量请求的 key 是不合理的，根本不存在于缓存中，也不存在于数据库中 。这就导致这些请求直接到了数据库上，根本没有经过缓存这一层，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n举个例子：某个黑客故意制造一些非法的 key 发起大量请求，导致大量请求落到数据库，结果数据库上也没有查到对应的数据。也就是说这些请求最终都落到了数据库上，对数据库造成了巨大的压力。\n有哪些解决办法？ # 最基本的就是首先做好参数校验，一些不合法的参数请求直接抛出异常信息返回给客户端。比如查询的数据库 id 不能小于 0、传入的邮箱格式不对的时候直接返回错误消息给客户端等等。\n1）缓存无效 key\n如果缓存和数据库都查不到某个 key 的数据就写一个到 Redis 中去并设置过期时间，具体命令如下： SET key value EX 10086 。这种方式可以解决请求的 key 变化不频繁的情况，如果黑客恶意攻击，每次构建不同的请求 key，会导致 Redis 中缓存大量无效的 key 。很明显，这种方案并不能从根本上解决此问题。如果非要用这种方式来解决穿透问题的话，尽量将无效的 key 的过期时间设置短一点比如 1 分钟。\n另外，这里多说一嘴，一般情况下我们是这样设计 key 的： 表名:列名:主键名:主键值 。\n如果用 Java 代码展示的话，差不多是下面这样的：\npublic Object getObjectInclNullById(Integer id) { // 从缓存中获取数据 Object cacheValue = cache.get(id); // 缓存为空 if (cacheValue == null) { // 从数据库中获取 Object storageValue = storage.get(key); // 缓存空对象 cache.set(key, storageValue); // 如果存储数据为空，需要设置一个过期时间(300秒) if (storageValue == null) { // 必须设置过期时间，否则有被攻击的风险 cache.expire(key, 60 * 5); } return storageValue; } return cacheValue; } 2）布隆过滤器\n布隆过滤器是一个非常神奇的数据结构，通过它我们可以非常方便地判断一个给定数据是否存在于海量数据中。我们需要的就是判断 key 是否合法，有没有感觉布隆过滤器就是我们想要找的那个“人”。\n具体是这样做的：把所有可能存在的请求的值都存放在布隆过滤器中，当用户请求过来，先判断用户发来的请求的值是否存在于布隆过滤器中。不存在的话，直接返回请求参数错误信息给客户端，存在的话才会走下面的流程。\n加入布隆过滤器之后的缓存处理流程图如下。\n但是，需要注意的是布隆过滤器可能会存在误判的情况。总结来说就是： 布隆过滤器说某个元素存在，小概率会误判。布隆过滤器说某个元素不在，那么这个元素一定不在。\n为什么会出现误判的情况呢? 我们还要从布隆过滤器的原理来说！\n我们先来看一下，当一个元素加入布隆过滤器中的时候，会进行哪些操作：\n使用布隆过滤器中的哈希函数对元素值进行计算，得到哈希值（有几个哈希函数得到几个哈希值）。 根据得到的哈希值，在位数组中把对应下标的值置为 1。 我们再来看一下，当我们需要判断一个元素是否存在于布隆过滤器的时候，会进行哪些操作：\n对给定元素再次进行相同的哈希计算； 得到值之后判断位数组中的每个元素是否都为 1，如果值都为 1，那么说明这个值在布隆过滤器中，如果存在一个值不为 1，说明该元素不在布隆过滤器中。 然后，一定会出现这样一种情况：不同的字符串可能哈希出来的位置相同。 （可以适当增加位数组大小或者调整我们的哈希函数来降低概率）\n更多关于布隆过滤器的内容可以看我的这篇原创：《不了解布隆过滤器？一文给你整的明明白白！》 ，强烈推荐，个人感觉网上应该找不到总结的这么明明白白的文章了。\n缓存击穿 # 什么是缓存击穿？ # 缓存击穿中，请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 。这就可能会导致瞬时大量的请求直接打到了数据库上，对数据库造成了巨大的压力，可能直接就被这么多请求弄宕机了。\n举个例子 ：秒杀进行过程中，缓存中的某个秒杀商品的数据突然过期，这就导致瞬时大量对该商品的请求直接落到数据库上，对数据库造成了巨大的压力。\n有哪些解决办法？ # 设置热点数据永不过期或者过期时间比较长。 针对热点数据提前预热，将其存入缓存中并设置合理的过期时间比如秒杀场景下的数据在秒杀结束之前不过期。 请求数据库写数据到缓存之前，先获取互斥锁，保证只有一个请求会落到数据库上，减少数据库的压力。 缓存穿透和缓存击穿有什么区别？ # 缓存穿透中，请求的 key 既不存在于缓存中，也不存在于数据库中。\n缓存击穿中，请求的 key 对应的是 热点数据 ，该数据 存在于数据库中，但不存在于缓存中（通常是因为缓存中的那份数据已经过期） 。\n缓存雪崩 # 什么是缓存雪崩？ # 我发现缓存雪崩这名字起的有点意思，哈哈。\n实际上，缓存雪崩描述的就是这样一个简单的场景：缓存在同一时间大面积的失效，导致大量的请求都直接落到了数据库上，对数据库造成了巨大的压力。 这就好比雪崩一样，摧枯拉朽之势，数据库的压力可想而知，可能直接就被这么多请求弄宕机了。\n另外，缓存服务宕机也会导致缓存雪崩现象，导致所有的请求都落到了数据库上。\n举个例子 ：数据库中的大量数据在同一时间过期，这个时候突然有大量的请求需要访问这些过期的数据。这就导致大量的请求直接落到数据库上，对数据库造成了巨大的压力。\n有哪些解决办法？ # 针对 Redis 服务不可用的情况：\n采用 Redis 集群，避免单机出现问题整个缓存服务都没办法使用。 限流，避免同时处理大量的请求。 针对热点缓存失效的情况：\n设置不同的失效时间比如随机设置缓存的失效时间。 缓存永不失效（不太推荐，实用性太差）。 设置二级缓存。 缓存雪崩和缓存击穿有什么区别？ # 缓存雪崩和缓存击穿比较像，但缓存雪崩导致的原因是缓存中的大量或者所有数据失效，缓存击穿导致的原因主要是某个热点数据不存在于缓存中（通常是因为缓存中的那份数据已经过期）。\n如何保证缓存和数据库数据的一致性？ # 细说的话可以扯很多，但是我觉得其实没太大必要（小声 BB：很多解决方案我也没太弄明白）。我个人觉得引入缓存之后，如果为了短时间的不一致性问题，选择让系统设计变得更加复杂的话，完全没必要。\n下面单独对 Cache Aside Pattern（旁路缓存模式） 来聊聊。\nCache Aside Pattern 中遇到写请求是这样的：更新 DB，然后直接删除 cache 。\n如果更新数据库成功，而删除缓存这一步失败的情况的话，简单说两个解决方案：\n缓存失效时间变短（不推荐，治标不治本） ：我们让缓存数据的过期时间变短，这样的话缓存就会从数据库中加载数据。另外，这种解决办法对于先操作缓存后操作数据库的场景不适用。 增加 cache 更新重试机制（常用）： 如果 cache 服务当前不可用导致缓存删除失败的话，我们就隔一段时间进行重试，重试次数可以自己定。如果多次重试还是失败的话，我们可以把当前更新失败的 key 存入队列中，等缓存服务可用之后，再将缓存中对应的 key 删除即可。 相关文章推荐：缓存和数据库一致性问题，看这篇就够了 - 水滴与银弹\nRedis 集群 # Redis Sentinel ：\n什么是 Sentinel？ 有什么用？ Sentinel 如何检测节点是否下线？主观下线与客观下线的区别? Sentinel 是如何实现故障转移的？ 为什么建议部署多个 sentinel 节点（哨兵集群）？ Sentinel 如何选择出新的 master（选举机制）? 如何从 Sentinel 集群中选择出 Leader ？ Sentinel 可以防止脑裂吗？ Redis Cluster ：\n为什么需要 Redis Cluster？解决了什么问题？有什么优势？ Redis Cluster 是如何分片的？ 为什么 Redis Cluster 的哈希槽是 16384 个? 如何确定给定 key 的应该分布到哪个哈希槽中？ Redis Cluster 支持重新分配哈希槽吗？ Redis Cluster 扩容缩容期间可以提供服务吗？ Redis Cluster 中的节点是怎么进行通信的？ 参考答案 ：Redis 集群详解（付费）。\n参考 # 《Redis 开发与运维》 《Redis 设计与实现》 Redis Transactions : https://redis.io/docs/manual/transactions/ 。 "},{"id":64,"href":"/zh/docs/technology/Review/java_guide/database/Redis/ly0705lyredis-questions-01/","title":"redis面试题01","section":"Redis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\nRedis 基础 # 什么是 Redis？ # Redis 是一个基于 C 语言开发的开源数据库（BSD 许可），与传统数据库不同的是 Redis 的数据是存在内存中的（内存数据库），读写速度非常快，被广泛应用于缓存方向。并且，Redis 存储的是 KV 键值对数据。\n为了满足不同的业务场景，Redis 内置了多种数据类型实现（比如 String、Hash、【List、Set、】Sorted Set、Bitmap）。并且，Redis 还支持事务 、持久化、Lua 脚本、多种开箱即用的集群方案（Redis Sentinel、Redis Cluster）。\nRedis 没有外部依赖，Linux 和 OS X 是 Redis 开发和测试最多的两个操作系统，官方推荐生产环境使用 Linux 部署 Redis。\n个人学习的话，你可以自己本机安装 Redis 或者通过 Redis 官网提供的在线 Redis 环境来实际体验 Redis。\n全世界有非常多的网站使用到了 Redis ，techstacks.io 专门维护了一个使用 Redis 的热门站点列表 ，感兴趣的话可以看看。\nRedis 为什么这么快？ # Redis 内部做了非常多的性能优化，比较重要的主要有下面 3 点：\nRedis 基于内存，内存的访问速度是磁盘的上千倍； Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型，主要是单线程事件循环和 IO 多路复用（Redis 线程模式后面会详细介绍到）； Redis 内置了多种优化过后的数据结构实现，性能非常高。 下面这张图片总结的挺不错的，分享一下，出自 Why is Redis so fast? 。\n分布式缓存常见的技术选型方案有哪些？ # 分布式缓存的话，比较老牌同时也是使用的比较多的还是 Memcached 和 Redis。不过，现在基本没有看过还有项目使用 Memcached 来做缓存，都是直接用 Redis。\nMemcached 是分布式缓存最开始兴起的那会，比较常用的。后来，随着 Redis 的发展，大家慢慢都转而使用更加强大的 Redis 了。\n另外，腾讯也开源了一款类似于 Redis 的分布式高性能 KV 存储数据库，基于知名的开源项目 RocksDB 作为存储引擎 ，100% 兼容 Redis 协议和 Redis4.0 所有数据模型，名为 Tendis （腾讯的）。\n关于 Redis 和 Tendis 的对比，腾讯官方曾经发过一篇文章：Redis vs Tendis：冷热混合存储版架构揭秘 ，可以简单参考一下。\n从这个项目的 Github 提交记录可以看出，Tendis 开源版几乎已经没有被维护更新了，加上其关注度并不高，使用的公司也比较少。因此，不建议你使用 Tendis 来实现分布式缓存。\n说一下 Redis 和 Memcached 的区别和共同点 # 现在公司一般都是用 Redis 来实现缓存，而且 Redis 自身也越来越强大了！不过，了解 Redis 和 Memcached 的区别和共同点，有助于我们在做相应的技术选型的时候，能够做到有理有据！\n共同点 ：\n都是基于内存的数据库，一般都用来当做缓存使用。 都有过期策略。 两者的性能都非常高。 区别 ：\nRedis 支持更丰富的数据类型（支持更复杂的应用场景）。Redis 不仅仅支持简单（string）的 k/v 类型的数据，同时还提供 list，set，zset，hash 等数据结构的存储。Memcached 只支持最简单的 k/v 数据类型。\nRedis 支持数据的持久化，可以将内存中的数据保持在磁盘中，重启的时候可以再次加载进行使用,而 Memcached 把数据全部存在内存之中。\nRedis 有灾难恢复机制。 因为可以把缓存中的数据持久化到磁盘上。\nRedis 在服务器内存使用完之后，可以将不用的数据放到磁盘上。但是，Memcached 在服务器内存使用完之后，就会直接报异常。\nMemcached 没有原生的集群模式，需要依靠客户端来实现往集群中分片写入数据；但是 Redis 目前是原生支持 cluster 模式的。\nMemcached 是多线程，非阻塞 IO 复用的网络模型；Redis 使用单线程的多路 IO 复用模型。 （Redis 6.0 引入了多线程 IO ）\n非阻塞的 IO复用\n单线程的多路IO复用\nRedis 支持发布订阅模型、Lua 脚本、事务等功能，而 Memcached 不支持。并且，Redis 支持更多的编程语言。\nMemcached 过期数据的删除策略只用了惰性删除，而 Redis 同时使用了惰性删除与定期删除。\n相信看了上面的对比之后，我们已经没有什么理由可以选择使用 Memcached 来作为自己项目的分布式缓存了。\n为什么要用 Redis/为什么要用缓存？ # 下面我们主要从“高性能”和“高并发”这两点来回答这个问题。\n高性能\n假如用户第一次访问数据库中的某些数据的话，这个过程是比较慢，毕竟是从硬盘中读取的。但是，如果说，用户访问的数据属于高频数据并且不会经常改变的话，那么我们就可以很放心地将该用户访问的数据存在缓存中。\n这样有什么好处呢？ 那就是保证用户下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。\n高并发\n一般像 MySQL 这类的数据库的 QPS 大概都在 1w 左右（4 核 8g） ，但是使用 Redis 缓存之后很容易达到 10w+，甚至最高能达到 30w+（就单机 Redis 的情况，Redis 集群的话会更高）。\nQPS（Query Per Second）：服务器每秒可以执行的查询次数；\n由此可见，直接操作缓存能够承受的数据库请求数量是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。进而，我们也就提高了系统整体的并发。\nRedis 除了做缓存，还能做什么？ # 分布式锁 ： 通过 Redis 来做分布式锁是一种比较常见的方式。通常情况下，我们都是基于 Redisson 来实现分布式锁。关于 Redis 实现分布式锁的详细介绍，可以看我写的这篇文章：分布式锁详解 。 限流 ：一般是通过 Redis + Lua 脚本的方式来实现限流。相关阅读：《我司用了 6 年的 Redis 分布式限流器，可以说是非常厉害了！》。 消息队列 ：Redis 自带的 list 数据结构可以作为一个简单的队列使用。Redis 5.0 中增加的 Stream 类型的数据结构更加适合用来做消息队列。它比较类似于 Kafka，有主题和消费组的概念，支持消息持久化以及 ACK 机制。 复杂业务场景 ：通过 Redis 以及 Redis 扩展（比如 Redisson）提供的数据结构，我们可以很方便地完成很多复杂的业务场景比如通过 bitmap 统计活跃用户、通过 sorted set 维护排行榜。 \u0026hellip;\u0026hellip; Redis 可以做消息队列么？ # Redis 5.0 新增加的一个数据结构 Stream 可以用来做消息队列，Stream 支持：\n发布 / 订阅模式 按照消费者组进行消费 消息持久化（ RDB 和 AOF） 不过，和专业的消息队列相比，还是有很多欠缺的地方比如消息丢失和堆积问题不好解决。因此，我们通常建议是不使用 Redis 来做消息队列的，你完全可以选择市面上比较成熟的一些消息队列比如 RocketMQ、Kafka。\n相关文章推荐：Redis 消息队列的三种方案（List、Streams、Pub/Sub）。\n如何基于 Redis 实现分布式锁？ # 关于 Redis 实现分布式锁的详细介绍，可以看我写的这篇文章：分布式锁详解 。\nRedis 数据结构 # Redis 常用的数据结构有哪些？ # 5 种基础数据结构 ：String（字符串）、List（列表）、Set（集合）、Hash（散列）、Zset（有序集合）。 3 种特殊数据结构 ：HyperLogLogs（基数统计）、Bitmap （位存储）、Geospatial (地理位置)。 关于 5 种基础数据结构的详细介绍请看这篇文章：Redis 5 种基本数据结构详解。\n关于 3 种特殊数据结构的详细介绍请看这篇文章：Redis 3 种特殊数据结构详解。\nString 的应用场景有哪些？ # 常规数据（比如 session、token、、序列化后的对象）的缓存； 计数比如用户单位时间的请求数（简单限流可以用到）、页面单位时间的访问数； 分布式锁(利用 SETNX key value 命令可以实现一个最简易的分布式锁)； \u0026hellip;\u0026hellip; 关于 String 的详细介绍请看这篇文章：Redis 5 种基本数据结构详解。\nString 还是 Hash 存储对象数据更好呢？ # String 存储的是序列化后的对象数据，存放的是整个对象。Hash 是对对象的每个字段单独存储，可以获取部分字段的信息，也可以修改或者添加部分字段，节省网络流量。如果对象中某些字段需要经常变动或者经常需要单独查询对象中的个别字段信息，Hash 就非常适合。 String 存储相对来说更加节省内存，缓存相同数量的对象数据，String 消耗的内存约是 Hash 的一半。并且，存储具有多层嵌套的对象时也方便很多。如果系统对性能和资源消耗非常敏感的话，String 就非常适合。 在绝大部分情况，我们建议使用 String 来存储对象数据即可！\nString 的底层实现是什么？ # Redis 是基于 C 语言编写的，但 Redis 的 String 类型的底层实现并不是 C 语言中的字符串（即以空字符 \\0 结尾的字符数组），而是自己编写了 SDS（Simple Dynamic String，简单动态字符串） 来作为底层实现。\n[daɪˈnæmɪk] 动态的\nSDS 最早是 Redis 作者为日常 C 语言开发而设计的 C 字符串，后来被应用到了 Redis 上，并经过了大量的修改完善以适合高性能操作。\nRedis7.0 的 SDS 的部分源码如下（https://github.com/redis/redis/blob/7.0/src/sds.h）：\n/* Note: sdshdr5 is never used, we just access the flags byte directly. * However is here to document the layout of type 5 SDS strings. */ struct __attribute__ ((__packed__)) sdshdr5 { unsigned char flags; /* 3 lsb of type, and 5 msb of string length */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr8 { uint8_t len; /* used */ uint8_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr16 { uint16_t len; /* used */ uint16_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr32 { uint32_t len; /* used */ uint32_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; struct __attribute__ ((__packed__)) sdshdr64 { uint64_t len; /* used */ uint64_t alloc; /* excluding the header and null terminator */ unsigned char flags; /* 3 lsb of type, 5 unused bits */ char buf[]; }; 通过源码可以看出，SDS 共有五种实现方式 SDS_TYPE_5（并未用到）、SDS_TYPE_8、SDS_TYPE_16、SDS_TYPE_32、SDS_TYPE_64，其中只有后四种实际用到。Redis 会根据初始化的长度决定使用哪种类型，从而减少内存的使用。\n类型 字节 位 sdshdr5 \u0026lt; 1 \u0026lt;8 sdshdr8 1 8 sdshdr16 2 16 sdshdr32 4 32 sdshdr64 8 64 对于后四种实现都包含了下面这 4 个属性：\nlen ：字符串的长度也就是已经使用的字节数 alloc：总共可用的字符空间大小，alloc-len 就是 SDS 剩余的空间大小 buf[] ：实际存储字符串的数组 flags ：低三位保存类型标志 SDS 相比于 C 语言中的字符串有如下提升：\n可以避免缓冲区溢出 ：C 语言中的字符串被修改（比如拼接）时，一旦没有分配足够长度的内存空间，就会造成缓冲区溢出。SDS 被修改时，会先根据 len 属性检查空间大小是否满足要求，如果不满足，则先扩展至所需大小再进行修改操作。 获取字符串长度的复杂度较低 ： C 语言中的字符串的长度通常是经过遍历计数来实现的，时间复杂度为 O(n)。SDS 的长度获取直接读取 len 属性即可，时间复杂度为 O(1)。 减少内存分配次数 ： 为了避免修改（增加/减少）字符串时，每次都需要重新分配内存（C 语言的字符串是这样的），SDS 实现了空间预分配和惰性空间释放两种优化策略。当 SDS 需要增加字符串时，Redis 会为 SDS 分配好内存，并且根据特定的算法分配多余的内存，这样可以减少连续执行字符串增长操作所需的内存重分配次数。当 SDS 需要减少字符串时，这部分内存不会立即被回收，会被记录下来，等待后续使用（支持手动释放，有对应的 API）。 二进制安全 ：C 语言中的字符串以空字符 \\0 作为字符串结束的标识，这存在一些问题，像一些二进制文件（比如图片、视频、音频）就可能包括空字符，C 字符串无法正确保存。SDS 使用 len 属性判断字符串是否结束，不存在这个问题。 多提一嘴，很多文章里 SDS 的定义是下面这样的：\nstruct sdshdr { unsigned int len; unsigned int free; char buf[]; }; 这个也没错，Redis 3.2 之前就是这样定义的。后来，由于这种方式的定义存在问题，len 和 free 的定义用了 4 个字节，造成了浪费。Redis 3.2 之后，Redis 改进了 SDS 的定义，将其划分为了现在的 5 种类型。\n购物车信息用 String 还是 Hash 存储更好呢? # 由于购物车中的商品频繁修改和变动，购物车信息建议使用 Hash 存储：\n用户 id 为 key 商品 id 为 field，商品数量为 value 那用户购物车信息的维护具体应该怎么操作呢？\n用户添加商品就是往 Hash 里面增加新的 field 与 value； 查询购物车信息就是遍历对应的 Hash； 更改商品数量直接修改对应的 value 值（直接 set 或者做运算皆可）； 删除商品就是删除 Hash 中对应的 field； 清空购物车直接删除对应的 key 即可。 这里只是以业务比较简单的购物车场景举例，实际电商场景下，field 只保存一个商品 id 是没办法满足需求的。\n使用 Redis 实现一个排行榜怎么做？ # Redis 中有一个叫做 sorted set 的数据结构经常被用在各种排行榜的场景，比如直播间送礼物的排行榜、朋友圈的微信步数排行榜、王者荣耀中的段位排行榜、话题热度排行榜等等。\n相关的一些 Redis 命令: ZRANGE (从小到大排序) 、 ZREVRANGE （从大到小排序）、ZREVRANK (指定元素排名)。\n《Java 面试指北》\n使用 Set 实现抽奖系统需要用到什么命令？ # SPOP key count ： 随机移除并获取指定集合中一个或多个元素，适合不允许重复中奖的场景。\nSRANDMEMBER key count : 随机获取指定集合中指定数量的元素，适合允许重复中奖的场景。\n重复中奖，这里说的是第一次中的是a，第二次可能也是a。而不是说一次中将的人有两个a\n使用 Bitmap 统计活跃用户怎么做？ # 使用日期（精确到天）作为 key，然后用户 ID 为 offset，如果当日活跃过就设置为 1。\n初始化数据：\n\u0026gt; SETBIT 20210308 1 1 (integer) 0 \u0026gt; SETBIT 20210308 2 1 (integer) 0 \u0026gt; SETBIT 20210309 1 1 (integer) 0 统计 20210308~20210309 总活跃用户数:\n\u0026gt; BITOP and desk1 20210308 20210309 (integer) 1 \u0026gt; BITCOUNT desk1 (integer) 1 统计 20210308~20210309 在线活跃用户数:\n\u0026gt; BITOP or desk2 20210308 20210309 (integer) 1 \u0026gt; BITCOUNT desk2 (integer) 2 使用 HyperLogLog 统计页面 UV 怎么做？ # Unique Visitor，即有多少个用户访问了我们的网站\n1、将访问指定页面的每个用户 ID 添加到 HyperLogLog 中。\nPFADD PAGE_1:UV USER1 USER2 ...... USERn 2、统计指定页面的 UV。\nPFCOUNT PAGE_1:UV #会自动扣除重复的 Redis 线程模型 # 对于读写命令来说，Redis 一直是单线程模型。不过，在 Redis 4.0 版本之后引入了多线程来执行一些大键值对的异步删除操作， Redis 6.0 版本之后引入了多线程来处理网络请求（提高网络 IO 读写性能）。\nRedis 单线程模型了解吗？ # Redis 基于 Reactor 模式设计开发了一套高效的事件处理模型 （Netty 的线程模型也基于 Reactor 模式，Reactor 模式不愧是高性能 IO 的基石），这套事件处理模型对应的是 Redis 中的文件事件处理器（file event handler）。由于**文件事件处理器（file event handler）**是单线程方式运行的，所以我们一般都说 Redis 是单线程模型。\n《Redis 设计与实现》有一段话是如是介绍文件事件处理器的，我觉得写得挺不错。\nRedis 基于 Reactor 模式开发了自己的网络事件处理器：这个处理器被称为文件事件处理器（file event handler）。\n文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字，并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关 闭（close）等操作时，与操作相对应的文件事件就会产生，这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行，但通过使用 I/O 多路复用程序来监听多个套接字，文件事件处理器既实现了高性能的网络通信模型，又可以很好地与 Redis 服务器中其他同样以单线程方式运行的模块进行对接，这保持了 Redis 内部单线程设计的简单性。\n既然是单线程，那怎么监听大量的客户端连接呢？\nRedis 通过 IO 多路复用程序 来监听来自客户端的大量连接（或者说是监听多个 socket），它会将感兴趣的事件及类型（读、写）注册到内核中并监听每个事件是否发生。\n这样的好处非常明显： I/O 多路复用技术的使用让 Redis 不需要额外创建多余的线程来监听客户端的大量连接，降低了资源的消耗（和 NIO 中的 Selector 组件很像）。\n文件事件处理器（file event handler）主要是包含 4 个部分：\n多个 socket（客户端连接）\nIO 多路复用程序（支持多个客户端连接的关键）\n文件事件分派器（将 socket 关联到相应的事件处理器）\n事件处理器（连接应答处理器、命令请求处理器、命令回复处理器）\n相关阅读：Redis 事件机制详解 。\nRedis6.0 之前为什么不使用多线程？ # 虽然说 Redis 是单线程模型，但是，实际上，Redis 在 4.0 之后的版本中就已经加入了对多线程的支持。\n不过，Redis 4.0 增加的多线程主要是针对一些大键值对的删除操作的命令，使用这些命令就会使用主线程之外的其他线程来“异步处理”。\n为此，Redis 4.0 之后新增了**UNLINK（可以看作是 DEL 的异步版本）、FLUSHALL ASYNC（清空所有数据库的所有 key，不仅仅是当前 SELECT 的数据库）、FLUSHDB ASYNC**（清空当前 SELECT 数据库中的所有 key）等异步命令。\n大体上来说，Redis 6.0 之前主要还是单线程处理。\n那 Redis6.0 之前为什么不使用多线程？ 我觉得主要原因有 3 点：\n单线程编程容易并且更容易维护； Redis 的性能瓶颈不在 CPU ，主要在内存和网络； 多线程就会存在死锁、线程上下文切换等问题，甚至会影响性能。 相关阅读：为什么 Redis 选择单线程模型 。\nRedis6.0 之后为何引入了多线程？ # Redis6.0 引入多线程主要是为了提高网络 IO 读写性能，因为这个算是 Redis 中的一个性能瓶颈（Redis 的瓶颈主要受限于内存和网络）。\n虽然，Redis6.0 引入了多线程，但是 Redis 的多线程只是在网络数据的读写这类耗时操作上使用了，执行命令仍然是单线程顺序执行。因此，你也不需要担心线程安全问题。\nRedis6.0 的多线程默认是禁用的，只使用主线程。如需开启需要设置IO线程数 \u0026gt; 1，需要修改 redis 配置文件 redis.conf ：\nio-threads 4 #设置1的话只会开启主线程，官网建议4核的机器建议设置为2或3个线程，8核的建议设置为6个线程 另外：\nio-threads的个数一旦设置，不能通过config动态设置 当设置ssl后，io-threads将不工作 开启多线程后，默认只会使用多线程进行IO写入writes，即发送数据给客户端，如果需要开启多线程IO读取reads，同样需要修改 redis 配置文件 redis.conf :\nio-threads-do-reads yes 但是官网描述开启多线程读并不能有太大提升，因此一般情况下并不建议开启\n相关阅读：\nRedis 6.0 新特性-多线程连环 13 问！ Redis 多线程网络模型全面揭秘（推荐） Redis 内存管理 # Redis 给缓存数据设置过期时间有啥用？ # 一般情况下，我们设置保存的缓存数据的时候都会设置一个过期时间。为什么呢？\n因为内存是有限的，如果缓存中的所有数据都是一直保存的话，分分钟直接 Out of memory。\nRedis 自带了给缓存数据设置过期时间的功能，比如：\n127.0.0.1:6379\u0026gt; expire key 60 # 数据在 60s 后过期 (integer) 1 127.0.0.1:6379\u0026gt; setex key 60 value # 数据在 60s 后过期 (setex:[set] + [ex]pire) OK 127.0.0.1:6379\u0026gt; ttl key # 查看数据还有多久过期 (integer) 56 注意：Redis 中除了字符串类型有自己独有设置过期时间的命令 setex 外，其他方法都需要依靠 expire 命令来设置过期时间 。另外， persist 命令可以移除一个键的过期时间。\n过期时间除了有助于缓解内存的消耗，还有什么其他用么？\n很多时候，我们的业务场景就是需要某个数据只在某一时间段内存在，比如我们的短信验证码可能只在 1 分钟内有效，用户登录的 token 可能只在 1 天内有效。\n如果使用传统的数据库来处理的话，一般都是自己判断过期，这样更麻烦并且性能要差很多。\nRedis 是如何判断数据是否过期的呢？ # Redis 通过一个叫做过期字典（可以看作是 hash 表）来保存数据过期的时间。过期字典的键指向 Redis 数据库中的某个 key(键)，过期字典的值是一个 long long 类型的整数，这个整数保存了 key 所指向的数据库键的过期时间（毫秒精度的 UNIX 时间戳）。\n过期字典是存储在 redisDb 这个结构里的：\ntypedef struct redisDb { ... dict *dict; //数据库键空间,保存着数据库中所有键值对 dict *expires // 过期字典,保存着键的过期时间 ... } redisDb; 过期的数据的删除策略了解么？ # 如果假设你设置了一批 key 只能存活 1 分钟，那么 1 分钟后，Redis 是怎么对这批 key 进行删除的呢？\n常用的过期数据的删除策略就两个（重要！自己造缓存轮子的时候需要格外考虑的东西）：\n惰性删除 ：只会在取出 key 的时候才对数据进行过期检查。这样对 CPU 最友好，但是可能会造成太多过期 key 没有被删除。 定期删除 ： 每隔一段时间抽取一批 key 执行删除过期 key 操作。并且，Redis 底层会通过限制删除操作执行的时长和频率来减少删除操作对 CPU 时间的影响。 定期删除对内存更加友好，惰性删除对 CPU 更加友好。两者各有千秋，所以 Redis 采用的是 定期删除+惰性/懒汉式删除 。\n但是，仅仅通过给 key 设置过期时间还是有问题的。因为还是可能存在定期删除和惰性删除漏掉了很多过期 key 的情况。这样就导致大量过期 key 堆积在内存里，然后就 Out of memory 了。\n怎么解决这个问题呢？答案就是：Redis 内存淘汰机制。\nRedis 内存淘汰机制了解么？ # 相关问题：MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据?\n当缓存数据越来越多，Redis 不可避免的会被写满，这时候就涉及到 Redis 的内存淘汰机制了\nRedis 提供 6 种数据淘汰策略：\nvolatile-lru（least recently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最近最少使用的数据淘汰 volatile-ttl：从已设置过期时间的数据集（server.db[i].expires）中挑选将要过期的数据淘汰 volatile-random：从已设置过期时间的数据集（server.db[i].expires）中任意选择数据淘汰 allkeys-lru（least recently used）：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的） allkeys-random：从数据集（server.db[i].dict）中任意选择数据淘汰 no-eviction：禁止驱逐数据，也就是说当内存不足以容纳新写入数据时，新写入操作会报错。这个应该没人使用吧！ 4.0 版本后增加以下两种：\nvolatile-lfu（least frequently used）：从已设置过期时间的数据集（server.db[i].expires）中挑选最不经常使用的数据淘汰 allkeys-lfu（least frequently used）：当内存不足以容纳新写入数据时，在键空间中，移除最不经常使用的 key 关于最近最少使用：\n链表尾部的数据会被丢弃\n长期不被使用的数据，在未来被用到的几率也不大。因此，当数据所占内存达到一定阈值时，要移除掉最近最少使用的数据。\n关于翻译问题：least，程度最轻的。recently，最近的。其实翻译应该是“非最近的，越远越要淘汰”\njava算法实现\npublic class LRUCache { class DLinkedNode { int key; int value; DLinkedNode prev; DLinkedNode next; public DLinkedNode() {} public DLinkedNode(int _key, int _value) {key = _key; value = _value;} } private Map\u0026lt;Integer, DLinkedNode\u0026gt; cache = new HashMap\u0026lt;Integer, DLinkedNode\u0026gt;(); private int size; private int capacity; private DLinkedNode head, tail; public LRUCache(int capacity) { this.size = 0; this.capacity = capacity; // 使用伪头部和伪尾部节点 head = new DLinkedNode(); tail = new DLinkedNode(); head.next = tail; tail.prev = head; } public int get(int key) { DLinkedNode node = cache.get(key); if (node == null) { return -1; } // 如果 key 存在，先通过哈希表定位，再移到头部 moveToHead(node); return node.value; } public void put(int key, int value) { DLinkedNode node = cache.get(key); if (node == null) { // 如果 key 不存在，创建一个新的节点 DLinkedNode newNode = new DLinkedNode(key, value); // 添加进哈希表 cache.put(key, newNode); // 添加至双向链表的头部 addToHead(newNode); ++size; if (size \u0026gt; capacity) { // 如果超出容量，删除双向链表的尾部节点 DLinkedNode tail = removeTail(); // 删除哈希表中对应的项 cache.remove(tail.key); --size; } } else { // 如果 key 存在，先通过哈希表定位，再修改 value，并移到头部 node.value = value; moveToHead(node); } } private void addToHead(DLinkedNode node) { node.prev = head; node.next = head.next; head.next.prev = node; head.next = node; } private void removeNode(DLinkedNode node) { node.prev.next = node.next; node.next.prev = node.prev; } private void moveToHead(DLinkedNode node) { removeNode(node); addToHead(node); } private DLinkedNode removeTail() { DLinkedNode res = tail.prev; removeNode(res); return res; } } Redis 持久化机制 # 怎么保证 Redis 挂掉之后再重启数据可以进行恢复？ # 很多时候我们需要持久化数据也就是将内存中的数据写入到硬盘里面，大部分原因是为了之后重用数据（比如重启机器、机器故障之后恢复数据），或者是为了防止系统故障而将数据备份到一个远程位置。\nRedis 不同于 Memcached 的很重要一点就是，Redis 支持持久化，而且支持两种不同的持久化操作。Redis 的一种持久化方式叫快照（snapshotting，RDB），另一种方式是只追加文件（append-only file, AOF）。这两种方法各有千秋，下面我会详细这两种持久化方法是什么，怎么用，如何选择适合自己的持久化方法。\n什么是 RDB 持久化？ # Redis 可以通过创建快照来获得存储在内存里面的数据在某个时间点上的副本。Redis 创建快照之后，可以对快照进行备份，可以将快照复制到其他服务器从而创建具有相同数据的服务器副本（Redis 主从结构，主要用来提高 Redis 性能），还可以将快照留在原地以便重启服务器的时候使用。\n快照持久化是 Redis 默认采用的持久化方式，在 redis.conf 配置文件中默认有此下配置：\nsave 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，Redis就会自动触发bgsave命令创建快照。 save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，Redis就会自动触发bgsave命令创建快照。 save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，Redis就会自动触发bgsave命令创建快照。 RDB 创建快照时会阻塞主线程吗？ # Redis 提供了两个命令来生成 RDB 快照文件：\nsave : 主线程执行，会阻塞主线程； bgsave : 子线程执行，不会阻塞主线程，默认选项。 什么是 AOF 持久化？ # 与快照持久化相比，AOF 持久化的实时性更好，因此已成为主流的持久化方案。默认情况下 Redis 没有开启 AOF（append only file）方式的持久化，可以通过 appendonly 参数开启：\nappendonly yes 开启 AOF 持久化后每执行一条会更改 Redis 中的数据的命令，Redis 就会将该命令写入到内存缓存 server.aof_buf 中，然后再根据 appendfsync 配置来决定何时将其同步到硬盘中的 AOF 文件。\nAOF 文件的保存位置和 RDB 文件的位置相同，都是通过 dir 参数设置的，默认的文件名是 appendonly.aof。\n在 Redis 的配置文件中存在三种不同的 AOF 持久化方式，它们分别是：\nappendfsync always #每次有数据修改发生时都会写入AOF文件,这样会严重降低Redis的速度 appendfsync everysec #每秒钟同步一次，显式地将多个写命令同步到硬盘 appendfsync no #让操作系统决定何时进行同步 为了兼顾数据和写入性能，用户可以考虑 appendfsync everysec 选项 ，让 Redis 每秒同步一次 AOF 文件，Redis 性能几乎没受到任何影响。而且这样即使出现系统崩溃，用户最多只会丢失一秒之内产生的数据。当硬盘忙于执行写入操作的时候，Redis 还会优雅的放慢自己的速度以便适应硬盘的最大写入速度。\n相关 issue ：\nRedis 的 AOF 方式 #783 Redis AOF 重写描述不准确 #1439 AOF 日志是如何实现的？ # 关系型数据库（如 MySQL）通常都是执行命令之前记录日志（方便故障恢复），而 Redis AOF 持久化机制是在执行完命令之后再记录日志。\n为什么是在执行完命令之后记录日志呢？\n避免额外的检查开销，AOF 记录日志不会对命令进行语法检查； 在命令执行完之后再记录，不会阻塞当前的命令执行。 这样也带来了风险（我在前面介绍 AOF 持久化的时候也提到过）：\n如果刚执行完命令 Redis 就宕机会导致对应的修改丢失； 可能会阻塞后续其他命令的执行（AOF 记录日志是在 Redis 主线程中进行的）。 AOF 重写了解吗？ # 当 AOF 变得太大时，Redis 能够在后台自动重写 AOF 产生一个新的 AOF 文件，这个新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。\nAOF 重写是一个有歧义的名字，该功能是通过读取数据库中的键值对来实现的，程序无须对现有 AOF 文件进行任何读入、分析或者写入操作。\n在执行 BGREWRITEAOF 命令时，Redis 服务器会维护一个 AOF 重写缓冲区，该缓冲区会在子进程创建新 AOF 文件期间，记录服务器执行的所有写命令。当子进程完成创建新 AOF 文件的工作之后，服务器会将重写缓冲区中的所有内容追加到新 AOF 文件的末尾，使得新的 AOF 文件保存的数据库状态与现有的数据库状态一致。最后，服务器用新的 AOF 文件替换旧的 AOF 文件，以此来完成 AOF 文件重写操作。\nRedis 7.0 版本之前，如果在重写期间有写入命令，AOF 可能会使用大量内存，重写期间到达的所有写入命令都会写入磁盘两次。\naof 文件重写是将 redis 中的数据转换为 写命令同步更新到 aof 文件的过程。\n重写 aof 后 为什么么可以变小\n清除了一些无效命令 eg. del srem 进程内超时的数据不再写入 aof 文件 多条写命令可以合并为批量写命令 eg. lpush list v1 lpush list v2 lpush list v3 合并为一条写入命令 lpush list v1 v2 v3 如何选择 RDB 和 AOF？ # 关于 RDB 和 AOF 的优缺点，官网上面也给了比较详细的说明Redis persistence，这里结合自己的理解简单总结一下。\nRDB 比 AOF 优秀的地方 ：\nRDB 文件存储的内容是经过压缩的二进制数据， 保存着某个时间点的数据集，文件很小，适合做数据的备份，灾难恢复。AOF 文件存储的是每一次写命令，类似于 MySQL 的 binlog 日志，通常会必 RDB 文件大很多。当 AOF 变得太大时，Redis 能够在后台自动重写 AOF。新的 AOF 文件和原有的 AOF 文件所保存的数据库状态一样，但体积更小。不过， Redis 7.0 版本之前，如果在重写期间有写入命令，AOF 可能会使用大量内存，重写期间到达的所有写入命令都会写入磁盘两次。 使用 RDB 文件恢复数据，直接解析还原数据即可，不需要一条一条地执行命令，速度非常快。而 AOF 则需要依次执行每个写命令，速度非常慢。也就是说，与 AOF 相比，恢复大数据集的时候，RDB 速度更快。 AOF 比 RDB 优秀的地方 ：\nRDB 的数据安全性不如 AOF，没有办法实时或者秒级持久化数据。生成 RDB 文件的过程是比繁重的， 虽然 BGSAVE 子进程写入 RDB 文件的工作不会阻塞主线程，但会对机器的 CPU 资源和内存资源产生影响，严重的情况下甚至会直接把 Redis 服务干宕机。AOF 支持秒级数据丢失（取决 fsync 策略，如果是 everysec，最多丢失 1 秒的数据），仅仅是追加命令到 AOF 文件，操作轻量。 RDB 文件是以特定的二进制格式保存的，并且在 Redis 版本演进中有多个版本的 RDB，所以存在老版本的 Redis 服务不兼容新版本的 RDB 格式的问题。 AOF 以一种易于理解和解析的格式包含所有操作的日志。你可以轻松地导出 AOF 文件进行分析，你也可以直接操作 AOF 文件来解决一些问题。比如，如果执行FLUSHALL命令意外地刷新了所有内容后，只要 AOF 文件没有被重写，删除最新命令并重启即可恢复之前的状态。 Redis 4.0 对于持久化机制做了什么优化？ # 由于 RDB 和 AOF 各有优势，于是，Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。\n如果把混合持久化打开，AOF 重写的时候就直接把 RDB 的内容写到 AOF 文件开头。这样做的好处是可以结合 RDB 和 AOF 的优点, 快速加载同时避免丢失过多的数据。当然缺点也是有的， AOF 里面的 RDB 部分是压缩格式不再是 AOF 格式，可读性较差。\n官方文档地址：https://redis.io/topics/persistence\n参考 # 《Redis 开发与运维》 《Redis 设计与实现》 Redis 命令手册：https://www.redis.com.cn/commands.html WHY Redis choose single thread (vs multi threads): https://medium.com/@jychen7/sharing-redis-single-thread-vs-multi-threads-5870bd44d153 The difference between AOF and RDB persistence：https://www.sobyte.net/post/2022-04/redis-rdb-and-aof/ "},{"id":65,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/web-real-time-message-push/","title":"web-real-time-message-push","section":"系统设计","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n原文地址：https://juejin.cn/post/7122014462181113887，JavaGuide 对本文进行了完善总结。\n我有一个朋友做了一个小破站，现在要实现一个站内信 Web 消息推送的功能，对，就是下图这个小红点，一个很常用的功能。\n不过他还没想好用什么方式做，这里我帮他整理了一下几种方案，并简单做了实现。\n# 什么是消息推送？ # 推送的场景比较多，比如有人关注我的公众号，这时我就会收到一条推送消息，以此来吸引我点击打开应用。\n消息推送通常是指网站的运营工作等人员，通过某种工具对用户当前网页或移动设备 APP 进行的主动消息推送。\n消息推送一般又分为 Web 端消息推送和移动端消息推送。\n移动端消息推送示例 ：\nWeb 端消息推送示例：\n在具体实现之前，咱们再来分析一下前边的需求，其实功能很简单，只要触发某个事件（主动分享了资源或者后台主动推送消息），Web 页面的通知小红点就会实时的 +1 就可以了。\n通常在服务端会有若干张消息推送表，用来记录用户触发不同事件所推送不同类型的消息，前端主动查询（拉）或者被动接收（推）用户所有未读的消息数。\n消息推送无非是推（push）和拉（pull）两种形式，下边我们逐个了解下。\n# 消息推送常见方案 # # 短轮询 # 轮询(polling) 应该是实现消息推送方案中最简单的一种，这里我们暂且将轮询分为短轮询和长轮询。\n短轮询很好理解，指定的时间间隔，由浏览器向服务器发出 HTTP 请求，服务器实时返回未读消息数据给客户端，浏览器再做渲染显示。\n一个简单的 JS 定时器就可以搞定，每秒钟请求一次未读消息数接口，返回的数据展示即可。\nsetInterval(() =\u0026gt; { // 方法请求 messageCount().then((res) =\u0026gt; { if (res.code === 200) { this.messageCount = res.data } }) }, 1000); 效果还是可以的，短轮询实现固然简单，缺点也是显而易见，由于推送数据并不会频繁变更，无论后端此时是否有新的消息产生，客户端都会进行请求，势必会对服务端造成很大压力，浪费带宽和服务器资源。\n# 长轮询 # 长轮询是对上边短轮询的一种改进版本，在尽可能减少对服务器资源浪费的同时，保证消息的相对实时性。长轮询在中间件中应用的很广泛，比如 Nacos 和 Apollo 配置中心，消息队列 Kafka、RocketMQ 中都有用到长轮询。\nNacos 配置中心交互模型是 push 还是 pull？open in new window一文中我详细介绍过 Nacos 长轮询的实现原理，感兴趣的小伙伴可以瞅瞅。\n长轮询其实原理跟轮询差不多，都是采用轮询的方式。不过，如果服务端的数据没有发生变更，会 一直 hold 住请求，直到服务端的数据发生变化，或者等待一定时间超时才会返回。返回后，客户端又会立即再次发起下一次长轮询。\n这次我使用 Apollo 配置中心实现长轮询的方式，应用了一个类DeferredResult，它是在 Servelet3.0 后经过 Spring 封装提供的一种异步请求机制，直意就是延迟结果。\nDeferredResult可以允许容器线程快速释放占用的资源，不阻塞请求线程，以此接受更多的请求提升系统的吞吐量，然后启动异步工作线程处理真正的业务逻辑，处理完成调用DeferredResult.setResult(200)提交响应结果。\n下边我们用长轮询来实现消息推送。\n因为一个 ID 可能会被多个长轮询请求监听，所以我采用了 Guava 包提供的Multimap结构存放长轮询，一个 key 可以对应多个 value。一旦监听到 key 发生变化，对应的所有长轮询都会响应。前端得到非请求超时的状态码，知晓数据变更，主动查询未读消息数接口，更新页面数据。\n@Controller @RequestMapping(\u0026#34;/polling\u0026#34;) public class PollingController { // 存放监听某个Id的长轮询集合 // 线程同步结构 public static Multimap\u0026lt;String, DeferredResult\u0026lt;String\u0026gt;\u0026gt; watchRequests = Multimaps.synchronizedMultimap(HashMultimap.create()); /** * 设置监听 */ @GetMapping(path = \u0026#34;watch/{id}\u0026#34;) @ResponseBody public DeferredResult\u0026lt;String\u0026gt; watch(@PathVariable String id) { // 延迟对象设置超时时间 DeferredResult\u0026lt;String\u0026gt; deferredResult = new DeferredResult\u0026lt;\u0026gt;(TIME_OUT); // 异步请求完成时移除 key，防止内存溢出 deferredResult.onCompletion(() -\u0026gt; { watchRequests.remove(id, deferredResult); }); // 注册长轮询请求 watchRequests.put(id, deferredResult); return deferredResult; } /** * 变更数据 */ @GetMapping(path = \u0026#34;publish/{id}\u0026#34;) @ResponseBody public String publish(@PathVariable String id) { // 数据变更 取出监听ID的所有长轮询请求，并一一响应处理 if (watchRequests.containsKey(id)) { Collection\u0026lt;DeferredResult\u0026lt;String\u0026gt;\u0026gt; deferredResults = watchRequests.get(id); for (DeferredResult\u0026lt;String\u0026gt; deferredResult : deferredResults) { deferredResult.setResult(\u0026#34;我更新了\u0026#34; + new Date()); } } return \u0026#34;success\u0026#34;; } 当请求超过设置的超时时间，会抛出AsyncRequestTimeoutException异常，这里直接用@ControllerAdvice全局捕获统一返回即可，前端获取约定好的状态码后再次发起长轮询请求，如此往复调用。\n@ControllerAdvice public class AsyncRequestTimeoutHandler { @ResponseStatus(HttpStatus.NOT_MODIFIED) @ResponseBody @ExceptionHandler(AsyncRequestTimeoutException.class) public String asyncRequestTimeoutHandler(AsyncRequestTimeoutException e) { System.out.println(\u0026#34;异步请求超时\u0026#34;); return \u0026#34;304\u0026#34;; } } 我们来测试一下，首先页面发起长轮询请求/polling/watch/10086监听消息更变，请求被挂起，不变更数据直至超时，再次发起了长轮询请求；紧接着手动变更数据/polling/publish/10086，长轮询得到响应，前端处理业务逻辑完成后再次发起请求，如此循环往复。\n长轮询相比于短轮询在性能上提升了很多，但依然会产生较多的请求，这是它的一点不完美的地方。\n# iframe 流 # iframe 流就是在页面中插入一个隐藏的\u0026lt;iframe\u0026gt;标签，通过在src中请求消息数量 API 接口，由此在服务端和客户端之间创建一条长连接，服务端持续向iframe传输数据。\n传输的数据通常是 HTML、或是内嵌的JavaScript 脚本，来达到实时更新页面的效果。\n这种方式实现简单，前端只要一个\u0026lt;iframe\u0026gt;标签搞定了\n\u0026lt;iframe src=\u0026#34;/iframe/message\u0026#34; style=\u0026#34;display:none\u0026#34;\u0026gt;\u0026lt;/iframe\u0026gt; 服务端直接组装 HTML、JS 脚本数据向 response 写入就行了\n@Controller @RequestMapping(\u0026#34;/iframe\u0026#34;) public class IframeController { @GetMapping(path = \u0026#34;message\u0026#34;) public void message(HttpServletResponse response) throws IOException, InterruptedException { while (true) { response.setHeader(\u0026#34;Pragma\u0026#34;, \u0026#34;no-cache\u0026#34;); response.setDateHeader(\u0026#34;Expires\u0026#34;, 0); response.setHeader(\u0026#34;Cache-Control\u0026#34;, \u0026#34;no-cache,no-store\u0026#34;); response.setStatus(HttpServletResponse.SC_OK); response.getWriter().print(\u0026#34; \u0026lt;script type=\\\u0026#34;text/javascript\\\u0026#34;\u0026gt;\\n\u0026#34; + \u0026#34;parent.document.getElementById(\u0026#39;clock\u0026#39;).innerHTML = \\\u0026#34;\u0026#34; + count.get() + \u0026#34;\\\u0026#34;;\u0026#34; + \u0026#34;parent.document.getElementById(\u0026#39;count\u0026#39;).innerHTML = \\\u0026#34;\u0026#34; + count.get() + \u0026#34;\\\u0026#34;;\u0026#34; + \u0026#34;\u0026lt;/script\u0026gt;\u0026#34;); } } } iframe 流的服务器开销很大，而且IE、Chrome等浏览器一直会处于 loading 状态，图标会不停旋转，简直是强迫症杀手。\niframe 流非常不友好，强烈不推荐。\n# SSE (我的方式) # 很多人可能不知道，服务端向客户端推送消息，其实除了可以用WebSocket这种耳熟能详的机制外，还有一种服务器发送事件(Server-Sent Events)，简称 SSE。这是一种服务器端到客户端(浏览器)的单向消息推送。\nSSE 基于 HTTP 协议的，我们知道一般意义上的 HTTP 协议是无法做到服务端主动向客户端推送消息的，但 SSE 是个例外，它变换了一种思路。\nSSE 在服务器和客户端之间打开一个单向通道，服务端响应的不再是一次性的数据包而是text/event-stream类型的数据流信息，在有数据变更时从服务器流式传输到客户端。\n整体的实现思路有点类似于在线视频播放，视频流会连续不断的推送到浏览器，你也可以理解成，客户端在完成一次用时很长（网络不畅）的下载。\nSSE 与 WebSocket 作用相似，都可以建立服务端与浏览器之间的通信，实现服务端向客户端推送消息，但还是有些许不同：\nSSE 是基于 HTTP 协议的，它们不需要特殊的协议或服务器实现即可工作；WebSocket 需单独服务器来处理协议。 SSE 单向通信，只能由服务端向客户端单向通信；WebSocket 全双工通信，即通信的双方可以同时发送和接受信息。 SSE 实现简单开发成本低，无需引入其他组件；WebSocket 传输数据需做二次解析，开发门槛高一些。 SSE 默认支持断线重连；WebSocket 则需要自己实现。 SSE 只能传送文本消息，二进制数据需要经过编码后传送；WebSocket 默认支持传送二进制数据。 SSE 与 WebSocket 该如何选择？\n技术并没有好坏之分，只有哪个更合适\nSSE 好像一直不被大家所熟知，一部分原因是出现了 WebSocket，这个提供了更丰富的协议来执行双向、全双工通信。对于游戏、即时通信以及需要双向近乎实时更新的场景，拥有双向通道更具吸引力。\n但是，在某些情况下，不需要从客户端发送数据。而你只需要一些服务器操作的更新。比如：站内信、未读消息数、状态更新、股票行情、监控数量等场景，SEE 不管是从实现的难易和成本上都更加有优势。此外，SSE 具有 WebSocket 在设计上缺乏的多种功能，例如：自动重新连接、事件 ID 和发送任意事件的能力。\n前端只需进行一次 HTTP 请求，带上唯一 ID，打开事件流，监听服务端推送的事件就可以了\n\u0026lt;script\u0026gt; let source = null; let userId = 7777 if (window.EventSource) { // 建立连接 source = new EventSource(\u0026#39;http://localhost:7777/sse/sub/\u0026#39;+userId); setMessageInnerHTML(\u0026#34;连接用户=\u0026#34; + userId); /** * 连接一旦建立，就会触发open事件 * 另一种写法：source.onopen = function (event) {} */ source.addEventListener(\u0026#39;open\u0026#39;, function (e) { setMessageInnerHTML(\u0026#34;建立连接。。。\u0026#34;); }, false); /** * 客户端收到服务器发来的数据 * 另一种写法：source.onmessage = function (event) {} */ source.addEventListener(\u0026#39;message\u0026#39;, function (e) { setMessageInnerHTML(e.data); }); } else { setMessageInnerHTML(\u0026#34;你的浏览器不支持SSE\u0026#34;); } \u0026lt;/script\u0026gt; 服务端的实现更简单，创建一个SseEmitter对象放入sseEmitterMap进行管理\nprivate static Map\u0026lt;String, SseEmitter\u0026gt; sseEmitterMap = new ConcurrentHashMap\u0026lt;\u0026gt;(); /** * 创建连接 */ public static SseEmitter connect(String userId) { try { // 设置超时时间，0表示不过期。默认30秒 SseEmitter sseEmitter = new SseEmitter(0L); // 注册回调 sseEmitter.onCompletion(completionCallBack(userId)); sseEmitter.onError(errorCallBack(userId)); sseEmitter.onTimeout(timeoutCallBack(userId)); sseEmitterMap.put(userId, sseEmitter); count.getAndIncrement(); return sseEmitter; } catch (Exception e) { log.info(\u0026#34;创建新的sse连接异常，当前用户：{}\u0026#34;, userId); } return null; } /** * 给指定用户发送消息 */ public static void sendMessage(String userId, String message) { if (sseEmitterMap.containsKey(userId)) { try { sseEmitterMap.get(userId).send(message); } catch (IOException e) { log.error(\u0026#34;用户[{}]推送异常:{}\u0026#34;, userId, e.getMessage()); removeUser(userId); } } } 注意： SSE 不支持 IE 浏览器，对其他主流浏览器兼容性做的还不错。\n# Websocket # Websocket 应该是大家都比较熟悉的一种实现消息推送的方式，上边我们在讲 SSE 的时候也和 Websocket 进行过比较。\n是一种在 TCP 连接上进行全双工通信的协议，建立客户端和服务器之间的通信渠道。浏览器和服务器仅需一次握手，两者之间就直接可以创建持久性的连接，并进行双向数据传输。\nSpringBoot 整合 Websocket，先引入 Websocket 相关的工具包，和 SSE 相比额外的开发成本。\n\u0026lt;!-- 引入websocket --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-websocket\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--可能需要排除springboot自带的tomcat--\u0026gt; 服务端使用@ServerEndpoint注解标注当前类为一个 WebSocket 服务器，客户端可以通过ws://localhost:7777/webSocket/10086来连接到 WebSocket 服务器端。\n@Component @Slf4j @ServerEndpoint(\u0026#34;/websocket/{userId}\u0026#34;) public class MyWebSocket { //与某个客户端的连接会话，需要通过它来给客户端发送数据 private Session session; private static final CopyOnWriteArraySet\u0026lt;MyWebSocket\u0026gt; webSockets = new CopyOnWriteArraySet\u0026lt;\u0026gt;(); // 用来存在线连接数 private static final Map\u0026lt;String, Session\u0026gt; sessionPool = new HashMap\u0026lt;String, Session\u0026gt;(); /** * 链接成功调用的方法 */ @OnOpen public void onOpen(Session session, @PathParam(value = \u0026#34;userId\u0026#34;) String userId) { try { this.session = session; webSockets.add(this); sessionPool.put(userId, session); log.info(\u0026#34;websocket消息: 有新的连接，总数为:\u0026#34; + webSockets.size()); } catch (Exception e) { } } /** * 收到客户端消息后调用的方法 */ @OnMessage public void onMessage(String message) { log.info(\u0026#34;websocket消息: 收到客户端消息:\u0026#34; + message); } /** * 此为单点消息 */ public void sendOneMessage(String userId, String message) { Session session = sessionPool.get(userId); if (session != null \u0026amp;\u0026amp; session.isOpen()) { try { log.info(\u0026#34;websocket消: 单点消息:\u0026#34; + message); session.getAsyncRemote().sendText(message); } catch (Exception e) { e.printStackTrace(); } } } } 前端初始化打开 WebSocket 连接，并监听连接状态，接收服务端数据或向服务端发送数据。\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;websocket测试\u0026lt;/title\u0026gt; \u0026lt;meta charset=\u0026#34;utf-8\u0026#34; /\u0026gt; \u0026lt;script src=\u0026#34;https://code.jquery.com/jquery-3.1.1.min.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;message\u0026#34;\u0026gt; \u0026lt;button onclick=\u0026#34;sendMessage()\u0026#34;\u0026gt;测试\u0026lt;/button\u0026gt; \u0026lt;script\u0026gt; //注意，地址不要填错了 var ws = new WebSocket(\u0026#39;ws://localhost:8089/websocket/10086\u0026#39;); // 获取连接状态 console.log(\u0026#39;ws连接状态：\u0026#39; + ws.readyState); //监听是否连接成功 ws.onopen = function () { console.log(\u0026#39;ws连接状态：\u0026#39; + ws.readyState); //连接成功则发送一个数据 ws.send(\u0026#39;test1\u0026#39;); } // 接听服务器发回的信息并处理展示 ws.onmessage = function (data) { console.log(\u0026#39;接收到来自服务器的消息：\u0026#39;); console.log(data); //完成通信后关闭WebSocket连接(这里不要关闭，让他持续发） //ws.close(); } // 监听连接关闭事件 ws.onclose = function () { // 监听整个过程中websocket的状态 console.log(\u0026#39;ws连接状态：\u0026#39; + ws.readyState); } // 监听并处理error事件 ws.onerror = function (error) { console.log(error); } //如果新开一个窗口，可以手动访问http://192.168.2.26:8089/socket/publish?userId=10086\u0026amp;message=abcde,那么就可以发送消息啦（原窗口可以继续接收消息） function sendMessage() { var content = $(\u0026#34;#message\u0026#34;).val(); //这里需要后台提供/socket/publish接口 $.ajax({ url: \u0026#39;http://192.168.2.26:8089/socket/publish?userId=10086\u0026amp;message=\u0026#39; + content, type: \u0026#39;GET\u0026#39;, data: { \u0026#34;id\u0026#34;: \u0026#34;7777\u0026#34;, \u0026#34;content\u0026#34;: content }, success: function (data) { console.log(data) } }) } \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 页面初始化建立 WebSocket 连接，之后就可以进行双向通信了，效果还不错。\n# MQTT # 什么是 MQTT 协议？\nMQTT (Message Queue Telemetry Transport)是一种基于发布/订阅（publish/subscribe）模式的轻量级通讯协议，通过订阅相应的主题来获取消息，是物联网（Internet of Thing）中的一个标准传输协议。\n该协议将消息的发布者（publisher）与订阅者（subscriber）进行分离，因此可以在不可靠的网络环境中，为远程连接的设备提供可靠的消息服务，使用方式与传统的 MQ 有点类似。\nTCP 协议位于传输层，MQTT 协议位于应用层，MQTT 协议构建于 TCP/IP 协议上，也就是说只要支持 TCP/IP 协议栈的地方，都可以使用 MQTT 协议。\n为什么要用 MQTT 协议？\nMQTT 协议为什么在物联网（IOT）中如此受偏爱？而不是其它协议，比如我们更为熟悉的 HTTP 协议呢？\n首先 HTTP 协议它是一种同步协议，客户端请求后需要等待服务器的响应。而在物联网（IOT）环境中，设备会很受制于环境的影响，比如带宽低、网络延迟高、网络通信不稳定等，显然异步消息协议更为适合 IOT 应用程序。 HTTP 是单向的，如果要获取消息客户端必须发起连接，而在物联网（IOT）应用程序中，设备或传感器往往都是客户端，这意味着它们无法被动地接收来自网络的命令。 通常需要将一条命令或者消息，发送到网络上的所有设备上。HTTP 要实现这样的功能不但很困难，而且成本极高。 具体的 MQTT 协议介绍和实践，这里我就不再赘述了，大家可以参考我之前的两篇文章，里边写的也都很详细了。\nMQTT 协议的介绍：我也没想到 SpringBoot + RabbitMQ 做智能家居，会这么简单open in new window MQTT 实现消息推送：未读消息（小红点），前端 与 RabbitMQ 实时消息推送实践，贼简单~open in new window # 总结 # 以下内容为 JavaGuide 补充\n介绍 优点 缺点 短轮询 客户端定时向服务端发送请求，服务端直接返回响应数据（即使没有数据更新） 简单、易理解、易实现 实时性太差，无效请求太多，频繁建立连接太耗费资源 长轮询 与短轮询不同是，长轮询接收到客户端请求之后等到有数据更新才返回请求 减少了无效请求 挂起请求会导致资源浪费 iframe 流 服务端和客户端之间创建一条长连接，服务端持续向iframe传输数据。 简单、易理解、易实现 维护一个长连接会增加开销，效果太差（图标会不停旋转） SSE 一种服务器端到客户端(浏览器)的单向消息推送。 简单、易实现，功能丰富 不支持双向通信 WebSocket 除了最初建立连接时用 HTTP 协议，其他时候都是直接基于 TCP 协议进行通信的，可以实现客户端和服务端的全双工通信。 性能高、开销小 对开发人员要求更高，实现相对复杂一些 MQTT 基于发布/订阅（publish/subscribe）模式的轻量级通讯协议，通过订阅相应的主题来获取消息。 成熟稳定，轻量级 对开发人员要求更高，实现相对复杂一些 著作权归所有 原文链接：https://javaguide.cn/system-design/web-real-time-message-push.html\n"},{"id":66,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/schedule-task/","title":"Java定时任务详解","section":"系统设计","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n为什么需要定时任务？ # 我们来看一下几个非常常见的业务场景：\n某系统凌晨要进行数据备份。 某电商平台，用户下单半个小时未支付的情况下需要自动取消订单。 某媒体聚合平台，每 10 分钟动态抓取某某网站的数据为自己所用。 某博客平台，支持定时发送文章。 某基金平台，每晚定时计算用户当日收益情况并推送给用户最新的数据。 \u0026hellip;\u0026hellip; 这些场景往往都要求我们在某个特定的时间去做某个事情。\n单机定时任务技术选型 # Timer # java.util.Timer是 JDK 1.3 开始就已经支持的一种定时任务的实现方式。\nTimer 内部使用一个叫做 TaskQueue 的类存放定时任务，它是一个基于最小堆实现的优先级队列。TaskQueue 会按照任务距离下一次执行时间的大小将任务排序，保证在堆顶的任务最先执行。这样在需要执行任务时，每次只需要取出堆顶的任务运行即可！\nTimer 使用起来比较简单，通过下面的方式我们就能创建一个 1s 之后执行的定时任务。\n// 示例代码： TimerTask task = new TimerTask() { public void run() { System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); } }; System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); Timer timer = new Timer(\u0026#34;Timer\u0026#34;); long delay = 1000L; timer.schedule(task, delay); //输出： 当前时间: Fri May 28 15:18:47 CST 2021n线程名称: main 当前时间: Fri May 28 15:18:48 CST 2021n线程名称: Timer 不过其缺陷较多，比如一个 Timer 一个线程，这就导致 Timer 的任务的执行只能串行执行，一个任务执行时间过长的话会影响其他任务（性能非常差），再比如发生异常时任务直接停止（Timer 只捕获了 InterruptedException ）。\nTimer 类上的有一段注释是这样写的：\n* This class does not offer real-time guarantees: it schedules * tasks using the \u0026lt;tt\u0026gt;Object.wait(long)\u0026lt;/tt\u0026gt; method. *Java 5.0 introduced the {@code java.util.concurrent} package and * one of the concurrency utilities therein is the {@link * java.util.concurrent.ScheduledThreadPoolExecutor * ScheduledThreadPoolExecutor} which is a thread pool for repeatedly * executing tasks at a given rate or delay. It is effectively a more * versatile replacement for the {@code Timer}/{@code TimerTask} * combination, as it allows multiple service threads, accepts various * time units, and doesn\u0026#39;t require subclassing {@code TimerTask} (just * implement {@code Runnable}). Configuring {@code * ScheduledThreadPoolExecutor} with one thread makes it equivalent to * {@code Timer}. 大概的意思就是： ScheduledThreadPoolExecutor 支持多线程执行定时任务并且功能更强大，是 Timer 的替代品。\nScheduledExecutorService # ScheduledExecutorService 是一个接口，有多个实现类，比较常用的是 ScheduledThreadPoolExecutor 。\nScheduledThreadPoolExecutor 本身就是一个线程池，支持任务并发执行。并且，其内部使用 DelayedWorkQueue 作为任务队列。\n// 示例代码： TimerTask repeatedTask = new TimerTask() { @SneakyThrows public void run() { System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); } }; System.out.println(\u0026#34;当前时间: \u0026#34; + new Date() + \u0026#34;n\u0026#34; + \u0026#34;线程名称: \u0026#34; + Thread.currentThread().getName()); ScheduledExecutorService executor = Executors.newScheduledThreadPool(3); long delay = 1000L; long period = 1000L; executor.scheduleAtFixedRate(repeatedTask, delay, period, TimeUnit.MILLISECONDS); Thread.sleep(delay + period * 5); executor.shutdown(); //输出： 当前时间: Fri May 28 15:40:46 CST 2021n线程名称: main 当前时间: Fri May 28 15:40:47 CST 2021n线程名称: pool-1-thread-1 当前时间: Fri May 28 15:40:48 CST 2021n线程名称: pool-1-thread-1 当前时间: Fri May 28 15:40:49 CST 2021n线程名称: pool-1-thread-2 当前时间: Fri May 28 15:40:50 CST 2021n线程名称: pool-1-thread-2 当前时间: Fri May 28 15:40:51 CST 2021n线程名称: pool-1-thread-2 当前时间: Fri May 28 15:40:52 CST 2021n线程名称: pool-1-thread-2 不论是使用 Timer 还是 ScheduledExecutorService 都无法使用 Cron 表达式指定任务执行的具体时间。\nSpring Task # [krɑn] cron\n我们直接通过 Spring 提供的 @Scheduled 注解即可定义定时任务，非常方便！\n/** * cron：使用Cron表达式。　每分钟的1，2秒运行 */ @Scheduled(cron = \u0026#34;1-2 * * * * ? \u0026#34;) public void reportCurrentTimeWithCronExpression() { log.info(\u0026#34;Cron Expression: The time is now {}\u0026#34;, dateFormat.format(new Date())); } 我在大学那会做的一个 SSM 的企业级项目，就是用的 Spring Task 来做的定时任务。\n并且，Spring Task 还是支持 Cron 表达式 的。Cron 表达式主要用于定时作业(定时任务)系统定义执行时间或执行频率的表达式，非常厉害，你可以通过 Cron 表达式进行设置定时任务每天或者每个月什么时候执行等等操作。咱们要学习定时任务的话，Cron 表达式是一定是要重点关注的。推荐一个在线 Cron 表达式生成器：http://cron.qqe2.com/ 。\n但是，Spring 自带的定时调度只支持单机，并且提供的功能比较单一。之前写过一篇文章:《5 分钟搞懂如何在 Spring Boot 中 Schedule Tasks》 ，不了解的小伙伴可以参考一下。\nSpring Task 底层是基于 JDK 的 ScheduledThreadPoolExecutor 线程池来实现的。\n优缺点总结：\n优点： 简单，轻量，支持 Cron 表达式 缺点 ：功能单一 时间轮 # Kafka、Dubbo、ZooKeeper、Netty 、Caffeine 、Akka 中都有对时间轮的实现。\n时间轮简单来说就是一个环形的队列（底层一般基于数组实现），队列中的每一个元素（时间格）都可以存放一个定时任务列表。\n时间轮中的每个时间格代表了时间轮的基本时间跨度或者说时间精度，加入时间一秒走一个时间格的话，那么这个时间轮的最高精度就是 1 秒（也就是说 3 s 和 3.9s 会在同一个时间格中）。\n下图是一个有 12 个时间格的时间轮，转完一圈需要 12 s。当我们需要新建一个 3s 后执行的定时任务，只需要将定时任务放在下标为 3 的时间格中即可。当我们需要新建一个 9s 后执行的定时任务，只需要将定时任务放在下标为 9 的时间格中即可。\n那当我们需要创建一个 13s 后执行的定时任务怎么办呢？这个时候可以引入一叫做 圈数/轮数 的概念，也就是说这个任务还是放在下标为 3 的时间格中， 不过它的圈数为 2 。\n除了增加圈数这种方法之外，还有一种 多层次时间轮 （类似手表），Kafka 采用的就是这种方案。\n针对下图的时间轮，我来举一个例子便于大家理解。\n上图的时间轮，第 1 层的时间精度为 1 ，第 2 层的时间精度为 20 ，第 3 层的时间精度为 400。假如我们需要添加一个 350s 后执行的任务 A 的话（当前时间是 0s），这个任务会被放在第 2 层（因为第二层的时间跨度为 20*20=400\u0026gt;350）的第 350/20=17 个时间格子。\n当第一层转了 17 圈之后，时间过去了 340s ，第 2 层的指针此时来到第 17 个时间格子。此时，第 2 层第 17 个格子的任务会被移动到第 1 层。\n任务 A 当前是 10s 之后执行，因此它会被移动到第 1 层的第 10 个时间格子。\n这里在层与层之间的移动也叫做时间轮的升降级。参考手表来理解就好！\n时间轮比较适合任务数量比较多的定时任务场景，它的任务写入和执行的时间复杂度都是 0（1）。\n分布式定时任务技术选型 # 上面提到的一些定时任务的解决方案都是在单机下执行的，适用于比较简单的定时任务场景比如每天凌晨备份一次数据。\n如果我们需要一些高级特性比如支持任务在分布式场景下的分片和高可用的话，我们就需要用到分布式任务调度框架了。\n通常情况下，一个定时任务的执行往往涉及到下面这些角色：\n任务 ： 首先肯定是要执行的任务，这个任务就是具体的业务逻辑比如定时发送文章。 调度器 ：其次是调度中心，调度中心主要负责任务管理，会分配任务给执行器。 执行器 ： 最后就是执行器，执行器接收调度器分派的任务并执行。 Quartz # 一个很火的开源任务调度框架，完全由Java写成。Quartz 可以说是 Java 定时任务领域的老大哥或者说参考标准，其他的任务调度框架基本都是基于 Quartz 开发的，比如当当网的elastic-job就是基于quartz二次开发之后的分布式调度解决方案。\n使用 Quartz 可以很方便地与 Spring 集成，并且支持动态添加任务和集群。但是，Quartz 使用起来也比较麻烦，API 繁琐。\n并且，Quzrtz 并没有内置 UI 管理控制台，不过你可以使用 quartzui 这个开源项目来解决这个问题。\n另外，Quartz 虽然也支持分布式任务。但是，它是在数据库层面，通过数据库的锁机制做的，有非常多的弊端比如系统侵入性严重、节点负载不均衡。有点伪分布式的味道。\n优缺点总结：\n优点： 可以与 Spring 集成，并且支持动态添加任务和集群。 缺点 ：分布式支持不友好，没有内置 UI 管理控制台、使用麻烦（相比于其他同类型框架来说） Elastic-Job # Elastic-Job 是当当网开源的一个基于Quartz和ZooKeeper的分布式调度解决方案，由两个相互独立的子项目 Elastic-Job-Lite 和 Elastic-Job-Cloud 组成，一般我们只要使用 Elastic-Job-Lite 就好。\nElasticJob 支持任务在分布式场景下的分片和高可用、任务可视化管理等功能。\nElasticJob-Lite 的架构设计如下图所示：\n从上图可以看出，Elastic-Job 没有调度中心这一概念，而是使用 ZooKeeper 作为注册中心，注册中心负责协调分配任务到不同的节点上。\nElastic-Job 中的定时调度都是由执行器自行触发，这种设计也被称为去中心化设计（调度和处理都是执行器单独完成）。\n@Component @ElasticJobConf(name = \u0026#34;dayJob\u0026#34;, cron = \u0026#34;0/10 * * * * ?\u0026#34;, shardingTotalCount = 2, shardingItemParameters = \u0026#34;0=AAAA,1=BBBB\u0026#34;, description = \u0026#34;简单任务\u0026#34;, failover = true) public class TestJob implements SimpleJob { @Override public void execute(ShardingContext shardingContext) { log.info(\u0026#34;TestJob任务名：【{}】, 片数：【{}】, param=【{}】\u0026#34;, shardingContext.getJobName(), shardingContext.getShardingTotalCount(), shardingContext.getShardingParameter()); } } 相关地址：\nGithub 地址：https://github.com/apache/shardingsphere-elasticjob。 官方网站：https://shardingsphere.apache.org/elasticjob/index_zh.html 。 优缺点总结：\n优点 ：可以与 Spring 集成、支持分布式、支持集群、性能不错 缺点 ：依赖了额外的中间件比如 Zookeeper（复杂度增加，可靠性降低、维护成本变高） XXL-JOB # XXL-JOB 于 2015 年开源，是一款优秀的轻量级分布式任务调度框架，支持任务可视化管理、弹性扩容缩容、任务失败重试和告警、任务分片等功能，\n根据 XXL-JOB 官网介绍，其解决了很多 Quartz 的不足。\nXXL-JOB 的架构设计如下图所示：\n从上图可以看出，XXL-JOB 由 调度中心 和 执行器 两大部分组成。调度中心主要负责任务管理、执行器管理以及日志管理。执行器主要是接收调度信号并处理。另外，调度中心进行任务调度时，是通过自研 RPC 来实现的。\n不同于 Elastic-Job 的去中心化设计， XXL-JOB 的这种设计也被称为中心化设计（调度中心调度多个执行器执行任务）。\n和 Quzrtz 类似 XXL-JOB 也是基于数据库锁调度任务，存在性能瓶颈。不过，一般在任务量不是特别大的情况下，没有什么影响的，可以满足绝大部分公司的要求。\n不要被 XXL-JOB 的架构图给吓着了，实际上，我们要用 XXL-JOB 的话，只需要重写 IJobHandler 自定义任务执行逻辑就可以了，非常易用！\n@JobHandler(value=\u0026#34;myApiJobHandler\u0026#34;) @Component public class MyApiJobHandler extends IJobHandler { @Override public ReturnT\u0026lt;String\u0026gt; execute(String param) throws Exception { //...... return ReturnT.SUCCESS; } } 还可以直接基于注解定义任务。\n@XxlJob(\u0026#34;myAnnotationJobHandler\u0026#34;) public ReturnT\u0026lt;String\u0026gt; myAnnotationJobHandler(String param) throws Exception { //...... return ReturnT.SUCCESS; } 相关地址：\nGithub 地址：https://github.com/xuxueli/xxl-job/。 官方介绍：https://www.xuxueli.com/xxl-job/ 。 优缺点总结：\n优点：开箱即用（学习成本比较低）、与 Spring 集成、支持分布式、支持集群、内置了 UI 管理控制台。 缺点：不支持动态添加任务（如果一定想要动态创建任务也是支持的，参见：xxl-job issue277）。 PowerJob # 非常值得关注的一个分布式任务调度框架，分布式任务调度领域的新星。目前，已经有很多公司接入比如 OPPO、京东、中通、思科。\n这个框架的诞生也挺有意思的，PowerJob 的作者当时在阿里巴巴实习过，阿里巴巴那会使用的是内部自研的 SchedulerX（阿里云付费产品）。实习期满之后，PowerJob 的作者离开了阿里巴巴。想着说自研一个 SchedulerX，防止哪天 SchedulerX 满足不了需求，于是 PowerJob 就诞生了。\n更多关于 PowerJob 的故事，小伙伴们可以去看看 PowerJob 作者的视频 《我和我的任务调度中间件》。简单点概括就是：“游戏没啥意思了，我要扛起了新一代分布式任务调度与计算框架的大旗！”。\n由于 SchedulerX 属于人民币产品，我这里就不过多介绍。PowerJob 官方也对比过其和 QuartZ、XXL-JOB 以及 SchedulerX。\n总结 # 这篇文章中，我主要介绍了：\n定时任务的相关概念 ：为什么需要定时任务、定时任务中的核心角色、分布式定时任务。 定时任务的技术选型 ： XXL-JOB 2015 年推出，已经经过了很多年的考验。XXL-JOB 轻量级，并且使用起来非常简单。虽然存在性能瓶颈，但是，在绝大多数情况下，对于企业的基本需求来说是没有影响的。PowerJob 属于分布式任务调度领域里的新星，其稳定性还有待继续考察。ElasticJob 由于在架构设计上是基于 Zookeeper ，而 XXL-JOB 是基于数据库，性能方面的话，ElasticJob 略胜一筹。 这篇文章并没有介绍到实际使用，但是，并不代表实际使用不重要。我在写这篇文章之前，已经动手写过相应的 Demo。像 Quartz，我在大学那会就用过。不过，当时用的是 Spring 。为了能够更好地体验，我自己又在 Spring Boot 上实际体验了一下。如果你并没有实际使用某个框架，就直接说它并不好用的话，是站不住脚的。\n最后，这篇文章要感谢艿艿的帮助，写这篇文章的时候向艿艿询问过一些问题。推荐一篇艿艿写的偏实战类型的硬核文章：《Spring Job？Quartz？XXL-Job？年轻人才做选择，艿艿全莽~》 。\n"},{"id":67,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/security/ly06ly_sentive-words-filter/","title":"敏感词过滤方案总结","section":"安全","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n系统需要对用户输入的文本进行敏感词过滤如色情、政治、暴力相关的词汇。\n敏感词过滤用的使用比较多的 Trie 树算法 和 DFA 算法。\n算法实现 # Trie 树 # Trie 树 也称为字典树、单词查找树，哈系树(这里是不是写错了，哈希树？)的一种变种，通常被用于字符串匹配，用来解决在一组字符串集合中快速查找某个字符串的问题。像浏览器搜索的关键词提示一般就是基于 Trie 树来做的。\n假如我们的敏感词库中有以下敏感词：\n高清有码 高清 AV 东京冷 东京热 我们构造出来的敏感词 Trie 树就是下面这样的：\n当我们要查找对应的字符串“东京热”的话，我们会把这个字符串切割成单个的字符“东”、“京”、“热”，然后我们从 Trie 树的根节点开始匹配。\n可以看出， Trie 树的核心原理其实很简单，就是通过公共前缀来提高字符串匹配效率。\nApache Commons Collecions 这个库中就有 Trie 树实现：\nTrie\u0026lt;String, String\u0026gt; trie = new PatriciaTrie\u0026lt;\u0026gt;(); trie.put(\u0026#34;Abigail\u0026#34;, \u0026#34;student\u0026#34;); trie.put(\u0026#34;Abi\u0026#34;, \u0026#34;doctor\u0026#34;); trie.put(\u0026#34;Annabel\u0026#34;, \u0026#34;teacher\u0026#34;); trie.put(\u0026#34;Christina\u0026#34;, \u0026#34;student\u0026#34;); trie.put(\u0026#34;Chris\u0026#34;, \u0026#34;doctor\u0026#34;); Assertions.assertTrue(trie.containsKey(\u0026#34;Abigail\u0026#34;)); assertEquals(\u0026#34;{Abi=doctor, Abigail=student}\u0026#34;, trie.prefixMap(\u0026#34;Abi\u0026#34;).toString()); assertEquals(\u0026#34;{Chris=doctor, Christina=student}\u0026#34;, trie.prefixMap(\u0026#34;Chr\u0026#34;).toString()); Aho-Corasick（AC）自动机是一种建立在 Trie 树上的一种改进算法，是一种多模式匹配算法，由贝尔实验室的研究人员 Alfred V. Aho 和 Margaret J.Corasick 发明。\nAC 自动机算法使用 Trie 树来存放模式串的前缀，通过失败匹配指针（失配指针）来处理匹配失败的跳转。\n相关阅读：地铁十分钟 | AC 自动机\nDFA # DFA（Deterministic Finite Automata)即确定有穷自动机，与之对应的是 NFA（Non-Deterministic Finite Automata，有穷自动机)。\n关于 DFA 的详细介绍可以看这篇文章：有穷自动机 DFA\u0026amp;NFA (学习笔记) - 小蜗牛的文章 - 知乎 。\nHutool 提供了 DFA 算法的实现：\nWordTree wordTree = new WordTree(); wordTree.addWord(\u0026#34;大\u0026#34;); wordTree.addWord(\u0026#34;大憨憨\u0026#34;); wordTree.addWord(\u0026#34;憨憨\u0026#34;); String text = \u0026#34;那人真是个大憨憨！\u0026#34;; // 获得第一个匹配的关键字 String matchStr = wordTree.match(text); System.out.println(matchStr); // 标准匹配，匹配到最短关键词，并跳过已经匹配的关键词 List\u0026lt;String\u0026gt; matchStrList = wordTree.matchAll(text, -1, false, false); System.out.println(matchStrList); //匹配到最长关键词，跳过已经匹配的关键词 List\u0026lt;String\u0026gt; matchStrList2 = wordTree.matchAll(text, -1, false, true); System.out.println(matchStrList2); 输出：\n大 [大, 憨憨] [大, 大憨憨] 开源项目 # ToolGood.Words ：一款高性能敏感词(非法词/脏字)检测过滤组件，附带繁体简体互换，支持全角半角互换，汉字转拼音，模糊搜索等功能。 sensitive-words-filter ：敏感词过滤项目，提供 TTMP、DFA、DAT、hash bucket、Tire 算法支持过滤。可以支持文本的高亮、过滤、判词、替换的接口支持。 论文 # 一种敏感词自动过滤管理系统 一种网络游戏中敏感词过滤方法及系统 "},{"id":68,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/security/ly05ly_design-of-authority-system/","title":"权限系统设计详解","section":"安全","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n作者：转转技术团队\n原文：https://mp.weixin.qq.com/s/ONMuELjdHYa0yQceTj01Iw\nly：比较繁琐，大概看了前面的部分\n老权限系统的问题与现状 # 转转公司在过去并没有一个统一的权限管理系统，权限管理由各业务自行研发或是使用其他业务的权限系统，权限管理的不统一带来了不少问题：\n各业务重复造轮子，维护成本高 各系统只解决部分场景问题，方案不够通用，新项目选型时没有可靠的权限管理方案 缺乏统一的日志管理与审批流程，在授权信息追溯上十分困难 基于上述问题，去年底公司启动建设转转统一权限系统，目标是开发一套灵活、易用、安全的权限管理系统，供各业务使用。\n业界权限系统的设计方式 # 目前业界主流的权限模型有两种，下面分别介绍下：\n基于角色的访问控制（RBAC） 基于属性的访问控制（ABAC） RBAC 模型 # 基于角色的访问控制（Role-Based Access Control，简称 RBAC） 指的是通过用户的角色（Role）授权其相关权限，实现了灵活的访问控制，相比直接授予用户权限，要更加简单、高效、可扩展。\n一个用户可以拥有若干角色，每一个角色又可以被分配若干权限这样，就构造成“用户-角色-权限” 的授权模型。在这种模型中，用户与角色、角色与权限之间构成了多对多的关系。\n用一个图来描述如下：\n当使用 RBAC模型 时，通过分析用户的实际情况，基于共同的职责和需求，授予他们不同角色。这种 用户 -\u0026gt; 角色 -\u0026gt; 权限 间的关系，让我们可以不用再单独管理单个用户权限，用户从授予的角色里面获取所需的权限。\n以一个简单的场景（Gitlab 的权限系统）为例，用户系统中有 Admin、Maintainer、Operator 三种角色，这三种角色分别具备不同的权限，比如只有 Admin 具备创建代码仓库、删除代码仓库的权限，其他的角色都不具备。我们授予某个用户 Admin 这个角色，他就具备了 创建代码仓库 和 删除代码仓库 这两个权限。\n通过 RBAC模型 ，当存在多个用户拥有相同权限时，我们只需要创建好拥有该权限的角色，然后给不同的用户分配不同的角色，后续只需要修改角色的权限，就能自动修改角色内所有用户的权限。\nABAC 模型 # 基于属性的访问控制（Attribute-Based Access Control，简称 ABAC） 是一种比 RBAC模型 更加灵活的授权模型，它的原理是通过各种属性来动态判断一个操作是否可以被允许。这个模型在云系统中使用的比较多，比如 AWS，阿里云等。\n考虑下面这些场景的权限控制：\n授权某个人具体某本书的编辑权限 当一个文档的所属部门跟用户的部门相同时，用户可以访问这个文档 当用户是一个文档的拥有者并且文档的状态是草稿，用户可以编辑这个文档 早上九点前禁止 A 部门的人访问 B 系统 在除了上海以外的地方禁止以管理员身份访问 A 系统 用户对 2022-06-07 之前创建的订单有操作权限 可以发现上述的场景通过 RBAC模型 很难去实现，因为 RBAC模型 仅仅描述了用户可以做什么操作，但是操作的条件，以及操作的数据，RBAC模型 本身是没有这些限制的。但这恰恰是 ABAC模型 的长处，ABAC模型 的思想是基于用户、访问的数据的属性、以及各种环境因素去动态计算用户是否有权限进行操作。\nABAC 模型的原理 # 在 ABAC模型 中，一个操作是否被允许是基于对象、资源、操作和环境信息共同动态计算决定的。\n对象：对象是当前请求访问资源的用户。用户的属性包括 ID，个人资源，角色，部门和组织成员身份等 资源：资源是当前用户要访问的资产或对象，例如文件，数据，服务器，甚至 API 操作：操作是用户试图对资源进行的操作。常见的操作包括“读取”，“写入”，“编辑”，“复制”和“删除” 环境：环境是每个访问请求的上下文。环境属性包含访问的时间和位置，对象的设备，通信协议和加密强度等 在 ABAC模型 的决策语句的执行过程中，决策引擎会根据定义好的决策语句，结合对象、资源、操作、环境等因素动态计算出决策结果。每当发生访问请求时，ABAC模型 决策系统都会分析属性值是否与已建立的策略匹配。如果有匹配的策略，访问请求就会被通过。\n新权限系统的设计思想 # 结合转转的业务现状，RBAC模型 满足了转转绝大部分业务场景，并且开发成本远低于 ABAC模型 的权限系统，所以新权限系统选择了基于 RBAC模型 来实现。对于实在无法满足的业务系统，我们选择了暂时性不支持，这样可以保障新权限系统的快速落地，更快的让业务使用起来。\n标准的 RBAC模型 是完全遵守 用户 -\u0026gt; 角色 -\u0026gt; 权限 这个链路的，也就是用户的权限完全由他所拥有的角色来控制，但是这样会有一个缺点，就是给用户加权限必须新增一个角色，导致实际操作起来效率比较低。所以我们在 RBAC模型 的基础上，新增了给用户直接增加权限的能力，也就是说既可以给用户添加角色，也可以给用户直接添加权限。最终用户的权限是由拥有的角色和权限点组合而成。\n新权限系统的权限模型：用户最终权限 = 用户拥有的角色带来的权限 + 用户独立配置的权限，两者取并集。\n新权限系统方案如下图 ：\n首先，将集团所有的用户（包括外部用户），通过 统一登录与注册 功能实现了统一管理，同时与公司的组织架构信息模块打通，实现了同一个人员在所有系统中信息的一致，这也为后续基于组织架构进行权限管理提供了可行性。 其次，因为新权限系统需要服务集团所有业务，所以需要支持多系统权限管理。用户进行权限管理前，需要先选择相应的系统，然后配置该系统的 菜单权限 和 数据权限 信息，建立好系统的各个权限点。PS：菜单权限和数据权限的具体说明，下文会详细介绍。 最后，创建该系统下的不同角色，给不同角色配置好权限点。比如店长角色，拥有店员操作权限、本店数据查看权限等，配置好这个角色后，后续只需要给店长增加这个角色，就可以让他拥有对应的权限。 完成上述配置后，就可以进行用户的权限管理了。有两种方式可以给用户加权限：\n先选用户，然后添加权限。该方式可以给用户添加任意角色或是菜单/数据权限点。 先选择角色，然后关联用户。该方式只可给用户添加角色，不能单独添加菜单/数据权限点。 这两种方式的具体设计方案，后文会详细说明。\n权限系统自身的权限管理 # 对于权限系统来说，首先需要设计好系统自身的权限管理，也就是需要管理好 ”谁可以进入权限系统，谁可以管理其他系统的权限“，对于权限系统自身的用户，会分为三类：\n超级管理员：拥有权限系统的全部操作权限，可以进行系统自身的任何操作，也可以管理接入权限的应用系统的管理操作。 权限操作用户：拥有至少一个已接入的应用系统的超级管理员角色的用户。该用户能进行的操作限定在所拥有的应用系统权限范围内。权限操作用户是一种身份，无需分配，而是根据规则自动获得的。 普通用户：普通用户也可以认为是一种身份，除去上述 2 类人，其余的都为普通用户。他们只能申请接入系统以及访问权限申请页面。 权限类型的定义 # 新权限系统中，我们把权限分为两大类，分别是 ：\n菜单功能权限：包括系统的目录导航、菜单的访问权限，以及按钮和 API 操作的权限 数据权限：包括定义数据的查询范围权限，在不同系统中，通常叫做 “组织”、”站点“等，在新权限系统中，统一称作 ”组织“ 来管理数据权限 默认角色的分类 # 每个系统中设计了三个默认角色，用来满足基本的权限管理需求，分别如下：\n超级管理员：该角色拥有该系统的全部权限，可以修改系统的角色权限等配置，可以给其他用户授权。 系统管理员：该角色拥有给其他用户授权以及修改系统的角色权限等配置能力，但角色本身不具有任何权限。 授权管理员：该角色拥有给其他用户授权的能力。但是授权的范围不超出自己所拥有的权限。 举个栗子：授权管理员 A 可以给 B 用户添加权限，但添加的范围 小于等于 A 用户已拥有的权限。\n经过这么区分，把 拥有权限 和 拥有授权能力 ，这两部分给分隔开来，可以满足所有的权限控制的场景。\n新权限系统的核心模块设计 # 上面介绍了新权限系统的整体设计思想，接下来分别介绍下核心模块的设计\n系统/菜单/数据权限管理 # 把一个新系统接入权限系统有下列步骤：\n创建系统 配置菜单功能权限 配置数据权限（可选） 创建系统的角色 其中，1、2、3 的步骤，都是在系统管理模块完成，具体流程如下图:\n用户可以对系统的基本信息进行增删改查的操作，不同系统之间通过 系统编码 作为唯一区分。同时 系统编码 也会用作于菜单和数据权限编码的前缀，通过这样的设计保证权限编码全局唯一性。\n例如系统的编码为 test_online，那么该系统的菜单编码格式便为 test_online:m_xxx。\n系统管理界面设计如下：\n菜单管理 # 新权限系统首先对菜单进行了分类，分别是 目录、菜单 和 操作，示意如下图\n它们分别代表的含义是：\n目录 ：指的是应用系统中最顶部的一级目录，通常在系统 Logo 的右边 菜单 ：指的是应用系统左侧的多层级菜单，通常在系统 Logo 的下面，也是最常用的菜单结构 操作 ：指页面中的按钮、接口等一系列可以定义为操作或页面元素的部分。 菜单管理界面设计如下： 菜单权限数据的使用，也提供两种方式：\n动态菜单模式 ：这种模式下，菜单的增删完全由权限系统接管。也就是说在权限系统增加菜单，应用系统会同步增加。这种模式好处是修改菜单无需项目上线。 静态菜单模式 ：菜单的增删由应用系统的前端控制，权限系统只控制访问权限。这种模式下，权限系统只能标识出用户是否拥有当前菜单的权限，而具体的显示控制是由前端根据权限数据来决定。 角色与用户管理 # 角色与用户管理都是可以直接改变用户权限的核心模块，整个设计思路如下图：\n这个模块设计重点是需要考虑到批量操作。无论是通过角色关联用户，还是给用户批量增加/删除/重置权限，批量操作的场景都是系统需要设计好的。\n权限申请 # 除了给其他用户添加权限外，新权限系统同时支持了用户自主申请权限。这个模块除了常规的审批流（申请、审批、查看）等，有一个比较特别的功能，就是如何让用户能选对自己要的权限。所以在该模块的设计上，除了直接选择角色外，还支持通过菜单/数据权限点，反向选择角色，如下图：\n操作日志 # 系统操作日志会分为两大类：\n操作流水日志 ：用户可看、可查的关键操作日志 服务 Log 日志 ：系统服务运行过程中产生的 Log 日志,其中，服务 Log 日志信息量大于操作流水日志，但是不方便搜索查看。所以权限系统需要提供操作流水日志功能。 在新权限系统中，用户所有的操作可以分为三类，分别为新增、更新、删除。所有的模块也可枚举，例如用户管理、角色管理、菜单管理等。明确这些信息后，那么一条日志就可以抽象为：什么人(Who)在什么时间(When)对哪些人(Target)的哪些模块做了哪些操作。 这样把所有的记录都入库，就可以方便的进行日志的查看和筛选了。\n总结与展望 # 至此，新权限系统的核心设计思路与模块都已介绍完成，新系统在转转内部有大量的业务接入使用，权限管理相比以前方便了许多。权限系统作为每家公司的一个基础系统，灵活且完备的设计可以助力日后业务的发展更加高效。\n后续两篇：\n转转统一权限系统的设计与实现（后端实现篇） 转转统一权限系统的设计与实现（前端实现篇） 参考 # 选择合适的权限模型：https://docs.authing.cn/v2/guides/access-control/choose-the-right-access-control-model.html "},{"id":69,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/security/ly04ly_sso-intro/","title":"sso单点登录","section":"安全","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n本文授权转载自 ： https://ken.io/note/sso-design-implement 作者：ken.io\nSSO 介绍 # 什么是 SSO？ # SSO 英文全称 Single Sign On，单点登录。SSO 是在多个应用系统中，用户只需要登录一次就可以访问所有相互信任的应用系统。\n例如你登录网易账号中心（https://reg.163.com/ ）之后访问以下站点都是登录状态。\n网易直播 https://v.163.com 网易博客 https://blog.163.com 网易花田 https://love.163.com 网易考拉 https://www.kaola.com 网易 Lofter http://www.lofter.com SSO 有什么好处？ # 用户角度 :用户能够做到一次登录多次使用，无需记录多套用户名和密码，省心。 系统管理员角度 : 管理员只需维护好一个统一的账号中心就可以了，方便。 新系统开发角度: 新系统开发时只需直接对接统一的账号中心即可，简化开发流程，省时。 SSO 设计与实现 # 本篇文章也主要是为了探讨如何设计\u0026amp;实现一个 SSO 系统\n以下为需要实现的核心功能：\n单点登录 单点登出 支持跨域单点登录 支持跨域单点登出 核心应用与依赖 # 应用/模块/对象 说明 前台站点 需要登录的站点 SSO 站点-登录 提供登录的页面 SSO 站点-登出 提供注销登录的入口 SSO 服务-登录 提供登录服务 SSO 服务-登录状态 提供登录状态校验/登录信息查询的服务 SSO 服务-登出 提供用户注销登录的服务 数据库 存储用户账户信息 缓存 存储用户的登录信息，通常使用 Redis 用户登录状态的存储与校验 # 常见的 Web 框架对于 Session 的实现都是生成一个 SessionId 存储在浏览器 Cookie 中。然后将 Session 内容存储在服务器端内存中，这个 ken.io 在之前Session 工作原理中也提到过。整体也是借鉴这个思路。\n用户登录成功之后，生成 AuthToken 交给客户端保存。如果是浏览器，就保存在 Cookie 中。如果是手机 App 就保存在 App 本地缓存中。本篇主要探讨基于 Web 站点的 SSO。\n用户在浏览需要登录的页面时，客户端将 AuthToken 提交给 SSO 服务校验登录状态/获取用户登录信息\n对于登录信息的存储，建议采用 Redis，使用 Redis 集群来存储登录信息，既可以保证高可用，又可以线性扩充。同时也可以让 SSO 服务满足负载均衡/可伸缩的需求。\n对象 说明 AuthToken 直接使用 UUID/GUID 即可，如果有验证 AuthToken 合法性需求，可以将 UserName+时间戳加密生成，服务端解密之后验证合法性 登录信息 通常是将 UserId，UserName 缓存起来 用户登录/登录校验 # 登录时序图\n按照上图，用户登录后 AuthToken 保存在 Cookie 中。 domain=test.com 浏览器会将 domain 设置成 .test.com，\n这样访问所有 .test.com 的 web 站点，都会将 AuthToken 携带到服务器端*。 然后通过 SSO 服务，完成对用户状态的校验/用户登录信息的获取\n登录信息获取/登录状态校验\n用户登出 # 用户登出时要做的事情很简单：\n服务端清除缓存（Redis）中的登录状态 客户端清除存储的 AuthToken 登出时序图\n跨域登录、登出 # 前面提到过，核心思路是客户端存储 AuthToken，服务器端通过 Redis 存储登录信息。由于客户端是将 AuthToken 存储在 Cookie 中的。所以跨域要解决的问题，就是如何解决 Cookie 的跨域读写问题。\n解决跨域的核心思路就是：\n登录完成之后通过回调的方式，将 AuthToken 传递给主域名之外的站点，该站点自行将 AuthToken 保存在当前域下的 Cookie 中。 登出完成之后通过回调的方式，调用非主域名站点的登出页面，完成设置 Cookie 中的 AuthToken 过期的操作。(过期：先让主域名过期，再让非主域名过期[token失效]) 跨域登录（主域名已登录） 跨域登录（主域名未登录）\n跨域登出\n说明 # 关于方案 ：这次设计方案更多是提供实现思路。如果涉及到 APP 用户登录等情况，在访问 SSO 服务时，增加对 APP 的签名验证就好了。当然，如果有无线网关，验证签名不是问题。 关于时序图：时序图中并没有包含所有场景，只列举了核心/主要场景，另外对于一些不影响理解思路的消息能省就省了。 "},{"id":70,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/security/ly03ly_jwt-advantages-disadvantages/","title":"jwt身份认证优缺点","section":"安全","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n在 JWT 基本概念详解这篇文章中，我介绍了：\n什么是 JWT? JWT 由哪些部分组成？ 如何基于 JWT 进行身份验证？ JWT 如何防止 Token 被篡改？ 如何加强 JWT 的安全性？ 这篇文章，我们一起探讨一下 JWT 身份认证的优缺点以及常见问题的解决办法。\nJWT 的优势 # 相比于 Session 认证的方式来说，使用 JWT 进行身份认证主要有下面 4 个优势。\n无状态 # JWT 自身包含了身份验证所需要的所有信息，因此，我们的服务器不需要存储 Session 信息。这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。\n不过，也正是由于 JWT 的无状态，也导致了它最大的缺点：不可控！\n就比如说，我们想要在 JWT 有效期内废弃一个 JWT 或者更改它的权限的话，并不会立即生效，通常需要等到有效期过后才可以。再比如说，当用户 Logout 的话，JWT 也还有效。除非，我们在后端增加额外的处理逻辑比如将失效的 JWT 存储起来，后端先验证 JWT 是否有效再进行处理。具体的解决办法，我们会在后面的内容中详细介绍到，这里只是简单提一下。\n有效避免了 CSRF 攻击 # [ˈfɔːdʒəri] forgery 伪造\nCSRF（Cross Site Request Forgery） 一般被翻译为 跨站请求伪造，属于网络攻击领域范围。相比于 SQL 脚本注入、XSS 等安全攻击方式，CSRF 的知名度并没有它们高。但是，它的确是我们开发系统时必须要考虑的安全隐患。就连业内技术标杆 Google 的产品 Gmail 也曾在 2007 年的时候爆出过 CSRF 漏洞，这给 Gmail 的用户造成了很大的损失。\n那么究竟什么是跨站请求伪造呢？ 简单来说就是用你的身份去做一些不好的事情（发送一些对你不友好的请求比如恶意转账）。\n举个简单的例子：小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了 10000 元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求，也就是通过你的 Cookie 向银行发出请求。\n\u0026lt;a src=\u0026#34;http://www.mybank.com/Transfer?bankId=11\u0026amp;money=10000\u0026#34;\u0026gt;科学理财，年盈利率过万\u0026lt;/a\u0026gt; CSRF 攻击需要依赖 Cookie ，Session 认证中 Cookie 中的 SessionID 是由浏览器发送到服务端的，只要发出请求，Cookie 就会被携带。借助这个特性，即使黑客无法获取你的 SessionID，只要让你误点攻击链接，就可以达到攻击效果。\n另外，并不是必须点击链接才可以达到攻击效果，很多时候，只要你打开了某个页面，CSRF 攻击就会发生。\n\u0026lt;img src=\u0026#34;http://www.mybank.com/Transfer?bankId=11\u0026amp;money=10000\u0026#34; /\u0026gt; 那为什么 JWT 不会存在这种问题呢？\n一般情况下我们使用 JWT 的话，在我们登录成功获得 JWT 之后，一般会选择存放在 localStorage 中。前端的每一个请求后续都会附带上这个 JWT，整个过程压根不会涉及到 Cookie。因此，即使你点击了非法链接发送了请求到服务端，这个非法请求也是不会携带 JWT 的，所以这个请求将是非法的。\n总结来说就一句话：使用 JWT 进行身份验证不需要依赖 Cookie ，因此可以避免 CSRF 攻击。\n不过，这样也会存在 XSS 攻击的风险。为了避免 XSS 攻击，你可以选择将 JWT 存储在标记为httpOnly 的 Cookie 中。但是，这样又导致了你必须自己提供 CSRF 保护，因此，实际项目中我们通常也不会这么做。\nXSS攻击又称为跨站脚本，XSS的重点不在于跨站点，而是在于脚本的执行。XSS是一种经常出现在Web应用程序中的计算机安全漏洞，是由于Web应用程序对用户的输入过滤不足而产生的，它允许恶意web用户将代码植入到提供给其它用户使用的页面中。\n常见的避免 XSS 攻击的方式是过滤掉请求中存在 XSS 攻击风险的可疑字符串。\n在 Spring 项目中，我们一般是通过创建 XSS 过滤器来实现的。\n@Component @Order(Ordered.HIGHEST_PRECEDENCE) public class XSSFilter implements Filter { @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { XSSRequestWrapper wrappedRequest = new XSSRequestWrapper((HttpServletRequest) request); chain.doFilter(wrappedRequest, response); } // other methods } 适合移动端应用 # 使用 Session 进行身份认证的话，需要保存一份信息在服务器端，而且这种方式会依赖到 Cookie（需要 Cookie 保存 SessionId），所以不适合移动端。\n但是，使用 JWT 进行身份认证就不会存在这种问题，因为只要 JWT 可以被客户端存储就能够使用，而且 JWT 还可以跨语言使用。\n单点登录友好 # 使用 Session 进行身份认证的话，实现单点登录，需要我们把用户的 Session 信息保存在一台电脑上，并且还会遇到常见的 Cookie 跨域的问题。但是，使用 JWT 进行认证的话， JWT 被保存在客户端，不会存在这些问题。\nJWT 身份认证常见问题及解决办法 # 注销登录等场景下 JWT 还有效 # 与之类似的具体相关场景有：\n退出登录; 修改密码; 服务端修改了某个用户具有的权限或者角色； 用户的帐户被封禁/删除； 用户被服务端强制注销； 用户被踢下线； \u0026hellip;\u0026hellip; 这个问题不存在于 Session 认证方式中，因为在 Session 认证方式中，遇到这种情况的话服务端删除对应的 Session 记录即可。但是，使用 JWT 认证的方式就不好解决了。我们也说过了，JWT 一旦派发出去，如果后端不增加其他逻辑的话，它在失效之前都是有效的。\n那我们如何解决这个问题呢？查阅了很多资料，我简单总结了下面 4 种方案：\n1、将 JWT 存入内存数据库\n将 JWT 存入 DB 中，Redis 内存数据库在这里是不错的选择。如果需要让某个 JWT 失效就直接从 Redis 中删除这个 JWT 即可。但是，这样会导致每次使用 JWT 发送请求都要先从 DB 中查询 JWT 是否存在的步骤，而且违背了 JWT 的无状态原则。\n2、黑名单机制\n和上面的方式类似，使用内存数据库比如 Redis 维护一个黑名单，如果想让某个 JWT 失效的话就直接将这个 JWT 加入到 黑名单 即可。然后，每次使用 JWT 进行请求的话都会先判断这个 JWT 是否存在于黑名单中。\n前两种方案的核心在于将有效的 JWT 存储起来或者将指定的 JWT 拉入黑名单。\n虽然这两种方案都违背了 JWT 的无状态原则，但是一般实际项目中我们通常还是会使用这两种方案。\n3、修改密钥 (Secret) :\n我们为每个用户都创建一个专属密钥，如果我们想让某个 JWT 失效，我们直接修改对应用户的密钥即可。但是，这样相比于前两种引入内存数据库带来了危害更大：\n如果服务是分布式的，则每次发出新的 JWT 时都必须在多台机器同步密钥。为此，你需要将密钥存储在数据库或其他外部服务中，这样和 Session 认证就没太大区别了。 如果用户同时在两个浏览器打开系统，或者在手机端也打开了系统，如果它从一个地方将账号退出，那么其他地方都要重新进行登录，这是不可取的。 4、保持令牌的有效期限短并经常轮换\n很简单的一种方式。但是，会导致用户登录状态不会被持久记录，而且需要用户经常登录。\n另外，对于修改密码后 JWT 还有效问题的解决还是比较容易的。说一种我觉得比较好的方式：使用用户的密码的哈希值对 JWT 进行签名。因此，如果密码更改，则任何先前的令牌将自动无法验证。\nJWT 的续签问题 # JWT 有效期一般都建议设置的不太长，那么 JWT 过期后如何认证，如何实现动态刷新 JWT，避免用户经常需要重新登录？\n我们先来看看在 Session 认证中一般的做法：假如 Session 的有效期 30 分钟，如果 30 分钟内用户有访问，就把 Session 有效期延长 30 分钟。\nJWT 认证的话，我们应该如何解决续签问题呢？查阅了很多资料，我简单总结了下面 4 种方案：\n1、类似于 Session 认证中的做法\n这种方案满足于大部分场景。假设服务端给的 JWT 有效期设置为 30 分钟，服务端每次进行校验时，如果发现 JWT 的有效期马上快过期了，服务端就重新生成 JWT 给客户端。客户端每次请求都检查新旧 JWT，如果不一致，则更新本地的 JWT。这种做法的问题是仅仅在快过期的时候请求才会更新 JWT ,对客户端不是很友好。\n2、每次请求都返回新 JWT\n这种方案的的思路很简单，但是，开销会比较大，尤其是在服务端要存储维护 JWT 的情况下。\n3、JWT 有效期设置到半夜\n这种方案是一种折衷的方案，保证了大部分用户白天可以正常登录，适用于对安全性要求不高的系统。\n4、用户登录返回两个 JWT\n第一个是 accessJWT ，它的过期时间 JWT 本身的过期时间比如半个小时，另外一个是 refreshJWT 它的过期时间更长一点比如为 1 天。客户端登录后，将 accessJWT 和 refreshJWT 保存在本地，每次访问将 accessJWT 传给服务端。服务端校验 accessJWT 的有效性，如果过期的话，就将 refreshJWT 传给服务端。如果有效，服务端就生成新的 accessJWT 给客户端。否则，客户端就重新登录即可。\n这种方案的不足是：\n需要客户端来配合；\n用户注销的时候需要同时保证两个 JWT 都无效；\n重新请求获取 JWT 的过程中会有短暂 JWT 不可用的情况（可以通过在客户端设置定时器，当 accessJWT 快过期的时候，提前去通过 refreshJWT 获取新的 accessJWT）;\n这里说的短暂，就是accessJWT失效而refreshJWT成功的那种情况\n存在安全问题，只要拿到了未过期的 refreshJWT 就一直可以获取到 accessJWT。\n总结 # JWT 其中一个很重要的优势是无状态，但实际上，我们想要在实际项目中合理使用 JWT 的话，也还是需要保存 JWT 信息。\nJWT 也不是银弹，也有很多缺陷，具体是选择 JWT 还是 Session 方案还是要看项目的具体需求。万万不可尬吹 JWT，而看不起其他身份认证方案。\n另外，不用 JWT 直接使用普通的 Token(随机生成，不包含具体的信息) 结合 Redis 来做身份认证也是可以的。我在 「优质开源项目推荐」的第 8 期推荐过的 Sa-Token 这个项目是一个比较完善的 基于 JWT 的身份认证解决方案，支持自动续签、踢人下线、账号封禁、同端互斥登录等功能，感兴趣的朋友可以看看。\n参考 # JWT 超详细分析：https://learnku.com/articles/17883 How to log out when using JWT：https://medium.com/devgorilla/how-to-log-out-when-using-jwt-a8c7823e8a6 CSRF protection with JSON Web JWTs：https://medium.com/@agungsantoso/csrf-protection-with-json-web-JWTs-83e0f2fcbcc Invalidating JSON Web JWTs：https://stackoverflow.com/questions/21978658/invalidating-json-web-JWTs "},{"id":71,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/security/ly02ly_jwt-intro/","title":"jwt-intro","section":"安全","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n什么是 JWT? # JWT （JSON Web Token） 是目前最流行的跨域认证解决方案，是一种基于 Token 的认证授权机制。 从 JWT 的全称可以看出，JWT 本身也是 Token，一种规范化之后的 JSON 结构的 Token。\n跨域认证的问题\n互联网服务离不开用户认证。一般流程是下面这样。\n这种模式的问题在于，扩展性（scaling）不好。单机当然没有问题，如果是服务器集群，或者是跨域的服务导向架构，就要求 session 数据共享，每台服务器都能够读取 session。\n举例来说，A 网站和 B 网站是同一家公司的关联服务。现在要求，用户只要在其中一个网站登录，再访问另一个网站就会自动登录，请问怎么实现？\n一种解决方案是 session 数据持久化，写入数据库或别的持久层。各种服务收到请求后，都向持久层请求数据。这种方案的优点是架构清晰，缺点是工程量比较大。另外，持久层万一挂了，就会单点失败。\n另一种方案是服务器索性不保存 session 数据了，所有数据都保存在客户端，每次请求都发回服务器。JWT 就是这种方案的一个代表。\nJWT 自身包含了身份验证所需要的所有信息，因此，我们的服务器不需要存储 Session 信息。这显然增加了系统的可用性和伸缩性，大大减轻了服务端的压力。\nly：我觉得这里的重点就是，服务器不存储Session以维护\u0026quot;用户\u0026quot;和cookie(session id)的关系了\n可以看出，JWT 更符合设计 RESTful API 时的「Stateless（无状态）」原则 。\n并且， 使用 JWT 认证可以有效避免 CSRF 攻击，因为 JWT 一般是存在在 localStorage 中，使用 JWT 进行身份验证的过程中是不会涉及到 Cookie 的。\n我在 JWT 优缺点分析这篇文章中有详细介绍到使用 JWT 做身份认证的优势和劣势。\n下面是 RFC 7519 对 JWT 做的较为正式的定义。\nJSON Web Token (JWT) is a compact, URL-safe means of representing claims to be transferred between two parties. The claims in a JWT are encoded as a JSON object that is used as the payload of a JSON Web Signature (JWS) structure or as the plaintext of a JSON Web Encryption (JWE) structure, enabling the claims to be digitally signed or integrity protected with a Message Authentication Code (MAC) and/or encrypted. ——JSON Web Token (JWT)\nJWT 由哪些部分组成？ # JWT 本质上就是一组字串，通过（.）切分成三个为 Base64 编码的部分：\nHeader : 描述 JWT 的元数据，定义了生成签名的算法以及 Token 的类型。 Payload : 用来存放实际需要传递的数据 Signature（签名） ：服务器通过 Payload、Header 和**一个密钥(Secret)**使用 Header 里面指定的签名算法（默认是 HMAC SHA256）生成。 JWT 通常是这样的：xxxxx.yyyyy.zzzzz。\n示例：\neyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9. eyJzdWIiOiIxMjM0NTY3ODkwIiwibmFtZSI6IkpvaG4gRG9lIiwiaWF0IjoxNTE2MjM5MDIyfQ. SflKxwRJSMeKKF2QT4fwpMeJf36POk6yJV_adQssw5c 你可以在 jwt.io 这个网站上对其 JWT 进行解码，解码之后得到的就是 Header、Payload、Signature 这三部分。\nHeader 和 Payload 都是 JSON 格式的数据，Signature 由 Payload、Header 和 **Secret(密钥)**通过特定的计算公式和加密算法得到。\nHeader # Header 通常由两部分组成：\ntyp（Type）：令牌类型，也就是 JWT。 alg（Algorithm） ：签名算法，比如 HS256。 示例：\n{ \u0026#34;alg\u0026#34;: \u0026#34;HS256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; } JSON 形式的 Header 被转换成 Base64 编码，成为 JWT 的第一部分。\nPayload # Payload 也是 JSON 格式数据，其中包含了 Claims(声明，包含 JWT 的相关信息)。\nClaims 分为三种类型：\nRegistered Claims（注册声明） ：预定义的一些声明，建议使用，但不是强制性的。 Public Claims（公有声明） ：JWT 签发方可以自定义的声明，但是为了避免冲突，应该在 IANA JSON Web Token Registry 中定义它们。 Private Claims（私有声明） ：JWT 签发方因为项目需要而自定义的声明，更符合实际项目场景使用。 下面是一些常见的注册声明：\niss（issuer）：JWT 签发方。 iat（issued at time）：JWT 签发时间。 sub（subject）：JWT 主题。 aud（audience）：JWT 接收方。 exp（expiration time）：JWT 的过期时间。 nbf（not before time）：JWT 生效时间，早于该定义的时间的 JWT 不能被接受处理。 jti（JWT ID）：JWT 唯一标识。 示例：\n{ \u0026#34;uid\u0026#34;: \u0026#34;ff1212f5-d8d1-4496-bf41-d2dda73de19a\u0026#34;, \u0026#34;sub\u0026#34;: \u0026#34;1234567890\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;exp\u0026#34;: 15323232, \u0026#34;iat\u0026#34;: 1516239022, \u0026#34;scope\u0026#34;: [\u0026#34;admin\u0026#34;, \u0026#34;user\u0026#34;] } Payload 部分默认是不加密的，一定不要将隐私信息存放在 Payload 当中！！！\nJSON 形式的 Payload 被转换成 Base64 编码，成为 JWT 的第二部分。\nSignature # Signature 部分是对前两部分的签名，作用是防止 JWT（主要是 payload） 被篡改。\n这个签名的生成需要用到：\nHeader + Payload。 存放在服务端的密钥(一定不要泄露出去)。 签名算法。 签名的计算公式如下：\nHMACSHA256( base64UrlEncode(header) + \u0026#34;.\u0026#34; + base64UrlEncode(payload), secret) 算出签名以后，把 Header、Payload、Signature 三个部分拼成一个字符串，每个部分之间用\u0026quot;点\u0026quot;（.）分隔，这个字符串就是 JWT 。\n如何基于 JWT 进行身份验证？ # 在基于 JWT 进行身份验证的的应用程序中，服务器通过 Payload、Header 和 Secret(密钥)创建 JWT 并将 JWT 发送给客户端。客户端接收到 JWT 之后，会将其保存在 Cookie 或者 localStorage 里面，以后客户端发出的所有请求都会携带这个令牌。\n简化后的步骤如下：\n用户向服务器发送用户名、密码以及验证码用于登陆系统。\n如果用户用户名、密码以及验证码校验正确的话，服务端会返回已经签名的 Token，也就是 JWT。\n注意，很重要，★★ JWT是服务器生成的！\n用户以后每次向后端发请求都在 Header 中带上这个 JWT 。\n服务端检查 JWT 并从中获取用户相关信息。\n两点建议：\n建议将 JWT 存放在 localStorage 中，放在 Cookie 中会有 CSRF 风险。 请求服务端并携带 JWT 的常见做法是将其放在 HTTP Header 的 Authorization 字段中（Authorization: Bearer Token）。 spring-security-jwt-guide 就是一个基于 JWT 来做身份认证的简单案例，感兴趣的可以看看。\n如何防止 JWT 被篡改？ # 有了签名之后，即使 JWT 被泄露或者截获，黑客也没办法同时篡改 Signature 、Header 、Payload。\n这是为什么呢？因为服务端拿到 JWT 之后，会解析出其中包含的 Header、Payload 以及 Signature 。服务端会根据 Header、Payload、密钥再次生成一个 Signature。拿新生成的 Signature 和 JWT 中的 Signature 作对比，如果一样就说明 Header 和 Payload 没有被修改。\n不过，如果服务端的秘钥也被泄露的话，黑客就可以同时篡改 Signature 、Header 、Payload 了。黑客直接修改了 Header 和 Payload 之后，再重新生成一个 Signature 就可以了。\n密钥一定保管好，一定不要泄露出去。JWT 安全的核心在于签名，签名安全的核心在密钥。\n如何加强 JWT 的安全性？ # 使用安全系数高的加密算法。 使用成熟的开源库，没必要造轮子。 JWT 存放在 localStorage 中而不是 Cookie 中，避免 CSRF 风险。 一定不要将隐私信息存放在 Payload 当中。 密钥一定保管好，一定不要泄露出去。JWT 安全的核心在于签名，签名安全的核心在密钥。 Payload 要加入 exp （JWT 的过期时间），永久有效的 JWT 不合理。并且，JWT 的过期时间不易过长。 \u0026hellip;\u0026hellip; "},{"id":72,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/security/ly01ly_basis-of-authority-certification/","title":"认证授权基础概念详解","section":"安全","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n认证 (Authentication) 和授权 (Authorization)的区别是什么？ # 这是一个绝大多数人都会混淆的问题。首先先从读音上来认识这两个名词，很多人都会把它俩的读音搞混，所以我建议你先先去查一查这两个单词到底该怎么读，他们的具体含义是什么。\n说简单点就是：\n认证 (Authentication)： 你是谁。[ɔːˌθentɪˈkeɪʃn] 身份验证 授权 (Authorization)： 你有权限干什么。[ˌɔːθəraɪˈzeɪʃn] 授权 稍微正式点（啰嗦点）的说法就是 ：\nAuthentication（认证） 是验证您的身份的凭据（例如用户名/用户 ID 和密码），通过这个凭据，系统得以知道你就是你，也就是说系统存在你这个用户。所以，Authentication 被称为身份/用户验证。 Authorization（授权） 发生在 Authentication（认证） 之后。授权嘛，光看意思大家应该就明白，它主要掌管我们访问系统的权限。比如有些特定资源只能具有特定权限的人才能访问比如 admin，有些对系统资源操作比如删除、添加、更新只能特定人才具有。 认证 ：\n授权：\n这两个一般在我们的系统中被结合在一起使用，目的就是为了保护我们系统的安全性。\nRBAC 模型了解吗？ # 系统权限控制最常采用的访问控制模型就是 RBAC 模型 。\n什么是 RBAC 呢？\nRBAC 即基于角色的权限访问控制（Role-Based Access Control）。这是一种通过角色关联权限，角色同时又关联用户的授权的方式。\n简单地说：一个用户可以拥有若干角色，每一个角色又可以被分配若干权限，这样就构造成“用户-角色-权限” 的授权模型。在这种模型中，用户与角色、角色与权限之间构成了多对多的关系，如下图\n在 RBAC 中，权限与角色相关联，用户通过成为适当角色的成员而得到这些角色的权限。这就极大地简化了权限的管理。\n本系统的权限设计相关的表如下（一共 5 张表，2 张用户建立表之间的联系）：\n通过这个权限模型，我们可以创建不同的角色并为不同的角色分配不同的权限范围（菜单）。\n通常来说，如果系统对于权限控制要求比较严格的话，一般都会选择使用 RBAC 模型来做权限控制。\n什么是 Cookie ? Cookie 的作用是什么? # ly：如上，可以看出 cookie的附属是域名\nCookie 和 Session 都是用来跟踪浏览器用户身份的会话方式，但是两者的应用场景不太一样。\n维基百科是这样定义 Cookie 的：\nCookies 是某些网站为了辨别用户身份而储存在用户本地终端上的数据（通常经过加密）。\n简单来说： Cookie 存放在客户端，一般用来保存用户信息。\n下面是 Cookie 的一些应用案例：\n我们在 Cookie 中保存已经登录过的用户信息，下次访问网站的时候页面可以自动帮你登录的一些基本信息给填了。除此之外，Cookie 还能保存用户首选项，主题和其他设置信息。\n使用 Cookie 保存 SessionId 或者 Token ，向后端发送请求的时候带上 Cookie，这样后端就能取到 Session 或者 Token 了。这样就能记录用户当前的状态了，因为 HTTP 协议是无状态的。\n无状态的意思就是通过http发的多次请求，没有特殊处理的话我们是不知道是否是同一个用户发的，也就是没有用户状态。\nCookie 还可以用来记录和分析用户行为。举个简单的例子你在网上购物的时候，因为 HTTP 协议是没有状态的，如果服务器想要获取你在某个页面的停留状态或者看了哪些商品，一种常用的实现方式就是将这些信息存放在 Cookie\n\u0026hellip;\u0026hellip;\n如何在项目中使用 Cookie 呢？ # 我这里以 Spring Boot 项目为例。\n1)设置 Cookie 返回给客户端\n@GetMapping(\u0026#34;/change-username\u0026#34;) public String setCookie(HttpServletResponse response) { // 创建一个 cookie Cookie cookie = new Cookie(\u0026#34;username\u0026#34;, \u0026#34;Jovan\u0026#34;); //设置 cookie过期时间 cookie.setMaxAge(7 * 24 * 60 * 60); // expires in 7 days //添加到 response 中 response.addCookie(cookie); return \u0026#34;Username is changed!\u0026#34;; } 2) 使用 Spring 框架提供的 @CookieValue 注解获取特定的 cookie 的值\n@GetMapping(\u0026#34;/\u0026#34;) public String readCookie(@CookieValue(value = \u0026#34;username\u0026#34;, defaultValue = \u0026#34;Atta\u0026#34;) String username) { return \u0026#34;Hey! My username is \u0026#34; + username; } 3) 读取所有的 Cookie 值\n@GetMapping(\u0026#34;/all-cookies\u0026#34;) public String readAllCookies(HttpServletRequest request) { Cookie[] cookies = request.getCookies(); if (cookies != null) { return Arrays.stream(cookies) .map(c -\u0026gt; c.getName() + \u0026#34;=\u0026#34; + c.getValue()).collect(Collectors.joining(\u0026#34;, \u0026#34;)); } return \u0026#34;No cookies\u0026#34;; } 更多关于如何在 Spring Boot 中使用 Cookie 的内容可以查看这篇文章：How to use cookies in Spring Boot 。\nCookie 和 Session 有什么区别？ # Session 的主要作用就是通过服务端记录用户的状态。 典型的场景是购物车，当你要添加商品到购物车的时候，★★重要：系统不知道是哪个用户操作的，因为 HTTP 协议是无状态的。服务端给特定的用户创建特定的 Session 之后就可以标识这个用户并且跟踪这个用户了。\nCookie 数据保存在客户端(浏览器端)，Session 数据保存在服务器端。相对来说 Session 安全性更高。如果使用 Cookie 的一些敏感信息不要写入 Cookie 中，最好能将 Cookie 信息加密然后使用到的时候再去服务器端解密。\n那么，如何使用 Session 进行身份验证？\n如何使用 Session-Cookie 方案进行身份验证？ # 很多时候我们都是通过 SessionID 来实现特定的用户，SessionID 一般会选择存放在 Redis 中。举个例子：\n用户成功登陆系统，然后返回给客户端具有 SessionID 的 Cookie 。 当用户向后端发起请求的时候会把 SessionID 带上，这样后端就知道你的身份状态了。 关于这种认证方式更详细的过程如下：\n用户向服务器发送用户名、密码、验证码用于登陆系统。 服务器验证通过后，服务器为用户创建一个 Session，并将 Session 信息存储起来（一般是Redis）。 服务器向用户返回一个 SessionID，写入用户的 Cookie。 当用户保持登录状态时，Cookie 将与每个后续请求一起被发送出去。 服务器可以将存储在 Cookie 上的 SessionID 与存储在内存中或者数据库中的 Session 信息进行比较，以验证用户的身份，返回给用户客户端响应信息的时候会附带用户当前的状态。 使用 Session 的时候需要注意下面几个点：\n依赖 Session 的关键业务一定要确保客户端开启了 Cookie。 注意 Session 的过期时间。 另外，Spring Session 提供了一种跨多个应用程序或实例管理用户会话信息的机制。如果想详细了解可以查看下面几篇很不错的文章：\nGetting Started with Spring Session Guide to Spring Session Sticky Sessions with Spring Session \u0026amp; Redis 多服务器节点下 Session-Cookie 方案如何做？ # Session-Cookie 方案在单体环境是一个非常好的身份认证方案。但是，当服务器水平拓展成多节点时，Session-Cookie 方案就要面临挑战了。\n举个例子：假如我们部署了两份相同的服务 A，B，用户第一次登陆的时候 ，Nginx 通过负载均衡机制将用户请求转发到 A 服务器，此时用户的 Session 信息保存在 A 服务器。结果，用户第二次访问的时候 Nginx 将请求路由到 B 服务器，由于 B 服务器没有保存 用户的 Session 信息，导致用户需要重新进行登陆。\n我们应该如何避免上面这种情况的出现呢？\n有几个方案可供大家参考：\n某个用户的所有请求都通过特性的哈希策略分配给同一个服务器处理。这样的话，每个服务器都保存了一部分用户的 Session 信息。服务器宕机，其保存的所有 Session 信息就完全丢失了。 每一个服务器保存的 Session 信息都是互相同步的，也就是说每一个服务器都保存了全量的 Session 信息。每当一个服务器的 Session 信息发生变化，我们就将其同步到其他服务器。这种方案成本太大，并且，节点越多时，同步成本也越高。 单独使用一个所有服务器都能访问到的数据节点（比如缓存）来存放 Session 信息。为了保证高可用，数据节点尽量要避免是单点。 如果没有 Cookie 的话 Session 还能用吗？ # 这是一道经典的面试题！\n一般是通过 Cookie 来保存 SessionID ，假如你使用了 Cookie 保存 SessionID 的方案的话， 如果客户端禁用了 Cookie，那么 Session 就无法正常工作。\n但是，并不是没有 Cookie 之后就不能用 Session 了，比如你可以将 SessionID 放在请求的 url 里面https://javaguide.cn/?Session_id=xxx 。这种方案的话可行，但是安全性和用户体验感降低。当然，为了你也可以对 SessionID 进行一次加密之后再传入后端。\n个人觉得localstorage也是可以的\n为什么 Cookie 无法防止 CSRF 攻击，而 Token 可以？ # CSRF(Cross Site Request Forgery) 一般被翻译为 跨站请求伪造 。那么什么是 跨站请求伪造 呢？说简单用你的身份去发送一些对你不友好的请求。举个简单的例子：\n小壮登录了某网上银行，他来到了网上银行的帖子区，看到一个帖子下面有一个链接写着“科学理财，年盈利率过万”，小壮好奇的点开了这个链接，结果发现自己的账户少了 10000 元。这是这么回事呢？原来黑客在链接中藏了一个请求，这个请求直接利用小壮的身份给银行发送了一个转账请求,也就是通过你的 Cookie 向银行发出请求。\n\u0026lt;a src=http://www.mybank.com/Transfer?bankId=11\u0026amp;money=10000\u0026gt;科学理财，年盈利率过万\u0026lt;/\u0026gt; 上面也提到过，进行 Session 认证的时候，我们一般使用 Cookie 来存储 SessionId,当我们登陆后后端生成一个 SessionId 放在 Cookie 中返回给客户端，服务端通过 Redis 或者其他存储工具记录保存着这个 SessionId，客户端登录以后每次请求都会带上这个 SessionId，服务端通过这个 SessionId 来标示你这个人。如果别人通过 Cookie 拿到了 SessionId 后就可以代替你的身份访问系统了。\nSession 认证中 Cookie 中的 SessionId 是由浏览器发送到服务端的，借助这个特性，攻击者就可以通过让用户误点攻击链接，达到攻击效果。\n但是，我们使用 Token 的话就不会存在这个问题，在我们登录成功获得 Token 之后，一般会选择存放在 localStorage （浏览器本地存储）中。然后我们在前端通过某些方式会给每个发到后端的请求加上这个 Token,这样就不会出现 CSRF 漏洞的问题。因为，即使有个你点击了非法链接发送了请求到服务端，这个非法请求是不会携带 Token 的，所以这个请求将是非法的。\n需要注意的是：不论是 Cookie 还是 Token 都无法避免 跨站脚本攻击（Cross Site Scripting）XSS 。\n跨站脚本攻击（Cross Site Scripting）缩写为 CSS 但这会与层叠样式表（Cascading Style Sheets，CSS）的缩写混淆。因此，有人将跨站脚本攻击缩写为 XSS。\nXSS 中攻击者会用各种方式将恶意代码注入到其他用户的页面中。就可以通过脚本盗用信息比如 Cookie 。\n推荐阅读：如何防止 CSRF 攻击？—美团技术团队\n什么是 JWT?JWT 由哪些部分组成？ # JWT 基础概念详解\n如何基于 JWT 进行身份验证？ 如何防止 JWT 被篡改？ # JWT 基础概念详解\n什么是 SSO? # SSO(Single Sign On)即单点登录说的是用户登陆多个子系统的其中一个就有权访问与其相关的其他系统。举个例子我们在登陆了京东金融之后，我们同时也成功登陆京东的京东超市、京东国际、京东生鲜等子系统。\nSSO 有什么好处？ # 用户角度 :用户能够做到一次登录多次使用，无需记录多套用户名和密码，省心。 系统管理员角度 : 管理员只需维护好一个统一的账号中心就可以了，方便。 新系统开发角度: 新系统开发时只需直接对接统一的账号中心即可，简化开发流程，省时。 如何设计实现一个 SSO 系统? # SSO 单点登录详解\n什么是 OAuth 2.0？ # OAuth 是一个行业的标准授权协议，主要用来授权第三方应用获取有限的权限。而 OAuth 2.0 是对 OAuth 1.0 的完全重新设计，OAuth 2.0 更快，更容易实现，OAuth 1.0 已经被废弃。详情请见：rfc6749。\n实际上它就是一种授权机制，它的最终目的是为第三方应用颁发一个有时效性的令牌 Token，使得第三方应用能够通过该令牌获取相关的资源。\nOAuth 2.0 比较常用的场景就是第三方登录，当你的网站接入了第三方登录的时候一般就是使用的 OAuth 2.0 协议。\n另外，现在 OAuth 2.0 也常见于支付场景（微信支付、支付宝支付）和开发平台（微信开放平台、阿里开放平台等等）。\n下图是 Slack OAuth 2.0 第三方登录的示意图：\n推荐阅读：\nOAuth 2.0 的一个简单解释 10 分钟理解什么是 OAuth 2.0 协议 OAuth 2.0 的四种方式 GitHub OAuth 第三方登录示例教程 参考 # 不要用 JWT 替代 session 管理（上）：全面了解 Token,JWT,OAuth,SAML,SSO：https://zhuanlan.zhihu.com/p/38942172 Introduction to JSON Web Tokens：https://jwt.io/introduction JSON Web Token Claims：https://auth0.com/docs/secure/tokens/json-web-tokens/json-web-token-claims "},{"id":73,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/basis/unit-test/","title":"单元测试","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n何谓单元测试？ # 维基百科是这样介绍单元测试的：\n在计算机编程中，单元测试（Unit Testing）是针对程序模块（软件设计的最小单位）进行的正确性检验测试工作。\n程序单元是应用的 最小可测试部件 。在过程化编程中，一个单元就是单个程序、函数、过程等；对于面向对象编程，最小单元就是方法，包括基类（超类）、抽象类、或者派生类（子类）中的方法。\n由于每个单元有独立的逻辑，在做单元测试时，为了隔离外部依赖，确保这些依赖不影响验证逻辑，我们经常会用到 Fake、Stub 与 Mock 。\n关于 Fake、Mock 与 Stub 这几个概念的解读，可以看看这篇文章：测试中 Fakes、Mocks 以及 Stubs 概念明晰 - 王下邀月熊 - 2018 。\n为什么需要单元测试？ # 为重构保驾护航 # 我在重构这篇文章中这样写到：\n单元测试可以为重构提供信心，降低重构的成本。我们要像重视生产代码那样，重视单元测试。\n每个开发者都会经历重构，重构后把代码改坏了的情况并不少见，很可能你只是修改了一个很简单的方法就导致系统出现了一个比较严重的错误。\n如果有了单元测试的话，就不会存在这个隐患了。写完一个类，把单元测试写了，确保这个类逻辑正确；写第二个类，单元测试\u0026hellip;..写 100 个类，道理一样，每个类做到第一点“保证逻辑正确性”，100 个类拼在一起肯定不出问题。你大可以放心一边重构，一边运行 APP；而不是整体重构完，提心吊胆地 run。\n提高代码质量 # 由于每个单元有独立的逻辑，做单元测试时需要隔离外部依赖，确保这些依赖不影响验证逻辑。因为要把各种依赖分离，单元测试会促进工程进行组件拆分，整理工程依赖关系，更大程度减少代码耦合。这样写出来的代码，更好维护，更好扩展，从而提高代码质量。\n减少 bug # 一个机器，由各种细小的零件组成，如果其中某件零件坏了，机器运行故障。必须保证每个零件都按设计图要求的规格，机器才能正常运行。\n一个可单元测试的工程，会把业务、功能分割成规模更小、有独立的逻辑部件，称为单元。单元测试的目标，就是保证各个单元的逻辑正确性。单元测试保障工程各个“零件”按“规格”（需求）执行，从而保证整个“机器”（项目）运行正确，最大限度减少 bug。\n快速定位 bug # 如果程序有 bug，我们运行一次全部单元测试，找到不通过的测试，可以很快地定位对应的执行代码。修复代码后，运行对应的单元测试；如还不通过，继续修改，运行测试\u0026hellip;..直到测试通过。\n持续集成依赖单元测试 # 持续集成需要依赖单元测试，当持续集成服务自动构建新代码之后，会自动运行单元测试来发现代码错误。\n谁逼你写单元测试？ # 领导要求 # 有些经验丰富的领导，或多或少都会要求团队写单元测试。对于有一定工作经验的队友，这要求挺合理；对于经验尚浅的、毕业生，恐怕要死要活了，连代码都写不好，还要写单元测试，are you kidding me？\n培训新人单元测试用法，是一项艰巨的任务。新人代码风格未形成，也不知道单元测试多重要，强制单元测试会让他们感到困惑，没办法按自己思路写代码。\n大牛都写单元测试 # 国外很多家喻户晓的开源项目，都有大量单元测试。例如，retrofit、okhttp、butterknife\u0026hellip;. 国外大牛都写单元测试，我们也写吧！\n很多读者都有这种想法，一开始满腔热血。当真要对自己项目单元测试时，便困难重重，很大原因是项目对单元测试不友好。最后只能对一些不痛不痒的工具类做单元测试，久而久之，当初美好愿望也不了了之。\n保住面子 # 都是有些许年经验的老鸟，还天天被测试同学追 bug，好意思么？花多一点时间写单元测试，确保没低级 bug，还能彰显大牛风范，何乐而不为？\n心虚 # 笔者也是个不太相信自己代码的人，总觉得哪里会突然冒出莫名其妙的 bug，也怕别人不小心改了自己的代码（被害妄想症），新版本上线提心吊胆\u0026hellip;\u0026hellip;花点时间写单元测试，有事没事跑一下测试，确保原逻辑没问题，至少能睡安稳一点。\nTDD 测试驱动开发 # 何谓 TDD？ # TDD 即 Test-Driven Development（ 测试驱动开发），这是敏捷开发的一项核心实践和技术，也是一种设计方法论。\nTDD 原理是开发功能代码之前，先编写测试用例代码，然后针对测试用例编写功能代码，使其能够通过。\nTDD 的节奏：“红 - 绿 - 重构”。\n由于 TDD 对开发人员要求非常高，跟传统开发思维不一样，因此实施起来相当困难。\nTDD 在很多人眼中是不实用的，一来他们并不理解测试“驱动”开发的含义，但更重要的是，他们很少会做任务分解。而任务分解是做好 TDD 的关键点。只有把任务分解到可以测试的地步，才能够有针对性地写测试。\nTDD 优缺点分析 # 测试驱动开发有好处也有坏处。因为每个测试用例都是根据需求来的，或者说把一个大需求分解成若干小需求编写测试用例，所以测试用例写出来后，开发者写的执行代码，必须满足测试用例。如果测试不通过，则修改执行代码，直到测试用例通过。\n优点 ：\n帮你整理需求，梳理思路； 帮你设计出更合理的接口（空想的话很容易设计出屎）； 减小代码出现 bug 的概率； 提高开发效率（前提是正确且熟练使用 TDD）。 缺点 ：\n能用好 TDD 的人非常少，看似简单，实则门槛很高； 投入开发资源（时间和精力）通常会更多； 由于测试用例在未进行代码设计前写；很有可能限制开发者对代码整体设计； 可能引起开发人员不满情绪，我觉得这点很严重，毕竟不是人人都喜欢单元测试，尽管单元测试会带给我们相当多的好处。 相关阅读：如何用正确的姿势打开 TDD？ - 陈天 - 2017 。\n总结 # 单元测试确实会带给你相当多的好处，但不是立刻体验出来。正如买重疾保险，交了很多保费，没病没痛，十几年甚至几十年都用不上，最好就是一辈子用不上理赔，身体健康最重要。单元测试也一样，写了可以买个放心，对代码的一种保障，有 bug 尽快测出来，没 bug 就最好，总不能说“写那么多单元测试，结果测不出 bug，浪费时间”吧？\n以下是个人对单元测试一些建议：\n越重要的代码，越要写单元测试； 代码做不到单元测试，多思考如何改进，而不是放弃； 边写业务代码，边写单元测试，而不是完成整个新功能后再写； 多思考如何改进、简化测试代码。 测试代码需要随着生产代码的演进而重构或者修改，如果测试不能保持整洁，只会越来越难修改。 作为一名经验丰富的程序员，写单元测试更多的是对自己的代码负责。有测试用例的代码，别人更容易看懂，以后别人接手你的代码时，也可能放心做改动。\n多敲代码实践，多跟有单元测试经验的工程师交流，你会发现写单元测试获得的收益会更多。\n"},{"id":74,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/basis/refactoring/","title":"代码重构指南","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n前段时间重读了《重构：改善代码既有设计》，收货颇多。于是，简单写了一篇文章来聊聊我对重构的看法。\n何谓重构？ # 学习重构必看的一本神书《重构：改善代码既有设计》从两个角度给出了重构的定义：\n重构（名词）：对软件内部结构的一种调整，目的是在不改变软件可观察行为的前提下，提高其可理解性，降低其修改成本。 重构（动词）：使用一系列重构手法，在不改变软件可观察行为的前提下，调整其结构。 用更贴近工程师的语言来说： 重构就是利用设计模式(如组合模式、策略模式、责任链模式)、软件设计原则（如 SOLID 原则、YAGNI 原则、KISS 原则）和重构手段（如封装、继承、构建测试体系）来让代码更容易理解，更易于修改。\n软件设计原则指导着我们组织和规范代码，同时，重构也是为了能够尽量设计出尽量满足软件设计原则的软件。\n正确重构的核心在于 步子一定要小，每一步的重构都不会影响软件的正常运行，可以随时停止重构。\n常见的设计模式如下 ：\n更全面的设计模式总结，可以看 java-design-patterns 这个开源项目。\n常见的软件设计原则如下 ：\n更全面的设计原则总结，可以看 java-design-patterns 和 hacker-laws-zh 这两个开源项目。\n为什么要重构？ # 在上面介绍重构定义的时候，我从比较抽象的角度介绍了重构的好处：重构的主要目的主要是提升代码\u0026amp;架构的灵活性/可扩展性以及复用性。\n如果对应到一个真实的项目，重构具体能为我们带来什么好处呢？\n让代码更容易理解 ： 通过添加注释、命名规范、逻辑优化等手段可以让我们的代码更容易被理解； 避免代码腐化 ：通过重构干掉坏味道代码； 加深对代码的理解 ：重构代码的过程会加深你对某部分代码的理解； 发现潜在 bug ：是这样的，很多潜在的 bug ，都是我们在重构的过程中发现的； \u0026hellip;\u0026hellip; 看了上面介绍的关于重构带来的好处之后，你会发现重构的最终目标是 提高软件开发速度和质量 。\n重构并不会减慢软件开发速度，相反，如果代码质量和软件设计较差，当我们想要添加新功能的话，开发速度会越来越慢。到了最后，甚至都有想要重写整个系统的冲动。\n[\n《重构：改善代码既有设计》这本书中这样说：\n重构的唯一目的就是让我们开发更快，用更少的工作量创造更大的价值。\n何时进行重构？ # 重构在是开发过程中随时可以进行的，见机行事即可，并不需要单独分配一两天的时间专门用来重构。\n提交代码之前 # 《重构：改善代码既有设计》这本书介绍了一个 营地法则 的概念:\n编程时，需要遵循营地法则：保证你离开时的代码库一定比来时更健康。\n这个概念表达的核心思想其实很简单：在你提交代码的之前，花一会时间想一想，我这次的提交是让项目代码变得更健康了，还是更腐化了，或者说没什么变化？\n项目团队的每一个人只有保证自己的提交没有让项目代码变得更腐化，项目代码才会朝着健康的方向发展。\n当我们离开营地（项目代码）的时候，请不要留下垃圾（代码坏味道）！尽量确保营地变得更干净了！\n开发一个新功能之后\u0026amp;之前 # 在开发一个新功能之后，我们应该回过头看看是不是有可以改进的地方。在添加一个新功能之前，我们可以思考一下自己是否可以重构代码以让新功能的开发更容易。\n一个新功能的开发不应该仅仅只有功能验证通过那么简单，我们还应该尽量保证代码质量。\n有一个两顶帽子的比喻：在我开发新功能之前，我发现重构可以让新功能的开发更容易，于是我戴上了重构的帽子。重构之后，我换回原来的帽子，继续开发新能功能。新功能开发完成之后，我又发现自己的代码难以理解，于是我又戴上了重构帽子。比较好的开发状态就是就是这样在重构和开发新功能之间来回切换。\nCode Review 之后 # Code Review 可以非常有效提高代码的整体质量，它会帮助我们发现代码中的坏味道以及可能存在问题的地方。并且， Code Review 可以帮助项目团队其他程序员理解你负责的业务模块，有效避免人员方面的单点风险。\n经历一次 Code Review ，你的代码可能会收到很多改进建议。\n捡垃圾式重构 # 当我们发现坏味道代码（垃圾）的时候，如果我们不想停下手头自己正在做的工作，但又不想放着垃圾不管，我们可以这样做：\n如果这个垃圾很容易重构的话，我们可以立即重构它。 如果这个垃圾不太容易重构的话，我们可以先记录下来，当****重构它。 阅读理解代码的时候 # 搞开发的小伙伴应该非常有体会：我们经常需要阅读项目团队中其他人写的代码，也经常需要阅读自己过去写的代码。阅读代码的时候，通常要比我们写代码的时间还要多很多。\n我们在阅读理解代码的时候，如果发现一些坏味道的话，我们就可以对其进行重构。\n就比如说你在阅读张三写的某段代码的时候，你发现这段代码逻辑过于复杂难以理解，你有更好的写法，那你就可以对张三的这段代码逻辑进行重构。\n重构有哪些注意事项？ # 单元测试是重构的保护网 # 单元测试可以为重构提供信心，降低重构的成本。我们要像重视生产代码那样，重视单元测试。\n另外，多提一句：持续集成也要依赖单元测试，当持续集成服务自动构建新代码之后，会自动运行单元测试来发现代码错误。\n怎样才能算单元测试呢？ 网上的定义很多，很抽象，很容易把人给看迷糊了。我觉得对于单元测试的定义主要取决于你的项目，一个函数甚至是一个类都可以看作是一个单元。就比如说我们写了一个计算个人股票收益率的方法，我们为了验证它的正确性专门为它写了一个单元测试。再比如说我们代码有一个类专门负责数据脱敏，我们为了验证脱敏是否符合预期专门为这个类写了一个单元测试。\n单元测试也是需要重构或者修改的。 《代码整洁之道:敏捷软件开发手册》这本书这样写到：\n测试代码需要随着生产代码的演进而修改，如果测试不能保持整洁，只会越来越难修改。\n不要为了重构而重构 # 重构一定是要为项目带来价值的！ 某些情况下我们不应该进行重构：\n学习了某个设计模式/工程实践之后，不顾项目实际情况，刻意使用在项目上（避免货物崇拜编程）； 项目进展比较急的时候，重构项目调用的某个 API 的底层代码（重构之后对项目调用这个 API 并没有带来什么价值）； 重写比重构更容易更省事； \u0026hellip;\u0026hellip; 遵循方法 # 《重构：改善代码既有设计》这本书中列举除了代码常见的一些坏味道（比如重复代码、过长函数）和重构手段（如提炼函数、提炼变量、提炼类）。我们应该花时间去学习这些重构相关的理论知识，并在代码中去实践这些重构理论。\n如何练习重构？ # 除了可以在重构项目代码的过程中练习精进重构之外，你还可以有下面这些手段：\n重构实战练习 ：通过几个小案例一步一步带你学习重构！ 设计模式+重构学习网站 ：免费在线学习代码重构、 设计模式、 SOLID 原则 （单一职责、 开闭原则、 里氏替换、 接口隔离以及依赖反转） 。 IDEA 官方文档的代码重构教程 ： 教你如何使用 IDEA 进行重构。 参考 # 再读《重构》- ThoughtWorks 洞见 - 2020 ：详细介绍了重构的要点比如小步重构、捡垃圾式的重构，主要是重构概念相关的介绍。 常见代码重构技巧 - VectorJin - 2021 ：从软件设计原则、设计模式、代码分层、命名规范等角度介绍了如何进行重构，比较偏实战。 "},{"id":75,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/basis/naming/","title":"代码命名指南","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n我还记得我刚工作那一段时间， 项目 Code Review 的时候，我经常因为变量命名不规范而被 “diss”!\n究其原因还是自己那会经验不足，而且，大学那会写项目的时候不太注意这些问题，想着只要把功能实现出来就行了。\n但是，工作中就不一样，为了代码的可读性、可维护性，项目组对于代码质量的要求还是很高的！\n前段时间，项目组新来的一个实习生也经常在 Code Review 因为变量命名不规范而被 “diss”，这让我想到自己刚到公司写代码那会的日子。\n于是，我就简单写了这篇关于变量命名规范的文章，希望能对同样有此困扰的小伙伴提供一些帮助。\n确实，编程过程中，有太多太多让我们头疼的事情了，比如命名、维护其他人的代码、写测试、与其他人沟通交流等等。\n据说之前在 Quora 网站，由接近 5000 名程序员票选出来的最难的事情就是“命名”。\n大名鼎鼎的《重构》的作者老马（Martin Fowler）曾经在TwoHardThings这篇文章中提到过CS 领域有两大最难的事情：一是 缓存失效 ，一是 程序命名 。\n这个句话实际上也是老马引用别人的，类似的表达还有很多。比如分布式系统领域有两大最难的事情：一是 保证消息顺序 ，一是 严格一次传递 。\n今天咱们就单独拎出 “命名” 来聊聊！\n这篇文章配合我之前发的 《编码 5 分钟，命名 2 小时？史上最全的 Java 命名规范参考！》 这篇文章阅读效果更佳哦！\n为什么需要重视命名？ # 咱们需要先搞懂为什么要重视编程中的命名这一行为，它对于我们的编码工作有着什么意义。\n为什么命名很重要呢？ 这是因为 好的命名即是注释，别人一看到你的命名就知道你的变量、方法或者类是做什么的！\n简单来说就是 别人根据你的命名就能知道你的代码要表达的意思 （不过，前提这个人也要有基本的英语知识，对于一些编程中常见的单词比较熟悉）。\n简单举个例子说明一下命名的重要性。\n《Clean Code》这本书明确指出：\n好的代码本身就是注释，我们要尽量规范和美化自己的代码来减少不必要的注释。\n若编程语言足够有表达力，就不需要注释，尽量通过代码来阐述。\n举个例子：\n去掉下面复杂的注释，只需要创建一个与注释所言同一事物的函数即可\n// check to see if the employee is eligible for full benefits if ((employee.flags \u0026amp; HOURLY_FLAG) \u0026amp;\u0026amp; (employee.age \u0026gt; 65)) 应替换为\nif (employee.isEligibleForFullBenefits()) 常见命名规则以及适用场景 # 这里只介绍 3 种最常见的命名规范。\n驼峰命名法（CamelCase） # 驼峰命名法应该我们最常见的一个，这种命名方式使用大小写混合的格式来区别各个单词，并且单词之间不使用空格隔开或者连接字符连接的命名方式\n大驼峰命名法（UpperCamelCase） # 类名需要使用大驼峰命名法（UpperCamelCase）\n正例：\nServiceDiscovery、ServiceInstance、LruCacheFactory 反例：\nserviceDiscovery、Serviceinstance、LRUCacheFactory 小驼峰命名法（lowerCamelCase） # 方法名、参数名、成员变量、局部变量需要使用小驼峰命名法（lowerCamelCase）。\n正例：\ngetUserInfo() createCustomThreadPool() setNameFormat(String nameFormat) Uservice userService; 反例：\nGetUserInfo()、CreateCustomThreadPool()、setNameFormat(String NameFormat) Uservice user_service 蛇形命名法（snake_case） # 测试方法名、常量、枚举名称需要使用蛇形命名法（snake_case）\n在蛇形命名法中，各个单词之间通过**下划线“_”**连接，比如should_get_200_status_code_when_request_is_valid、CLIENT_CONNECT_SERVER_FAILURE。\n蛇形命名法的优势是命名所需要的单词比较多的时候，比如我把上面的命名通过小驼峰命名法给大家看一下：“shouldGet200StatusCodeWhenRequestIsValid”。\n感觉如何？ 相比于使用蛇形命名法（snake_case）来说是不是不那么易读？\n正例：\n@Test void should_get_200_status_code_when_request_is_valid() { ...... } 反例：\n@Test void shouldGet200StatusCodeWhenRequestIsValid() { ...... } 串式命名法（kebab-case） # 在串式命名法中，各个单词之间通过连接符“-”连接，比如dubbo-registry。\n建议项目文件夹名称使用串式命名法（kebab-case），比如 dubbo 项目的各个模块的命名是下面这样的。\n常见命名规范 # Java 语言基本命名规范 # 1、类名需要使用大驼峰命名法（UpperCamelCase）风格。方法名、参数名、成员变量、局部变量需要使用小驼峰命名法（lowerCamelCase）。\n2、测试方法名、常量、枚举名称需要使用蛇形命名法（snake_case），比如should_get_200_status_code_when_request_is_valid、CLIENT_CONNECT_SERVER_FAILURE。并且，测试方法名称要求全部小写，常量以及枚举名称需要全部大写。\n3、项目文件夹名称使用串式命名法（kebab-case），比如dubbo-registry。\n4、包名统一使用小写，尽量使用单个名词作为包名，各个单词通过 \u0026ldquo;.\u0026rdquo; 分隔符连接，并且各个单词必须为单数。\n正例： org.apache.dubbo.common.threadlocal\n反例： org.apache_dubbo.Common.threadLocals\n5、抽象类命名使用 Abstract 开头。\n//为远程传输部分抽象出来的一个抽象类（出处：Dubbo源码） public abstract class AbstractClient extends AbstractEndpoint implements Client { } 6、异常类命名使用 Exception 结尾。\n//自定义的 NoSuchMethodException（出处：Dubbo源码） public class NoSuchMethodException extends RuntimeException { private static final long serialVersionUID = -2725364246023268766L; public NoSuchMethodException() { super(); } public NoSuchMethodException(String msg) { super(msg); } } 7、测试类命名以它要测试的类的名称开始，以 Test 结尾。\n//为 AnnotationUtils 类写的测试类（出处：Dubbo源码） public class AnnotationUtilsTest { ...... } POJO 类中布尔类型的变量，都不要加 is 前缀，否则部分框架解析会引起序列化错误。\n如果模块、接口、类、方法使用了设计模式，在命名时需体现出具体模式。\n命名易读性规范 # 1、为了能让命名更加易懂和易读，尽量不要缩写/简写单词，除非这些单词已经被公认可以被这样缩写/简写。比如 CustomThreadFactory 不可以被写成 ~~CustomTF 。\n2、命名不像函数一样要尽量追求短，可读性强的名字优先于简短的名字，虽然可读性强的名字会比较长一点。 这个对应我们上面说的第 1 点。\n3、避免无意义的命名，你起的每一个名字都要能表明意思。\n正例：UserService userService; int userCount;\n反例: UserService service int count\n4、避免命名过长（50 个字符以内最好），过长的命名难以阅读并且丑陋。\n5、不要使用拼音，更不要使用中文。 不过像 alibaba 、wuhan、taobao 这种国际通用名词可以当做英文来看待。\n正例：discount\n反例：dazhe\nCodelf:变量命名神器? # 这是一个由国人开发的网站，网上有很多人称其为变量命名神器， 我在实际使用了几天之后感觉没那么好用。小伙伴们可以自行体验一下，然后再给出自己的判断。\nCodelf 提供了在线网站版本，网址：https://unbug.github.io/codelf/，具体使用情况如下：\n我选择了 Java 编程语言，然后搜索了“序列化”这个关键词，然后它就返回了很多关于序列化的命名。\n并且，Codelf 还提供了 VS code 插件，看这个评价，看来大家还是很喜欢这款命名工具的。\n相关阅读推荐 # 《阿里巴巴 Java 开发手册》 《Clean Code》 Google Java 代码指南：https://google.github.io/styleguide/javaguide.html 告别编码5分钟，命名2小时！史上最全的Java命名规范参考：https://www.cnblogs.com/liqiangchn/p/12000361.html 总结 # 作为一个合格的程序员，小伙伴们应该都知道代码表义的重要性。想要写出高质量代码，好的命名就是第一步！\n好的命名对于其他人（包括你自己）理解你的代码有着很大的帮助！你的代码越容易被理解，可维护性就越强，侧面也就说明你的代码设计的也就越好！\n在日常编码过程中，我们需要谨记常见命名规范比如类名需要使用大驼峰命名法、不要使用拼音，更不要使用中文\u0026hellip;\u0026hellip;。\n另外，国人开发的一个叫做 Codelf 的网站被很多人称为“变量命名神器”，当你为命名而头疼的时候，你可以去参考一下上面提供的一些命名示例。\n最后，祝愿大家都不用再为命名而困扰!\n"},{"id":76,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/basis/software-engineering/","title":"软件工程简明教程","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n大部分软件开发从业者，都会忽略软件开发中的一些最基础、最底层的一些概念。但是，这些软件开发的概念对于软件开发来说非常重要，就像是软件开发的基石一样。这也是我写这篇文章的原因。\n何为软件工程？ # 1968 年 NATO（北大西洋公约组织）提出了软件危机（Software crisis）一词。同年，为了解决软件危机问题，“软件工程”的概念诞生了。一门叫做软件工程的学科也就应运而生。\n随着时间的推移，软件工程这门学科也经历了一轮又一轮的完善，其中的一些核心内容比如软件开发模型越来越丰富实用！\n什么是软件危机呢？\n简单来说，软件危机描述了当时软件开发的一个痛点：我们很难高效地开发出质量高的软件。\nDijkstra（Dijkstra算法的作者） 在 1972年图灵奖获奖感言中也提高过软件危机，他是这样说的：“导致软件危机的主要原因是机器变得功能强大了几个数量级！坦率地说：只要没有机器，编程就完全没有问题。当我们有一些弱小的计算机时，编程成为一个温和的问题，而现在我们有了庞大的计算机，编程也同样成为一个巨大的问题”。\n说了这么多，到底什么是软件工程呢？\n工程是为了解决实际的问题将理论应用于实践。软件工程指的就是将工程思想应用于软件开发。\n上面是我对软件工程的定义，我们再来看看比较权威的定义。IEEE 软件工程汇刊给出的定义是这样的：　(1)将系统化的、规范的、可量化的方法应用到软件的开发、运行及维护中，即将工程化方法应用于软件。　(2)在(1)中所述方法的研究。\n总之，软件工程的终极目标就是：在更少资源消耗的情况下，创造出更好、更容易维护的软件。\n软件开发过程 # 维基百科是这样定义软件开发过程的：\n软件开发过程（英语：software development process），或软件过程（英语：software process），是软件开发的开发生命周期（software development life cycle），其各个阶段实现了软件的需求定义与分析、设计、实现、测试、交付和维护。软件过程是在开发与构建系统时应遵循的步骤，是软件开发的路线图。\n需求分析 ：分析用户的需求，建立逻辑模型。 软件设计 ： 根据需求分析的结果对软件架构进行设计。 编码 ：编写程序运行的源代码。 测试 : 确定测试用例，编写测试报告。 交付 ：将做好的软件交付给客户。 维护 ：对软件进行维护比如解决 bug，完善功能。 软件开发过程只是比较笼统的层面上，一定义了一个软件开发可能涉及到的一些流程。\n软件开发模型更具体地定义了软件开发过程，对开发过程提供了强有力的理论支持。\n软件开发模型 # 软件开发模型有很多种，比如瀑布模型（Waterfall Model）、快速原型模型（Rapid Prototype Model）、V模型（V-model）、W模型（W-model）、敏捷开发模型。其中最具有代表性的还是 瀑布模型 和 敏捷开发 。\n瀑布模型 定义了一套完成的软件开发周期，完整地展示了一个软件的的生命周期。\n敏捷开发模型 是目前使用的最多的一种软件开发模型。MBA智库百科对敏捷开发的描述是这样的:\n敏捷开发 是一种以人为核心、迭代、循序渐进的开发方法。在敏捷开发中，软件项目的构建被切分成多个子项目，各个子项目的成果都经过测试，具备集成和可运行的特征。换言之，就是把一个大项目分为多个相互联系，但也可独立运行的小项目，并分别完成，在此过程中软件一直处于可使用状态。\n像现在比较常见的一些概念比如 持续集成 、重构 、小版本发布 、低文档 、站会 、结对编程 、测试驱动开发 都是敏捷开发的核心。\n软件开发的基本策略 # 软件复用 # 我们在构建一个新的软件的时候，不需要从零开始，通过复用已有的一些轮子（框架、第三方库等）、设计模式、设计原则等等现成的物料，我们可以更快地构建出一个满足要求的软件。\n像我们平时接触的开源项目就是最好的例子。我想，如果不是开源，我们构建出一个满足要求的软件，耗费的精力和时间要比现在多的多！\n分而治之 # 构建软件的过程中，我们会遇到很多问题。我们可以将一些比较复杂的问题拆解为一些小问题，然后，一一攻克。\n我结合现在比较火的软件设计方法—**领域驱动设计（Domain Driven Design，简称 DDD）**来说说。\n在领域驱动设计中，很重要的一个概念就是领域（Domain），它就是我们要解决的问题。在领域驱动设计中，我们要做的就是把比较大的领域（问题）拆解为若干的小领域（子域）。\n除此之外，分而治之也是一个比较常用的算法思想，对应的就是分治算法。如果你想了解分治算法的话，推荐你看一下北大的《算法设计与分析 Design and Analysis of Algorithms》。\n逐步演进 # 软件开发是一个逐步演进的过程，我们需要不断进行迭代式增量开发，最终交付符合客户价值的产品。\n这里补充一个在软件开发领域，非常重要的概念：MVP（Minimum Viable Product，最小可行产品）。\n这个最小可行产品，可以理解为刚好能够满足客户需求的产品。下面这张图片把这个思想展示的非常精髓。\n利用最小可行产品，我们可以也可以提早进行市场分析，这对于我们在探索产品不确定性的道路上非常有帮助。可以非常有效地指导我们下一步该往哪里走。\n优化折中 # 软件开发是一个不断优化改进的过程。任何软件都有很多可以优化的点，不可能完美。我们需要不断改进和提升软件的质量。\n但是，也不要陷入这个怪圈。要学会折中，在有限的投入内，以最有效的方式提高现有软件的质量。\n参考 # 软件工程的基本概念-清华大学软件学院 刘强：https://www.xuetangx.com/course/THU08091000367 软件开发过程-维基百科 ：https://zh.wikipedia.org/wiki/软件开发过程 "},{"id":77,"href":"/zh/docs/technology/Review/java_guide/lycly_system-design/basis/restful/","title":"restFul","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n这篇文章简单聊聊后端程序员必备的 RESTful API 相关的知识。\n开始正式介绍 RESTful API 之前，我们需要首先搞清 ：API 到底是什么？\n# 何为 API？ # API（Application Programming Interface） 翻译过来是应用程序编程接口的意思。\n我们在进行后端开发的时候，主要的工作就是为前端或者其他后端服务提供 API 比如查询用户数据的 API 。\n但是， API 不仅仅代表后端系统暴露的接口，像框架中提供的方法也属于 API 的范畴。\n为了方便大家理解，我再列举几个例子 🌰：\n你通过某电商网站搜索某某商品，电商网站的前端就调用了后端提供了搜索商品相关的 API。 你使用 JDK 开发 Java 程序，想要读取用户的输入的话，你就需要使用 JDK 提供的 IO 相关的 API。 \u0026hellip;\u0026hellip; 你可以把 API 理解为程序与程序之间通信的桥梁，其本质就是一个函数而已。另外，API 的使用也不是没有章法的，它的规则由（比如数据输入和输出的格式）API 提供方制定。\n# 何为 RESTful API？ # RESTful API 经常也被叫做 REST API，它是基于 REST 构建的 API。这个 REST 到底是什么，我们后文在讲，涉及到的概念比较多。\n如果你看 RESTful API 相关的文章的话一般都比较晦涩难懂，主要是因为 REST 涉及到的一些概念比较难以理解。但是，实际上，我们平时开发用到的 RESTful API 的知识非常简单也很容易概括！\n举个例子，如果我给你下面两个 API 你是不是立马能知道它们是干什么用的！这就是 RESTful API 的强大之处！\nGET /classes：列出所有班级 POST /classes：新建一个班级 RESTful API 可以让你看到 URL+Http Method 就知道这个 URL 是干什么的，让你看到了 HTTP 状态码（status code）就知道请求结果如何。\n像咱们在开发过程中设计 API 的时候也应该至少要满足 RESTful API 的最基本的要求（比如接口中尽量使用名词，使用 POST 请求创建资源，DELETE 请求删除资源等等，示例：GET /notes/id：获取某个指定 id 的笔记的信息）。\n# 解读 REST # REST 是 REpresentational State Transfer 的缩写。这个词组的翻译过来就是“表现层状态转化”。\n这样理解起来甚是晦涩，实际上 REST 的全称是 Resource Representational State Transfer ，直白地翻译过来就是 “资源”在网络传输中以某种“表现形式”进行“状态转移” 。如果还是不能继续理解，请继续往下看，相信下面的讲解一定能让你理解到底啥是 REST 。\n我们分别对上面涉及到的概念进行解读，以便加深理解，实际上你不需要搞懂下面这些概念，也能看懂我下一部分要介绍到的内容。不过，为了更好地能跟别人扯扯 “RESTful API”我建议你还是要好好理解一下！\n资源（Resource） ：我们可以把真实的对象数据称为资源。一个资源既可以是一个集合，也可以是单个个体。比如我们的班级 classes 是代表一个集合形式的资源，而特定的 class 代表单个个体资源。每一种资源都有特定的 URI（统一资源标识符）与之对应，如果我们需要获取这个资源，访问这个 URI 就可以了，比如获取特定的班级：/class/12。另外，资源也可以包含子资源，比如 /classes/classId/teachers：列出某个指定班级的所有老师的信息 表现形式（Representational）：\u0026ldquo;资源\u0026quot;是一种信息实体，它可以有多种外在表现形式。我们把\u0026quot;资源\u0026quot;具体呈现出来的形式比如 json，xml，image,txt 等等叫做它的**\u0026ldquo;表现层/表现形式\u0026rdquo;**。 状态转移（State Transfer） ：大家第一眼看到这个词语一定会很懵逼？内心 BB：这尼玛是啥啊？ 大白话来说 REST 中的状态转移更多地描述的服务器端资源的状态，比如你通过增删改查（通过 HTTP 动词实现）引起资源状态的改变。ps:互联网通信协议 HTTP 协议，是一个无状态协议，所有的资源状态都保存在服务器端。 综合上面的解释，我们总结一下什么是 RESTful 架构：\n每一个 URI 代表一种资源； 客户端和服务器之间，传递这种资源的某种表现形式比如 json，xml，image,txt 等等； 客户端通过特定的 HTTP 动词，对服务器端资源进行操作，实现**\u0026ldquo;表现层状态转化\u0026rdquo;**。 # RESTful API 规范 # # 动作 # GET：请求从服务器获取特定资源。举个例子：GET /classes（获取所有班级） POST ：在服务器上创建一个新的资源。举个例子：POST /classes（创建班级） PUT ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：PUT /classes/12（更新编号为 12 的班级） DELETE ：从服务器删除特定的资源。举个例子：DELETE /classes/12（删除编号为 12 的班级） PATCH ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，这里就不举例子了。 # 路径（接口命名） # 路径又称\u0026quot;终点\u0026rdquo;（endpoint），表示 API 的具体网址。实际开发中常见的规范如下：\n网址中不能有动词，只能有名词，API 中的名词也应该使用复数。 因为 REST 中的资源往往和数据库中的表对应，而数据库中的表都是同种记录的\u0026quot;集合\u0026quot;（collection）。如果 API 调用并不涉及资源（如计算，翻译等操作）的话，可以用动词。比如：GET /calculate?param1=11\u0026amp;param2=33 。 不用大写字母，建议用中杠 - 不用下杠 _ 。比如邀请码写成 invitation-code而不是 invitation_code 。 善用版本化 API。当我们的 API 发生了重大改变而不兼容前期版本的时候，我们可以通过 URL 来实现版本化，比如 http://api.example.com/v1、http://apiv1.example.com 。版本不必非要是数字，只是数字用的最多，日期、季节都可以作为版本标识符，项目团队达成共识就可。 接口尽量使用名词，避免使用动词。 RESTful API 操作（HTTP Method）的是资源（名词）而不是动作（动词）。 Talk is cheap！来举个实际的例子来说明一下吧！现在有这样一个 API 提供班级（class）的信息，还包括班级中的学生和教师的信息，则它的路径应该设计成下面这样。\nGET /classes：列出所有班级 POST /classes：新建一个班级 GET /classes/{classId}：获取某个指定班级的信息 PUT /classes/{classId}：更新某个指定班级的信息（一般倾向整体更新） PATCH /classes/{classId}：更新某个指定班级的信息（一般倾向部分更新） DELETE /classes/{classId}：删除某个班级 GET /classes/{classId}/teachers：列出某个指定班级的所有老师的信息 GET /classes/{classId}/students：列出某个指定班级的所有学生的信息 DELETE /classes/{classId}/teachers/{ID}：删除某个指定班级下的指定的老师的信息 反例：\n/getAllclasses /createNewclass /deleteAllActiveclasses 理清资源的层次结构，比如业务针对的范围是学校，那么学校会是一级资源:/schools，老师: /schools/teachers，学生: /schools/students 就是二级资源。\n# 过滤信息（Filtering） # 如果我们在查询的时候需要添加特定条件的话，建议使用 url 参数的形式。比如我们要查询 state 状态为 active 并且 name 为 guidegege 的班级：\nGET /classes?state=active\u0026amp;name=guidegege 比如我们要实现分页查询：\nGET /classes?page=1\u0026amp;size=10 //指定第1页，每页10个数据 # 状态码（Status Codes） # 状态码范围：\n2xx：成功 3xx：重定向 4xx：客户端错误 5xx：服务器错误 200 成功 301 永久重定向 400 错误请求 500 服务器错误 201 创建 304 资源未修改 401 未授权 502 网关错误 403 禁止访问 504 网关超时 404 未找到 405 请求方法不对 # RESTful 的极致 HATEOAS # RESTful 的极致是 hateoas ，但是这个基本不会在实际项目中用到。\n上面是 RESTful API 最基本的东西，也是我们平时开发过程中最容易实践到的。实际上，RESTful API 最好做到 Hypermedia，即返回结果中提供链接，连向其他 API 方法，使得用户不查文档，也知道下一步应该做什么。\n比如，当用户向 api.example.com 的根目录发出请求，会得到这样一个返回结果\n{\u0026#34;link\u0026#34;: { \u0026#34;rel\u0026#34;: \u0026#34;collection https://www.example.com/classes\u0026#34;, \u0026#34;href\u0026#34;: \u0026#34;https://api.example.com/classes\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;List of classes\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;application/vnd.yourformat+json\u0026#34; }} 上面代码表示，文档中有一个 link 属性，用户读取这个属性就知道下一步该调用什么 API 了。rel 表示这个 API 与当前网址的关系（collection 关系，并给出该 collection 的网址），href 表示 API 的路径，title 表示 API 的标题，type 表示返回类型 Hypermedia API 的设计被称为HATEOASopen in new window。\n在 Spring 中有一个叫做 HATEOAS 的 API 库，通过它我们可以更轻松的创建出符合 HATEOAS 设计的 API。相关文章：\n在 Spring Boot 中使用 HATEOASopen in new window Building REST services with Springopen in new window (Spring 官网 ) An Intro to Spring HATEOASopen in new window spring-hateoas-examplesopen in new window Spring HATEOASopen in new window (Spring 官网 ) # 参考 # https://RESTfulapi.net/ https://www.ruanyifeng.com/blog/2014/05/restful_api.html https://juejin.im/entry/59e460c951882542f578f2f0 https://phauer.com/2016/testing-RESTful-services-java-best-practices/ https://www.seobility.net/en/wiki/REST_API https://dev.to/duomly/rest-api-vs-graphql-comparison-3j6g 著作权归所有 原文链接：https://javaguide.cn/system-design/basis/RESTfulAPI.html\n"},{"id":78,"href":"/zh/docs/technology/Review/java_guide/lyfly_high-availability/ly05ly_performance-test/","title":"性能测试入门","section":"高可用","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n性能测试一般情况下都是由测试这个职位去做的，那还需要我们开发学这个干嘛呢？了解性能测试的指标、分类以及工具等知识有助于我们更好地去写出性能更好的程序，另外作为开发这个角色，如果你会性能测试的话，相信也会为你的履历加分不少。\n这篇文章是我会结合自己的实际经历以及在测试这里取的经所得，除此之外，我还借鉴了一些优秀书籍，希望对你有帮助。\n本文思维导图：\n# 一 不同角色看网站性能 # # 1.1 用户 # 当用户打开一个网站的时候，最关注的是什么？当然是网站响应速度的快慢。比如我们点击了淘宝的主页，淘宝需要多久将首页的内容呈现在我的面前，我点击了提交订单按钮需要多久返回结果等等。\n所以，用户在体验我们系统的时候往往根据你的响应速度的快慢来评判你的网站的性能。\n# 1.2 开发人员 # 用户与开发人员都关注速度，这个速度实际上就是我们的系统处理用户请求的速度。\n开发人员一般情况下很难直观的去评判自己网站的性能，我们往往会根据网站当前的架构以及基础设施情况给一个大概的值,比如：\n项目架构是分布式的吗？ 用到了缓存和消息队列没有？ 高并发的业务有没有特殊处理？ 数据库设计是否合理？ 系统用到的算法是否还需要优化？ 系统是否存在内存泄露的问题？ 项目使用的 Redis 缓存多大？服务器性能如何？用的是机械硬盘还是固态硬盘？ \u0026hellip;\u0026hellip; # 1.3 测试人员 # 测试人员一般会根据性能测试工具来测试，然后一般会做出一个表格。这个表格可能会涵盖下面这些重要的内容：\n响应时间； 请求成功率； 吞吐量； \u0026hellip;\u0026hellip; # 1.4 运维人员 # 运维人员会倾向于根据基础设施和资源的利用率来判断网站的性能，比如我们的服务器资源使用是否合理、数据库资源是否存在滥用的情况、当然，这是传统的运维人员，现在 Devpos 火起来后，单纯干运维的很少了。我们这里暂且还保留有这个角色。\n# 二 性能测试需要注意的点 # 几乎没有文章在讲性能测试的时候提到这个问题，大家都会讲如何去性能测试，有哪些性能测试指标这些东西。\n# 2.1 了解系统的业务场景 # 性能测试之前更需要你了解当前的系统的业务场景。 对系统业务了解的不够深刻，我们很容易犯测试方向偏执的错误，从而导致我们忽略了对系统某些更需要性能测试的地方进行测试。比如我们的系统可以为用户提供发送邮件的功能，用户配置成功邮箱后只需输入相应的邮箱之后就能发送，系统每天大概能处理上万次发邮件的请求。很多人看到这个可能就直接开始使用相关工具测试邮箱发送接口，但是，发送邮件这个场景可能不是当前系统的性能瓶颈，这么多人用我们的系统发邮件， 还可能有很多人一起发邮件，单单这个场景就这么人用，那用户管理可能才是性能瓶颈吧！\n# 2.2 历史数据非常有用 # 当前系统所留下的历史数据非常重要，一般情况下，我们可以通过相应的些历史数据初步判定这个系统哪些接口调用的比较多、哪些 service 承受的压力最大，这样的话，我们就可以针对这些地方进行更细致的性能测试与分析。\n另外，这些地方也就像这个系统的一个短板一样，优化好了这些地方会为我们的系统带来质的提升。\n# 三 性能测试的指标 # # 3.1 响应时间 # 响应时间就是用户发出请求到用户收到系统处理结果所需要的时间。 重要吗？实在太重要！\n比较出名的 2-5-8 原则是这样描述的：通常来说，2到5秒，页面体验会比较好，5到8秒还可以接受，8秒以上基本就很难接受了。另外，据统计当网站慢一秒就会流失十分之一的客户。\n但是，在某些场景下我们也并不需要太看重 2-5-8 原则 ，比如我觉得系统导出导入大数据量这种就不需要，系统生成系统报告这种也不需要。\n# 3.2 并发数 # 并发数是系统能同时处理请求的数目即同时提交请求的用户数目。\n不得不说，高并发是现在后端架构中非常非常火热的一个词了，这个与当前的互联网环境以及中国整体的互联网用户量都有很大关系。一般情况下，你的系统并发量越大，说明你的产品做的就越大。但是，并不是每个系统都需要达到像淘宝、12306 这种亿级并发量的。\n# 3.3 吞吐量 # 吞吐量指的是系统单位时间内系统处理的请求数量。衡量吞吐量有几个重要的参数：QPS（TPS）、并发数、响应时间。\nQPS（Query Per Second）：服务器每秒可以执行的查询次数； TPS（Transaction Per Second）：服务器每秒处理的事务数（这里的一个事务可以理解为客户发出请求到收到服务器的过程）； 并发数；系统能同时处理请求的数目即同时提交请求的用户数目。 响应时间： 一般取多次请求的平均响应时间 理清他们的概念，就很容易搞清楚他们之间的关系了。\nQPS（TPS） = 并发数/平均响应时间 并发数 = QPS*平均响应时间 书中是这样描述 QPS 和 TPS 的区别的。\nQPS vs TPS：QPS 基本类似于 TPS，但是不同的是，对于一个页面的一次访问，形成一个TPS；但一次页面请求，可能产生多次对服务器的请求，服务器对这些请求，就可计入“QPS”之中。如，访问一个页面会请求服务器2次，一次访问，产生一个“T”，产生2个“Q”。\n# 3.4 性能计数器 # 性能计数器是描述服务器或者操作系统的一些数据指标如内存使用、CPU使用、磁盘与网络I/O等情况。\n# 四 几种常见的性能测试 # # 性能测试 # 性能测试方法是通过测试工具模拟用户请求系统，目的主要是为了测试系统的性能是否满足要求。通俗地说，这种方法就是要在特定的运行条件下验证系统的能力状态。\n性能测试是你在对系统性能已经有了解的前提之后进行的，并且有明确的性能指标。\n# 负载测试 # 对被测试的系统继续加大请求压力，直到服务器的某个资源已经达到饱和了，比如系统的缓存已经不够用了或者系统的响应时间已经不满足要求了。\n负载测试说白点就是测试系统的上限。\n# 压力测试 # 不去管系统资源的使用情况，对系统继续加大请求压力，直到服务器崩溃无法再继续提供服务。\n# 稳定性测试 # 模拟真实场景，给系统一定压力，看看业务是否能稳定运行。\n# 五 常用性能测试工具 # 这里就不多扩展了，有时间的话会单独拎一个熟悉的说一下。\n# 5.1 后端常用 # 没记错的话，除了 LoadRunner 其他几款性能测试工具都是开源免费的。\nJmeter ：Apache JMeter 是 JAVA 开发的性能测试工具。 LoadRunner：一款商业的性能测试工具。 Galtling ：一款基于Scala 开发的高性能服务器性能测试工具。 ab ：全称为 Apache Bench 。Apache 旗下的一款测试工具，非常实用。 # 5.2 前端常用 # Fiddler：抓包工具，它可以修改请求的数据，甚至可以修改服务器返回的数据，功能非常强大，是Web 调试的利器。 HttpWatch: 可用于录制HTTP请求信息的工具。 # 六 常见的性能优化策略 # 性能优化之前我们需要对请求经历的各个环节进行分析，排查出可能出现性能瓶颈的地方，定位问题。\n下面是一些性能优化时，我经常拿来自问的一些问题：\n系统是否需要缓存？ 系统架构本身是不是就有问题？ 系统是否存在死锁的地方？ 系统是否存在内存泄漏？（Java 的自动回收内存虽然很方便，但是，有时候代码写的不好真的会造成内存泄漏） 数据库索引使用是否合理？ \u0026hellip;\u0026hellip; 著作权归所有 原文链接：https://javaguide.cn/high-availability/performance-test.html\n"},{"id":79,"href":"/zh/docs/technology/Review/java_guide/lyfly_high-availability/ly04ly_timout-and-retry/","title":"超时\u0026重试详解","section":"高可用","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n由于网络问题、系统或者服务内部的 Bug、服务器宕机、操作系统崩溃等问题的不确定性，我们的系统或者服务永远不可能保证时刻都是可用的状态。\n为了最大限度的减小系统或者服务出现故障之后带来的影响，我们需要用到的 超时（Timeout） 和 重试（Retry） 机制。\n想要把超时和重试机制讲清楚其实很简单，因为它俩本身就不是什么高深的概念。\n虽然超时和重试机制的思想很简单，但是它俩是真的非常实用。你平时接触到的绝大部分涉及到远程调用的系统或者服务都会应用超时和重试机制。尤其是对于微服务系统来说，正确设置超时和重试非常重要。单体服务通常只涉及数据库、缓存、第三方 API、中间件等的网络调用，而微服务系统内部各个服务之间还存在着网络调用。\n# 超时机制 # # 什么是超时机制？ # 超时机制说的是当一个请求超过指定的时间（比如 1s）还没有被处理的话，这个请求就会直接被取消并抛出指定的异常或者错误（比如 504 Gateway Timeout）。\n我们平时接触到的超时可以简单分为下面 2 种：\n连接超时（ConnectTimeout） ：客户端与服务端建立连接的最长等待时间。 读取超时（ReadTimeout） ：客户端和服务端已经建立连接，客户端等待服务端处理完请求的最长时间。实际项目中，我们关注比较多的还是读取超时。 一些连接池客户端框架中可能还会有获取连接超时和空闲连接清理超时**。\n如果没有设置超时的话，就可能会导致服务端连接数爆炸和大量请求堆积的问题。\n这些堆积的连接和请求会消耗系统资源，影响新收到的请求的处理。严重的情况下，甚至会拖垮整个系统或者服务。\n我之前在实际项目就遇到过类似的问题，整个网站无法正常处理请求，服务器负载直接快被拉满。后面发现原因是项目超时设置错误加上客户端请求处理异常，导致服务端连接数直接接近 40w+，这么多堆积的连接直接把系统干趴了。\n# 超时时间应该如何设置？ # 超时到底设置多长时间是一个难题！超时值设置太高或者太低都有风险。如果设置太高的话，会降低超时机制的有效性，比如你设置超时为 10s 的话，那设置超时就没啥意义了，系统依然可能会出现大量慢请求堆积的问题。如果设置太低的话，就可能会导致在系统或者服务在某些处理请求速度变慢的情况下（比如请求突然增多），大量请求重试（超时通常会结合重试）继续加重系统或者服务的压力，进而导致整个系统或者服务被拖垮的问题。\n通常情况下，我们建议读取超时设置为 1500ms ,这是一个比较普适的值。如果你的系统或者服务对于延迟比较敏感的话，那读取超时值可以适当在 1500ms 的基础上进行缩短。反之，读取超时值也可以在 1500ms 的基础上进行加长，不过，尽量还是不要超过 1500ms 。连接超时可以适当设置长一些，建议在 1000ms ~ 5000ms 之内。\n没有银弹！超时值具体该设置多大，还是要根据实际项目的需求和情况慢慢调整优化得到。\n更上一层，参考美团的Java线程池参数动态配置open in new window思想，我们也可以将超时弄成可配置化的参数而不是固定的，比较简单的一种办法就是将超时的值放在配置中心中。这样的话，我们就可以根据系统或者服务的状态动态调整超时值了。\n# 重试机制 # # 什么是重试机制？ # 重试机制一般配合超时机制一起使用，指的是多次发送相同的请求来避免瞬态故障和偶然性故障。\n瞬态故障可以简单理解为某一瞬间系统偶然出现的故障，并不会持久。偶然性故障可以理解为哪些在某些情况下偶尔出现的故障，频率通常较低。\n重试的核心思想是通过消耗服务器的资源来尽可能获得请求更大概率被成功处理。由于瞬态故障和偶然性故障是很少发生的，因此，重试对于服务器的资源消耗几乎是可以被忽略的。\n# 重试的次数如何设置？ # 重试的次数不宜过多，否则依然会对系统负载造成比较大的压力。\n重试的次数通常建议设为 3 次。并且，我们通常还会设置重试的间隔，比如说我们要重试 3 次的话，第 1 次请求失败后，等待 1 秒再进行重试，第 2 次请求失败后，等待 2 秒再进行重试，第 3 次请求失败后，等待 3 秒再进行重试。\n# 重试幂等 # 超时和重试机制在实际项目中使用的话，需要注意保证同一个请求没有被多次执行。\n这里说的同一个请求，指的是\u0026quot;\u0026ldquo;业务上的概念\u0026rdquo;\u0026quot;\n什么情况下会出现一个请求被多次执行呢？客户端等待服务端完成请求完成超时但此时服务端已经执行了请求，只是由于短暂的网络波动导致响应在发送给客户端的过程中延迟了。\n举个例子：用户支付购买某个课程，结果用户支付的请求由于重试的问题导致用户购买同一门课程支付了两次。对于这种情况，我们在执行用户购买课程的请求的时候需要判断一下用户是否已经购买过。这样的话，就不会因为重试的问题导致重复购买了。\n# 参考 # 微服务之间调用超时的设置治理：https://www.infoq.cn/article/eyrslar53l6hjm5yjgyx 超时、重试和抖动回退：https://aws.amazon.com/cn/builders-library/timeouts-retries-and-backoff-with-jitter/ 著作权归所有 原文链接：https://javaguide.cn/high-availability/timeout-and-retry.html\n"},{"id":80,"href":"/zh/docs/technology/Review/java_guide/lyfly_high-availability/ly03ly_limit-request/","title":"服务限流详解","section":"高可用","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n针对软件系统来说，限流就是对请求的速率进行限制，避免瞬时的大量请求击垮软件系统。毕竟，软件系统的处理能力是有限的。如果说超过了其处理能力的范围，软件系统可能直接就挂掉了。\n限流可能会导致用户的请求无法被正确处理，不过，这往往也是权衡了软件系统的稳定性之后得到的最优解。\n现实生活中，处处都有限流的实际应用，就比如排队买票是为了避免大量用户涌入购票而导致售票员无法处理。\n常见限流算法有哪些？ # 简单介绍 4 种非常好理解并且容易实现的限流算法！\n图片来源于 InfoQ 的一篇文章《分布式服务限流实战，已经为你排好坑了》。\n固定窗口计数器算法 # 固定窗口其实就是时间窗口。固定窗口计数器算法 规定了我们单位时间处理的请求数量。\n假如我们规定系统中某个接口 1 分钟只能访问 33 次的话，使用固定窗口计数器算法的实现思路如下：\n给定一个变量 counter 来记录当前接口处理的请求数量，初始值为 0（代表接口当前 1 分钟内还未处理请求）。 1 分钟之内每处理一个请求之后就将 counter+1 ，当 counter=33 之后（也就是说在这 1 分钟内接口已经被访问 33 次的话），后续的请求就会被全部拒绝。 等到 1 分钟结束后，将 counter 重置 0，重新开始计数。 这种限流算法无法保证限流速率，因而无法保证突然激增的流量。\n就比如说我们限制某个接口 1 分钟只能访问 1000 次，该接口的 QPS 为 500，前 55s 这个接口 1 个请求没有接收，后 1s 突然接收了 1000 个请求。然后，在当前场景下，这 1000 个请求在 1s 内是没办法被处理的，系统直接就被瞬时的大量请求给击垮了。\n滑动窗口计数器算法 # 滑动窗口计数器算法 算的上是固定窗口计数器算法的升级版。\n滑动窗口计数器算法相比于固定窗口计数器算法的优化在于：它把时间以一定比例分片(即分片后应用\u0026quot;固定窗口计数器\u0026quot;算法) 。\n例如我们的接口限流每分钟处理 60 个请求，我们可以把 1 分钟分为 60 个窗口(每个窗口时1秒)。每隔 1 秒移动一次，每个窗口一秒只能处理 不大于 60(请求数)/60（窗口数） 的请求， 如果当前窗口的请求计数总和超过了限制的数量的话就不再处理其他请求。\n感觉上面的例子和图没匹配上，应该是\n例如我们的接口限流每分钟处理 300 个请求，我们可以把 1 分钟分为 60 个窗口(每个窗口时1秒)。每隔 1 秒移动一次，每个窗口一秒只能处理 不大于 300(请求数)/60（窗口数） 的请求， 如果当前窗口的请求计数总和超过了限制的数量的话就不再处理其他请求。\n很显然， 当滑动窗口的格子划分的越多，滑动窗口的滚动就越平滑，限流的统计就会越精确。\n]\n漏桶算法 # 我们可以把发请求的动作比作成注水到桶中，我们处理请求的过程可以比喻为漏桶漏水。我们往桶中以任意速率流入水，以一定速率流出水。当水超过桶流量则丢弃，因为桶容量是不变的，保证了整体的速率。\n如果想要实现这个算法的话也很简单，准备一个队列用来保存请求，然后我们定期从队列中拿请求来执行就好了（和消息队列削峰/限流的思想是一样的）。\n令牌桶算法 # 令牌桶算法也比较简单。和漏桶算法算法一样，我们的主角还是桶（这限流算法和桶过不去啊）。不过现在桶里装的是令牌了，请求在被处理之前需要拿到一个令牌，请求处理完毕之后将这个令牌丢弃（删除）。我们根据限流大小，按照一定的速率往桶里添加令牌。如果桶装满了，就不能继续往里面继续添加令牌了。\n单机限流怎么做？ # 单机限流针对的是单体架构应用。\n单机限流可以直接使用 Google Guava 自带的限流工具类 RateLimiter 。 RateLimiter 基于令牌桶算法，可以应对突发流量。\nGuava 地址：https://github.com/google/guava\n除了最基本的令牌桶算法(平滑突发限流)实现之外，Guava 的RateLimiter还提供了 平滑预热限流 的算法实现。\n平滑突发限流就是按照指定的速率放令牌到桶里，而平滑预热限流会有一段预热时间，预热时间之内，速率会逐渐提升到配置的速率。\n我们下面通过两个简单的小例子来详细了解吧！\n我们直接在项目中引入 Guava 相关的依赖即可使用。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.google.guava\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;guava\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;31.0.1-jre\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 下面是一个简单的 Guava 平滑突发限流的 Demo。\nimport com.google.common.util.concurrent.RateLimiter; /** * 微信搜 JavaGuide 回复\u0026#34;面试突击\u0026#34;即可免费领取个人原创的 Java 面试手册 * * @author Guide哥 * @date 2021/10/08 19:12 **/ public class RateLimiterDemo { public static void main(String[] args) { // 1s 放 5 个令牌到桶里也就是 0.2s 放 1个令牌到桶里 //也就是1s放几个 //public static RateLimiter create(double permitsPerSecond) {} RateLimiter rateLimiter = RateLimiter.create(5); for (int i = 0; i \u0026lt; 10; i++) { //阻塞直到得到一个令牌 ,会返回阻塞的时间 double sleepingTime = rateLimiter.acquire(1); System.out.printf(\u0026#34;get 1 tokens: %ss%n\u0026#34;, sleepingTime); } } } //源码 /** * Creates a {@code RateLimiter} with the specified stable throughput, given as \u0026#34;permits per * second\u0026#34; (commonly referred to as \u0026lt;i\u0026gt;QPS\u0026lt;/i\u0026gt;, queries per second). * * \u0026lt;p\u0026gt;The returned {@code RateLimiter} ensures that on average no more than {@code * permitsPerSecond} are issued during any given second, with sustained requests being smoothly * spread over each second. When the incoming request rate exceeds {@code permitsPerSecond} the * rate limiter will release one permit every {@code (1.0 / permitsPerSecond)} seconds. When the * rate limiter is unused, bursts of up to {@code permitsPerSecond} permits will be allowed, with * subsequent requests being smoothly limited at the stable rate of {@code permitsPerSecond}. * * @param permitsPerSecond the rate of the returned {@code RateLimiter}, measured in how many * permits become available per second * @throws IllegalArgumentException if {@code permitsPerSecond} is negative or zero */ // TODO(user): \u0026#34;This is equivalent to // {@code createWithCapacity(permitsPerSecond, 1, TimeUnit.SECONDS)}\u0026#34;. // public static RateLimiter create(double permitsPerSecond) {} //源码 /** * Acquires the given number of permits from this {@code RateLimiter}, blocking until the request * can be granted. Tells the amount of time slept, if any. * * @param permits the number of permits to acquire * @return time spent sleeping to enforce rate, in seconds; 0.0 if not rate-limited * @throws IllegalArgumentException if the requested number of permits is negative or zero * @since 16.0 (present in 13.0 with {@code void} return type}) */ /* @CanIgnoreReturnValue public double acquire(int permits) { long microsToWait = reserve(permits); stopwatch.sleepMicrosUninterruptibly(microsToWait); return 1.0 * microsToWait / SECONDS.toMicros(1L); } */ 输出：\nget 1 tokens: 0.0s get 1 tokens: 0.188413s get 1 tokens: 0.197811s get 1 tokens: 0.198316s get 1 tokens: 0.19864s get 1 tokens: 0.199363s get 1 tokens: 0.193997s get 1 tokens: 0.199623s get 1 tokens: 0.199357s get 1 tokens: 0.195676s 下面是一个简单的 Guava 平滑预热限流的 Demo。\nimport com.google.common.util.concurrent.RateLimiter; import java.util.concurrent.TimeUnit; /** * 微信搜 JavaGuide 回复\u0026#34;面试突击\u0026#34;即可免费领取个人原创的 Java 面试手册 * * @author Guide哥 * @date 2021/10/08 19:12 **/ public class RateLimiterDemo { public static void main(String[] args) { // 1s 放 5 个令牌到桶里也就是 0.2s 放 1个令牌到桶里 // 预热时间为3s,也就说刚开始的 3s 内发牌速率会逐渐提升到 0.2s 放 1 个令牌到桶里(所以前面的速率比较小，获取需要的时间比较长) RateLimiter rateLimiter = RateLimiter.create(5, 3, TimeUnit.SECONDS); for (int i = 0; i \u0026lt; 20; i++) { double sleepingTime = rateLimiter.acquire(1); System.out.printf(\u0026#34;get 1 tokens: %sds%n\u0026#34;, sleepingTime); } } } 输出：\nget 1 tokens: 0.0s get 1 tokens: 0.561919s get 1 tokens: 0.516931s get 1 tokens: 0.463798s get 1 tokens: 0.41286s get 1 tokens: 0.356172s get 1 tokens: 0.300489s get 1 tokens: 0.252545s get 1 tokens: 0.203996s get 1 tokens: 0.198359s 另外，Bucket4j 是一个非常不错的基于令牌/漏桶算法的限流库。\nBucket4j 地址：https://github.com/vladimir-bukhtoyarov/bucket4j\n相对于，Guava 的限流工具类来说，Bucket4j 提供的限流功能更加全面。不仅支持单机限流和分布式限流，还可以集成监控，搭配 Prometheus 和 Grafana 使用。\n不过，毕竟 Guava 也只是一个功能全面的工具类库，其提供的开箱即用的限流功能在很多单机场景下还是比较实用的。\nSpring Cloud Gateway 中自带的单机限流的早期版本就是基于 Bucket4j 实现的。后来，替换成了 Resilience4j。\nResilience4j 是一个轻量级的容错组件，其灵感来自于 Hystrix。自Netflix 宣布不再积极开发 Hystrix 之后，Spring 官方和 Netflix 都更推荐使用 Resilience4j 来做限流熔断。\nResilience4j 地址: https://github.com/resilience4j/resilience4j\n一般情况下，为了保证系统的高可用，项目的限流和熔断都是要一起做的。\nResilience4j 不仅提供限流，还提供了熔断、负载保护、自动重试等保障系统高可用开箱即用的功能。并且，Resilience4j 的生态也更好，很多网关都使用 Resilience4j 来做限流熔断的。\n因此，在绝大部分场景下 Resilience4j 或许会是更好的选择。如果是一些比较简单的限流场景的话，Guava 或者 Bucket4j 也是不错的选择。\n分布式限流怎么做？ # 分布式限流针对的分布式/微服务应用架构应用，在这种架构下，单机限流就不适用了，因为会存在多种服务，并且一种服务也可能会被部署多份。\n分布式限流常见的方案：\n借助中间件架限流 ：可以借助 Sentinel 或者使用 Redis 来自己实现对应的限流逻辑。 网关层限流 ：比较常用的一种方案，直接在网关层把限流给安排上了。不过，通常网关层限流通常也需要借助到中间件/框架。就比如 Spring Cloud Gateway 的分布式限流实现**RedisRateLimiter**就是基于 Redis+Lua 来实现的，再比如 Spring Cloud Gateway 还可以整合 Sentinel 来做限流。 如果你要基于 Redis 来手动实现限流逻辑的话，建议配合 Lua 脚本来做。\n为什么建议 Redis+Lua 的方式？ 主要有两点原因：\n减少了网络开销 ：我们可以利用 Lua 脚本来批量执行多条 Redis 命令，这些 Redis 命令会被提交到 Redis 服务器一次性执行完成，大幅减小了网络开销。 原子性 ：一段 Lua 脚本可以视作一条命令执行，一段 Lua 脚本执行过程中不会有其他脚本或 Redis 命令同时执行，保证了操作不会被其他指令插入或打扰。 我这里就不放具体的限流脚本代码了，网上也有很多现成的优秀的限流脚本供你参考，就比如 Apache 网关项目 ShenYu 的 RateLimiter 限流插件就基于 Redis + Lua 实现了令牌桶算法/并发令牌桶算法、漏桶算法、滑动窗口算法。\nShenYu 地址: https://github.com/apache/incubator-shenyu\n服务治理之轻量级熔断框架 Resilience4j ：https://xie.infoq.cn/article/14786e571c1a4143ad1ef8f19 超详细的 Guava RateLimiter 限流原理解析：https://cloud.tencent.com/developer/article/1408819 实战 Spring Cloud Gateway 之限流篇 👍：https://www.aneasystone.com/archives/2020/08/spring-cloud-gateway-current-limiting.html "},{"id":81,"href":"/zh/docs/technology/Review/java_guide/lyfly_high-availability/ly02ly_redundancy/","title":"冗余设计","section":"高可用","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\ntitle category 冗余设计详解 高可用 冗余设计是保证系统和数据高可用的最常的手段。\n对于服务来说，冗余的思想就是相同的服务部署多份，如果正在使用的服务突然挂掉的话，系统可以很快切换到备份服务上，大大减少系统的不可用时间，提高系统的可用性。\n对于数据来说，冗余的思想就是相同的数据备份多份，这样就可以很简单地提高数据的安全性。\n实际上，日常生活中就有非常多的冗余思想的应用。\n拿我自己来说，我对于重要文件的保存方法就是冗余思想的应用。我日常所使用的重要文件都会同步一份在 Github 以及个人云盘上，这样就可以保证即使电脑硬盘损坏，我也可以通过 Github 或者个人云盘找回自己的重要文件。\n高可用集群（High Availability Cluster，简称 HA Cluster）、同城灾备、异地灾备、同城多活和异地多活是冗余思想在高可用系统设计中最典型的应用。\n高可用集群 : 同一份服务部署两份或者多份，当正在使用的服务突然挂掉的话，可以切换到另外一台服务，从而保证服务的高可用。 同城灾备 ：一整个集群可以部署在同一个机房，而同城灾备中相同服务部署在同一个城市的不同机房中。并且，备用服务不处理请求。这样可以避免机房出现意外情况比如停电、火灾。 异地灾备 ：类似于同城灾备，不同的是，相同服务部署在异地（通常距离较远，甚至是在不同的城市或者国家）的不同机房中 同城多活 ：类似于同城灾备，但备用服务可以处理请求，这样可以充分利用系统资源，提高系统的并发。 异地多活 : 将服务部署在异地的不同机房中，并且，它们可以同时对外提供服务。 高可用集群单纯是服务的冗余，并没有强调地域。同城灾备、异地灾备、同城多活和异地多活实现了地域上的冗余。\n同城和异地的主要区别在于机房之间的距离。异地通常距离较远，甚至是在不同的城市或者国家。\n和传统的灾备设计相比，同城多活和异地多活最明显的改变在于**“多活”，即所有站点都是同时在对外提供服务的。异地多活是为了应对突发状况比如火灾**、地震等自然或者人为灾害。\n光做好冗余还不够，必须要配合上 故障转移 才可以！ 所谓故障转移，简单来说就是实现不可用服务快速且自动地切换到可用服务，整个过程不需要人为干涉。\n举个例子：哨兵模式的 Redis 集群中，如果 Sentinel（哨兵） 检测到 master 节点出现故障的话， 它就会帮助我们实现故障转移，自动将某一台 slave 升级为 master，确保整个 Redis 系统的可用性。整个过程完全自动，不需要人工介入。我在《Java 面试指北》的「技术面试题篇」中的数据库部分详细介绍了 Redis 集群相关的知识点\u0026amp;面试题，感兴趣的小伙伴可以看看。\n再举个例子：Nginx 可以结合 Keepalived 来实现高可用。如果 Nginx 主服务器宕机的话，Keepalived 可以自动进行故障转移，备用 Nginx 主服务器升级为主服务。并且，这个切换对外是透明的，因为使用的虚拟 IP，虚拟 IP 不会改变。我在《Java 面试指北》的「技术面试题篇」中的「服务器」部分详细介绍了 Nginx 相关的知识点\u0026amp;面试题，感兴趣的小伙伴可以看看。\n异地多活架构实施起来非常难，需要考虑的因素非常多。本人不才，实际项目中并没有实践过异地多活架构，我对其了解还停留在书本知识。\n如果你想要深入学习异地多活相关的知识，我这里推荐几篇我觉得还不错的文章：\n搞懂异地多活，看这篇就够了- 水滴与银弹 - 2021 四步构建异地多活 《从零开始学架构》— 28 | 业务高可用的保障：异地多活架构 不过，这些文章大多也都是在介绍概念知识。目前，网上还缺少真正介绍具体要如何去实践落地异地多活架构的资料。\n灾备 = 容灾+备份。\n备份 ： 将系统所产生的的所有重要数据多备份几份。 容灾 ： 在异地建立两个完全相同的系统。当某个地方的系统突然挂掉，整个应用系统可以切换到另一个，这样系统就可以正常提供服务了。 异地多活 描述的是将服务部署在异地并且服务同时对外提供服务。和传统的灾备设计的最主要区别在于“多活”，即所有站点都是同时在对外提供服务的。异地多活是为了应对突发状况比如火灾、地震等自然或者人为灾害。\n"},{"id":82,"href":"/zh/docs/technology/Review/java_guide/lyfly_high-availability/ly01ly_high-availability-system-design/","title":"高可用系统设计指南","section":"高可用","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n什么是高可用？可用性的判断标准是啥？ # 高可用描述的是一个系统在大部分时间都是可用的，可以为我们提供服务的。高可用代表系统即使在发生硬件故障或者系统升级的时候，服务仍然是可用的。\n一般情况下，我们使用多少个 9 来评判一个系统的可用性，比如 99.9999% 就是代表该系统在所有的运行时间中只有 0.0001% 的时间是不可用的，这样的系统就是非常非常高可用的了！当然，也会有系统如果可用性不太好的话，可能连 9 都上不了。\n除此之外，系统的可用性还可以用某功能的失败次数与总的请求次数之比来衡量，比如对网站请求 1000 次，其中有 10 次请求失败，那么可用性就是 99%。\n哪些情况会导致系统不可用？ # 黑客攻击； 硬件故障，比如服务器坏掉。 并发量/用户请求量激增导致整个服务宕掉或者部分服务不可用。 代码中的坏味道导致内存泄漏或者其他问题导致程序挂掉。 网站架构某个重要的角色比如 Nginx 或者数据库突然不可用。 自然灾害或者人为破坏。 \u0026hellip;\u0026hellip; 有哪些提高系统可用性的方法？ # 注重代码质量，测试严格把关 # 我觉得这个是最最最重要的，代码质量有问题比如比较常见的内存泄漏、循环依赖都是对系统可用性极大的损害。大家都喜欢谈限流、降级、熔断，但是我觉得从代码质量这个源头把关是首先要做好的一件很重要的事情。如何提高代码质量？比较实际可用的就是 CodeReview，不要在乎每天多花的那 1 个小时左右的时间，作用可大着呢！\n另外，安利几个对提高代码质量有实际效果的神器：\nSonarqube； Alibaba 开源的 Java 诊断工具 Arthas； 阿里巴巴 Java 代码规范（Alibaba Java Code Guidelines）； IDEA 自带的代码分析等工具。 使用集群，减少单点故障 # 先拿常用的 Redis 举个例子！我们如何保证我们的 Redis 缓存高可用呢？答案就是使用集群，避免单点故障。当我们使用一个 Redis 实例作为缓存的时候，这个 Redis 实例挂了之后，整个缓存服务可能就挂了。使用了集群之后，即使一台 Redis 实例挂了，不到一秒就会有另外一台 Redis 实例顶上。\n限流 # 流量控制（flow control），其原理是监控应用流量的 QPS 或并发线程数等指标，当达到指定的阈值时对流量进行控制，以避免被瞬时的流量高峰冲垮，从而保障应用的高可用性。——来自 alibaba-Sentinel 的 wiki。\n超时和重试机制设置 # 一旦用户请求超过某个时间的得不到响应，就抛出异常。这个是非常重要的，很多线上系统故障都是因为没有进行超时设置或者超时设置的方式不对导致的。我们在读取第三方服务的时候，尤其适合设置超时和重试机制。一般我们使用一些 RPC 框架的时候，这些框架都自带的超时重试的配置。如果不进行超时设置可能会导致请求响应速度慢，甚至导致请求堆积进而让系统无法再处理请求。重试的次数一般设为 3 次，再多次的重试没有好处，反而会加重服务器压力（部分场景使用失败重试机制会不太适合）。\n熔断机制 # 超时和重试机制设置之外，熔断机制也是很重要的。 熔断机制说的是系统自动收集所依赖服务的资源使用情况和性能指标，当所依赖的服务恶化或者调用失败次数达到某个阈值的时候就迅速失败，让当前系统立即切换依赖其他备用服务。 比较常用的流量控制和熔断降级框架是 Netflix 的 Hystrix 和 alibaba 的 Sentinel。\n异步调用 # 异步调用的话我们不需要关心最后的结果，这样我们就可以用户请求完成之后就立即返回结果，具体处理我们可以后续再做，秒杀场景用这个还是蛮多的。但是，使用异步之后我们可能需要 适当修改业务流程进行配合，比如用户在提交订单之后，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功。除了可以在程序中实现异步之外，我们常常还使用消息队列，消息队列可以通过异步处理提高系统性能（削峰、减少响应所需时间）并且可以降低系统耦合性。\n使用缓存 # 如果我们的系统属于并发量比较高的话，如果我们单纯使用数据库的话，当大量请求直接落到数据库可能数据库就会直接挂掉。使用缓存缓存热点数据，因为缓存存储在内存中，所以速度相当地快！\n其他 # 核心应用和服务优先使用更好的硬件 监控系统资源使用情况增加报警设置。 注意备份，必要时候回滚。 灰度发布： 将服务器集群分成若干部分，每天只发布一部分机器，观察运行稳定没有故障，第二天继续发布一部分机器，持续几天才把整个集群全部发布完毕，期间如果发现问题，只需要回滚已发布的一部分服务器即可 定期检查/更换硬件： 如果不是购买的云服务的话，定期还是需要对硬件进行一波检查的，对于一些需要更换或者升级的硬件，要及时更换或者升级。 \u0026hellip;.. "},{"id":83,"href":"/zh/docs/technology/Review/java_guide/lyely_high-performance/message-mq/rocketmq-questions/","title":"rocketmq常见面试题","section":"RocketMQ","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n本文来自读者 PR。\n主要是rocket mq的几个问题\n1 单机版消息中心 # 一个消息中心，最基本的需要支持多生产者、多消费者，例如下：\nclass Scratch { public static void main(String[] args) { // 实际中会有 nameserver 服务来找到 broker 具体位置以及 broker 主从信息 Broker broker = new Broker(); Producer producer1 = new Producer(); producer1.connectBroker(broker); Producer producer2 = new Producer(); producer2.connectBroker(broker); Consumer consumer1 = new Consumer(); consumer1.connectBroker(broker); Consumer consumer2 = new Consumer(); consumer2.connectBroker(broker); for (int i = 0; i \u0026lt; 2; i++) { producer1.asyncSendMsg(\u0026#34;producer1 send msg\u0026#34; + i); producer2.asyncSendMsg(\u0026#34;producer2 send msg\u0026#34; + i); } System.out.println(\u0026#34;broker has msg:\u0026#34; + broker.getAllMagByDisk()); for (int i = 0; i \u0026lt; 1; i++) { System.out.println(\u0026#34;consumer1 consume msg：\u0026#34; + consumer1.syncPullMsg()); } for (int i = 0; i \u0026lt; 3; i++) { System.out.println(\u0026#34;consumer2 consume msg：\u0026#34; + consumer2.syncPullMsg()); } } } class Producer { private Broker broker; public void connectBroker(Broker broker) { this.broker = broker; } public void asyncSendMsg(String msg) { if (broker == null) { throw new RuntimeException(\u0026#34;please connect broker first\u0026#34;); } new Thread(() -\u0026gt; { broker.sendMsg(msg); }).start(); } } class Consumer { private Broker broker; public void connectBroker(Broker broker) { this.broker = broker; } public String syncPullMsg() { return broker.getMsg(); } } class Broker { // 对应 RocketMQ 中 MessageQueue，默认情况下 1 个 Topic 包含 4 个 MessageQueue private LinkedBlockingQueue\u0026lt;String\u0026gt; messageQueue = new LinkedBlockingQueue(Integer.MAX_VALUE); // 实际发送消息到 broker 服务器使用 Netty 发送 public void sendMsg(String msg) { try { messageQueue.put(msg); // 实际会同步或异步落盘，异步落盘使用的定时任务定时扫描落盘 } catch (InterruptedException e) { } } public String getMsg() { try { return messageQueue.take(); } catch (InterruptedException e) { } return null; } public String getAllMagByDisk() { StringBuilder sb = new StringBuilder(\u0026#34;\\n\u0026#34;); messageQueue.iterator().forEachRemaining((msg) -\u0026gt; { sb.append(msg + \u0026#34;\\n\u0026#34;); }); return sb.toString(); } } 问题：\n没有实现真正执行消息存储落盘 没有实现 NameServer 去作为注册中心，定位服务 使用 LinkedBlockingQueue 作为消息队列，注意，参数是无限大，在真正 RocketMQ 也是如此是无限大，理论上不会出现对进来的数据进行抛弃，但是会有内存泄漏问题（阿里巴巴开发手册也因为这个问题，建议我们使用自制线程池） 没有使用多个队列（即多个 LinkedBlockingQueue），RocketMQ 的顺序消息是通过生产者和消费者同时使用同一个 MessageQueue 来实现，但是如果我们只有一个 MessageQueue，那我们天然就支持顺序消息 没有使用 MappedByteBuffer 来实现文件映射从而使消息数据落盘非常的快（实际 RocketMQ 使用的是 FileChannel+DirectBuffer） 2 分布式消息中心 # 2.1 问题与解决 # 2.1.1 消息丢失的问题 # 当你系统需要保证百分百消息不丢失，你可以使用生产者每发送一个消息，Broker 同步返回一个消息发送成功的反馈消息 即每发送一个消息，同步落盘后才返回生产者消息发送成功，这样只要生产者得到了消息发送生成的返回，事后除了硬盘损坏，都可以保证不会消息丢失 但是这同时引入了一个问题，同步落盘怎么才能快？ 2.1.2 同步落盘怎么才能快 # 使用 FileChannel + DirectBuffer 池，使用堆外内存，加快内存拷贝 使用数据和索引分离，当消息需要写入时，使用 commitlog 文件顺序写，当需要定位某个消息时，查询index 文件来定位，从而减少文件IO随机读写的性能损耗 2.1.3 消息堆积的问题 # 后台定时任务每隔72小时，删除旧的没有使用过的消息信息 根据不同的业务实现不同的丢弃任务，具体参考线程池的 AbortPolicy，例如FIFO/LRU等（RocketMQ没有此策略） 消息定时转移，或者对某些重要的 TAG 型（支付型）消息真正落库 2.1.4 定时消息的实现 # 实际 RocketMQ 没有实现任意精度的定时消息，它只支持某些特定的时间精度的定时消息 实现定时消息的原理是：创建特定时间精度的 MessageQueue，例如生产者需要定时1s之后被消费者消费，你只需要将此消息发送到特定的 Topic，例如：MessageQueue-1 表示这个 MessageQueue 里面的消息都会延迟一秒被消费，然后 Broker 会在 1s 后发送到消费者消费此消息，使用 newSingleThreadScheduledExecutor 实现 2.1.5 顺序消息的实现 # 与定时消息同原理，生产者生产消息时指定特定的 MessageQueue ，消费者消费消息时，消费特定的 MessageQueue，其实单机版的消息中心在一个 MessageQueue 就天然支持了顺序消息 注意：同一个 MessageQueue 保证里面的消息是顺序消费的前提是：消费者是串行的消费该 MessageQueue，因为就算 MessageQueue 是顺序的，但是当并行消费时，还是会有顺序问题，但是串行消费也同时引入了两个问题： 引入锁来实现串行 前一个消费阻塞时后面都会被阻塞 2.1.6 分布式消息的实现 # 需要前置知识：2PC RocketMQ4.3 起支持，原理为2PC，即两阶段提交，prepared-\u0026gt;commit/rollback 生产者发送事务消息，假设该事务消息 Topic 为 Topic1-Trans，Broker 得到后首先更改该消息的 Topic 为 Topic1-Prepared，该 Topic1-Prepared 对消费者不可见。然后定时回调生产者的本地事务A执行状态，根据本地事务A执行状态，来是否将该消息修改为 Topic1-Commit 或 Topic1-Rollback，消费者就可以正常找到该事务消息或者不执行等 注意，就算是事务消息最后回滚了也不会物理删除，只会逻辑删除该消息\n2.1.7 消息的 push 实现 # 注意，RocketMQ 已经说了自己会有低延迟问题，其中就包括这个消息的 push 延迟问题 因为这并不是真正的将消息主动的推送到消费者，而是 Broker 定时任务每5s将消息推送到消费者 pull模式需要我们手动调用consumer拉消息，而push模式则只需要我们提供一个listener即可实现对消息的监听，而实际上，RocketMQ的push模式是基于pull模式实现的，它没有实现真正的push。 push方式里，consumer把轮询过程封装了，并注册MessageListener监听器，取到消息后，唤醒MessageListener的consumeMessage()来消费，对用户而言，感觉消息是被推送过来的。 2.1.8 消息重复发送的避免 # RocketMQ 会出现消息重复发送的问题，因为在网络延迟的情况下，这种问题不可避免的发生，如果非要实现消息不可重复发送，那基本太难，因为网络环境无法预知，还会使程序复杂度加大，因此默认允许消息重复发送 RocketMQ 让使用者在消费者端去解决该问题，即需要消费者端在消费消息时支持幂等性的去消费消息 最简单的解决方案是每条消费记录有个消费状态字段，根据这个消费状态字段来判断是否消费或者使用一个集中式的表，来存储所有消息的消费状态，从而避免重复消费 具体实现可以查询关于消息幂等消费的解决方案 2.1.9 广播消费与集群消费 # 消息消费区别：广播消费，订阅该 Topic 的消息者们都会消费每个消息。集群消费，订阅该 Topic 的消息者们只会有一个去消费某个消息 消息落盘区别：具体表现在消息消费进度的保存上。广播消费，由于每个消费者都独立的去消费每个消息，因此每个消费者各自保存自己的消息消费进度。而集群消费下，订阅了某个 Topic，而旗下又有多个 MessageQueue，每个消费者都可能会去消费不同的 MessageQueue，因此总体的消费进度保存在 Broker 上集中的管理 2.1.10 RocketMQ 不使用 ZooKeeper 作为注册中心的原因，以及自制的 NameServer 优缺点？ # ZooKeeper 作为支持顺序一致性的中间件，在某些情况下，它为了满足一致性，会丢失一定时间内的可用性，RocketMQ 需要注册中心只是为了发现组件地址，在某些情况下，RocketMQ 的注册中心可以出现数据不一致性，这同时也是 NameServer 的缺点，因为 NameServer 集群间互不通信，它们之间的注册信息可能会不一致 另外，当有新的服务器加入时，NameServer 并不会立马通知到 Producer，而是由 Producer 定时去请求 NameServer 获取最新的 Broker/Consumer 信息（这种情况是通过 Producer 发送消息时，负载均衡解决） 2.1.11 其它 # 加分项咯\n包括组件通信间使用 Netty 的自定义协议 消息重试负载均衡策略（具体参考 Dubbo 负载均衡策略） 消息过滤器（Producer 发送消息到 Broker，Broker 存储消息信息，Consumer 消费时请求 Broker 端从磁盘文件查询消息文件时,在 Broker 端就使用过滤服务器进行过滤） Broker 同步双写和异步双写中 Master 和 Slave 的交互 Broker 在 4.5.0 版本更新中引入了基于 Raft 协议的多副本选举，之前这是商业版才有的特性 ISSUE-1046 3 参考 # 《RocketMQ技术内幕》：https://blog.csdn.net/prestigeding/article/details/85233529 关于 RocketMQ 对 MappedByteBuffer 的一点优化：https://lishoubo.github.io/2017/09/27/MappedByteBuffer%E7%9A%84%E4%B8%80%E7%82%B9%E4%BC%98%E5%8C%96/ 十分钟入门RocketMQ：https://developer.aliyun.com/article/66101 分布式事务的种类以及 RocketMQ 支持的分布式消息：https://www.infoq.cn/article/2018/08/rocketmq-4.3-release 滴滴出行基于RocketMQ构建企业级消息队列服务的实践：https://yq.aliyun.com/articles/664608 基于《RocketMQ技术内幕》源码注释：https://github.com/LiWenGu/awesome-rocketmq "},{"id":84,"href":"/zh/docs/technology/Review/java_guide/lyely_high-performance/message-mq/rocketmq-intro/","title":"rocketmq介绍","section":"RocketMQ","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n消息队列扫盲 # 消息队列顾名思义就是存放消息的队列，队列我就不解释了，别告诉我你连队列都不知道是啥吧？\n所以问题并不是消息队列是什么，而是 消息队列为什么会出现？消息队列能用来干什么？用它来干这些事会带来什么好处？消息队列会带来副作用吗？\n# 消息队列为什么会出现？ # 消息队列算是作为后端程序员的一个必备技能吧，因为分布式应用必定涉及到各个系统之间的通信问题，这个时候消息队列也应运而生了。可以说分布式的产生是消息队列的基础，而分布式怕是一个很古老的概念了吧，所以消息队列也是一个很古老的中间件了。\n# 消息队列能用来干什么？ # # 异步 # 你可能会反驳我，应用之间的通信又不是只能由消息队列解决，好好的通信为什么中间非要插一个消息队列呢？我不能直接进行通信吗？\n很好👍，你又提出了一个概念，同步通信。就比如现在业界使用比较多的 Dubbo 就是一个适用于各个系统之间同步通信的 RPC 框架。\n我来举个🌰吧，比如我们有一个购票系统，需求是用户在购买完之后能接收到购买完成的短信。\n我们省略中间的网络通信时间消耗，假如购票系统处理需要 150ms ，短信系统处理需要 200ms ，那么整个处理流程的时间消耗就是 150ms + 200ms = 350ms。\n当然，乍看没什么问题。可是仔细一想你就感觉有点问题，我用户购票在购票系统的时候其实就已经完成了购买，而我现在通过同步调用非要让整个请求拉长时间，而短信系统这玩意又不是很有必要，它仅仅是一个辅助功能增强用户体验感而已。我现在整个调用流程就有点 头重脚轻 的感觉了，购票是一个不太耗时的流程，而我现在因为同步调用，非要等待发送短信这个比较耗时的操作才返回结果。那我如果再加一个发送邮件呢？\n这样整个系统的调用链又变长了，整个时间就变成了550ms。\n当我们在学生时代需要在食堂排队的时候，我们和食堂大妈就是一个同步的模型。\n我们需要告诉食堂大妈：“姐姐，给我加个鸡腿，再加个酸辣土豆丝，帮我浇点汁上去，多打点饭哦😋😋😋” 咦~~~ 为了多吃点，真恶心。\n然后大妈帮我们打饭配菜，我们看着大妈那颤抖的手和掉落的土豆丝不禁咽了咽口水。\n最终我们从大妈手中接过饭菜然后去寻找座位了\u0026hellip;\n回想一下，我们在给大妈发送需要的信息之后我们是 同步等待大妈给我配好饭菜 的，上面我们只是加了鸡腿和土豆丝，万一我再加一个番茄牛腩，韭菜鸡蛋，这样是不是大妈打饭配菜的流程就会变长，我们等待的时间也会相应的变长。\n那后来，我们工作赚钱了有钱去饭店吃饭了，我们告诉服务员来一碗牛肉面加个荷包蛋 (传达一个消息) ，然后我们就可以在饭桌上安心的玩手机了 (干自己其他事情) ，等到我们的牛肉面上了我们就可以吃了。这其中我们也就传达了一个消息，然后我们又转过头干其他事情了。这其中虽然做面的时间没有变短，但是我们只需要传达一个消息就可以干其他事情了，这是一个 异步 的概念。\n所以，为了解决这一个问题，聪明的程序员在中间也加了个类似于服务员的中间件——消息队列。这个时候我们就可以把模型给改造了。\n这样，我们在将消息存入消息队列之后我们就可以直接返回了(我们告诉服务员我们要吃什么然后玩手机)，所以整个耗时只是 150ms + 10ms = 160ms。\n但是你需要注意的是，整个流程的时长是没变的，就像你仅仅告诉服务员要吃什么是不会影响到做面的速度的。\n# 解耦 # 回到最初同步调用的过程，我们写个伪代码简单概括一下。\n那么第二步，我们又添加了一个发送邮件，我们就得重新去修改代码，如果我们又加一个需求：用户购买完还需要给他加积分，这个时候我们是不是又得改代码？\n如果你觉得还行，那么我这个时候不要发邮件这个服务了呢，我是不是又得改代码，又得重启应用？\n这样改来改去是不是很麻烦，那么 此时我们就用一个消息队列在中间进行解耦 。你需要注意的是，我们后面的发送短信、发送邮件、添加积分等一些操作都依赖于上面的 result ，这东西抽象出来就是购票的处理结果呀，比如订单号，用户账号等等，也就是说我们后面的一系列服务都是需要同样的消息来进行处理。既然这样，我们是不是可以通过 “广播消息” 来实现。\n我上面所讲的“广播”并不是真正的广播，而是接下来的系统作为消费者去 订阅 特定的主题。比如我们这里的主题就可以叫做 订票 ，我们购买系统作为一个生产者去生产这条消息放入消息队列，然后消费者订阅了这个主题，会从消息队列中拉取消息并消费。就比如我们刚刚画的那张图，你会发现，在生产者这边我们只需要关注 生产消息到指定主题中 ，而 消费者只需要关注从指定主题中拉取消息 就行了。\n如果没有消息队列，每当一个新的业务接入，我们都要在主系统调用新接口、或者当我们取消某些业务，我们也得在主系统删除某些接口调用。有了消息队列，我们只需要关心消息是否送达了队列，至于谁希望订阅，接下来收到消息如何处理，是下游的事情，无疑极大地减少了开发和联调的工作量。\n# 削峰（xue 1) # 我们再次回到一开始我们使用同步调用系统的情况，并且思考一下，如果此时有大量用户请求购票整个系统会变成什么样？\n如果，此时有一万的请求进入购票系统，我们知道运行我们主业务的服务器配置一般会比较好，所以这里我们假设购票系统能承受这一万的用户请求，那么也就意味着我们同时也会出现一万调用发短信服务的请求。而对于短信系统来说并不是我们的主要业务，所以我们配备的硬件资源并不会太高，那么你觉得现在这个短信系统能承受这一万的峰值么，且不说能不能承受，系统会不会 直接崩溃 了？\n短信业务又不是我们的主业务，我们能不能 折中处理 呢？如果我们把购买完成的信息发送到消息队列中，而短信系统 尽自己所能地去消息队列中取消息和消费消息 ，即使处理速度慢一点也无所谓，只要我们的系统没有崩溃就行了。\n留得江山在，还怕没柴烧？你敢说每次发送验证码的时候是一发你就收到了的么？\n# 消息队列能带来什么好处？ # 其实上面我已经说了。异步、解耦、削峰。 哪怕你上面的都没看懂也千万要记住这六个字，因为他不仅是消息队列的精华，更是编程和架构的精华。\n# 消息队列会带来副作用吗？ # 没有哪一门技术是“银弹”，消息队列也有它的副作用。\n比如，本来好好的两个系统之间的调用，我中间加了个消息队列，如果消息队列挂了怎么办呢？是不是 降低了系统的可用性 ？\n那这样是不是要保证HA(高可用)？是不是要搞集群？那么我 整个系统的复杂度是不是上升了 ？\n抛开上面的问题不讲，万一我发送方发送失败了，然后执行重试，这样就可能产生重复的消息。\n或者我消费端处理失败了，请求重发，这样也会产生重复的消息。\n对于一些微服务来说，消费重复消息会带来更大的麻烦，比如增加积分，这个时候我加了多次是不是对其他用户不公平？\n那么，又 如何解决重复消费消息的问题 呢？\n如果我们此时的消息需要保证严格的顺序性怎么办呢？比如生产者生产了一系列的有序消息(对一个id为1的记录进行删除增加修改)，但是我们知道在发布订阅模型中，对于主题是无顺序的，那么这个时候就会导致对于消费者消费消息的时候没有按照生产者的发送顺序消费，比如这个时候我们消费的顺序为修改删除增加，如果该记录涉及到金额的话是不是会出大事情？\n那么，又 如何解决消息的顺序消费问题 呢？\n就拿我们上面所讲的分布式系统来说，用户购票完成之后是不是需要增加账户积分？在同一个系统中我们一般会使用事务来进行解决，如果用 Spring 的话我们在上面伪代码中加入 @Transactional 注解就好了。但是在不同系统中如何保证事务呢？总不能这个系统我扣钱成功了你那积分系统积分没加吧？或者说我这扣钱明明失败了，你那积分系统给我加了积分。\n那么，又如何 解决分布式事务问题 呢？\n我们刚刚说了，消息队列可以进行削峰操作，那如果我的消费者如果消费很慢或者生产者生产消息很快，这样是不是会将消息堆积在消息队列中？\n那么，又如何 解决消息堆积的问题 呢？\n可用性降低，复杂度上升，又带来一系列的重复消费，顺序消费，分布式事务，消息堆积的问题，这消息队列还怎么用啊😵？\n别急，办法总是有的。\n# RocketMQ是什么？ # 原理 来源： https://www.bilibili.com/video/BV1GY4y1F7og\n哇，你个混蛋！上面给我抛出那么多问题，你现在又讲 RocketMQ ，还让不让人活了？！🤬\n别急别急，话说你现在清楚 MQ 的构造吗，我还没讲呢，我们先搞明白 MQ 的内部构造，再来看看如何解决上面的一系列问题吧，不过你最好带着问题去阅读和了解喔。\nRocketMQ 是一个 队列模型 的消息中间件，具有高性能、高可靠、高实时、分布式 的特点。它是一个采用 Java 语言开发的分布式的消息系统，由阿里巴巴团队开发，在2016年底贡献给 Apache，成为了 Apache 的一个顶级项目。 在阿里内部，RocketMQ 很好地服务了集团大大小小上千个应用，在每年的双十一当天，更有不可思议的万亿级消息通过 RocketMQ 流转。\n废话不多说，想要了解 RocketMQ 历史的同学可以自己去搜寻资料。听完上面的介绍，你只要知道 RocketMQ 很快、很牛、而且经历过双十一的实践就行了！\n# 队列模型和主题模型 # 在谈 RocketMQ 的技术架构之前，我们先来了解一下两个名词概念——队列模型 和 主题模型 。\n首先我问一个问题，消息队列为什么要叫消息队列？\n你可能觉得很弱智，这玩意不就是存放消息的队列嘛？不叫消息队列叫什么？\n的确，早期的消息中间件是通过 队列 这一模型来实现的，可能是历史原因，我们都习惯把消息中间件称为消息队列。\n但是，如今例如 RocketMQ 、Kafka 这些优秀的消息中间件不仅仅是通过一个 队列 来实现消息存储的。\n# 队列模型 # 就像我们理解队列一样，消息中间件的队列模型就真的只是一个队列。。。我画一张图给大家理解。\n在一开始我跟你提到了一个 “广播” 的概念，也就是说如果我们此时我们需要将一个消息发送给多个消费者(比如此时我需要将信息发送给短信系统和邮件系统)，这个时候单个队列即不能满足需求了。\n当然你可以让 Producer 生产消息放入多个队列中，然后每个队列去对应每一个消费者。问题是可以解决，创建多个队列并且复制多份消息是会很影响资源和性能的。而且，这样子就会导致生产者需要知道具体消费者个数然后去复制对应数量的消息队列，这就违背我们消息中间件的 解耦 这一原则。\n# 主题模型 # 那么有没有好的方法去解决这一个问题呢？有，那就是 主题模型 或者可以称为 发布订阅模型 。\n感兴趣的同学可以去了解一下设计模式里面的观察者模式并且手动实现一下，我相信你会有所收获的。\n在主题模型中，消息的生产者称为 发布者(Publisher) ，消息的消费者称为 订阅者(Subscriber) ，存放消息的容器称为 主题(Topic) 。\n其中，发布者将消息发送到指定主题中，订阅者需要 提前订阅主题 才能接受特定主题的消息。\n# RocketMQ中的消息模型 # RocketMQ 中的消息模型就是按照 主题模型 所实现的。你可能会好奇这个 主题 到底是怎么实现的呢？你上面也没有讲到呀！\n其实对于主题模型的实现来说每个消息中间件的底层设计都是不一样的，就比如 Kafka 中的 分区 ，RocketMQ 中的 队列 ，RabbitMQ 中的 Exchange 。我们可以理解为 主题模型/发布订阅模型 就是一个标准，那些中间件只不过照着这个标准去实现而已。\n所以，RocketMQ 中的 主题模型 到底是如何实现的呢？首先我画一张图，大家尝试着去理解一下。\n我们可以看到在整个图中有 Producer Group 、Topic 、Consumer Group 三个角色，我来分别介绍一下他们。\nProducer Group 生产者组： 代表某一类的生产者，比如我们有多个秒杀系统作为生产者，这多个合在一起就是一个 Producer Group 生产者组，它们一般生产相同的消息。 Consumer Group 消费者组： 代表某一类的消费者，比如我们有多个短信系统作为消费者，这多个合在一起就是一个 Consumer Group 消费者组，它们一般消费相同的消息。 Topic 主题： 代表一类消息，比如订单消息，物流消息等等。 你可以看到图中生产者组中的生产者会向主题发送消息，而 主题中存在多个队列，生产者每次生产消息之后是指定主题中的某个队列发送消息的。\nly：我的理解是，一般情况下，同一个生产者组生产的消息，会发到同一个topic中\n每个主题中都有多个队列(这里还不涉及到 Broker)，集群消费模式下，一个消费者集群多台机器共同消费一个 topic 的多个队列，一个队列只会被一个消费者消费。如果某个消费者挂掉，分组内其它消费者会接替挂掉的消费者继续消费。就像上图中 Consumer1 和 Consumer2 分别对应着两个队列，而 Consumer3 是没有队列对应的，所以一般来讲要控制 消费者组中的消费者个数和主题中队列个数相同 。\n当然也可以消费者个数小于队列个数，只不过不太建议。如下图。\n每个消费组在每个队列上维护一个消费位置 ，为什么呢？\n因为我们刚刚画的仅仅是一个消费者组，我们知道在发布订阅模式中一般会涉及到多个消费者组，而每个消费者组在每个队列中的消费位置都是不同的。如果此时有多个消费者组，那么消息被一个消费者组消费完之后是不会删除的(因为其它消费者组也需要呀 【注意重点，是消费完之后】)，它仅仅是为每个消费者组维护一个 消费位移(offset) ，每次消费者组消费完会返回一个成功的响应，然后队列再把维护的消费位移加一，这样就不会出现刚刚消费过的消息再一次被消费了。\n可能你还有一个问题，为什么一个主题中需要维护多个队列 ？\n答案是 提高并发能力 。的确，每个主题中只存在一个队列也是可行的。你想一下，如果每个主题中只存在一个队列，这个队列中也维护着每个消费者组的消费位置，这样也可以做到 发布订阅模式 。如下图。\n但是，这样我生产者是不是只能向一个队列发送消息？又因为需要维护消费位置所以一个队列只能对应一个消费者组中的消费者，这样是不是其他的 Consumer 就没有用武之地了？从这两个角度来讲，并发度一下子就小了很多。\n所以总结来说，RocketMQ 通过使用在一个 Topic 中配置多个队列并且每个队列维护每个消费者组的消费位置 实现了 主题模式/发布订阅模式 。\n# RocketMQ的架构图 # 讲完了消息模型，我们理解起 RocketMQ 的技术架构起来就容易多了。\nRocketMQ 技术架构中有四大角色 NameServer 、Broker 、Producer 、Consumer 。我来向大家分别解释一下这四个角色是干啥的。\nBroker： 主要负责消息的存储、投递和查询以及服务高可用保证。说白了就是消息队列服务器嘛，生产者生产消息到 Broker ，消费者从 Broker 拉取消息并消费。\n这里，我还得普及一下关于 Broker 、Topic 和 队列的关系。上面我讲解了 Topic 和队列的关系——一个 Topic 中存在多个队列，那么这个 Topic 和队列存放在哪呢？\n一个 Topic 分布在多个 Broker上，一个 Broker 可以配置多个 Topic ，它们是多对多的关系。\n如果某个 Topic 消息量很大，应该给它多配置几个队列(上文中提到了提高并发能力)，并且 尽量多分布在不同 Broker 上，以减轻某个 Broker 的压力 。\nTopic 消息量都比较均匀的情况下，如果某个 broker 上的队列越多，则该 broker 压力越大。\n感觉下面这个图有一点点误解，就是Queue 重复（比如多个Queue），我觉得同一个Queue不会分布在多个Topic上面的\n所以说我们需要配置多个Broker。\nNameServer： 不知道你们有没有接触过 ZooKeeper 和 Spring Cloud 中的 Eureka ，它其实也是一个 注册中心 ，主要提供两个功能：Broker管理 和 路由信息管理 。说白了就是 Broker 会将自己的信息注册到 NameServer 中，此时 NameServer 就存放了很多 Broker 的信息(Broker的路由表)，消费者和生产者就从 NameServer 中获取路由表然后照着路由表的信息和对应的 Broker 进行通信(生产者和消费者定期会向 NameServer 去查询相关的 Broker 的信息)。\nProducer： 消息发布的角色，支持分布式集群方式部署。说白了就是生产者。\nConsumer： 消息消费的角色，支持分布式集群方式部署。支持以push推，pull拉两种模式对消息进行消费。同时也支持集群方式和广播方式的消费，它提供实时消息订阅机制。说白了就是消费者。\n听完了上面的解释你可能会觉得，这玩意好简单。不就是这样的么？\n嗯？你可能会发现一个问题，这老家伙 NameServer 干啥用的，这不多余吗？直接 Producer 、Consumer 和 Broker 直接进行生产消息，消费消息不就好了么？\n但是，我们上文提到过 Broker 是需要保证高可用的，如果整个系统仅仅靠着一个 Broker 来维持的话，那么这个 Broker 的压力会不会很大？所以我们需要使用多个 Broker 来保证 负载均衡 。\n如果说，我们的消费者和生产者直接和多个 Broker 相连，那么当 Broker 修改的时候必定会牵连着每个生产者和消费者，这样就会产生耦合问题，而 NameServer 注册中心就是用来解决这个问题的。\n如果还不是很理解的话，可以去看我介绍 Spring Cloud 的那篇文章，其中介绍了 Eureka 注册中心。\n当然，RocketMQ 中的技术架构肯定不止前面那么简单，因为上面图中的四个角色都是需要做集群的。我给出一张官网的架构图，大家尝试理解一下。\n其实和我们最开始画的那张乞丐版的架构图也没什么区别，主要是一些细节上的差别。听我细细道来🤨。\n第一、我们的 Broker 做了集群并且还进行了主从部署 ，由于消息分布在各个 Broker 上，一旦某个 Broker 宕机，则该Broker 上的消息读写都会受到影响。所以 Rocketmq 提供了 master/slave 的结构， salve 定时从 master 同步数据(同步刷盘或者异步刷盘)，如果 master 宕机，则 slave 提供消费服务，但是不能写入消息 (后面我还会提到哦)。\n第二、为了保证 HA ，我们的 NameServer 也做了集群部署，但是请注意它是 去中心化 的。也就意味着它没有主节点，你可以很明显地看出 NameServer 的所有节点是没有进行 Info Replicate 的，在 RocketMQ 中是通过 单个Broker和所有NameServer保持长连接 ，并且在每隔30秒 Broker 会向所有 Nameserver 发送心跳，心跳包含了自身的 Topic 配置信息，这个步骤就对应这上面的 Routing Info 。\n第三、在生产者需要向 Broker 发送消息的时候，需要先从 NameServer 获取关于 Broker 的路由信息，然后通过 轮询 的方法去向每个队列中生产数据以达到 负载均衡 的效果。\n第四、消费者通过 NameServer 获取所有 Broker 的路由信息后，向 Broker 发送 Pull 请求来获取消息数据。Consumer 可以以两种模式启动—— 广播（Broadcast）和集群（Cluster）。广播模式下，一条消息会发送给 同一个消费组中的所有消费者 ，集群模式下消息只会发送给一个消费者。\n# 如何解决 顺序消费、重复消费 # 其实，这些东西都是我在介绍消息队列带来的一些副作用的时候提到的，也就是说，这些问题不仅仅挂钩于 RocketMQ ，而是应该每个消息中间件都需要去解决的。\n在上面我介绍 RocketMQ 的技术架构的时候我已经向你展示了 它是如何保证高可用的 ，这里不涉及运维方面的搭建，如果你感兴趣可以自己去官网上照着例子搭建属于你自己的 RocketMQ 集群。\n其实 Kafka 的架构基本和 RocketMQ 类似，只是它注册中心使用了 Zookeeper 、它的 分区 就相当于 RocketMQ 中的 队列 。还有一些小细节不同会在后面提到。\n# 顺序消费 # 在上面的技术架构介绍中，我们已经知道了 RocketMQ 在主题上是无序的、它只有在队列层面才是保证有序 的。\n这又扯到两个概念——普通顺序 和 严格顺序 。\n所谓普通顺序是指 消费者通过 同一个消费队列收到的消息是有顺序的 ，不同消息队列收到的消息则可能是无顺序的。普通顺序消息在 Broker 重启情况下不会保证消息顺序性 (短暂时间) 。\n所谓严格顺序是指 消费者收到的 所有消息 均是有顺序的。严格顺序消息 即使在异常情况下也会保证消息的顺序性 。\n但是，严格顺序看起来虽好，实现它可会付出巨大的代价。如果你使用严格顺序模式，Broker 集群中只要有一台机器不可用，则整个集群都不可用。你还用啥？现在主要场景也就在 binlog 同步。\n一般而言，我们的 MQ 都是能容忍短暂的乱序，所以推荐使用普通顺序模式。\n那么，我们现在使用了 普通顺序模式 ，我们从上面学习知道了在 Producer 生产消息的时候会进行轮询(取决你的负载均衡策略)来向同一主题的不同消息队列发送消息。那么如果此时我有几个消息分别是同一个订单的创建、支付、发货，在轮询的策略下这 三个消息会被发送到不同队列 ，因为在不同的队列此时就无法使用 RocketMQ 带来的队列有序特性来保证消息有序性了。\n那么，怎么解决呢？\n其实很简单，我们需要处理的仅仅是将同一语义下的消息放入同一个队列(比如这里是同一个订单)，那我们就可以使用 Hash取模法 来保证同一个订单在同一个队列中就行了。\n# 重复消费 # emmm，就两个字—— 幂等 。在编程中一个幂等 操作的特点是其任意多次执行所产生的影响均与一次执行的影响相同。比如说，这个时候我们有一个订单的处理积分的系统，每当来一个消息的时候它就负责为创建这个订单的用户的积分加上相应的数值。可是有一次，消息队列发送给订单系统 FrancisQ 的订单信息，其要求是给 FrancisQ 的积分加上 500。但是积分系统在收到 FrancisQ 的订单信息处理完成之后返回给消息队列处理成功的信息的时候出现了网络波动(当然还有很多种情况，比如Broker意外重启等等)，这条回应没有发送成功。\n那么，消息队列没收到积分系统的回应会不会尝试重发这个消息？问题就来了，我再发这个消息，万一它又给 FrancisQ 的账户加上 500 积分怎么办呢？\n所以我们需要给我们的消费者实现 幂等 ，也就是对同一个消息的处理结果，执行多少次都不变。\n那么如何给业务实现幂等呢？这个还是需要结合具体的业务的。你可以使用 写入 Redis 来保证，因为 Redis 的 key 和 value 就是天然支持幂等的（如果mq处理过就给redis设置值，而后每次mq处理之前查询一下redis才知道mq是否已经处理过）。当然还有使用 数据库插入法 ，基于数据库的唯一键来保证重复数据不会被插入多条。\n不过最主要的还是需要 根据特定场景使用特定的解决方案 ，你要知道你的消息消费是否是完全不可重复消费还是可以忍受重复消费的，然后再选择强校验和弱校验的方式。毕竟在 CS 领域还是很少有技术银弹的说法。\n而在整个互联网领域，幂等不仅仅适用于消息队列的重复消费问题，这些实现幂等的方法，也同样适用于，在其他场景中来解决重复请求或者重复调用的问题 。比如将HTTP服务设计成幂等的，解决前端或者APP重复提交表单数据的问题 ，也可以将一个微服务设计成幂等的，解决 RPC 框架自动重试导致的 重复调用问题 。\n# 分布式事务 # 如何解释分布式事务呢？事务大家都知道吧？要么都执行要么都不执行 。在同一个系统中我们可以轻松地实现事务，但是在分布式架构中，我们有很多服务是部署在不同系统之间的，而不同服务之间又需要进行调用。比如此时我下订单然后增加积分，如果保证不了分布式事务的话，就会出现A系统下了订单，但是B系统增加积分失败或者A系统没有下订单，B系统却增加了积分。前者对用户不友好，后者对运营商不利，这是我们都不愿意见到的。\n那么，如何去解决这个问题呢？\n如今比较常见的分布式事务实现有 2PC、TCC 和事务消息(half 半消息机制)。每一种实现都有其特定的使用场景，但是也有各自的问题，都不是完美的解决方案。\n在 RocketMQ 中使用的是 事务消息加上事务反查机制 来解决分布式事务问题的。我画了张图，大家可以对照着图进行理解。\n在第一步发送的 half 消息 ，它的意思是 在事务提交之前，对于消费者来说，这个消息是不可见的 。\n那么，如何做到写入消息但是对用户不可见呢？RocketMQ事务消息的做法是：如果消息是half消息，将备份原消息的主题与消息消费队列，然后 改变主题 为RMQ_SYS_TRANS_HALF_TOPIC。由于消费组未订阅该主题，故消费端无法消费half类型的消息，然后RocketMQ会开启一个定时任务，从Topic为RMQ_SYS_TRANS_HALF_TOPIC中拉取消息进行消费，根据生产者组获取一个服务提供者发送回查事务状态请求，根据事务状态来决定是提交或回滚消息。\n你可以试想一下，如果没有从第5步开始的 事务反查机制 ，如果出现网路波动第4步没有发送成功，这样就会产生 MQ 不知道是不是需要给消费者消费的问题，他就像一个无头苍蝇一样。在 RocketMQ 中就是使用的上述的事务反查来解决的，而在 Kafka 中通常是直接抛出一个异常让用户来自行解决。\n你还需要注意的是，在 MQ Server 指向系统B的操作已经和系统A不相关了，也就是说在消息队列中的分布式事务是——本地事务和存储消息到消息队列才是同一个事务。这样也就产生了事务的最终一致性，因为整个过程是异步的，每个系统只要保证它自己那一部分的事务就行了。 # 消息堆积问题 # 在上面我们提到了消息队列一个很重要的功能——削峰 。那么如果这个峰值太大了导致消息堆积在队列中怎么办呢？\n其实这个问题可以将它广义化，因为产生消息堆积的根源其实就只有两个——生产者生产太快或者消费者消费太慢。\n我们可以从多个角度去思考解决这个问题，当流量到峰值的时候是因为生产者生产太快，我们可以使用一些 限流降级 的方法，当然你也可以增加多个消费者实例去水平扩展增加消费能力来匹配生产的激增。如果消费者消费过慢的话，我们可以先检查 是否是消费者出现了大量的消费错误 ，或者打印一下日志查看是否是哪一个线程卡死，出现了锁资源不释放等等的问题。\n当然，最快速解决消息堆积问题的方法还是增加消费者实例，不过 同时你还需要增加每个主题的队列数量 。\n别忘了在 RocketMQ 中，一个队列只会被一个消费者消费 ，如果你仅仅是增加消费者实例就会出现我一开始给你画架构图的那种情况。\n# 回溯消费 # 回溯消费是指 Consumer 已经消费成功的消息，由于业务上需求需要重新消费，在RocketMQ 中， Broker 在向Consumer 投递成功消息后，消息仍然需要保留 。并且重新消费一般是按照时间维度，例如由于 Consumer 系统故障，恢复后需要重新消费1小时前的数据，那么 Broker 要提供一种机制，可以按照时间维度来回退消费进度。RocketMQ 支持按照时间回溯消费，时间维度精确到毫秒。\n这是官方文档的解释，我直接照搬过来就当科普了😁😁😁。\n# RocketMQ 的刷盘机制 # 上面我讲了那么多的 RocketMQ 的架构和设计原理，你有没有好奇\n在 Topic 中的 队列是以什么样的形式存在的？\n队列中的消息又是如何进行存储持久化的呢？\n我在上文中提到的 同步刷盘 和 异步刷盘 又是什么呢？它们会给持久化带来什么样的影响呢？\n下面我将给你们一一解释。\n# 同步刷盘和异步刷盘 # 如上图所示，在同步刷盘中需要等待一个刷盘成功的 ACK ，同步刷盘对 MQ 消息可靠性来说是一种不错的保障，但是 性能上会有较大影响 ，一般地适用于金融等特定业务场景。\n而异步刷盘往往是开启一个线程去异步地执行刷盘操作。消息刷盘采用后台异步线程提交的方式进行， 降低了读写延迟 ，提高了 MQ 的性能和吞吐量，一般适用于如发验证码等对于消息保证要求不太高的业务场景。\n一般地，异步刷盘只有在 Broker 意外宕机的时候会丢失部分数据，你可以设置 Broker 的参数 FlushDiskType 来调整你的刷盘策略(ASYNC_FLUSH 或者 SYNC_FLUSH)。\n# 同步复制和异步复制 # 上面的同步刷盘和异步刷盘是在单个结点层面的，而同步复制和异步复制主要是指的 Borker 主从模式下，主节点返回消息给客户端的时候是否需要同步从节点。\n同步复制： 也叫 “同步双写”，也就是说，只有消息同步双写到主从节点上时才返回写入成功 。 异步复制： 消息写入主节点之后就直接返回写入成功 。 然而，很多事情是没有完美的方案的，就比如我们进行消息写入的节点越多就更能保证消息的可靠性，但是随之的性能也会下降，所以需要程序员根据特定业务场景去选择适应的主从复制方案。\n那么，异步复制会不会也像异步刷盘那样影响消息的可靠性呢？\n答案是不会的，因为两者就是不同的概念，对于消息可靠性是通过不同的刷盘策略保证的，而像异步同步复制策略仅仅是影响到了 可用性 。为什么呢？其主要原因是 RocketMQ 是不支持自动主从切换的，当主节点挂掉之后，生产者就不能再给这个主节点生产消息了。\n比如这个时候采用异步复制的方式，在主节点还未发送完需要同步的消息的时候主节点挂掉了，这个时候从节点就少了一部分消息。但是此时生产者无法再给主节点生产消息了，消费者可以自动切换到从节点进行消费(仅仅是消费)，所以在主节点挂掉的时间只会产生主从结点短暂的消息不一致的情况，降低了可用性，而当主节点重启之后，从节点那部分未来得及复制的消息还会继续复制。\n在单主从架构中，如果一个主节点挂掉了，那么也就意味着整个系统不能再生产了。那么这个可用性的问题能否解决呢？一个主从不行那就多个主从的呗，别忘了在我们最初的架构图中，每个 Topic 是分布在不同 Broker 中的。\n但是这种复制方式同样也会带来一个问题，那就是无法保证 严格顺序 。在上文中我们提到了如何保证的消息顺序性是通过将一个语义的消息发送在同一个队列中，使用 Topic 下的队列来保证顺序性的。如果此时我们主节点A负责的是订单A的一系列语义消息，然后它挂了，这样其他节点是无法代替主节点A的，如果我们任意节点都可以存入任何消息，那就没有顺序性可言了。\n而在 RocketMQ 中采用了 Dledger 解决这个问题。他要求在写入消息的时候，要求至少消息复制到半数以上的节点之后，才给客⼾端返回写⼊成功，并且它是⽀持通过选举来动态切换主节点的。这里我就不展开说明了，读者可以自己去了解。\n也不是说 Dledger 是个完美的方案，至少在 Dledger 选举过程中是无法提供服务的，而且他必须要使用三个节点或以上，如果多数节点同时挂掉他也是无法保证可用性的，而且要求消息复制半数以上节点的效率和直接异步复制还是有一定的差距的。\n# 存储机制 # ly：详细的有点复杂，暂时跳过。大概就是有三个东西，CommitLog（实际存储消息的东西），ConsumeQueue（相当于CommitLog的索引）\n还记得上面我们一开始的三个问题吗？到这里第三个问题已经解决了。\n但是，在 Topic 中的 队列是以什么样的形式存在的？队列中的消息又是如何进行存储持久化的呢？ 还未解决，其实这里涉及到了 RocketMQ 是如何设计它的存储结构了。我首先想大家介绍 RocketMQ 消息存储架构中的三大角色——CommitLog 、ConsumeQueue 和 IndexFile 。\nCommitLog： 消息主体以及元数据的存储主体，存储 Producer 端写入的消息主体内容,消息内容不是定长的。单个文件大小默认1G ，文件名长度为20位，左边补零，剩余为起始偏移量，比如00000000000000000000代表了第一个文件，起始偏移量为0，文件大小为1G=1073741824；当第一个文件写满了，第二个文件为00000000001073741824，起始偏移量为1073741824，以此类推。消息主要是顺序写入日志文件，当文件满了，写入下一个文件。\nConsumeQueue： 消息消费队列，引入的目的主要是提高消息消费的性能(我们再前面也讲了)，由于RocketMQ 是基于主题 Topic 的订阅模式，消息消费是针对主题进行的，如果要遍历 commitlog 文件中根据 Topic 检索消息是非常低效的。Consumer 即可根据 ConsumeQueue 来查找待消费的消息。其中，ConsumeQueue（逻辑消费队列）作为消费消息的索引，保存了指定 Topic 下的队列消息在 CommitLog 中的起始物理偏移量 offset *，消息大小 size 和消息 Tag 的 HashCode 值。*consumequeue 文件可以看成是基于 topic 的 commitlog 索引文件，故 consumequeue 文件夹的组织方式如下：topic/queue/file三层组织结构，具体存储路径为：$HOME/store/consumequeue/{topic}/{queueId}/{fileName}。同样 consumequeue 文件采取定长设计，每一个条目共20个字节，分别为8字节的 commitlog 物理偏移量、4字节的消息长度、8字节tag hashcode，单个文件由30W个条目组成，可以像数组一样随机访问每一个条目，每个 ConsumeQueue文件大小约5.72M；\n保存了指定 Topic 下的队列消息在 CommitLog 中的**起始物理偏移量 offset **，消息大小 size 和消息 Tag 的 HashCode 值\nIndexFile： IndexFile（索引文件）提供了一种可以通过key或时间区间来查询消息的方法。这里只做科普不做详细介绍。\n总结来说，整个消息存储的结构，最主要的就是 CommitLoq 和 ConsumeQueue 。而 ConsumeQueue 你可以大概理解为 Topic 中的队列。\n我的理解是，通过ConsumeQueue files去查询CommitLog\nRocketMQ 采用的是 混合型的存储结构 ，即为 Broker 单个实例下所有的队列共用一个日志数据文件来存储消息。有意思的是在同样高并发的 Kafka 中会为每个 Topic 分配一个存储文件。这就有点类似于我们有一大堆书需要装上书架，RockeMQ 是不分书的种类直接成批的塞上去的，而 Kafka 是将书本放入指定的分类区域的。\n而 RocketMQ 为什么要这么做呢？原因是 提高数据的写入效率 ，不分 Topic 意味着我们有更大的几率获取 成批 的消息进行数据写入，但也会带来一个麻烦就是读取消息的时候需要遍历整个大文件，这是非常耗时的。\n所以，在 RocketMQ 中又使用了 ConsumeQueue 作为每个队列的索引文件来 提升读取消息的效率。我们可以直接根据队列的消息序号，计算出索引的全局位置（索引序号*索引固定⻓度20），然后直接读取这条索引，再根据索引中记录的消息的全局位置，找到消息。\n讲到这里，你可能对 RockeMQ 的存储架构还有些模糊，没事，我们结合着图来理解一下。\nemmm，是不是有一点复杂🤣，看英文图片和英文文档的时候就不要怂，硬着头皮往下看就行。\n如果上面没看懂的读者一定要认真看下面的流程分析！\n首先，在最上面的那一块就是我刚刚讲的你现在可以直接 把 ConsumerQueue 理解为 Queue。\n在图中最左边说明了红色方块代表被写入的消息，虚线方块代表等待被写入的。左边的生产者发送消息会指定 Topic 、QueueId 和具体消息内容，而在 Broker 中管你是哪门子消息，他直接 全部顺序存储到了 CommitLog。而根据生产者指定的 Topic 和 QueueId 将这条消息本身在 CommitLog 的偏移(offset)，消息本身大小，和tag的hash值存入对应的 ConsumeQueue 索引文件中。而在每个队列中都保存了 ConsumeOffset 即每个消费者组的消费位置(我在架构那里提到了，忘了的同学可以回去看一下)，而消费者拉取消息进行消费的时候只需要根据 ConsumeOffset 获取下一个未被消费的消息就行了。\n上述就是我对于整个消息存储架构的大概理解(这里不涉及到一些细节讨论，比如稀疏索引等等问题)，希望对你有帮助。\n因为有一个知识点因为写嗨了忘讲了，想想在哪里加也不好，所以我留给大家去思考🤔🤔一下吧。\n为什么 CommitLog 文件要设计成固定大小的长度呢？提醒：内存映射机制。\n# 总结 # 总算把这篇博客写完了。我讲的你们还记得吗😅？\n这篇文章中我主要想大家介绍了\n消息队列出现的原因 消息队列的作用(异步，解耦，削峰) 消息队列带来的一系列问题(消息堆积、重复消费、顺序消费、分布式事务等等) 消息队列的两种消息模型——队列和主题模式 分析了 RocketMQ 的技术架构(NameServer 、Broker 、Producer 、Comsumer) 结合 RocketMQ 回答了消息队列副作用的解决方案 介绍了 RocketMQ 的存储机制和刷盘策略。 等等。。。\n如果喜欢可以点赞哟👍👍👍。\n著作权归所有 原文链接：https://javaguide.cn/high-performance/message-queue/rocketmq-intro.html\n"},{"id":85,"href":"/zh/docs/technology/Review/java_guide/lyely_high-performance/message-mq/base/","title":"message-queue","section":"RocketMQ","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n“RabbitMQ？”“Kafka？”“RocketMQ？”\u0026hellip;在日常学习与开发过程中，我们常常听到消息队列这个关键词。我也在我的多篇文章中提到了这个概念。可能你是熟练使用消息队列的老手，又或者你是不懂消息队列的新手，不论你了不了解消息队列，本文都将带你搞懂消息队列的一些基本理论。\n如果你是老手，你可能从本文学到你之前不曾注意的一些关于消息队列的重要概念，如果你是新手，相信本文将是你打开消息队列大门的一板砖。\n什么是消息队列？ # 我们可以把消息队列看作是一个存放消息的容器，当我们需要使用消息的时候，直接从容器中取出消息供自己使用即可。由于队列 Queue 是一种先进先出的数据结构，所以消费消息时也是按照顺序来消费的。\n参与消息传递的双方称为生产者和消费者，生产者负责发送消息，消费者负责处理消息。\n我们知道操作系统中的进程通信的一种很重要的方式就是消息队列。我们这里提到的消息队列稍微有点区别，更多指的是各个服务以及系统内部各个组件/模块之前的通信，属于一种中间件。\n随着分布式和微服务系统的发展，消息队列在系统设计中有了更大的发挥空间，使用消息队列可以降低系统耦合性、实现任务异步、有效地进行流量削峰，是分布式和微服务系统中重要的组件之一。\n消息队列有什么用？ # 通常来说，使用消息队列能为我们的系统带来下面三点好处：\n通过异步处理提高系统性能（减少响应所需时间）。 削峰/限流 降低系统耦合性。 如果在面试的时候你被面试官问到这个问题的话，一般情况是你在你的简历上涉及到消息队列这方面的内容，这个时候推荐你结合你自己的项目来回答。\n通过异步处理提高系统性能（减少响应所需时间） # 将用户的请求数据存储到消息队列之后就立即返回结果。随后，系统再对消息进行消费。\n因为用户请求数据写入消息队列之后就立即返回给用户了，但是请求数据在后续的业务校验、写数据库等操作中可能失败。因此，使用消息队列进行异步处理之后，需要适当修改业务流程进行配合，比如用户在提交订单之后，订单数据写入消息队列，不能立即返回用户订单提交成功，需要在消息队列的订单消费者进程真正处理完该订单之后，甚至出库后，再通过电子邮件或短信通知用户订单成功，以免交易纠纷。这就类似我们平时手机订火车票和电影票。\n削峰/限流 # 先将短时间高并发产生的事务消息存储在消息队列中，然后后端服务再慢慢根据自己的能力去消费这些消息，这样就避免直接把后端服务打垮掉。\n举例：在电子商务一些秒杀、促销活动中，合理使用消息队列可以有效抵御促销活动刚开始大量订单涌入对系统的冲击。如下图所示：\n降低系统耦合性 # 使用消息队列还可以降低系统耦合性。我们知道如果模块之间不存在直接调用，那么新增模块或者修改模块就对其他模块影响较小，这样系统的可扩展性无疑更好一些。还是直接上图吧：\n生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合，这显然也提高了系统的扩展性。\n消息队列使用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。\n消息接受者对消息进行过滤、处理、包装后，构造成一个新的消息类型，将消息继续发送出去，等待其他消息接受者订阅该消息。因此基于事件（消息对象）驱动的业务架构可以是一系列流程。\n另外，为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。\n备注： 不要认为消息队列只能利用发布-订阅模式工作，只不过在解耦这个特定业务环境下是使用发布-订阅模式的。除了发布-订阅模式，还有点对点订阅模式（一个消息只有一个消费者），我们比较常用的是发布-订阅模式。另外，这两种消息模型是 JMS 提供的，AMQP 协议还提供了 5 种消息模型。\n使用消息队列哪些问题？ # 系统可用性降低： 系统可用性在某种程度上降低，为什么这样说呢？在加入 MQ 之前，你不用考虑消息丢失或者说 MQ 挂掉等等的情况，但是，引入 MQ 之后你就需要去考虑了！ 系统复杂性提高： 加入 MQ 之后，你需要保证消息没有被重复消费、处理消息丢失的情况、保证消息传递的顺序性等等问题！ 一致性问题： 我上面讲了消息队列可以实现异步，消息队列带来的异步确实可以提高系统响应速度。但是，万一消息的真正消费者并没有正确消费消息怎么办？这样就会导致数据不一致的情况了! JMS 和 AMQP # JMS 是什么？ # JMS（JAVA Message Service,java 消息服务）是 java 的消息服务，JMS 的客户端之间可以通过 JMS 服务进行异步的消息传输。JMS（JAVA Message Service，Java 消息服务）API 是一个消息服务的标准或者说是规范，允许应用程序组件基于 JavaEE 平台创建、发送、接收和读取消息。它使分布式通信耦合度更低，消息服务更加可靠以及异步性。\nJMS 定义了五种不同的消息正文格式以及调用的消息类型，允许你发送并接收以一些不同形式的数据：\nStreamMessage：Java 原始值的数据流 MapMessage：一套名称-值对 TextMessage：一个字符串对象 ObjectMessage：一个序列化的 Java 对象 BytesMessage：一个字节的数据流 ActiveMQ（已被淘汰） 就是基于 JMS 规范实现的。\nJMS 两种消息模型 # 点到点（P2P）模型 # 使用**队列（Queue）*作为消息通信载体；满足*生产者与消费者模式，一条消息只能被一个消费者使用，未被消费的消息在队列中保留直到被消费或超时。比如：我们生产者发送 100 条消息的话，两个消费者来消费一般情况下两个消费者会按照消息发送的顺序各自消费一半（也就是你一个我一个的消费。）\n发布/订阅（Pub/Sub）模型 # 发布订阅模型（Pub/Sub） 使用**主题（Topic）*作为消息通信载体，类似于*广播模式；发布者发布一条消息，该消息通过主题传递给所有的订阅者，在一条消息广播之后才订阅的用户则是收不到该条消息的。\nAMQP 是什么？ # AMQP，即 Advanced Message Queuing Protocol，一个提供统一消息服务的应用层标准 高级消息队列协议（二进制应用层协议），是应用层协议的一个开放标准，为面向消息的中间件设计，兼容 JMS。基于此协议的客户端与消息中间件可传递消息，并不受客户端/中间件同产品，不同的开发语言等条件的限制。\nRabbitMQ 就是基于 AMQP 协议实现的。\nJMS vs AMQP # 对比方向 JMS AMQP 定义 Java API 协议 跨语言 否 是 跨平台 否 是 支持消息类型 提供两种消息模型：①Peer-2-Peer;②Pub/sub 提供了五种消息模型：①direct exchange；②fanout exchange；③topic change；④headers exchange；⑤system exchange。本质来讲，后四种和 JMS 的 pub/sub 模型没有太大差别，仅是在路由机制上做了更详细的划分； 支持消息类型 支持多种消息类型 ，我们在上面提到过 byte[]（二进制） 总结：\nAMQP 为消息定义了线路层（wire-level protocol）的协议，而 JMS 所定义的是 API 规范。在 Java 体系中，多个 client 均可以通过 JMS 进行交互，不需要应用修改代码，但是其对跨平台的支持较差。而 AMQP 天然具有跨平台、跨语言特性。 JMS 支持 TextMessage、MapMessage 等复杂的消息类型；而 AMQP 仅支持 byte[] 消息类型（复杂的类型可序列化后发送）。 由于 Exchange 提供的路由算法，AMQP 可以提供多样化的路由方式来传递消息到消息队列，而 JMS 仅支持 队列 和 主题/订阅 方式两种。 消息队列技术选型 # 常见的消息队列有哪些？ # Kafka # Kafka 是 LinkedIn 开源的一个分布式流式处理平台，已经成为 Apache 顶级项目，早期被用来用于处理海量的日志，后面才慢慢发展成了一款功能全面的高性能消息队列。\n流式处理平台具有三个关键功能：\n消息队列：发布和订阅消息流，这个功能类似于消息队列，这也是 Kafka 也被归类为消息队列的原因。 容错的持久方式存储记录消息流： Kafka 会把消息持久化到磁盘，有效避免了消息丢失的风险。 流式处理平台： 在消息发布的时候进行处理，Kafka 提供了一个完整的流式处理类库。 Kafka 是一个分布式系统，由通过高性能 TCP 网络协议进行通信的服务器和客户端组成，可以部署在在本地和云环境中的裸机硬件、虚拟机和容器上。\n在 Kafka 2.8 之前，Kafka 最被大家诟病的就是其重度依赖于 Zookeeper 做元数据管理和集群的高可用。在 Kafka 2.8 之后，引入了基于 Raft 协议的 KRaft 模式，不再依赖 Zookeeper，大大简化了 Kafka 的架构，让你可以以一种轻量级的方式来使用 Kafka。\n不过，要提示一下：如果要使用 KRaft 模式的话，建议选择较高版本的 Kafka，因为这个功能还在持续完善优化中。Kafka 3.3.1 版本是第一个将 KRaft（Kafka Raft）共识协议标记为生产就绪的版本。\nKafka 官网：http://kafka.apache.org/\nKafka 更新记录（可以直观看到项目是否还在维护）：https://kafka.apache.org/downloads\nRocketMQ # RocketMQ 是阿里开源的一款云原生“消息、事件、流”实时数据处理平台，借鉴了 Kafka，已经成为 Apache 顶级项目。\nRocketMQ 的核心特性（摘自 RocketMQ 官网）：\n云原生：生与云，长与云，无限弹性扩缩，K8s 友好 高吞吐：万亿级吞吐保证，同时满足微服务与大数据场景。 流处理：提供轻量、高扩展、高性能和丰富功能的流计算引擎。 金融级：金融级的稳定性，广泛用于交易核心链路。 架构极简：零外部依赖，Shared-nothing 架构。 生态友好：无缝对接微服务、实时计算、数据湖等周边生态。 根据官网介绍：\nApache RocketMQ 自诞生以来，因其架构简单、业务功能丰富、具备极强可扩展性等特点被众多企业开发者以及云厂商广泛采用。历经十余年的大规模场景打磨，RocketMQ 已经成为业内共识的金融级可靠业务消息首选方案，被广泛应用于互联网、大数据、移动互联网、物联网等领域的业务场景。\nRocketMQ 官网：https://rocketmq.apache.org/ （文档很详细，推荐阅读）\nRocketMQ 更新记录（可以直观看到项目是否还在维护）：https://github.com/apache/rocketmq/releases\nRabbitMQ # RabbitMQ 是采用 Erlang 语言实现 AMQP(Advanced Message Queuing Protocol，高级消息队列协议）的消息中间件，它最初起源于金融系统，用于在分布式系统中存储转发消息。\nRabbitMQ 发展到今天，被越来越多的人认可，这和它在易用性、扩展性、可靠性和高可用性等方面的卓著表现是分不开的。RabbitMQ 的具体特点可以概括为以下几点：\n可靠性： RabbitMQ 使用一些机制来保证消息的可靠性，如持久化、传输确认及发布确认等。 灵活的路由： 在消息进入队列之前，通过交换器来路由消息。对于典型的路由功能，RabbitMQ 己经提供了一些内置的交换器来实现。针对更复杂的路由功能，可以将多个交换器绑定在一起，也可以通过插件机制来实现自己的交换器。这个后面会在我们讲 RabbitMQ 核心概念的时候详细介绍到。 扩展性： 多个 RabbitMQ 节点可以组成一个集群，也可以根据实际业务情况动态地扩展集群中节点。 高可用性： 队列可以在集群中的机器上设置镜像，使得在部分节点出现问题的情况下队列仍然可用。 支持多种协议： RabbitMQ 除了原生支持 AMQP 协议，还支持 STOMP、MQTT 等多种消息中间件协议。 多语言客户端： RabbitMQ 几乎支持所有常用语言，比如 Java、Python、Ruby、PHP、C#、JavaScript 等。 易用的管理界面： RabbitMQ 提供了一个易用的用户界面，使得用户可以监控和管理消息、集群中的节点等。在安装 RabbitMQ 的时候会介绍到，安装好 RabbitMQ 就自带管理界面。 插件机制： RabbitMQ 提供了许多插件，以实现从多方面进行扩展，当然也可以编写自己的插件。感觉这个有点类似 Dubbo 的 SPI 机制 RabbitMQ 官网：https://www.rabbitmq.com/ 。\nRabbitMQ 更新记录（可以直观看到项目是否还在维护）：https://www.rabbitmq.com/news.html\nPulsar # Pulsar 是下一代云原生分布式消息流平台，最初由 Yahoo 开发 ，已经成为 Apache 顶级项目。\nPulsar 集消息、存储、轻量化函数式计算为一体，采用计算与存储分离架构设计，支持多租户、持久化存储、多机房跨区域数据复制，具有强一致性、高吞吐、低延时及高可扩展性等流数据存储特性，被看作是云原生时代实时消息流传输、存储和计算最佳解决方案。\nPulsar 的关键特性如下（摘自官网）：\n是下一代云原生分布式消息流平台。 Pulsar 的单个实例原生支持多个集群，可跨机房在集群间无缝地完成消息复制。 极低的发布延迟和端到端延迟。 可无缝扩展到超过一百万个 topic。 简单的客户端 API，支持 Java、Go、Python 和 C++。 主题的多种订阅模式（独占、共享和故障转移）。 通过 Apache BookKeeper 提供的持久化消息存储机制保证消息传递 。 由轻量级的 serverless 计算框架 Pulsar Functions 实现流原生的数据处理。 基于 Pulsar Functions 的 serverless connector 框架 Pulsar IO 使得数据更易移入、移出 Apache Pulsar。 分层式存储可在数据陈旧时，将数据从热存储卸载到冷/长期存储（如 S3、GCS）中。 Pulsar 官网：https://pulsar.apache.org/\nPulsar 更新记录（可以直观看到项目是否还在维护）：https://github.com/apache/pulsar/releases\nActiveMQ # 目前已经被淘汰，不推荐使用，不建议学习。\n如何选择？ # 参考《Java 工程师面试突击第 1 季-中华石杉老师》\n对比方向 概要 吞吐量 万级的 ActiveMQ 和 RabbitMQ 的吞吐量（ActiveMQ 的性能最差）要比十万级甚至是百万级的 RocketMQ 和 Kafka 低一个数量级。 可用性 都可以实现高可用。ActiveMQ 和 RabbitMQ 都是基于主从架构实现高可用性。RocketMQ 基于分布式架构。 Kafka 也是分布式的，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 时效性 RabbitMQ 基于 Erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级，其他几个都是 ms 级。 功能支持 Pulsar 的功能更全面，支持多租户、多种消费模式和持久性模式等功能，是下一代云原生分布式消息流平台。 消息丢失 ActiveMQ 和 RabbitMQ 丢失的可能性非常低， Kafka、RocketMQ 和 Pulsar 理论上可以做到 0 丢失。 总结：\nActiveMQ 的社区算是比较成熟，但是较目前来说，ActiveMQ 的性能比较差，而且版本迭代很慢，不推荐使用，已经被淘汰了。 RabbitMQ 在吞吐量方面虽然稍逊于 Kafka 、RocketMQ 和 Pulsar，但是由于它基于 Erlang 开发，所以并发能力很强，性能极其好，延时很低，达到微秒级。但是也因为 RabbitMQ 基于 Erlang 开发，所以国内很少有公司有实力做 Erlang 源码级别的研究和定制。如果业务场景对并发量要求不是太高（十万级、百万级），那这几种消息队列中，RabbitMQ 或许是你的首选。 RocketMQ 和 Pulsar 支持强一致性，对消息一致性要求比较高的场景可以使用。 RocketMQ 阿里出品，Java 系开源项目，源代码我们可以直接阅读，然后可以定制自己公司的 MQ，并且 RocketMQ 有阿里巴巴的实际业务场景的实战考验。 Kafka 的特点其实很明显，就是仅仅提供较少的核心功能，但是提供超高的吞吐量，ms 级的延迟，极高的可用性以及可靠性，而且分布式可以任意扩展。同时 kafka 最好是支撑较少的 topic 数量即可，保证其超高吞吐量。Kafka 唯一的一点劣势是有可能消息重复消费，那么对数据准确性会造成极其轻微的影响，在大数据领域中以及日志采集中，这点轻微影响可以忽略这个特性天然适合大数据实时计算以及日志收集。如果是大数据领域的实时计算、日志采集等场景，用 Kafka 是业内标准的，绝对没问题，社区活跃度很高，绝对不会黄，何况几乎是全世界这个领域的事实性规范。 参考 # KRaft: Apache Kafka Without ZooKeeper：https://developer.confluent.io/learn/kraft/ "},{"id":86,"href":"/zh/docs/technology/Review/java_guide/lyely_high-performance/ly02ly_cdn/","title":"cdn","section":"高性能","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n什么是 CDN ？ # CDN 全称是 Content Delivery Network/Content Distribution Network，翻译过的意思是 内容分发网络 。\n我们可以将内容分发网络拆开来看：\n内容 ：指的是静态资源比如图片、视频、文档、JS、CSS、HTML。 分发网络 ：指的是将这些静态资源分发到位于多个不同的地理位置机房中的服务器上，这样，就可以实现静态资源的就近访问比如北京的用户直接访问北京机房的数据。 所以，简单来说，CDN 就是将静态资源分发到多个不同的地方以实现就近访问，进而加快静态资源的访问速度，减轻服务器以及带宽的负担。\n类似于京东建立的庞大的仓储运输体系，京东物流在全国拥有非常多的仓库，仓储网络几乎覆盖全国所有区县。这样的话，用户下单的第一时间，商品就从距离用户最近的仓库，直接发往对应的配送站，再由京东小哥送到你家。\n你可以将 CDN 看作是服务上一层的特殊缓存服务，分布在全国各地，主要用来处理静态资源的请求。\n我们经常拿全站加速和内容分发网络做对比，不要把两者搞混了！全站加速（不同云服务商叫法不同，腾讯云叫 ECDN、阿里云叫 DCDN）既可以加速静态资源又可以加速动态资源，**内容分发网络（CDN）**主要针对的是 静态资源 。\n绝大部分公司都会在项目开发中交使用 CDN 服务，但很少会有自建 CDN 服务的公司。基于成本、稳定性和易用性考虑，建议直接选择专业的云厂商（比如阿里云、腾讯云、华为云、青云）或者 CDN 厂商（比如网宿、蓝汛）提供的开箱即用的 CDN 服务。\n很多朋友可能要问了：既然是就近访问，为什么不直接将服务部署在多个不同的地方呢？\n成本太高，需要部署多份相同的服务。 静态资源通常占用空间比较大且经常会被访问到，如果直接使用服务器或者缓存来处理静态资源请求的话，对系统资源消耗非常大，可能会影响到系统其他服务的正常运行。 同一个服务在在多个不同的地方部署多份（比如同城灾备、异地灾备、同城多活、异地多活）是为了实现系统的高可用而不是就近访问。\nCDN 工作原理是什么？ # 搞懂下面 3 个问题也就搞懂了 CDN 的工作原理：\n静态资源是如何被缓存到 CDN 节点中的？ 如何找到最合适的 CDN 节点？ 如何防止静态资源被盗用？ 静态资源是如何被缓存到 CDN 节点中的？ # 你可以通过预热的方式将源站的资源同步到 CDN 的节点中。这样的话，用户首次请求资源可以直接从 CDN 节点中取，无需回源。这样可以降低源站压力，提升用户体验。\n如果不预热的话，你访问的资源可能不再 CDN 节点中，这个时候 CDN 节点将请求源站获取资源，这个过程是大家经常说的 回源。\n命中率 和 回源率 是衡量 CDN 服务质量两个重要指标。命中率越高越好，回源率越低越好。\n如果资源有更新的话，你也可以对其 刷新 ，删除 CDN 节点上缓存的资源，当用户访问对应的资源时直接回源获取最新的资源，并重新缓存。\n如何找到最合适的 CDN 节点？ # **GSLB （Global Server Load Balance，全局负载均衡）**是 CDN 的大脑，负责多个CDN节点之间相互协作，最常用的是基于 DNS 的 GSLB。\nCDN 会通过 GSLB 找到最合适的 CDN 节点，更具体点来说是下面这样的：\n浏览器向 DNS 服务器发送域名请求； DNS 服务器向根据 CNAME( Canonical Name ) 别名记录向 GSLB 发送请求； GSLB 返回性能最好（通常距离请求地址最近）的 CDN 节点（边缘服务器，真正缓存内容的地方）的地址给浏览器； 浏览器直接访问指定的 CDN 节点。 为了方便理解，上图其实做了一点简化。GSLB 内部可以看作是 CDN 专用 DNS 服务器和负载均衡系统组合。CDN 专用 DNS 服务器会返回负载均衡系统 IP 地址给浏览器，浏览器使用 IP 地址请求负载均衡系统进而找到对应的 CDN 节点。\nGSLB 是如何选择出最合适的 CDN 节点呢？ GSLB 会根据请求的 IP 地址、CDN 节点状态（比如负载情况、性能、响应时间、带宽）等指标来综合判断具体返回哪一个 CDN 节点的地址。\n如何防止资源被盗刷？ # 如果我们的资源被其他用户或者网站非法盗刷的话，将会是一笔不小的开支。\n解决这个问题最常用最简单的办法设置 Referer 防盗链，具体来说就是根据 HTTP 请求的头信息里面的 Referer 字段对请求进行限制。我们可以通过 Referer 字段获取到当前请求页面的来源页面的网站地址，这样我们就能确定请求是否来自合法的网站。\nCDN 服务提供商几乎都提供了这种比较基础的防盗链机制。\n不过，如果站点的防盗链配置允许 Referer 为空的话，通过隐藏 Referer，可以直接绕开防盗链。\n通常情况下，我们会配合其他机制来确保静态资源被盗用，一种常用的机制是 时间戳防盗链 。相比之下，时间戳防盗链 的安全性更强一些。时间戳防盗链加密的 URL 具有时效性，过期之后就无法再被允许访问。\n时间戳防盗链的 URL 通常会有两个参数一个是签名字符串，一个是过期时间。签名字符串一般是通过对用户设定的加密字符串、请求路径、过期时间通过 MD5 哈希算法取哈希的方式获得。\n注意，这个用户设定的加密字符串，是在后台配置的，而签名是后台给前端的\n（1）用户管理员在七牛 CDN 控制台 配置 key ，并将 key配置进业务服务器。 （2）当客户端请求资源时，将原始 url 发送至业务服务器。 （3）业务服务器根据计算逻辑，将带有时间戳签名的 url 返回至客户端。 （4）客户端使用带有时间戳签名的 url 请求资源。 （5）CDN 检查 url 签名的合法性。\n时间戳防盗链 URL示例：\nhttp://cdn.wangsu.com/4/123.mp3? wsSecret=79aead3bd7b5db4adeffb93a010298b5\u0026amp;wsTime=1601026312 wsSecret ：签名字符串。 wsTime: 过期时间。 时间戳防盗链的实现也比较简单，并且可靠性较高，推荐使用。并且，绝大部分 CDN 服务提供商都提供了开箱即用的时间戳防盗链机制。\n除了 Referer 防盗链和时间戳防盗链之外，你还可以 IP 黑白名单配置、IP 访问限频配置等机制来防盗刷。\n总结 # CDN 就是将静态资源分发到多个不同的地方以实现就近访问，进而加快静态资源的访问速度，减轻服务器以及带宽的负担。 基于成本、稳定性和易用性考虑，建议直接选择专业的云厂商（比如阿里云、腾讯云、华为云、青云）或者 CDN 厂商（比如网宿、蓝汛）提供的开箱即用的 CDN 服务。 GSLB （Global Server Load Balance，全局负载均衡）是 CDN 的大脑，负责多个 CDN 节点之间相互协作，最常用的是基于 DNS 的 GSLB。CDN 会通过 GSLB 找到最合适的 CDN 节点。 为了防止静态资源被盗用，我们可以利用 Referer 防盗链 + 时间戳防盗链 。 参考 # 时间戳防盗链 - 七牛云 CDN：https://developer.qiniu.com/fusion/kb/1670/timestamp-hotlinking-prevention CDN是个啥玩意？一文说个明白：https://mp.weixin.qq.com/s/Pp0C8ALUXsmYCUkM5QnkQw 《透视 HTTP 协议》- 37 | CDN：加速我们的网络服务：http://gk.link/a/11yOG "},{"id":87,"href":"/zh/docs/technology/Review/java_guide/lyely_high-performance/ly01ly_read-and-write-separation-and-library-subtable/","title":"数据库读写分离\u0026分库分表详解","section":"高性能","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n读写分离 # 什么是读写分离？ # 见名思意，根据读写分离的名字，我们就可以知道：读写分离主要是为了将对数据库的读写操作分散到不同的数据库节点上。 这样的话，就能够小幅提升写性能，大幅提升读性能。\n我简单画了一张图来帮助不太清楚读写分离的小伙伴理解。\n一般情况下，我们都会选择一主多从，也就是一台主数据库负责写，其他的从数据库负责读。主库和从库之间会进行数据同步，以保证从库中数据的准确性。这样的架构实现起来比较简单，并且也符合系统的写少读多的特点。\n读写分离会带来什么问题？如何解决？ # 读写分离对于提升数据库的并发非常有效，但是，同时也会引来一个问题：主库和从库的数据存在延迟，比如你写完主库之后，主库的数据同步到从库是需要时间的，这个时间差就导致了主库和从库的数据不一致性问题。这也就是我们经常说的 主从同步延迟 。\n主从同步延迟问题的解决，没有特别好的一种方案（可能是我太菜了，欢迎评论区补充）。你可以根据自己的业务场景，参考一下下面几种解决办法。\n1.强制将读请求路由到主库处理。\n既然你从库的数据过期了，那我就直接从主库读取嘛！这种方案虽然会增加主库的压力，但是，实现起来比较简单，也是我了解到的使用最多的一种方式。\n比如 Sharding-JDBC 就是采用的这种方案。通过使用 Sharding-JDBC 的 HintManager 分片键值管理器，我们可以强制使用主库。\nHintManager hintManager = HintManager.getInstance(); hintManager.setMasterRouteOnly(); // 继续JDBC操作 对于这种方案，你可以将那些必须获取最新数据的读请求都交给主库处理。\n2.延迟读取。\n还有一些朋友肯定会想既然主从同步存在延迟，那我就在延迟之后读取啊，比如主从同步延迟 0.5s,那我就 1s 之后再读取数据。这样多方便啊！方便是方便，但是也很扯淡。\n不过，如果你是这样设计业务流程就会好很多：对于一些对数据比较敏感的场景，你可以在完成写请求之后，避免立即进行请求操作。比如你支付成功之后，跳转到一个支付成功的页面，当你点击返回之后才返回自己的账户。\n另外，《MySQL 实战 45 讲》这个专栏中的《读写分离有哪些坑？》这篇文章还介绍了很多其他比较实际的解决办法，感兴趣的小伙伴可以自行研究一下。\n如何实现读写分离？ # 不论是使用哪一种读写分离具体的实现方案，想要实现读写分离一般包含如下几步：\n部署多台数据库，选择其中的一台作为主数据库，其他的一台或者多台作为从数据库。 保证主数据库和从数据库之间的数据是实时同步的，这个过程也就是我们常说的主从复制。 系统将写请求交给主数据库处理，读请求交给从数据库处理。[ 使用上 ] 落实到项目本身的话，常用的方式有两种：\n1.代理方式\n我们可以在应用和数据中间加了一个代理层。应用程序所有的数据请求都交给代理层处理，代理层负责分离读写请求，将它们路由到对应的数据库中。\n提供类似功能的中间件有 MySQL Router（官方）、Atlas（基于 MySQL Proxy）、Maxscale、MyCat。\n2.组件方式\n在这种方式中，我们可以通过引入第三方组件来帮助我们读写请求。\n这也是我比较推荐的一种方式。这种方式目前在各种互联网公司中用的最多的，相关的实际的案例也非常多。如果你要采用这种方式的话，推荐使用 sharding-jdbc ，直接引入 jar 包即可使用，非常方便。同时，也节省了很多运维的成本。\n你可以在 shardingsphere 官方找到 sharding-jdbc 关于读写分离的操作。\n主从复制原理是什么？ # MySQL binlog(binary log 即二进制日志文件) 主要记录了 MySQL 数据库中数据的所有变化(数据库执行的所有 DDL 和 DML 语句)。因此，我们根据主库的 MySQL binlog 日志就能够将主库的数据同步到从库中。\n更具体和详细的过程是这个样子的（图片来自于：《MySQL Master-Slave Replication on the Same Machine》）：\n主库将数据库中数据的变化写入到 binlog 从库连接主库 从库会创建一个 I/O 线程向主库请求更新的 binlog 主库会创建一个 binlog dump 线程来发送 binlog ，从库中的 I/O 线程负责接收 从库的 I/O 线程将接收的 binlog 写入到 relay log 中。 从库的 SQL 线程读取 relay log 同步数据本地（也就是再执行一遍 SQL ）。 怎么样？看了我对主从复制这个过程的讲解，你应该搞明白了吧!\n你一般看到 binlog 就要想到主从复制。当然，除了主从复制之外，binlog 还能帮助我们实现数据恢复。\n🌈 拓展一下：\n不知道大家有没有使用过阿里开源的一个叫做 canal 的工具。这个工具可以帮助我们实现 MySQL 和其他数据源比如 Elasticsearch 或者另外一台 MySQL 数据库之间的数据同步。很显然，这个工具的底层原理肯定也是依赖 binlog。canal 的原理就是模拟 MySQL 主从复制的过程，解析 binlog 将数据同步到其他的数据源。\n另外，像咱们常用的分布式缓存组件 Redis 也是通过主从复制实现的读写分离。\n🌕 简单总结一下：\nMySQL 主从复制是依赖于 binlog 。另外，常见的一些同步 MySQL 数据到其他数据源的工具（比如 canal）的底层一般也是依赖 binlog 。\n分库分表 # 读写分离主要应对的是数据库读并发，没有解决数据库存储问题。试想一下：如果 MySQL 一张表的数据量过大怎么办?\n换言之，我们该如何解决 MySQL 的存储压力呢？\n答案之一就是 分库分表。\n什么是分库？ # 分库 就是将数据库中的数据分散到不同的数据库上，可以垂直分库，也可以水平分库。\n垂直分库 就是把单一数据库按照业务进行划分，不同的业务使用不同的数据库，进而将一个数据库的压力分担到多个数据库。\n举个例子：说你将数据库中的用户表、订单表和商品表分别单独拆分为用户数据库、订单数据库和商品数据库。\n水平分库 是把同一个表按一定规则拆分到不同的数据库中，每个库可以位于不同的服务器上，这样就实现了水平扩展，解决了单表的存储和性能瓶颈的问题。\n举个例子：订单表数据量太大，你对订单表进行了水平切分（水平分表），然后将切分后的 2 张订单表分别放在两个不同的数据库。\n什么是分表？ # 分表 就是对单表的数据进行拆分，可以是垂直拆分，也可以是水平拆分。\n垂直分表 是对数据表列的拆分，把一张列比较多的表拆分为多张表。\n举个例子：我们可以将用户信息表中的一些列单独抽出来作为一个表。\n水平分表 是对数据表行的拆分，把一张行比较多的表拆分为多张表，可以解决单一表数据量过大的问题。\n举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。\n水平拆分只能解决单表数据量大的问题，为了提升性能，我们通常会选择将拆分后的多张表放在不同的数据库中。也就是说，水平分表通常和水平分库同时出现。\n什么情况下需要分库分表？ # 遇到下面几种场景可以考虑分库分表：\n单表的数据达到千万级别以上，数据库读写速度比较缓慢。 数据库中的数据占用的空间越来越大，备份时间越来越长。 应用的并发量太大。 常见的分片算法有哪些？ # 分片算法主要解决了数据被水平分片之后，数据究竟该存放在哪个表的问题。\n哈希分片 ：求指定 key（比如 id） 的哈希，然后根据哈希值确定数据应被放置在哪个表中。哈希分片比较适合随机读写的场景，不太适合经常需要范围查询的场景。 范围分片 ：按照特性的范围区间（比如时间区间、ID区间）来分配数据，比如 将 id 为 1~299999 的记录分到第一个库， 300000~599999 的分到第二个库。范围分片适合需要经常进行范围查找的场景，不太适合随机读写的场景（数据未被分散，容易出现热点数据的问题）。 地理位置分片 ：很多 NewSQL 数据库都支持地理位置分片算法，也就是根据地理位置（如城市、地域）来分配数据。 融合算法 ：灵活组合多种分片算法，比如将哈希分片和范围分片组合。 \u0026hellip;\u0026hellip; 分库分表会带来什么问题呢？ # 记住，你在公司做的任何技术决策，不光是要考虑这个技术能不能满足我们的要求，是否适合当前业务场景，还要重点考虑其带来的成本。\n引入分库分表之后，会给系统带来什么挑战呢？\njoin 操作 ： 同一个数据库中的表分布在了不同的数据库中，导致无法使用 join 操作。这样就导致我们需要手动进行数据的封装，比如你在一个数据库中查询到一个数据之后，再根据这个数据去另外一个数据库中找对应的数据。 事务问题 ：同一个数据库中的表分布在了不同的数据库中，如果单个操作涉及到多个数据库，那么数据库自带的事务就无法满足我们的要求了。 分布式 id ：分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？这个时候，我们就需要为我们的系统引入分布式 id 了。 \u0026hellip;\u0026hellip; 另外，引入分库分表之后，一般需要 DBA 的参与，同时还需要更多的数据库服务器，这些都属于成本。\n分库分表有没有什么比较推荐的方案？ # ShardingSphere 项目（包括 Sharding-JDBC、Sharding-Proxy 和 Sharding-Sidecar）是当当捐入 Apache 的，目前主要由京东数科的一些巨佬维护。\nShardingSphere 绝对可以说是当前分库分表的首选！ShardingSphere 的功能完善，除了支持读写分离和分库分表，还提供分布式事务、数据库治理等功能。\n另外，ShardingSphere 的生态体系完善，社区活跃，文档完善，更新和发布比较频繁。\n艿艿之前写了一篇分库分表的实战文章，各位朋友可以看看：《芋道 Spring Boot 分库分表入门》 。\n分库分表后，数据怎么迁移呢？ # 分库分表之后，我们如何将老库（单库单表）的数据迁移到新库（分库分表后的数据库系统）呢？\n比较简单同时也是非常常用的方案就是停机迁移，写个脚本老库的数据写到新库中。比如你在凌晨 2 点，系统使用的人数非常少的时候，挂一个公告说系统要维护升级预计 1 小时。然后，你写一个脚本将老库的数据都同步到新库中。\n如果你不想停机迁移数据的话，也可以考虑双写方案。双写方案是针对那种不能停机迁移的场景，实现起来要稍微麻烦一些。具体原理是这样的：\n我们对老库的更新操作（增删改），同时也要写入新库（双写）。如果操作的数据不存在于新库的话，需要插入到新库中。 这样就能保证，咱们新库里的数据是最新的。 在迁移过程，双写只会让被更新操作过的老库中的数据同步到新库，我们还需要自己写脚本将老库中的数据(这里说的就是原来老库中的数据但是没有设计更新操作)和新库的数据做比对。如果新库中没有，那咱们就把数据插入到新库。如果新库有，旧库没有，就把新库对应的数据删除（冗余数据清理）。 重复上一步的操作，直到老库和新库的数据一致为止。 想要在项目中实施双写还是比较麻烦的，很容易会出现问题。我们可以借助上面提到的数据库同步工具 Canal 做增量数据迁移（还是依赖 binlog，开发和维护成本较低）。\n总结 # 读写分离主要是为了将对数据库的读写操作分散到不同的数据库节点上。 这样的话，就能够小幅提升写性能，大幅提升读性能。 读写分离基于主从复制，MySQL 主从复制是依赖于 binlog 。 分库 就是将数据库中的数据分散到不同的数据库上。分表 就是对单表的数据进行拆分，可以是垂直拆分，也可以是水平拆分。 引入分库分表之后，需要系统解决事务、分布式 id、无法 join 操作问题。 ShardingSphere 绝对可以说是当前分库分表的首选！ShardingSphere 的功能完善，除了支持读写分离和分库分表，还提供分布式事务、数据库治理等功能。另外，ShardingSphere 的生态体系完善，社区活跃，文档完善，更新和发布比较频繁。 "},{"id":88,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly08ly_zookeeper-in-action/","title":"zookeeper实战","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n1. 前言 # 这篇文章简单给演示一下 ZooKeeper 常见命令的使用以及 ZooKeeper Java客户端 Curator 的基本使用。介绍到的内容都是最基本的操作，能满足日常工作的基本需要。\n如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！\n2. ZooKeeper 安装和使用 # 2.1. 使用Docker 安装 zookeeper # a.使用 Docker 下载 ZooKeeper\ndocker pull zookeeper:3.5.8 b.运行 ZooKeeper\ndocker run -d --name zookeeper -p 2181:2181 zookeeper:3.5.8 2.2. 连接 ZooKeeper 服务 # a.进入ZooKeeper容器中\n先使用 docker ps 查看 ZooKeeper 的 ContainerID，然后使用 docker exec -it ContainerID /bin/bash 命令进入容器中。\nb.先进入 bin 目录,然后通过 ./zkCli.sh -server 127.0.0.1:2181命令连接ZooKeeper 服务\nroot@eaf70fc620cb:/apache-zookeeper-3.5.8-bin# cd bin 如果你看到控制台成功打印出如下信息的话，说明你已经成功连接 ZooKeeper 服务。\n2.3. 常用命令演示 # 2.3.1. 查看常用命令(help 命令) # 通过 help 命令查看 ZooKeeper 常用命令\n2.3.2. 创建节点(create 命令) # 通过 create 命令在根目录创建了 node1 节点，与它关联的字符串是\u0026quot;node1\u0026quot;\n[zk: 127.0.0.1:2181(CONNECTED) 34] create /node1 “node1” 通过 create 命令在根目录创建了 node1 节点，与它关联的内容是数字 123\n这个是不是写错了，应该是在node1目录下 ，创建了 node1.1节点\n[zk: 127.0.0.1:2181(CONNECTED) 1] create /node1/node1.1 123 Created /node1/node1.1 2.3.3. 更新节点数据内容(set 命令) # [zk: 127.0.0.1:2181(CONNECTED) 11] set /node1 \u0026#34;set node1\u0026#34; 2.3.4. 获取节点的数据(get 命令) # get 命令可以获取指定节点的数据内容和节点的状态,可以看出我们通过 set 命令已经将节点数据内容改为 \u0026ldquo;set node1\u0026rdquo;。\nset node1 cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x4b mtime = Sun Jan 20 10:41:10 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 1 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 9 numChildren = 1 2.3.5. 查看某个目录下的子节点(ls 命令) # 通过 ls 命令查看根目录下的节点\n[zk: 127.0.0.1:2181(CONNECTED) 37] ls / [dubbo, ZooKeeper, node1] 通过 ls 命令查看 node1 目录下的节点\n[zk: 127.0.0.1:2181(CONNECTED) 5] ls /node1 [node1.1] ZooKeeper 中的 ls 命令和 linux 命令中的 ls 类似， 这个命令将列出绝对路径 path 下的所有子节点信息（列出 1 级，并不递归）\n2.3.6. 查看节点状态(stat 命令) # 通过 stat 命令查看节点状态\n[zk: 127.0.0.1:2181(CONNECTED) 10] stat /node1 cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x47 mtime = Sun Jan 20 10:22:59 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 1 上面显示的一些信息比如 cversion、aclVersion、numChildren 等等，我在上面 “ZooKeeper 相关概念总结(入门)” 这篇文章中已经介绍到。\n2.3.7. 查看节点信息和状态(ls2 命令) # ls2 命令更像是 ls 命令和 stat 命令的结合。 ls2 命令返回的信息包括 2 部分：\n子节点列表 当前节点的 stat 信息。 [zk: 127.0.0.1:2181(CONNECTED) 7] ls2 /node1 [node1.1] cZxid = 0x47 ctime = Sun Jan 20 10:22:59 CST 2019 mZxid = 0x47 mtime = Sun Jan 20 10:22:59 CST 2019 pZxid = 0x4a cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 11 numChildren = 1 2.3.8. 删除节点(delete 命令) # 这个命令很简单，但是需要注意的一点是如果你要删除某一个节点，那么这个节点必须无子节点才行。\n[zk: 127.0.0.1:2181(CONNECTED) 3] delete /node1/node1.1 在后面我会介绍到 Java 客户端 API 的使用以及开源 ZooKeeper 客户端 ZkClient 和 Curator 的使用。\n3. ZooKeeper Java客户端 Curator简单使用 # Curator 是Netflix公司开源的一套 ZooKeeper Java客户端框架，相比于 Zookeeper 自带的客户端 zookeeper 来说，Curator 的封装更加完善，各种 API 都可以比较方便地使用。\n下面我们就来简单地演示一下 Curator 的使用吧！\nCurator4.0+版本对ZooKeeper 3.5.x支持比较好。开始之前，请先将下面的依赖添加进你的项目。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.curator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;curator-framework\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.curator\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;curator-recipes\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.2.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 3.1. 连接 ZooKeeper 客户端 # 通过 CuratorFrameworkFactory 创建 CuratorFramework 对象，然后再调用 CuratorFramework 对象的 start() 方法即可！\nprivate static final int BASE_SLEEP_TIME = 1000; private static final int MAX_RETRIES = 3; // Retry strategy. Retry 3 times, and will increase the sleep time between retries. RetryPolicy retryPolicy = new ExponentialBackoffRetry(BASE_SLEEP_TIME, MAX_RETRIES); CuratorFramework zkClient = CuratorFrameworkFactory.builder() // the server to connect to (can be a server list) .connectString(\u0026#34;127.0.0.1:2181\u0026#34;). .retryPolicy(retryPolicy) .build(); zkClient.start(); 对于一些基本参数的说明：\nbaseSleepTimeMs：重试之间等待的初始时间 maxRetries ：最大重试次数 connectString ：要连接的服务器列表 retryPolicy ：重试策略 3.2. 数据节点的增删改查 # 3.2.1. 创建节点 # 我们在 ZooKeeper常见概念解读 中介绍到，我们通常是将 znode 分为 4 大类：\n持久（PERSISTENT）节点 ：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。 临时（EPHEMERAL）节点 ：临时节点的生命周期是与 客户端会话（session） 绑定的，会话消失则节点消失 。并且，临时节点 只能做叶子节点 ，不能创建子节点。 持久顺序（PERSISTENT_SEQUENTIAL）节点 ：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 /node1/app0000000001 、/node1/app0000000002 。 临时顺序（EPHEMERAL_SEQUENTIAL）节点 ：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。 你在使用的ZooKeeper 的时候，会发现 CreateMode 类中实际有 7种 znode 类型 ，但是用的最多的还是上面介绍的 4 种。\na.创建持久化节点\n你可以通过下面两种方式创建持久化的节点。\n//注意:下面的代码会报错，下文说了具体原因 zkClient.create().forPath(\u0026#34;/node1/00001\u0026#34;); zkClient.create().withMode(CreateMode.PERSISTENT).forPath(\u0026#34;/node1/00002\u0026#34;); 但是，你运行上面的代码会报错，这是因为的父节点node1还未创建。\n你可以先创建父节点 node1 ，然后再执行上面的代码就不会报错了。\nzkClient.create().forPath(\u0026#34;/node1\u0026#34;); 更推荐的方式是通过下面这行代码， creatingParentsIfNeeded() 可以保证父节点不存在的时候自动创建父节点，这是非常有用的。\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(\u0026#34;/node1/00001\u0026#34;); b.创建临时节点\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\u0026#34;/node1/00001\u0026#34;); c.创建节点并指定数据内容\nzkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\u0026#34;/node1/00001\u0026#34;,\u0026#34;java\u0026#34;.getBytes()); zkClient.getData().forPath(\u0026#34;/node1/00001\u0026#34;);//获取节点的数据内容，获取到的是 byte数组 d.检测节点是否创建成功\nzkClient.checkExists().forPath(\u0026#34;/node1/00001\u0026#34;);//不为null的话，说明节点创建成功 3.2.2. 删除节点 # a.删除一个子节点\nzkClient.delete().forPath(\u0026#34;/node1/00001\u0026#34;); b.删除一个节点以及其下的所有子节点\nzkClient.delete().deletingChildrenIfNeeded().forPath(\u0026#34;/node1\u0026#34;); 3.2.3. 获取/更新节点数据内容 # zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(\u0026#34;/node1/00001\u0026#34;,\u0026#34;java\u0026#34;.getBytes()); zkClient.getData().forPath(\u0026#34;/node1/00001\u0026#34;);//获取节点的数据内容 zkClient.setData().forPath(\u0026#34;/node1/00001\u0026#34;,\u0026#34;c++\u0026#34;.getBytes());//更新节点数据内容 3.2.4. 获取某个节点的所有子节点路径 # List\u0026lt;String\u0026gt; childrenPaths = zkClient.getChildren().forPath(\u0026#34;/node1\u0026#34;); "},{"id":89,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly07ly_zookeeper-plus/","title":"zookeeper进阶","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\nFrancisQ 投稿。\n1. 好久不见 # 离上一篇文章的发布也快一个月了，想想已经快一个月没写东西了，其中可能有期末考试、课程设计和驾照考试，但这都不是借口！\n一到冬天就懒的不行，望广大掘友督促我🙄🙄✍️✍️。\n文章很长，先赞后看，养成习惯。❤️ 🧡 💛 💚 💙 💜\n2. 什么是ZooKeeper # ZooKeeper 由 Yahoo 开发，后来捐赠给了 Apache ，现已成为 Apache 顶级项目。ZooKeeper 是一个开源的分布式应用程序协调服务器，其为分布式系统提供一致性服务。其一致性是通过基于 Paxos 算法的 ZAB 协议完成的。其主要功能包括：配置维护、分布式同步、集群管理、分布式事务等。\n简单来说， ZooKeeper 是一个 分布式协调服务框架 。分布式？协调服务？这啥玩意？🤔🤔\n其实解释到分布式这个概念的时候，我发现有些同学并不是能把 分布式和集群 这两个概念很好的理解透。前段时间有同学和我探讨起分布式的东西，他说分布式不就是加机器吗？一台机器不够用再加一台抗压呗。当然加机器这种说法也无可厚非，你一个分布式系统必定涉及到多个机器，但是你别忘了，计算机学科中还有一个相似的概念—— Cluster ，集群不也是加机器吗？但是 集群 和 分布式 其实就是两个完全不同的概念。\n比如，我现在有一个秒杀服务，并发量太大单机系统承受不住，那我加几台服务器也 一样 提供秒杀服务，这个时候就是 Cluster 集群 。\n但是，我现在换一种方式，我将一个秒杀服务 拆分成多个子服务 ，比如创建订单服务，增加积分服务，扣优惠券服务等等，然后我将这些子服务都部署在不同的服务器上 ，这个时候就是 Distributed 分布式 。\n而我为什么反驳同学所说的分布式就是加机器呢？因为我认为加机器更加适用于构建集群，因为它真是只有加机器。而对于分布式来说，你首先需要将业务进行拆分，然后再加机器（不仅仅是加机器那么简单），同时你还要去解决分布式带来的一系列问题。\n比如各个分布式组件如何协调起来，如何减少各个系统之间的耦合度，分布式事务的处理，如何去配置整个分布式系统等等。ZooKeeper 主要就是解决这些问题的。\n3. 一致性问题 # 设计一个分布式系统必定会遇到一个问题—— 因为分区容忍性（partition tolerance）的存在，就必定要求我们需要在系统可用性（availability）和数据一致性（consistency）中做出权衡 。这就是著名的 CAP 定理。\n理解起来其实很简单，比如说把一个班级作为整个系统，而学生是系统中的一个个独立的子系统。这个时候班里的小红小明偷偷谈恋爱被班里的大嘴巴小花发现了，小花欣喜若狂告诉了周围的人，然后小红小明谈恋爱的消息在班级里传播起来了。当在消息的传播（散布）过程中，你抓到一个同学问他们的情况，如果回答你不知道（或者说的是他们的前男/女朋友，也就是消息不一致），那么说明整个班级系统出现了数据不一致的问题（因为小花已经知道这个消息了）。而如果他直接不回答你，因为整个班级有消息在进行传播（为了保证一致性，需要所有人都知道才可提供服务），这个时候就出现了系统的可用性问题。\n而上述前者就是 Eureka 的处理方式，它保证了AP（可用性），后者就是我们今天所要讲的 ZooKeeper 的处理方式，它保证了CP（数据一致性(即同步期间不可用)）。\n【ly总结】也就是说两台机器同步期间，如果要保证可用性，那么必然会出现一致性问题；而如果要保证一致性，那必然要等到完全同步完（也就是期间会让请求不可用）\n4. 一致性协议和算法 # 而为了解决数据一致性问题，在科学家和程序员的不断探索中，就出现了很多的一致性协议和算法。比如 2PC（两阶段提交），3PC（三阶段提交），Paxos算法等等。\n这时候请你思考一个问题，同学之间如果采用传纸条的方式去传播消息，那么就会出现一个问题——我咋知道我的小纸条有没有传到我想要传递的那个人手中呢？万一被哪个小家伙给劫持篡改了呢，对吧？\n这个时候就引申出一个概念—— 拜占庭将军问题 。它意指 在不可靠信道上试图通过消息传递的方式达到一致性是不可能的， 所以所有的一致性算法的 必要前提 就是安全可靠的消息通道。\n而为什么要去解决数据一致性的问题？你想想，如果一个秒杀系统将服务拆分成了下订单和加积分服务，这两个服务部署在不同的机器上了，万一在消息的传播过程中积分系统宕机了，总不能你这边下了订单却没加积分吧？你总得保证两边的数据需要一致吧？\n4.1. 2PC（两阶段提交） # 两阶段提交是一种保证分布式系统数据一致性的协议，现在很多数据库都是采用的两阶段提交协议来完成 分布式事务 的处理。\n在介绍2PC之前，我们先来想想分布式事务到底有什么问题呢？\n还拿秒杀系统的下订单和加积分两个系统来举例吧（我想你们可能都吐了🤮🤮🤮），我们此时下完订单会发个消息给积分系统告诉它下面该增加积分了。如果我们仅仅是发送一个消息也不收回复，那么我们的订单系统怎么能知道积分系统的收到消息的情况呢？如果我们增加一个收回复的过程，那么当积分系统收到消息后返回给订单系统一个 Response ，但在中间出现了网络波动，那个回复消息没有发送成功，订单系统是不是以为积分系统消息接收失败了？它是不是会回滚事务？但此时（实际情况：）积分系统是成功收到消息的，它就会去处理消息然后给用户增加积分，这个时候就会出现积分加了但是订单没下成功。\n所以我们所需要解决的是在分布式系统中，整个调用链中，我们所有服务的数据处理要么都成功要么都失败，即所有服务的 原子性问题 。\n在两阶段提交中，主要涉及到两个角色，分别是协调者和参与者。\n第一阶段：当要执行一个分布式事务的时候，事务发起者首先向协调者发起事务请求，然后协调者会给所有参与者发送 prepare 请求（其中包括事务内容）告诉参与者你们需要执行事务了，如果能执行我发的事务内容那么就先执行但不提交，执行后请给我回复。然后参与者收到 prepare 消息后，他们会开始执行事务（但不提交），并将 Undo 和 Redo 信息记入事务日志中，之后参与者就向协调者反馈是否准备好了。\n第二阶段：第二阶段主要是协调者根据参与者反馈的情况来决定接下来是否可以进行事务的提交操作，即提交事务或者回滚事务。\n比如这个时候 所有的参与者 都返回了准备好了的消息，这个时候就进行事务的提交，协调者此时会给所有的参与者发送 Commit 请求 ，当参与者收到 Commit 请求的时候会执行前面执行的事务的 提交操作 ，提交完毕之后将给协调者发送提交成功的响应。\n而如果在第一阶段并不是所有参与者都返回了准备好了的消息，那么此时协调者将会给所有参与者发送 回滚事务的 rollback 请求，参与者收到之后将会 回滚它在第一阶段所做的事务处理 ，然后再将处理情况返回给协调者，最终协调者收到响应后便给事务发起者返回处理失败的结果。\n个人觉得 2PC 实现得还是比较鸡肋的，因为事实上它只解决了各个事务的原子性问题，随之也带来了很多的问题。\n单点故障问题，如果协调者挂了那么整个系统都处于不可用的状态了。 阻塞问题，即当协调者发送 prepare 请求，参与者收到之后如果能处理那么它将会进行事务的处理但并不提交，这个时候会一直占用着资源不释放，如果此时协调者挂了，那么这些资源都不会再释放了，这会极大影响性能。 数据不一致问题，比如当第二阶段，协调者只发送了一部分的 commit 请求就挂了，那么也就意味着，收到消息的参与者会进行事务的提交，而后面没收到的则不会进行事务提交，那么这时候就会产生数据不一致性问题。 4.2. 3PC（三阶段提交） # 因为2PC存在的一系列问题，比如单点，容错机制缺陷等等，从而产生了 3PC（三阶段提交） 。那么这三阶段又分别是什么呢？\n千万不要吧PC理解成个人电脑了，其实他们是 phase-commit 的缩写，即阶段提交。\nCanCommit阶段：协调者向所有参与者发送 CanCommit 请求，参与者收到请求后会根据自身情况查看是否能执行事务，如果可以则返回 YES 响应并进入预备状态，否则返回 NO 。 PreCommit阶段：协调者根据参与者返回的响应来决定是否可以进行下面的 PreCommit 操作。如果上面参与者返回的都是 YES，那么协调者将向所有参与者发送 PreCommit 预提交请求，参与者收到预提交请求后，会进行事务的执行操作，并将 Undo 和 Redo 信息写入事务日志中 ，最后如果参与者顺利执行了事务则给协调者返回成功的响应。如果在第一阶段协调者收到了 任何一个 NO 的信息，或者 在一定时间内 并没有收到全部的参与者的响应，那么就会中断事务，它会向所有参与者发送中断请求（abort），参与者收到中断请求之后会立即中断事务，或者在一定时间内没有收到协调者的请求，它也会中断事务。 DoCommit阶段：这个阶段其实和 2PC 的第二阶段差不多，如果协调者收到了所有参与者在 PreCommit 阶段的 YES 响应，那么协调者将会给所有参与者发送 DoCommit 请求，参与者收到 DoCommit 请求后则会进行事务的提交工作，完成后则会给协调者返回响应，协调者收到所有参与者返回的事务提交成功的响应之后则完成事务。若协调者在 PreCommit 阶段 收到了任何一个 NO 或者在一定时间内没有收到所有参与者的响应 ，那么就会进行中断请求的发送，参与者收到中断请求后则会 通过上面记录的回滚日志 来进行事务的回滚操作，并向协调者反馈回滚状况，协调者收到参与者返回的消息后，中断事务。 这里是 3PC 在成功的环境下的流程图，你可以看到 3PC 在很多地方进行了超时中断的处理，比如协调者在指定时间内为收到全部的确认消息则进行事务中断的处理，这样能 减少同步阻塞的时间 。还有需要注意的是，3PC 在 DoCommit 阶段参与者如未收到协调者发送的提交事务的请求，它会在一定时间内进行事务的提交。为什么这么做呢？是因为这个时候我们肯定保证了在第一阶段所有的协调者全部返回了可以执行事务的响应，这个时候我们有理由相信其他系统都能进行事务的执行和提交，所以不管协调者有没有发消息给参与者，进入第三阶段参与者都会进行事务的提交操作。\n总之，3PC 通过一系列的超时机制很好的缓解了阻塞问题，但是最重要的一致性并没有得到根本的解决，比如在 PreCommit 阶段，当一个参与者收到了请求之后其他参与者和协调者挂了或者出现了网络分区，这个时候收到消息的参与者都会进行事务提交，这就会出现数据不一致性问题。\n所以，要解决一致性问题还需要靠 Paxos 算法⭐️ ⭐️ ⭐️ 。\n4.3. Paxos 算法 [ly:看不懂，跳过] # Paxos 算法是基于消息传递且具有高度容错特性的一致性算法，是目前公认的解决分布式一致性问题最有效的算法之一，其解决的问题就是在分布式系统中如何就某个值（决议）达成一致 。\n在 Paxos 中主要有三个角色，分别为 Proposer提案者、Acceptor表决者、Learner学习者。Paxos 算法和 2PC 一样，也有两个阶段，分别为 Prepare 和 accept 阶段。\n4.3.1. prepare 阶段 # Proposer提案者：负责提出 proposal，每个提案者在提出提案时都会首先获取到一个 具有全局唯一性的、递增的提案编号N，即在整个集群中是唯一的编号 N，然后将该编号赋予其要提出的提案，在第一阶段是只将提案编号发送给所有的表决者。 Acceptor表决者：每个表决者在 accept 某提案后，会将该提案编号N记录在本地，这样每个表决者中保存的已经被 accept 的提案中会存在一个编号最大的提案，其编号假设为 maxN。每个表决者仅会 accept 编号大于自己本地 maxN 的提案，在批准提案时表决者会将以前接受过的最大编号的提案作为响应反馈给 Proposer 。 下面是 prepare 阶段的流程图，你可以对照着参考一下。\n4.3.2. accept 阶段 # 当一个提案被 Proposer 提出后，如果 Proposer 收到了超过半数的 Acceptor 的批准（Proposer 本身同意），那么此时 Proposer 会给所有的 Acceptor 发送真正的提案（你可以理解为第一阶段为试探），这个时候 Proposer 就会发送提案的内容和提案编号。\n表决者收到提案请求后会再次比较本身已经批准过的最大提案编号和该提案编号，如果该提案编号 大于等于 已经批准过的最大提案编号，那么就 accept 该提案（此时执行提案内容但不提交），随后将情况返回给 Proposer 。如果不满足则不回应或者返回 NO 。\n当 Proposer 收到超过半数的 accept ，那么它这个时候会向所有的 acceptor 发送提案的提交请求。需要注意的是，因为上述仅仅是超过半数的 acceptor 批准执行了该提案内容，其他没有批准的并没有执行该提案内容，所以这个时候需要向未批准的 acceptor 发送提案内容和提案编号并让它无条件执行和提交，而对于前面已经批准过该提案的 acceptor 来说 仅仅需要发送该提案的编号 ，让 acceptor 执行提交就行了。\n而如果 Proposer 如果没有收到超过半数的 accept 那么它将会将 递增 该 Proposal 的编号，然后 重新进入 Prepare 阶段 。\n对于 Learner 来说如何去学习 Acceptor 批准的提案内容，这有很多方式，读者可以自己去了解一下，这里不做过多解释。\n4.3.3. paxos 算法的死循环问题 # 其实就有点类似于两个人吵架，小明说我是对的，小红说我才是对的，两个人据理力争的谁也不让谁🤬🤬。\n比如说，此时提案者 P1 提出一个方案 M1，完成了 Prepare 阶段的工作，这个时候 acceptor 则批准了 M1，但是此时提案者 P2 同时也提出了一个方案 M2，它也完成了 Prepare 阶段的工作。然后 P1 的方案已经不能在第二阶段被批准了（因为 acceptor 已经批准了比 M1 更大的 M2），所以 P1 自增方案变为 M3 重新进入 Prepare 阶段，然后 acceptor ，又批准了新的 M3 方案，它又不能批准 M2 了，这个时候 M2 又自增进入 Prepare 阶段。。。\n就这样无休无止的永远提案下去，这就是 paxos 算法的死循环问题。\n那么如何解决呢？很简单，人多了容易吵架，我现在 就允许一个能提案 就行了。\n5. 引出 ZAB # 5.1. Zookeeper 架构 # 作为一个优秀高效且可靠的分布式协调框架，ZooKeeper 在解决分布式数据一致性问题时并没有直接使用 Paxos ，而是专门定制了一致性协议叫做 ZAB(ZooKeeper Atomic Broadcast) 原子广播协议，该协议能够很好地支持 崩溃恢复 。\n5.2. ZAB 中的三个角色 # 和介绍 Paxos 一样，在介绍 ZAB 协议之前，我们首先来了解一下在 ZAB 中三个主要的角色，Leader 领导者、Follower跟随者、Observer观察者 。\nLeader ：集群中 唯一的写请求处理者 ，能够发起投票（投票也是为了进行写请求）。 Follower：能够接收客户端的请求，如果是读请求则可以自己处理，如果是写请求则要转发给 Leader 。在选举过程中会参与投票，有选举权和被选举权 。 Observer ：就是没有选举权和被选举权的 Follower 。 观察者[əbˈzɜːvə(r)] 在 ZAB 协议中对 zkServer(即上面我们说的三个角色的总称) 还有两种模式的定义，分别是 消息广播 和 崩溃恢复 。\n5.3. 消息广播模式 # 说白了就是 ZAB 协议是如何处理写请求的，上面我们不是说只有 Leader 能处理写请求嘛？那么我们的 Follower 和 Observer 是不是也需要 同步更新数据 呢？总不能数据只在 Leader 中更新了，其他角色都没有得到更新吧？\n不就是 在整个集群中保持数据的一致性 嘛？如果是你，你会怎么做呢？\n废话，第一步肯定需要 Leader 将写请求 广播 出去呀，让 Leader 问问 Followers 是否同意更新，如果超过半数以上的同意那么就进行 Follower 和 Observer 的更新（和 Paxos 一样）。当然这么说有点虚，画张图理解一下。\n嗯。。。看起来很简单，貌似懂了🤥🤥🤥。这两个 Queue 哪冒出来的？答案是 ZAB 需要让 Follower 和 Observer 保证顺序性 。何为顺序性，比如我现在有一个写请求A，此时 Leader 将请求A广播出去，因为只需要半数同意就行，所以可能这个时候有一个 Follower F1因为网络原因没有收到，而 Leader 又广播了一个请求B，因为网络原因，F1竟然先收到了请求B然后才收到了请求A，这个时候请求处理的顺序不同就会导致数据的不同，从而 产生数据不一致问题 。\n所以在 Leader 这端，它为每个其他的 zkServer 准备了一个 队列 ，采用先进先出的方式发送消息。由于协议是 通过 TCP 来进行网络通信的，保证了消息的发送顺序性，接受顺序性也得到了保证。\n队列的先进先出+tcp的发送顺序性，保证了接收顺序的一致性\n除此之外，在 ZAB 中还定义了一个 全局单调递增的事务ID ZXID ，它是一个64位long型，其中高32位表示 epoch 年代，低32位表示事务id。epoch 是会根据 Leader 的变化而变化的，当一个 Leader 挂了，新的 Leader 上位的时候，年代（epoch）就变了。而低32位可以简单理解为递增的事务id。\n定义这个的原因也是为了顺序性，每个 proposal 在 Leader 中生成后需要 通过其 ZXID 来进行排序 ，才能得到处理。\n5.4. 崩溃恢复模式 # 说到崩溃恢复我们首先要提到 ZAB 中的 Leader 选举算法，当系统出现崩溃影响最大应该是 Leader 的崩溃，因为我们只有一个 Leader ，所以当 Leader 出现问题的时候我们势必需要重新选举 Leader 。\nLeader 选举可以分为两个不同的阶段，第一个是我们提到的 Leader 宕机需要重新选举，第二则是当 Zookeeper 启动时需要进行系统的 Leader 初始化选举。下面我先来介绍一下 ZAB 是如何进行初始化选举的。\n假设我们集群中有3台机器，那也就意味着我们需要两台以上同意（超过半数）。比如这个时候我们启动了 server1 ，它会首先 投票给自己 ，投票内容为服务器的 myid 和 ZXID ，因为初始化所以 ZXID 都为0，此时 server1 发出的投票为 (1,0)。但此时 server1 的投票仅为1，所以不能作为 Leader ，此时还在选举阶段所以整个集群处于 Looking 状态。\n接着 server2 启动了，它首先也会将投票选给自己(2,0)，并将投票信息广播出去（server1也会，只是它那时没有其他的服务器了），server1 在收到 server2 的投票信息后会将投票信息与自己的作比较。首先它会比较 ZXID ，ZXID 大的优先为 Leader，如果相同则比较 myid，myid 大的优先作为 Leader。所以此时server1 发现 server2 更适合做 Leader，它就会将自己的投票信息更改为(2,0)然后再广播出去，之后server2 收到之后发现和自己的一样无需做更改，并且自己的 投票已经超过半数 ，则 确定 server2 为 Leader，server1 也会将自己服务器设置为 Following 变为 Follower。整个服务器就从 Looking 变为了正常状态。\n当 server3 启动发现集群没有处于 Looking 状态时，它会直接以 Follower 的身份加入集群。\n还是前面三个 server 的例子，如果在整个集群运行的过程中 server2 挂了，那么整个集群会如何重新选举 Leader 呢？其实和初始化选举差不多。\n首先毫无疑问的是剩下的两个 Follower 会将自己的状态 从 Following 变为 Looking 状态 ，然后每个 server 会向初始化投票一样首先给自己投票（这不过这里的 zxid 可能不是0了，这里为了方便随便取个数字）。\n假设 server1 给自己投票为(1,99)，然后广播给其他 server，server3 首先也会给自己投票(3,95)，然后也广播给其他 server。server1 和 server3 此时会收到彼此的投票信息，和一开始选举一样，他们也会比较自己的投票和收到的投票（zxid 大的优先，如果相同那么就 myid 大的优先）。这个时候 server1 收到了 server3 的投票发现没自己的合适故不变，server3 收到 server1 的投票结果后发现比自己的合适于是更改投票为(1,99)然后广播出去，最后 server1 收到了发现自己的投票已经超过半数就把自己设为 Leader，server3 也随之变为 Follower。\n请注意 ZooKeeper 为什么要设置奇数个结点？比如这里我们是三个，挂了一个我们还能正常工作，挂了两个我们就不能正常工作了（已经没有超过半数的节点数了，所以无法进行投票等操作了）。而假设我们现在有四个，挂了一个也能工作，但是挂了两个也不能正常工作了，这是和三个一样的，而三个比四个还少一个，带来的效益是一样的，所以 Zookeeper 推荐奇数个 server 。\n那么说完了 ZAB 中的 Leader 选举方式之后我们再来了解一下 崩溃恢复 是什么玩意？\n其实主要就是 当集群中有机器挂了，我们整个集群如何保证数据一致性？\n如果只是 Follower 挂了，而且挂的（总数）没超过半数的时候，因为我们一开始讲了在 Leader 中会维护队列，所以不用担心后面的数据没接收到导致数据不一致性。\n如果 Leader 挂了那就麻烦了，我们肯定需要先暂停服务变为 Looking 状态然后进行 Leader 的重新选举（上面我讲过了），但这个就要分为两种情况了，分别是 确保已经被Leader提交的提案最终能够被所有的Follower提交 和 跳过那些已经被丢弃的提案 。\n确保已经被Leader提交的提案最终能够被所有的Follower提交是什么意思呢？\n假设 Leader (server2) 发送 commit 请求（忘了请看上面的消息广播模式），他发送给了 server3，然后要发给 server1 的时候突然挂了。这个时候重新选举的时候我们如果把 server1 作为 Leader 的话，那么肯定会产生数据不一致性，因为 server3 肯定会提交刚刚 server2 发送的 commit 请求的提案，而 server1 根本没收到所以会丢弃。\n那怎么解决呢？\n聪明的同学肯定会质疑，这个时候 server1 已经不可能成为 Leader 了，因为 server1 和 server3 进行投票选举的时候会比较 ZXID ，而此时 server3 的 ZXID 肯定比 server1 的大了。(不理解可以看前面的选举算法)\n那么跳过那些已经被丢弃的提案又是什么意思呢？\n假设 Leader (server2) 此时同意了提案N1，自身提交了这个事务并且要发送给所有 Follower 要 commit 的请求，却在这个时候挂了，此时肯定要重新进行 Leader 的选举，比如说此时选 server1 为 Leader （这无所谓）。但是过了一会，这个 挂掉的 Leader 又重新恢复了 ，此时它肯定会作为 Follower 的身份进入集群中，需要注意的是刚刚 server2 已经同意提交了提案N1，但其他 server 并没有收到它的 commit 信息，所以其他 server 不可能再提交这个提案N1了，这样就会出现数据不一致性问题了，所以 该提案N1最终需要被抛弃掉 。\n6. Zookeeper的几个理论知识 # 了解了 ZAB 协议还不够，它仅仅是 Zookeeper 内部实现的一种方式，而我们如何通过 Zookeeper 去做一些典型的应用场景呢？比如说集群管理，分布式锁，Master 选举等等。\n这就涉及到如何使用 Zookeeper 了，但在使用之前我们还需要掌握几个概念。比如 Zookeeper 的 数据模型 、会话机制、ACL、Watcher机制 等等。\n6.1. 数据模型 # zookeeper 数据存储结构与标准的 Unix 文件系统非常相似，都是在根节点下挂很多子节点(树型)。但是 zookeeper 中没有文件系统中目录与文件的概念，而是 使用了 znode 作为数据节点 。znode 是 zookeeper 中的最小数据单元，每个 znode 上都可以保存数据，同时还可以挂载子节点，形成一个树形化命名空间。\n每个 znode 都有自己所属的 节点类型 和 节点状态。\n其中节点类型可以分为 持久节点、持久顺序节点、临时节点 和 临时顺序节点。\n持久节点：一旦创建就一直存在，直到将其删除。 持久顺序节点：一个父节点可以为其子节点 维护一个创建的先后顺序 ，这个顺序体现在 节点名称 上，是节点名称后自动添加一个由 10 位数字组成的数字串，从 0 开始计数。 临时节点：临时节点的生命周期是与 客户端会话 绑定的，会话消失则节点消失 。临时节点 只能做叶子节点 ，不能创建子节点。 临时顺序节点：父节点可以创建一个维持了顺序的临时节点(和前面的持久顺序性节点一样)。 节点状态中包含了很多节点的属性比如 czxid 、mzxid 等等，在 zookeeper 中是使用 Stat 这个类来维护的。下面我列举一些属性解释。\nczxid：Created ZXID，该数据节点被 创建 时的事务ID。 mzxid：Modified ZXID，节点 最后一次被更新时 的事务ID。 ctime：Created Time，该节点被创建的时间。 mtime： Modified Time，该节点最后一次被修改的时间。 version：节点的版本号。 cversion：子节点 的版本号。 aversion：节点的 ACL 版本号。 ephemeralOwner：创建该节点的会话的 sessionID ，如果该节点为持久节点，该值为0。 dataLength：节点数据内容的长度。 numChildre：该节点的子节点个数，如果为临时节点为0。 pzxid：该节点子节点列表最后一次被修改时的事务ID，注意是子节点的 列表 ，不是内容。 6.2. 会话 # 我想这个对于后端开发的朋友肯定不陌生，不就是 session 吗？只不过 zk 客户端和服务端是通过 TCP 长连接 维持的会话机制，其实对于会话来说你可以理解为 保持连接状态 。\n在 zookeeper 中，会话还有对应的事件，比如 CONNECTION_LOSS 连接丢失事件 、SESSION_MOVED 会话转移事件 、SESSION_EXPIRED 会话超时失效事件 。\n6.3. ACL # ACL 为 Access Control Lists ，它是一种权限控制。在 zookeeper 中定义了5种权限，它们分别为：\nCREATE ：创建子节点的权限。 READ：获取节点数据和子节点列表的权限。 WRITE：更新节点数据的权限。 DELETE：删除子节点的权限。 ADMIN：设置节点 ACL 的权限。 6.4. Watcher机制 # Watcher 为事件监听器，是 zk 非常重要的一个特性，很多功能都依赖于它，它有点类似于订阅的方式，即客户端向服务端 注册 指定的 watcher ，当服务端符合了 watcher 的某些事件或要求则会 向客户端发送事件通知 ，客户端收到通知后找到自己定义的 Watcher 然后 执行相应的回调方法 。\n7. Zookeeper的几个典型应用场景 # 前面说了这么多的理论知识，你可能听得一头雾水，这些玩意有啥用？能干啥事？别急，听我慢慢道来。\n7.1. 选主 # 还记得上面我们的所说的临时节点吗？因为 Zookeeper 的强一致性，能够很好地在保证 在高并发的情况下保证节点创建的全局唯一性 (即无法重复创建同样的节点)。\n利用这个特性，我们可以 让多个客户端创建一个指定的节点 ，创建成功的就是 master。\n但是，如果这个 master 挂了怎么办？？？\n你想想为什么我们要创建临时节点？还记得临时节点的生命周期吗？master 挂了是不是代表会话断了？会话断了是不是意味着这个节点没了？还记得 watcher 吗？我们是不是可以 让其他不是 master 的节点监听节点的状态 ，比如说我们监听这个临时节点的父节点，如果子节点个数变了就代表 master 挂了，这个时候我们 触发回调函数进行重新选举 ，或者我们直接监听节点的状态，我们可以通过节点是否已经失去连接来判断 master 是否挂了等等。\n总的来说，我们可以完全 利用 临时节点、节点状态 和 watcher 来实现选主的功能，临时节点主要用来选举，节点状态和**watcher** 可以用来判断 master 的活性和进行重新选举。\n7.2. 分布式锁 # 分布式锁的实现方式有很多种，比如 Redis 、数据库 、zookeeper 等。个人认为 zookeeper 在实现分布式锁这方面是非常非常简单的。\n上面我们已经提到过了 zk在高并发的情况下保证节点创建的全局唯一性，这玩意一看就知道能干啥了。实现互斥锁呗，又因为能在分布式的情况下，所以能实现分布式锁呗。\n如何实现呢？这玩意其实跟选主基本一样，我们也可以利用临时节点的创建来实现。\n首先肯定是如何获取锁，因为创建节点的唯一性，我们可以让多个客户端同时创建一个临时节点，创建成功的就说明获取到了锁 。然后没有获取到锁的客户端也像上面选主的非主节点创建一个 watcher 进行节点状态的监听，如果这个互斥锁被释放了（可能获取锁的客户端宕机了，或者那个客户端主动释放了锁）可以调用回调函数重新获得锁。\nzk 中不需要向 redis 那样考虑锁得不到释放的问题了，因为当客户端挂了，节点也挂了，锁也释放了。是不是很简单？\n那能不能使用 zookeeper 同时实现 共享锁和独占锁 呢？答案是可以的，不过稍微有点复杂而已。\n还记得 有序的节点 吗？\n这个时候我规定所有创建节点必须有序，当你是读请求（要获取共享锁）的话，如果 没有比自己更小的节点，或比自己小的节点都是读请求 ，则可以获取到读锁，然后就可以开始读了。若比自己小的节点中有写请求 ，则当前客户端无法获取到读锁，只能等待前面的写请求完成。\n如果你是写请求（获取独占锁），若 没有比自己更小的节点 ，则表示当前客户端可以直接获取到写锁，对数据进行修改。若发现 有比自己更小的节点，无论是读操作还是写操作，当前客户端都无法获取到写锁 ，等待所有前面的操作完成。\n这就很好地同时实现了共享锁和独占锁，当然还有优化的地方，比如当一个锁得到释放它会通知所有等待的客户端从而造成 羊群效应 。此时你可以通过让等待的节点只监听他们前面的节点。\n具体怎么做呢？其实也很简单，你可以让 读请求监听比自己小的最后一个写请求节点，写请求只监听比自己小的最后一个节点 ，感兴趣的小伙伴可以自己去研究一下。\n7.3. 命名服务 # 如何给一个对象设置ID，大家可能都会想到 UUID，但是 UUID 最大的问题就在于它太长了。。。(太长不一定是好事，嘿嘿嘿)。那么在条件允许的情况下，我们能不能使用 zookeeper 来实现呢？\n我们之前提到过 zookeeper 是通过 树形结构 来存储数据节点的，那也就是说，对于每个节点的 全路径，它必定是唯一的，我们可以使用节点的全路径作为命名方式了。而且更重要的是，路径是我们可以自己定义的，这对于我们对有些有语意的对象的ID设置可以更加便于理解。\n7.4. 集群管理和注册中心 # 看到这里是不是觉得 zookeeper 实在是太强大了，它怎么能这么能干！\n别急，它能干的事情还很多呢。可能我们会有这样的需求，我们需要了解整个集群中有多少机器在工作，我们想对集群中的每台机器的运行时状态进行数据采集，对集群中机器进行上下线操作等等。\n而 zookeeper 天然支持的 watcher 和 临时节点能很好的实现这些需求。我们可以为每条机器创建临时节点，并监控其父节点，如果子节点列表有变动（我们可能创建删除了临时节点），那么我们可以使用在其父节点绑定的 watcher 进行状态监控和回调。\n至于注册中心也很简单，我们同样也是让 服务提供者 在 zookeeper 中创建一个临时节点并且将自己的 ip、port、调用方式 写入节点，当 服务消费者 需要进行调用的时候会 通过注册中心找到相应的服务的地址列表(IP端口什么的) ，并缓存到本地(方便以后调用)，当消费者调用服务时，不会再去请求注册中心，而是直接通过负载均衡算法从地址列表中取一个服务提供者的服务器调用服务。\n当服务提供者的某台服务器宕机或下线时，相应的地址会从服务提供者地址列表中移除。同时，注册中心会将新的服务地址列表发送给服务消费者的机器并缓存在消费者本机（当然你可以让消费者进行节点监听，我记得 Eureka 会先试错，然后再更新）。\n8. 总结 # 看到这里的同学实在是太有耐心了👍👍👍，如果觉得我写得不错的话点个赞哈。\n不知道大家是否还记得我讲了什么😒。\n这篇文章中我带大家入门了 zookeeper 这个强大的分布式协调框架。现在我们来简单梳理一下整篇文章的内容。\n分布式与集群的区别\n2PC 、3PC 以及 paxos 算法这些一致性框架的原理和实现。\nzookeeper 专门的一致性算法 ZAB 原子广播协议的内容（Leader 选举、崩溃恢复、消息广播）。\nzookeeper 中的一些基本概念，比如 ACL，数据节点，会话，watcher机制等等。\nzookeeper 的典型应用场景，比如选主，注册中心等等。\n如果忘了可以回去看看再次理解一下，如果有疑问和建议欢迎提出🤝🤝🤝。\n"},{"id":90,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly06ly_zookeeper-intro/","title":"zookeeper介绍","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n1. 前言 # 相信大家对 ZooKeeper 应该不算陌生。但是你真的了解 ZooKeeper 到底有啥用不？如果别人/面试官让你给他讲讲对于 ZooKeeper 的认识，你能回答到什么地步呢？\n拿我自己来说吧！我本人曾经使用 Dubbo 来做分布式项目的时候，使用了 ZooKeeper 作为注册中心。为了保证分布式系统能够同步访问某个资源，我还使用 ZooKeeper 做过分布式锁。另外，我在学习 Kafka 的时候，知道 Kafka 很多功能的实现依赖了 ZooKeeper。\n前几天，总结项目经验的时候，我突然问自己 ZooKeeper 到底是个什么东西？想了半天，脑海中只是简单的能浮现出几句话：\nZooKeeper 可以被用作注册中心、分布式锁； ZooKeeper 是 Hadoop 生态系统的一员； 构建 ZooKeeper 集群的时候，使用的服务器最好是奇数台。 由此可见，我对于 ZooKeeper 的理解仅仅是停留在了表面。\n所以，通过本文，希望带大家稍微详细的了解一下 ZooKeeper 。如果没有学过 ZooKeeper ，那么本文将会是你进入 ZooKeeper 大门的垫脚砖。如果你已经接触过 ZooKeeper ，那么本文将带你回顾一下 ZooKeeper 的一些基础概念。\n另外，本文不光会涉及到 ZooKeeper 的一些概念，后面的文章会介绍到 ZooKeeper 常见命令的使用以及使用 Apache Curator 作为 ZooKeeper 的客户端。\n如果文章有任何需要改善和完善的地方，欢迎在评论区指出，共同进步！\n2. ZooKeeper 介绍 # 2.1. ZooKeeper 由来 # 正式介绍 ZooKeeper 之前，我们先来看看 ZooKeeper 的由来，还挺有意思的。\n下面这段内容摘自《从 Paxos 到 ZooKeeper 》第四章第一节，推荐大家阅读一下：\nZooKeeper 最早起源于雅虎研究院的一个研究小组。在当时，研究人员发现，在雅虎内部很多大型系统基本都需要依赖一个类似的系统来进行分布式协调，但是这些系统往往都存在分布式单点问题。所以，雅虎的开发人员就试图开发一个通用的无单点问题的分布式协调框架，以便让开发人员将精力集中在处理业务逻辑上。\n关于“ZooKeeper”这个项目的名字，其实也有一段趣闻。在立项初期，考虑到之前内部很多项目都是使用动物的名字来命名的（例如著名的 Pig 项目),雅虎的工程师希望给这个项目也取一个动物的名字。时任研究院的首席科学家 RaghuRamakrishnan 开玩笑地说：“在这样下去，我们这儿就变成动物园了！”此话一出，大家纷纷表示就叫动物园管理员吧一一一因为各个以动物命名的分布式组件放在一起，雅虎的整个分布式系统看上去就像一个大型的动物园了，而 ZooKeeper 正好要用来进行分布式环境的协调一一于是，ZooKeeper 的名字也就由此诞生了。\n2.2. ZooKeeper 概览 # ZooKeeper 是一个开源的分布式协调服务，它的设计目标是将那些复杂且容易出错的分布式一致性服务封装起来，构成一个高效可靠的原语集，并以一系列简单易用的接口提供给用户使用。\n原语： 操作系统或计算机网络用语范畴。是由若干条指令组成的，用于完成一定功能的一个过程。具有不可分割性·即原语的执行必须是连续的，在执行过程中不允许被中断。\nZooKeeper 为我们提供了高可用、高性能、稳定的分布式数据一致性解决方案，通常被用于实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。\n另外，ZooKeeper 将数据保存在内存中，性能是非常棒的。 在“读”多于“写”的应用程序中尤其地高性能，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景）。\n2.3. ZooKeeper 特点 # 顺序一致性： 从同一客户端发起的事务请求，最终将会严格地按照顺序被应用到 ZooKeeper 中去。 原子性： 所有事务请求的处理结果在整个集群中所有机器上的应用情况是一致的，也就是说，要么整个集群中所有的机器都成功应用了某一个事务，要么都没有应用。 单一系统映像 ： 无论客户端连到哪一个 ZooKeeper 服务器上，其看到的服务端数据模型都是一致的。 可靠性： 一旦一次更改请求被应用，更改的结果就会被持久化，直到被下一次更改覆盖。 2.4. ZooKeeper 典型应用场景 # ZooKeeper 概览中，我们介绍到使用其通常被用于实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。\n下面选 3 个典型的应用场景来专门说说：\n分布式锁 ： 通过创建唯一节点获得分布式锁，当获得锁的一方执行完相关代码或者是挂掉之后就释放锁。 命名服务 ：可以通过 ZooKeeper 的顺序节点生成全局唯一 ID 数据发布/订阅 ：通过 Watcher 机制 可以很方便地实现数据发布/订阅。当你将数据发布到 ZooKeeper 被监听的节点上，其他机器可通过监听 ZooKeeper 上节点的变化来实现配置的动态更新。 实际上，这些功能的实现基本都得益于 ZooKeeper 可以保存数据的功能，但是 ZooKeeper 不适合保存大量数据，这一点需要注意。\n2.5. 有哪些著名的开源项目用到了 ZooKeeper? # Kafka : ZooKeeper 主要为 Kafka 提供 Broker 和 Topic 的注册以及多个 Partition 的负载均衡等功能。 Hbase : ZooKeeper 为 Hbase 提供确保整个集群只有一个 Master 以及保存和提供 regionserver 状态信息（是否在线）等功能。 Hadoop : ZooKeeper 为 Namenode 提供高可用支持。 3. ZooKeeper 重要概念解读 # 破音：拿出小本本，下面的内容非常重要哦！\n3.1. Data model（数据模型） # ZooKeeper 数据模型采用层次化的多叉树形结构，每个节点上都可以存储数据，这些数据可以是数字、字符串或者是二级制序列。并且。每个节点还可以拥有 N 个子节点，最上层是根节点以“/”来代表。每个数据节点在 ZooKeeper 中被称为 znode，它是 ZooKeeper 中数据的最小单元。并且，每个 znode 都一个唯一的路径标识。\n强调一句：ZooKeeper 主要是用来协调服务的，而不是用来存储业务数据的，所以不要放比较大的数据在 znode 上，ZooKeeper 给出的上限是每个结点的数据大小最大是 1M。\n从下图可以更直观地看出：ZooKeeper 节点路径标识方式和 Unix 文件系统路径非常相似，都是由一系列使用斜杠\u0026quot;/\u0026ldquo;进行分割的路径表示，开发人员可以向这个节点中写入数据，也可以在节点下面创建子节点。这些操作我们后面都会介绍到。\n3.2. znode（数据节点） # 介绍了 ZooKeeper 树形数据模型之后，我们知道每个数据节点在 ZooKeeper 中被称为 znode，它是 ZooKeeper 中数据的最小单元。你要存放的数据就放在上面，是你使用 ZooKeeper 过程中经常需要接触到的一个概念。\n3.2.1. znode 4 种类型 # 我们通常是将 znode 分为 4 大类：\n持久（PERSISTENT）节点 ：一旦创建就一直存在即使 ZooKeeper 集群宕机，直到将其删除。 临时（EPHEMERAL）节点 ：临时节点的生命周期是与 客户端会话（session） 绑定的，会话消失则节点消失 。并且，临时节点只能做叶子节点 ，不能创建子节点。 持久顺序（PERSISTENT_SEQUENTIAL）节点 ：除了具有持久（PERSISTENT）节点的特性之外， 子节点的名称还具有顺序性。比如 /node1/app0000000001 、/node1/app0000000002 。 临时顺序（EPHEMERAL_SEQUENTIAL）节点 ：除了具备临时（EPHEMERAL）节点的特性之外，子节点的名称还具有顺序性。 3.2.2. znode 数据结构 # 每个 znode 由 2 部分组成:\nstat ：状态信息 data ： 节点存放的数据的具体内容 如下所示，我通过 get 命令来获取 根目录下的 dubbo 节点的内容。（get 命令在下面会介绍到）。\n[zk: 127.0.0.1:2181(CONNECTED) 6] get /dubbo # 该数据节点关联的数据内容为空 null # 下面是该数据节点的一些状态信息，其实就是 Stat 对象的格式化输出 cZxid = 0x2 ctime = Tue Nov 27 11:05:34 CST 2018 mZxid = 0x2 mtime = Tue Nov 27 11:05:34 CST 2018 pZxid = 0x3 cversion = 1 dataVersion = 0 aclVersion = 0 ephemeralOwner = 0x0 dataLength = 0 numChildren = 1 Stat 类中包含了一个数据节点的所有状态信息的字段，包括事务 ID-cZxid、节点创建时间-ctime 和子节点个数-numChildren 等等。\n下面我们来看一下每个 znode 状态信息究竟代表的是什么吧！（下面的内容来源于《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》，因为 Guide 确实也不是特别清楚，要学会参考资料的嘛！ ） ：\nznode 状态信息 解释 cZxid create ZXID，即该数据节点被创建时的事务 id ctime create time，即该节点的创建时间 mZxid modified ZXID，即该节点最终一次更新时的事务 id mtime modified time，即该节点最后一次的更新时间 pZxid 该节点的子节点列表最后一次修改时的事务 id，只有子节点列表变更才会更新 pZxid，子节点内容变更不会更新 cversion 子节点版本号，当前节点的子节点每次变化时值增加 1 dataVersion 数据节点内容版本号，节点创建时为 0，每更新一次节点内容(不管内容有无变化)该版本号的值增加 1 aclVersion 节点的 ACL 版本号，表示该节点 ACL 信息变更次数 ephemeralOwner 创建该临时节点的会话的 sessionId；如果当前节点为持久节点，则 ephemeralOwner=0 dataLength 数据节点内容长度 numChildren 当前节点的子节点个数 3.3. 版本（version） # 在前面我们已经提到，对应于每个 znode，ZooKeeper 都会为其维护一个叫作 Stat 的数据结构，Stat 中记录了这个 znode 的三个相关的版本：\ndataVersion ：当前 znode 节点的版本号 cversion ： 当前 znode 子节点的版本 aclVersion ： 当前 znode 的 ACL 的版本。 3.4. ACL（权限控制） # ZooKeeper 采用 ACL（AccessControlLists）策略来进行权限控制，类似于 UNIX 文件系统的权限控制。\n对于 znode 操作的权限，ZooKeeper 提供了以下 5 种：\nCREATE : 能创建子节点 READ ：能获取节点数据和列出其子节点 WRITE : 能设置/更新节点数据 DELETE : 能删除子节点 ADMIN : 能设置节点 ACL 的权限 其中尤其需要注意的是，CREATE 和 DELETE 这两种权限都是针对 子节点 的权限控制。\n对于身份认证，提供了以下几种方式：\nworld ： 默认方式，所有用户都可无条件访问。 auth :不使用任何 id，代表任何已认证的用户。 digest :用户名:密码认证方式： username:password 。 ip : 对指定 ip 进行限制。 3.5. Watcher（事件监听器） # Watcher（事件监听器），是 ZooKeeper 中的一个很重要的特性。ZooKeeper 允许用户在指定节点上注册一些 Watcher，并且在一些特定事件触发的时候，ZooKeeper 服务端会将事件通知到感兴趣的客户端上去，该机制是 ZooKeeper 实现分布式协调服务的重要特性。\n破音：非常有用的一个特性，都拿出小本本记好了，后面用到 ZooKeeper 基本离不开 Watcher（事件监听器）机制。\n3.6. 会话（Session） # Session 可以看作是 ZooKeeper 服务器与客户端的之间的一个 TCP 长连接，通过这个连接，客户端能够通过心跳检测与服务器保持有效的会话，也能够向 ZooKeeper 服务器发送请求并接受响应，同时还能够通过该连接接收来自服务器的 Watcher 事件通知。\nSession 有一个属性叫做：sessionTimeout ，sessionTimeout 代表会话的超时时间。当由于服务器压力太大、网络故障或是客户端主动断开连接等各种原因导致客户端连接断开时，只要在**sessionTimeout规定的时间内能够重新连接上集群中任意一台服务器，那么之前创建的会话仍然有效**。\n另外，在为客户端创建会话之前，服务端首先会为每个客户端都分配一个 sessionID。由于 sessionID是 ZooKeeper 会话的一个重要标识，许多与会话相关的运行机制都是基于这个 sessionID 的，因此，无论是哪台服务器为客户端分配的 sessionID，都务必保证全局唯一。\n4. ZooKeeper 集群 # 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。通常 3 台服务器就可以构成一个 ZooKeeper 集群了。ZooKeeper 官方提供的架构图就是一个 ZooKeeper 集群整体对外提供服务。\n[!\n上图中每一个 Server 代表一个安装 ZooKeeper 服务的服务器。组成 ZooKeeper 服务的服务器都会在内存中维护当前的服务器状态，并且每台服务器之间都互相保持着通信。集群间通过 **ZAB 协议（ZooKeeper Atomic Broadcast）**来保持数据的一致性。\n[ zookeeper中不是使用这个 ]\n最典型集群模式： Master/Slave 模式（主备模式）。在这种模式中，通常 Master 服务器作为主服务器提供写服务，其他的 Slave 服务器从服务器通过异步复制的方式获取 Master 服务器最新的数据提供读服务。\n4.1. ZooKeeper 集群角色 # 但是，在 ZooKeeper 中没有选择传统的 Master/Slave 概念，而是引入了 Leader、Follower 和 Observer 三种角色。如下图所示\n[!\nZooKeeper 集群中的所有机器通过一个 Leader 选举过程 来选定一台称为 “Leader” 的机器，Leader 既可以为客户端提供写服务又能提供读服务。除了 Leader 外，Follower 和 Observer 都只能提供读服务。Follower 和 Observer 唯一的区别在于 Observer 机器不参与 Leader 的选举过程，也不参与写操作的“过半写成功”策略，因此 Observer 机器可以在不影响写性能的情况下提升集群的读性能。\n角色 说明 Leader 为客户端提供读和写的服务，负责投票的发起和决议，更新系统状态。 Follower 为客户端提供读服务，如果是写服务则转发给 Leader。参与选举过程中的投票。 Observer 为客户端提供读服务，如果是写服务则转发给 Leader。不参与选举过程中的投票，也**不参与“过半写成功”**策略。在不影响写性能的情况下提升集群的读性能。此角色于 ZooKeeper3.3 系列新增的角色。 当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，就会进入 Leader 选举过程，这个过程会选举产生新的 Leader 服务器。\n这个过程大致是这样的：\nLeader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。 Discovery（发现阶段） ：在这个阶段，followers 跟准 leader 进行通信，同步 followers 最近接收的事务提议。 Synchronization（同步阶段） :同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。同步完成之后 准 leader 才会成为真正的 leader。 Broadcast（广播阶段） :到了这个阶段，ZooKeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步。 4.2. ZooKeeper 集群中的服务器状态 # LOOKING ：寻找 Leader。 LEADING ：Leader 状态，对应的节点为 Leader。 FOLLOWING ：Follower 状态，对应的节点为 Follower。 OBSERVING ：Observer 状态，对应节点为 Observer，该节点不参与 Leader 选举。 4.3. ZooKeeper 集群为啥最好奇数台？ # ZooKeeper 集群在宕掉几个 ZooKeeper 服务器之后，如果剩下的 ZooKeeper 服务器个数大于宕掉的个数的话整个 ZooKeeper 才依然可用。假如我们的集群中有 n 台 ZooKeeper 服务器，那么也就是剩下的服务数必须大于 n/2。\n有点绕，换句话就是说最多的宕机数必须小于一半（等于也不行），那么如果是奇数x，那他只能小于x/2 即为除之后的整数部分，就算再加一台，也最多只能宕机(x+1)/2 -1（等于奇数 (x+1)/2 -1），所以没必要再多一台，并不能增加可宕机数 先说一下结论，2n 和 2n-1 的容忍度是一样的，都是 n-1，大家可以先自己仔细想一想，这应该是一个很简单的数学问题了。 比如假如我们有 3 台，那么最大允许宕掉 1 台 ZooKeeper 服务器，如果我们有 4 台的的时候也同样只允许宕掉 1 台。 假如我们有 5 台，那么最大允许宕掉 2 台 ZooKeeper 服务器，如果我们有 6 台的的时候也同样只允许宕掉 2 台。\n综上，何必增加那一个不必要的 ZooKeeper 呢？\n4.4. ZooKeeper 选举的过半机制防止脑裂 # 何为集群脑裂？\n对于一个集群，通常多台机器会部署在不同机房，来提高这个集群的可用性。保证可用性的同时，会发生一种机房间网络线路故障，导致机房间网络不通，而集群被割裂成几个小集群。这时候子集群各自选主导致“脑裂”的情况。\n举例说明：比如现在有一个由 6 台服务器所组成的一个集群，部署在了 2 个机房，每个机房 3 台。正常情况下只有 1 个 leader，但是当两个机房中间网络断开的时候，每个机房的 3 台服务器都会认为另一个机房的 3 台服务器下线，而选出自己的 leader 并对外提供服务。若没有过半机制，当网络恢复的时候会发现有 2 个 leader。仿佛是 1 个大脑（leader）分散成了 2 个大脑，这就发生了脑裂现象。脑裂期间 2 个大脑都可能对外提供了服务，这将会带来数据一致性等问题。\n过半机制是如何防止脑裂现象产生的？\nZooKeeper 的过半机制导致不可能产生 2 个 leader，因为少于等于一半是不可能产生 leader 的，这就使得不论机房的机器如何分配都不可能发生脑裂。\n5. ZAB 协议和 Paxos 算法 # Paxos 算法应该可以说是 ZooKeeper 的灵魂了。但是，ZooKeeper 并没有完全采用 Paxos 算法 ，而是使用 ZAB 协议作为其保证数据一致性的核心算法。另外，在 ZooKeeper 的官方文档中也指出，ZAB 协议并不像 Paxos 算法那样，是一种通用的分布式一致性算法，它是一种特别为 Zookeeper 设计的崩溃可恢复的原子消息广播算法。\n5.1. ZAB 协议介绍 # ZAB（ZooKeeper Atomic Broadcast 原子广播） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。 在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议，ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。\n5.2. ZAB 协议两种基本的模式：崩溃恢复和消息广播 # ZAB 协议包括两种基本的模式，分别是\n崩溃恢复 ：当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB 协议就会进入恢复模式并选举产生新的 Leader 服务器。当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出恢复模式。其中，所谓的状态同步是指数据同步，用来保证集群中存在过半的机器能够和 Leader 服务器的数据状态保持一致。 消息广播 ：当集群中已经有过半的 Follower 服务器完成了和 Leader 服务器的状态同步，那么整个服务框架就可以进入消息广播模式了。 当一台同样遵守 ZAB 协议的服务器启动后加入到集群中时，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播，那么新加入的服务器就会自觉地进入数据恢复模式：找到 Leader 所在的服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 关于 ZAB 协议\u0026amp;Paxos 算法 需要讲和理解的东西太多了，具体可以看下面这两篇文章：\n图解 Paxos 一致性协议 Zookeeper ZAB 协议分析 6. 总结 # ZooKeeper 本身就是一个分布式程序（只要半数以上节点存活，ZooKeeper 就能正常服务）。 为了保证高可用，最好是以集群形态来部署 ZooKeeper，这样只要集群中大部分机器是可用的（能够容忍一定的机器故障），那么 ZooKeeper 本身仍然是可用的。 ZooKeeper 将数据保存在内存中，这也就保证了 高吞吐量和低延迟（但是内存限制了能够存储的容量不太大，此限制也是保持 znode 中存储的数据量较小的进一步原因）。 ZooKeeper 是高性能的。 在“读”多于“写”的应用程序中尤其地明显，因为“写”会导致所有的服务器间同步状态。（“读”多于“写”是协调服务的典型场景。） ZooKeeper 有临时节点的概念。 当创建临时节点的客户端会话一直保持活动，瞬时节点就一直存在。而当会话终结时，瞬时节点被删除。持久节点是指一旦这个 znode 被创建了，除非主动进行 znode 的移除操作，否则这个 znode 将一直保存在 ZooKeeper 上。 ZooKeeper 底层其实只提供了两个功能：① 管理（存储、读取）用户程序提交的数据；② 为用户程序提供数据节点监听服务。 7. 参考 # 《从 Paxos 到 ZooKeeper 分布式一致性原理与实践》 "},{"id":91,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly04ly_rpc-http/","title":"rpc_http","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n我正在参与掘金技术社区创作者签约计划招募活动，点击链接报名投稿。\n我想起了我刚工作的时候，第一次接触RPC协议，当时就很懵，我HTTP协议用的好好的，为什么还要用RPC协议？\n于是就到网上去搜。\n不少解释显得非常官方，我相信大家在各种平台上也都看到过，解释了又好像没解释，都在用一个我们不认识的概念去解释另外一个我们不认识的概念，懂的人不需要看，不懂的人看了还是不懂。\n这种看了，又好像没看的感觉，云里雾里的很难受，我懂。\n为了避免大家有强烈的审丑疲劳，今天我们来尝试重新换个方式讲一讲。\n从TCP聊起 # 作为一个程序员，假设我们需要在A电脑的进程发一段数据到B电脑的进程，我们一般会在代码里使用socket进行编程。\n这时候，我们可选项一般也就TCP和UDP二选一。TCP可靠，UDP不可靠。 除非是马总这种神级程序员（早期QQ大量使用UDP），否则，只要稍微对可靠性有些要求，普通人一般无脑选TCP就对了。\n类似下面这样。\nfd = socket(AF_INET,SOCK_STREAM,0); 复制代码 其中SOCK_STREAM，是指使用字节流传输数据，说白了就是TCP协议。\n在定义了socket之后，我们就可以愉快的对这个socket进行操作，比如用bind()绑定IP端口，用connect()发起建连。\n在连接建立之后，我们就可以使用send()发送数据，recv()接收数据。\n光这样一个纯裸的TCP连接，就可以做到收发数据了，那是不是就够了？\n不行，这么用会有问题。\n使用纯裸TCP会有什么问题 # 八股文常背，TCP是有三个特点，面向连接、可靠、基于字节流。\n这三个特点真的概括的非常精辟，这个八股文我们没白背。\n每个特点展开都能聊一篇文章，而今天我们需要关注的是基于字节流这一点。\n字节流可以理解为一个双向的通道里流淌的数据，这个数据其实就是我们常说的二进制数据，简单来说就是一大堆 01 串。纯裸TCP收发的这些 01 串之间是没有任何边界的，你根本不知道到哪个地方才算一条完整消息。 正因为这个没有任何边界的特点，所以当我们选择使用TCP发送 \u0026ldquo;夏洛\u0026quot;和\u0026quot;特烦恼\u0026rdquo; 的时候，接收端收到的就是 \u0026ldquo;夏洛特烦恼\u0026rdquo; ，这时候接收端没发区分你是想要表达 \u0026ldquo;夏洛\u0026rdquo;+\u0026ldquo;特烦恼\u0026rdquo; 还是 \u0026ldquo;夏洛特\u0026rdquo;+\u0026ldquo;烦恼\u0026rdquo; 。\n这就是所谓的粘包问题，之前也写过一篇专门的文章聊过这个问题。\n说这个的目的是为了告诉大家，纯裸TCP是不能直接拿来用的，你需要在这个基础上加入一些自定义的规则，用于区分消息边界。\n于是我们会把每条要发送的数据都包装一下，比如加入消息头，消息头里写清楚一个完整的包长度是多少，根据这个长度可以继续接收数据，截取出来后它们就是我们真正要传输的消息体。\n而这里头提到的消息头，还可以放各种东西，比如消息体是否被压缩过和消息体格式之类的，只要上下游都约定好了，互相都认就可以了，这就是所谓的协议。\n每个使用TCP的项目都可能会定义一套类似这样的协议解析标准，他们可能有区别，但原理都类似。\n于是基于TCP，就衍生了非常多的协议，比如HTTP和RPC。\nHTTP和RPC # 我们回过头来看网络的分层图。\nTCP是传输层的协议，而基于TCP造出来的HTTP和各类RPC协议，它们都只是定义了不同消息格式的应用层协议而已。\nHTTP协议（Hyper Text Transfer Protocol），又叫做超文本传输协议。我们用的比较多，平时上网在浏览器上敲个网址就能访问网页，这里用到的就是HTTP协议。\n而RPC（Remote Procedure Call），又叫做远程过程调用。它本身并不是一个具体的协议，而是一种调用方式。\n举个例子，我们平时调用一个本地方法就像下面这样。\nres = localFunc(req) 复制代码 如果现在这不是个本地方法，而是个远端服务器暴露出来的一个方法remoteFunc，如果我们还能像调用本地方法那样去调用它，这样就可以屏蔽掉一些网络细节，用起来更方便，岂不美哉？\nres = remoteFunc(req) 复制代码 基于这个思路，大佬们造出了非常多款式的RPC协议，比如比较有名的gRPC，thrift。\n值得注意的是，虽然大部分RPC协议底层使用TCP，但实际上它们不一定非得使用TCP，改用UDP或者HTTP，其实也可以做到类似的功能。\n到这里，我们回到文章标题的问题。\n既然有HTTP协议，为什么还要有RPC？\n其实，TCP是70年代出来的协议，而HTTP是90年代才开始流行的。而直接使用裸TCP会有问题，可想而知，这中间这么多年有多少自定义的协议，而这里面就有80年代出来的RPC。\n所以我们该问的不是既然有HTTP协议为什么要有RPC，而是为什么有RPC还要有HTTP协议。\n那既然有RPC了，为什么还要有HTTP呢？ # 现在电脑上装的各种联网软件，比如xx管家，xx卫士，它们都作为客户端（client） 需要跟服务端（server） 建立连接收发消息，此时都会用到应用层协议，在这种client/server (c/s) 架构下，它们可以使用自家造的RPC协议，因为它只管连自己公司的服务器就ok了。\n但有个软件不同，浏览器（browser） ，不管是chrome还是IE，它们不仅要能访问自家公司的服务器（server） ，还需要访问其他公司的网站服务器，因此它们需要有个统一的标准，不然大家没法交流。于是，HTTP就是那个时代用于统一 browser/server (b/s) 的协议。\n也就是说在多年以前，HTTP主要用于b/s架构，而RPC更多用于c/s架构。但现在其实已经没分那么清了，b/s和c/s在慢慢融合。 很多软件同时支持多端，比如某度云盘，既要支持网页版，还要支持手机端和pc端，如果通信协议都用HTTP的话，那服务器只用同一套就够了。而RPC就开始退居幕后，一般用于公司内部集群里，各个微服务之间的通讯。\n那这么说的话，都用HTTP得了，还用什么RPC？\n仿佛又回到了文章开头的样子，那这就要从它们之间的区别开始说起。\nHTTP和RPC有什么区别 # 我们来看看RPC和HTTP区别比较明显的几个点。\n服务发现 # 首先要向某个服务器发起请求，你得先建立连接，而建立连接的前提是，你得知道IP地址和端口。这个找到服务对应的IP端口的过程，其实就是服务发现。\n在HTTP中，你知道服务的域名，就可以通过DNS服务去解析得到它背后的IP地址，默认80端口。\n而RPC的话，就有些区别，一般会有专门的中间服务去保存服务名和IP信息，比如consul或者etcd，甚至是redis。想要访问某个服务，就去这些中间服务去获得IP和端口信息。由于dns也是服务发现的一种，所以也有基于dns去做服务发现的组件，比如CoreDNS。\n可以看出服务发现这一块，两者是有些区别，但不太能分高低。\n底层连接形式 # 以主流的HTTP1.1协议为例，其默认在建立底层TCP连接之后会一直保持这个连接（keep alive），之后的请求和响应都会复用这条连接。\n而RPC协议，也跟HTTP类似，也是通过建立TCP长链接进行数据交互，但不同的地方在于，RPC协议一般还会再建个连接池，在请求量大的时候，建立多条连接放在池内，要发数据的时候就从池里取一条连接出来，用完放回去，下次再复用，可以说非常环保。\n由于连接池有利于提升网络请求性能，所以不少编程语言的网络库里都会给HTTP加个连接池，比如go就是这么干的。\n可以看出这一块两者也没太大区别，所以也不是关键。\n传输的内容 # 基于TCP传输的消息，说到底，无非都是消息头header和消息体body。\nheader是用于标记一些特殊信息，其中最重要的是消息体长度。\nbody则是放我们真正需要传输的内容，而这些内容只能是二进制01串，毕竟计算机只认识这玩意。所以TCP传字符串和数字都问题不大，因为字符串可以转成编码再变成01串，而数字本身也能直接转为二进制。但结构体呢，我们得想个办法将它也转为二进制01串，这样的方案现在也有很多现成的，比如json，protobuf。\n这个将结构体转为二进制数组的过程就叫序列化，反过来将二进制数组复原成结构体的过程叫反序列化。\n对于主流的HTTP1.1，虽然它现在叫超文本协议，支持音频视频，但HTTP设计初是用于做网页文本展示的，所以它传的内容以字符串为主。header和body都是如此。在body这块，它使用json来序列化结构体数据。\n我们可以随便截个图直观看下。\n可以看到这里面的内容非常多的冗余，显得非常啰嗦。最明显的，像header里的那些信息，其实如果我们约定好头部的第几位是content-type，就不需要每次都真的把\u0026quot;content-type\u0026quot;这个字段都传过来，类似的情况其实在body的json结构里也特别明显。\n而RPC，因为它定制化程度更高，可以采用体积更小的protobuf或其他序列化协议去保存结构体数据，同时也不需要像HTTP那样考虑各种浏览器行为，比如302重定向跳转啥的。因此性能也会更好一些，这也是在公司内部微服务中抛弃HTTP，选择使用RPC的最主要原因。\n====缺一张图片====\n当然上面说的HTTP，其实特指的是现在主流使用的HTTP1.1，HTTP2在前者的基础上做了很多改进，所以性能可能比很多RPC协议还要好，甚至连gRPC底层都直接用的HTTP2。\n那么问题又来了。\n为什么既然有了HTTP2，还要有RPC协议？ # 这个是由于HTTP2是2015年出来的。那时候很多公司内部的RPC协议都已经跑了好些年了，基于历史原因，一般也没必要去换了。\n总结 # 纯裸TCP是能收发数据，但它是个无边界的数据流，上层需要定义消息格式用于定义消息边界。于是就有了各种协议，HTTP和各类RPC协议就是在TCP之上定义的应用层协议。 RPC本质上不算是协议，而是一种调用方式，而像gRPC和thrift这样的具体实现，才是协议，它们是实现了RPC调用的协议。目的是希望程序员能像调用本地方法那样去调用远端的服务方法。同时RPC有很多种实现方式，不一定非得基于TCP协议。 从发展历史来说，HTTP主要用于b/s架构，而RPC更多用于c/s架构。但现在其实已经没分那么清了，b/s和c/s在慢慢融合。 很多软件同时支持多端，所以对外一般用HTTP协议，而内部集群的微服务之间则采用RPC协议进行通讯。 RPC其实比HTTP出现的要早，且比目前主流的HTTP1.1性能要更好，所以大部分公司内部都还在使用RPC。 HTTP2.0在HTTP1.1的基础上做了优化，性能可能比很多RPC协议都要好，但由于是这几年才出来的，所以也不太可能取代掉RPC。 最后留个问题吧，大家有没有发现，不管是HTTP还是RPC，它们都有个特点，那就是消息都是客户端请求，服务端响应。客户端没问，服务端肯定就不答，这就有点僵了，但现实中肯定有需要下游主动发送消息给上游的场景，比如打个网页游戏，站在那啥也不操作，怪也会主动攻击我，这种情况该怎么办呢？\n最后 # 按照惯例，我应该在这里唯唯诺诺的求大家叫我两声靓仔的。\n但还是算了。因为我最近一直在想一个问题，希望兄弟们能在评论区告诉我答案。\n最近手机借给别人玩了一下午，现在老是给我推荐练习时长两年半的练习生视频。\n每个视频都在声嘶力竭的告诉我，鸡你太美。\n所以我很想问，兄弟们。\n鸡，到底美不美？\n头疼。\n右下角的点赞和再看还是可以走一波的。\n先这样。\n我是小白，我们下期见。\n别说了，一起在知识的海洋里呛水吧 # "},{"id":92,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly05ly_rpc-intro/","title":"rpc基础及面试题","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n简单介绍一下 RPC 相关的基础概念。\n何为 RPC? # RPC（Remote Procedure Call） 即远程过程调用，通过名字我们就能看出 RPC 关注的是远程调用而非本地调用。\n为什么要 RPC ？ 因为，两个不同的服务器上的服务提供的方法不在一个内存空间，所以，需要通过网络编程才能传递方法调用所需要的参数。并且，方法调用的结果也需要通过网络编程来接收。但是，如果我们自己手动网络编程来实现这个调用过程的话工作量是非常大的，因为，我们需要考虑底层传输方式（TCP还是UDP）、序列化方式等等方面。\nRPC 能帮助我们做什么呢？ 简单来说，通过 RPC 可以帮助我们调用远程计算机上某个服务的方法，这个过程就像调用本地方法一样简单。并且！我们不需要了解底层网络编程的具体细节。\n举个例子：两个不同的服务 A、B 部署在两台不同的机器上，服务 A 如果想要调用服务 B 中的某个方法的话就可以通过 RPC 来做。\n一言蔽之：RPC 的出现就是为了让你调用远程方法像调用本地方法一样简单。\nRPC 的原理是什么? # 为了能够帮助小伙伴们理解 RPC 原理，我们可以将整个 RPC的 核心功能看作是下面👇 5 个部分实现的：\n客户端（服务消费端） ：调用远程方法的一端。 客户端 Stub（桩） ： 这其实就是一代理类。代理类主要做的事情很简单，就是把你调用方法、类、方法参数等信息传递到服务端。 网络传输 ： 网络传输就是你要把你调用的方法的信息比如说参数啊这些东西传输到服务端，然后服务端执行完之后再把返回结果通过网络传输给你传输回来。网络传输的实现方式有很多种比如最近基本的 Socket或者性能以及封装更加优秀的 Netty（推荐）。 服务端 Stub（桩） ：这个桩就不是代理类了。我觉得理解为桩实际不太好，大家注意一下就好。这里的服务端 Stub 实际指的就是接收到客户端执行方法的请求后，去指定对应的方法然后返回结果给客户端的类。 服务端（服务提供端） ：提供远程方法的一端。 具体原理图如下，后面我会串起来将整个RPC的过程给大家说一下。\n服务消费端（client）以本地调用的方式调用远程服务； 客户端 Stub（client stub） 接收到调用后负责将方法、参数等组装成能够进行网络传输的消息体（序列化）：RpcRequest； 客户端 Stub（client stub） 找到远程服务的地址，并将消息发送到服务提供端； 服务端 Stub（桩）收到消息将消息反序列化为Java对象: RpcRequest； 服务端 Stub（桩）根据RpcRequest中的类、方法、方法参数等信息调用本地的方法； 服务端 Stub（桩）得到方法执行结果并将组装成能够进行网络传输的消息体：RpcResponse（序列化）发送至消费方； 客户端 Stub（client stub）接收到消息并将消息反序列化为Java对象:RpcResponse ，这样也就得到了最终结果。over! 相信小伙伴们看完上面的讲解之后，已经了解了 RPC 的原理。\n虽然篇幅不多，但是基本把 RPC 框架的核心原理讲清楚了！另外，对于上面的技术细节，我会在后面的章节介绍到。\n最后，对于 RPC 的原理，希望小伙伴不单单要理解，还要能够自己画出来并且能够给别人讲出来。因为，在面试中这个问题在面试官问到 RPC 相关内容的时候基本都会碰到。\n有哪些常见的 RPC 框架？ # 我们这里说的 RPC 框架指的是可以让客户端直接调用服务端方法，就像调用本地方法一样简单的框架，比如我下面介绍的 Dubbo、Motan、gRPC这些。 如果需要和 HTTP 协议打交道，解析和封装 HTTP 请求和响应。这类框架并不能算是“RPC 框架”，比如Feign。\nDubbo # Apache Dubbo 是一款微服务框架，为大规模微服务实践提供高性能 RPC 通信、流量治理、可观测性等解决方案， 涵盖 Java、Golang 等多种语言 SDK 实现。\nDubbo 提供了从服务定义、服务发现、服务通信到流量管控等几乎所有的服务治理能力，支持 Triple 协议（基于 HTTP/2 之上定义的下一代 RPC 通信协议）、应用级服务发现、Dubbo Mesh （Dubbo3 赋予了很多云原生友好的新特性）等特性。\nDubbo 是由阿里开源，后来加入了 Apache 。正是由于 Dubbo 的出现，才使得越来越多的公司开始使用以及接受分布式架构。\nDubbo 算的是比较优秀的国产开源项目了，它的源码也是非常值得学习和阅读的！\nGithub ：https://github.com/apache/incubator-dubbo 官网：https://dubbo.apache.org/zh/ Motan # Motan 是新浪微博开源的一款 RPC 框架，据说在新浪微博正支撑着千亿次调用。不过笔者倒是很少看到有公司使用，而且网上的资料也比较少。\n很多人喜欢拿 Motan 和 Dubbo 作比较，毕竟都是国内大公司开源的。笔者在查阅了很多资料，以及简单查看了其源码之后发现：Motan 更像是一个精简版的 Dubbo，可能是借鉴了 Dubbo 的思想，Motan 的设计更加精简，功能更加纯粹。\n不过，我不推荐你在实际项目中使用 Motan。如果你要是公司实际使用的话，还是推荐 Dubbo ，其社区活跃度以及生态都要好很多。\n从 Motan 看 RPC 框架设计：http://kriszhang.com/motan-rpc-impl/ Motan 中文文档：https://github.com/weibocom/motan/wiki/zh_overview gRPC # gRPC 是 Google 开源的一个高性能、通用的开源 RPC 框架。其由主要面向移动应用开发并基于 HTTP/2 协议标准而设计（支持双向流、消息头压缩等功能，更加节省带宽），基于 ProtoBuf 序列化协议开发，并且支持众多开发语言。\n何谓 ProtoBuf？ ProtoBuf（ Protocol Buffer） 是一种更加灵活、高效的数据格式，可用于通讯协议、数据存储等领域，基本支持所有主流编程语言且与平台无关。不过，通过 ProtoBuf 定义接口和数据类型还挺繁琐的，这是一个小问题。\n不得不说，gRPC 的通信层的设计还是非常优秀的，Dubbo-go 3.0 的通信层改进主要借鉴了 gRPC。\n不过，gRPC 的设计导致其几乎没有服务治理能力。如果你想要解决这个问题的话，就需要依赖其他组件比如腾讯的 PolarisMesh（北极星）了。\nGithub：https://github.com/grpc/grpc 官网：https://grpc.io/ Thrift # Apache Thrift 是 Facebook 开源的跨语言的 RPC 通信框架，目前已经捐献给 Apache 基金会管理，由于其跨语言特性和出色的性能，在很多互联网公司得到应用，有能力的公司甚至会基于 thrift 研发一套分布式服务框架，增加诸如服务注册、服务发现等功能。\nThrift支持多种不同的编程语言，包括C++、Java、Python、PHP、Ruby等（相比于 gRPC 支持的语言更多 ）。\n官网：https://thrift.apache.org/ Thrift 简单介绍：https://www.jianshu.com/p/8f25d057a5a9 总结 # gRPC 和 Thrift 虽然支持跨语言的 RPC 调用，但是它们只提供了最基本的 RPC 框架功能，缺乏一系列配套的服务化组件和服务治理功能的支撑。\nDubbo 不论是从功能完善程度、生态系统还是社区活跃度来说都是最优秀的。而且，Dubbo在国内有很多成功的案例比如当当网、滴滴等等，是一款经得起生产考验的成熟稳定的 RPC 框架。最重要的是你还能找到非常多的 Dubbo 参考资料，学习成本相对也较低。\n下图展示了 Dubbo 的生态系统。\nDubbo 也是 Spring Cloud Alibaba 里面的一个组件。\n但是，Dubbo 和 Motan 主要是给 Java 语言使用。虽然，Dubbo 和 Motan 目前也能兼容部分语言，但是不太推荐。如果需要跨多种语言调用的话，可以考虑使用 gRPC。\n综上，如果是 Java 后端技术栈，并且你在纠结选择哪一种 RPC 框架的话，我推荐你考虑一下 Dubbo。\n如何设计并实现一个 RPC 框架？ # 《手写 RPC 框架》 是我的知识星球的一个内部小册，我写了 12 篇文章来讲解如何从零开始基于 Netty+Kyro+Zookeeper 实现一个简易的 RPC 框架。\n麻雀虽小五脏俱全，项目代码注释详细，结构清晰，并且集成了 Check Style 规范代码结构，非常适合阅读和学习。\n内容概览 ：\n既然有了 HTTP 协议，为什么还要有 RPC ？ # HTTP 和 RPC 详细对比 。\n"},{"id":93,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly03ly_distributed-lock/","title":"分布式锁","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n网上有很多分布式锁相关的文章，写了一个相对简洁易懂的版本，针对面试和工作应该够用了。\n什么是分布式锁？ # 对于单机多线程来说，在 Java 中，我们通常使用 ReetrantLock 类、synchronized 关键字这类 JDK 自带的 本地锁 来控制一个 JVM 进程内的多个线程对本地共享资源的访问。\n下面是我对本地锁画的一张示意图。\n从图中可以看出，这些线程访问共享资源是互斥的，同一时刻只有一个线程可以获取到本地锁访问共享资源。\n分布式系统下，不同的服务/客户端通常运行在独立的 JVM 进程上。如果多个 JVM 进程共享同一份资源的话，使用本地锁就没办法实现资源的互斥访问了。于是，分布式锁 就诞生了。\n举个例子：系统的订单服务一共部署了 3 份，都对外提供服务。用户下订单之前需要检查库存，为了防止超卖，这里需要加锁以实现对检查库存操作的同步访问。由于订单服务位于不同的 JVM 进程中，本地锁在这种情况下就没办法正常工作了。我们需要用到分布式锁，这样的话，即使多个线程不在同一个 JVM 进程中也能获取到同一把锁，进而实现共享资源的互斥访问。\n下面是我对分布式锁画的一张示意图。\n从图中可以看出，这些独立的进程中的线程访问共享资源是互斥的，同一时刻只有一个线程可以获取到分布式锁访问共享资源。\n一个最基本的分布式锁需要满足：\n互斥 ：任意一个时刻，锁只能被一个线程持有； 高可用 ：锁服务是高可用的。并且，即使客户端的释放锁的代码逻辑出现问题(这里说的是异常，不是说代码写的有问题)，锁最终一定还是会被释放，不会影响其他线程对共享资源的访问。 可重入：(同)一个节点获取了锁之后，还可以再次获取锁。 通常情况下，我们一般会选择基于 Redis 或者 ZooKeeper 实现分布式锁，Redis 用的要更多一点，我这里也以 Redis 为例介绍分布式锁的实现。\n基于 Redis 实现分布式锁 # 如何基于 Redis 实现一个最简易的分布式锁？ # 不论是实现锁(本地)还是分布式锁，核心都在于**“互斥”**。\n在 Redis 中， SETNX 命令是可以帮助我们实现互斥。SETNX 即 SET if Not eXists (对应 Java 中的 setIfAbsent 方法)，如果 key 不存在的话，才会设置 key 的值。如果 key 已经存在， SETNX 啥也不做。\n\u0026gt; SETNX lockKey uniqueValue (integer) 1 \u0026gt; SETNX lockKey uniqueValue (integer) 0 #如上成功为1，失败为0 释放锁的话，直接通过 DEL 命令删除对应的 key 即可。\n\u0026gt; DEL lockKey (integer) 1 # 成功为1 为了误删到其他的锁，这里我们建议使用 Lua 脚本通过 key 对应的 value（唯一值）来判断。\n选用 Lua 脚本是为了保证解锁操作的原子性。因为 Redis 在执行 Lua 脚本时，可以以原子性的方式执行，从而保证了锁释放操作的原子性。\n// 释放锁时，先比较锁对应的 value 值是否相等，避免锁的误释放 if redis.call(\u0026#34;get\u0026#34;,KEYS[1]) == ARGV[1] then return redis.call(\u0026#34;del\u0026#34;,KEYS[1]) else return 0 end 这是一种最简易的 Redis 分布式锁实现，实现方式比较简单，性能也很高效。不过，这种方式实现分布式锁存在一些问题。就比如应用程序遇到一些问题比如释放锁的逻辑突然挂掉，可能会导致锁无法被释放，进而造成共享资源无法再被其他线程/进程访问。\n为什么要给锁设置一个过期时间？ # 为了避免锁无法被释放，我们可以想到的一个解决办法就是： 给这个 key（也就是锁） 设置一个过期时间 。\n127.0.0.1:6379\u0026gt; SET lockKey uniqueValue EX 3 NX OK lockKey ：加锁的锁名； uniqueValue ：能够唯一标示锁的随机字符串； NX ：只有当 lockKey 对应的 key 值不存在的时候才能 SET 成功； EX ：过期时间设置（秒为单位）EX 3 标示这个锁有一个 3 秒的自动过期时间。与 EX 对应的是 PX（毫秒为单位），这两个都是过期时间设置。 一定要保证设置指定 key 的值和过期时间是一个原子操作！！！ 不然的话，依然可能会出现锁无法被释放的问题。\n这样确实可以解决问题，不过，这种解决办法同样存在漏洞：如果操作共享资源的时间大于过期时间，就会出现锁提前过期的问题，进而导致分布式锁直接失效。如果锁的超时时间设置过长，又会影响到性能。\n你或许在想： 如果操作共享资源的操作还未完成，锁过期时间能够自己续期就好了！\n如何实现锁的优雅续期？ # 对于 Java 开发的小伙伴来说，已经有了现成的解决方案：Redisson 。其他语言的解决方案，可以在 Redis 官方文档中找到，地址：https://redis.io/topics/distlock 。\nRedisson 是一个开源的 Java 语言 Redis 客户端，提供了很多开箱即用的功能，不仅仅包括多种分布式锁的实现。并且，Redisson 还支持 Redis 单机、Redis Sentinel 、Redis Cluster 等多种部署架构。\nRedisson 中的分布式锁自带自动续期机制，使用起来非常简单，原理也比较简单，其提供了一个专门用来监控和续期锁的 Watch Dog（ 看门狗），如果操作共享资源的线程还未执行完成的话，Watch Dog 会不断地延长锁的过期时间，进而保证锁不会因为超时而被释放。\n如图，续期之前也是要检测是否为持锁线程\n看门狗名字的由来于 getLockWatchdogTimeout() 方法，这个方法返回的是看门狗给锁续期的过期时间，默认为 30 秒（redisson-3.17.6）。\n//默认 30秒，支持修改 private long lockWatchdogTimeout = 30 * 1000; public Config setLockWatchdogTimeout(long lockWatchdogTimeout) { this.lockWatchdogTimeout = lockWatchdogTimeout; return this; } public long getLockWatchdogTimeout() { return lockWatchdogTimeout; } renewExpiration() 方法包含了看门狗的主要逻辑：\nprivate void renewExpiration() { //...... Timeout task = commandExecutor.getConnectionManager().newTimeout(new TimerTask() { @Override public void run(Timeout timeout) throws Exception { //...... // 异步续期，基于 Lua 脚本(ly:我觉得是为了保证原子性所以用了Lua脚本) CompletionStage\u0026lt;Boolean\u0026gt; future = renewExpirationAsync(threadId); future.whenComplete((res, e) -\u0026gt; { if (e != null) { // 无法续期 log.error(\u0026#34;Can\u0026#39;t update lock \u0026#34; + getRawName() + \u0026#34; expiration\u0026#34;, e); EXPIRATION_RENEWAL_MAP.remove(getEntryName()); return; } if (res) { // 递归调用实现续期 renewExpiration(); } else { // 取消续期 cancelExpirationRenewal(null); } }); } // 延迟 internalLockLeaseTime/3（默认 10s，也就是 30/3） 再调用 }, internalLockLeaseTime / 3, TimeUnit.MILLISECONDS); ee.setTimeout(task); } 默认情况下，每过 10 秒，看门狗就会执行续期操作，将锁的超时时间设置为 30 秒。看门狗续期前也会先判断是否需要执行续期操作，需要才会执行续期，否则取消续期操作。\nWatch Dog 通过调用 renewExpirationAsync() 方法实现锁的异步续期：\nprotected CompletionStage\u0026lt;Boolean\u0026gt; renewExpirationAsync(long threadId) { return evalWriteAsync(getRawName(), LongCodec.INSTANCE, RedisCommands.EVAL_BOOLEAN, // 判断是否为持锁线程，如果是就执行续期操作，就锁的过期时间设置为 30s（默认） \u0026#34;if (redis.call(\u0026#39;hexists\u0026#39;, KEYS[1], ARGV[2]) == 1) then \u0026#34; + \u0026#34;redis.call(\u0026#39;pexpire\u0026#39;, KEYS[1], ARGV[1]); \u0026#34; + \u0026#34;return 1; \u0026#34; + \u0026#34;end; \u0026#34; + \u0026#34;return 0;\u0026#34;, Collections.singletonList(getRawName()), internalLockLeaseTime, getLockName(threadId)); } 可以看出， renewExpirationAsync 方法其实是调用 Lua 脚本实现的续期，这样做主要是为了保证续期操作的原子性。\n我这里以 Redisson 的分布式可重入锁 RLock 为例来说明如何使用 Redisson 实现分布式锁：\n// 1.获取指定的分布式锁对象 RLock lock = redisson.getLock(\u0026#34;lock\u0026#34;); // 2.拿锁且不设置锁超时时间，具备 Watch Dog 自动续期机制 lock.lock(); // 3.执行业务 ... // 4.释放锁 lock.unlock(); 只有未指定锁超时时间，才会使用到 Watch Dog 自动续期机制。\n// 手动给锁设置过期时间，不具备 Watch Dog 自动续期机制 lock.lock(10, TimeUnit.SECONDS); 如果使用 Redis 来实现分布式锁的话，还是比较推荐直接基于 Redisson 来做的。\n如何实现可重入锁？ # 所谓可重入锁指的是在一个线程中可以多次获取同一把锁，比如一个线程在执行一个带锁的方法，该方法中又调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法即可重入 ，而无需重新获得锁。像 Java 中的 synchronized 和 ReentrantLock 都属于可重入锁。\n不可重入的分布式锁基本可以满足绝大部分业务场景了，一些特殊的场景可能会需要使用可重入的分布式锁。\n可重入分布式锁的实现核心思路是线程在获取锁的时候判断是否为自己的锁，如果是的话，就不用再重新获取了。为此，我们可以为每个锁关联一个可重入计数器和一个占有它的线程。当可重入计数器大于 0 时，则锁被占有，需要判断占有该锁的线程和请求获取锁的线程是否为同一个。\n实际项目中，我们不需要自己手动实现，推荐使用我们上面提到的 Redisson ，其内置了多种类型的锁比如可重入锁（Reentrant Lock）、自旋锁（Spin Lock）、公平锁（Fair Lock）、多重锁（MultiLock）、 红锁（RedLock）、 读写锁（ReadWriteLock）。\nRedis 如何解决集群情况下分布式锁的可靠性？ # 为了避免单点故障（也就是只部署在一台机器，导致一台机器挂了服务就无法运行并提供功能），生产环境下的 Redis 服务通常是集群化部署的。\nRedis 集群下，上面介绍到的分布式锁的实现会存在一些问题。由于 Redis 集群数据同步到各个节点时是异步的，如果在 Redis 主节点获取到锁后，在没有同步到其他节点时，Redis 主节点宕机了，此时新的 Redis 主节点依然可以获取锁，所以多个应用服务就可以同时获取到锁。\n针对这个问题，Redis 之父 antirez 设计了 Redlock 算法 来解决。\nRedlock 算法的思想是让客户端向 Redis 集群中的多个独立的 Redis 实例 依次请求申请加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁，否则加锁失败。\n即使部分 Redis 节点出现问题，只要保证 Redis 集群中有半数以上的 Redis 节点可用，分布式锁服务就是正常的。\nRedlock 是直接操作 Redis 节点的，并不是通过 Redis 集群操作的，这样才可以避免 Redis 集群主从切换导致的锁丢失问题。\n注意，不是通过Redis集群做的哦\nRedlock 实现比较复杂，性能比较差，发生时钟变迁的情况下还存在安全性隐患。《数据密集型应用系统设计》一书的作者 Martin Kleppmann 曾经专门发文（How to do distributed locking - Martin Kleppmann - 2016）怼过 Redlock，他认为这是一个很差的分布式锁实现。感兴趣的朋友可以看看Redis 锁从面试连环炮聊到神仙打架这篇文章，有详细介绍到 antirez 和 Martin Kleppmann 关于 Redlock 的激烈辩论。\n实际项目中不建议使用 Redlock 算法，成本和收益不成正比。\n如果不是非要实现绝对可靠的分布式锁的话，其实单机版 Redis 就完全够了，实现简单，性能也非常高。如果你必须要实现一个绝对可靠的分布式锁的话，可以基于 Zookeeper 来做，只是性能会差一些。\n"},{"id":94,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly02ly_distributed-id/","title":"分布式id","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n分布式 ID 介绍 # 什么是 ID？ # 日常开发中，我们需要对系统中的各种数据使用 ID 唯一表示，比如用户 ID 对应且仅对应一个人，商品 ID 对应且仅对应一件商品，订单 ID 对应且仅对应一个订单。\n我们现实生活中也有各种 ID，比如身份证 ID 对应且仅对应一个人、地址 ID 对应且仅对应\n简单来说，ID 就是数据的唯一标识。\n什么是分布式 ID？ # 分布式 ID 是分布式系统下的 ID。分布式 ID 不存在与现实生活中(属于技术上的问题，跟业务无关)，属于计算机系统中的一个概念。\n我简单举一个分库分表的例子。\n我司的一个项目，使用的是单机 MySQL 。但是，没想到的是，项目上线一个月之后，随着使用人数越来越多，整个系统的数据量将越来越大。单机 MySQL 已经没办法支撑了，需要进行分库分表（推荐 Sharding-JDBC）。\n在分库之后， 数据遍布在不同服务器上的数据库，数据库的自增主键已经没办法满足生成的主键唯一了。我们如何为不同的数据节点生成全局唯一主键呢？\n这个时候就需要生成分布式 ID了。\n分布式 ID 需要满足哪些要求? # 分布式 ID 作为分布式系统中必不可少的一环，很多地方都要用到分布式 ID。\n一个最基本的分布式 ID 需要满足下面这些要求：\n全局唯一 ：ID 的全局唯一性肯定是首先要满足的！ 高性能 ： 分布式 ID 的生成速度要快，对本地资源消耗要小。 高可用 ：生成分布式 ID 的服务要保证可用性无限接近于 100%。 方便易用 ：拿来即用，使用方便，快速接入！ 除了这些之外，一个比较好的分布式 ID 还应保证：\n安全 ：ID 中不包含敏感信息。 有序递增 ：如果要把 ID 存放在数据库的话，ID 的有序性可以提升数据库写入速度。并且，很多时候 ，我们还很有可能会直接通过 ID 来进行排序。 有具体的业务含义 ：生成的 ID 如果能有具体的业务含义，可以让定位问题以及开发更透明化（通过 ID 就能确定是哪个业务）。 独立部署 ：也就是分布式系统单独有一个发号器服务，专门用来生成分布式 ID。这样就生成 ID 的服务可以和业务相关的服务解耦。不过，这样同样带来了网络调用消耗增加的问题。总的来说，如果需要用到分布式 ID 的场景比较多的话，独立部署的发号器服务还是很有必要的。 分布式 ID 常见解决方案 # 这里说的是如何获取到一个分布式ID，而不是具体分布式ID的使用\n数据库 # 数据库主键自增 # 这种方式就比较简单直白了，就是通过关系型数据库的自增主键产生来唯一的 ID。\n以 MySQL 举例，我们通过下面的方式即可。\n1.创建一个数据库表。\nCREATE TABLE `sequence_id` ( `id` bigint(20) unsigned NOT NULL AUTO_INCREMENT, `stub` char(10) NOT NULL DEFAULT \u0026#39;\u0026#39;, PRIMARY KEY (`id`), UNIQUE KEY `stub` (`stub`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; stub 字段无意义，只是为了占位，便于我们插入或者修改数据。并且，给 stub 字段创建了唯一索引，保证其唯一性。\n2.通过 replace into 来插入数据。\nBEGIN; REPLACE INTO sequence_id (stub) VALUES (\u0026#39;stub\u0026#39;); SELECT LAST_INSERT_ID(); COMMIT; 插入数据这里，我们没有使用 insert into 而是使用 replace into 来插入数据，具体步骤是这样的：\n1)第一步： 尝试把数据插入到表中。\n2)第二步： 如果主键或唯一索引字段出现重复数据错误而插入失败时，先从表中删除含有重复关键字值的冲突行，然后再次尝试把数据插入到表中。\n使用replace只是用来删除行，没有什么特殊含义\n这种方式的优缺点也比较明显：\n优点 ：实现起来比较简单、ID 有序递增、存储消耗空间小 缺点 ： 支持的并发量不大、存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）、ID 没有具体业务含义、安全问题（比如根据订单 ID 的递增规律就能推算出每天的订单量，商业机密啊！ ）、每次获取 ID 都要访问一次数据库（增加了对数据库的压力，获取速度也慢） 数据库号段模式 # 数据库主键自增这种模式，每次获取 ID 都要访问一次数据库，ID 需求比较大的时候，肯定是不行的。\n如果我们可以批量获取，然后存在在内存里面，需要用到的时候，直接从内存里面拿就舒服了！这也就是我们说的 基于数据库的号段模式来生成分布式 ID。\n数据库的号段模式也是目前比较主流的一种分布式 ID 生成方式。像滴滴开源的Tinyid 就是基于这种方式来做的。不过，TinyId 使用了双号段缓存、增加多 db 支持等方式来进一步优化。\n以 MySQL 举例，我们通过下面的方式即可。\n1.创建一个数据库表。\nCREATE TABLE `sequence_id_generator` ( `id` int(10) NOT NULL, `current_max_id` bigint(20) NOT NULL COMMENT \u0026#39;当前最大id\u0026#39;, `step` int(10) NOT NULL COMMENT \u0026#39;号段的长度\u0026#39;, `version` int(20) NOT NULL COMMENT \u0026#39;版本号\u0026#39;, `biz_type` int(20) NOT NULL COMMENT \u0026#39;业务类型\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; current_max_id 字段和step字段主要用于获取批量 ID，获取的批量 id 为： current_max_id ~ current_max_id+step。\nversion 字段主要用于解决并发问题（乐观锁）,biz_type 主要用于表示业务类型。\n2.先插入一行数据。\nINSERT INTO `sequence_id_generator` (`id`, `current_max_id`, `step`, `version`, `biz_type`) VALUES (1, 0, 100, 0, 101); 3.通过 SELECT 获取指定业务下的批量唯一 ID\nSELECT `current_max_id`, `step`,`version` FROM `sequence_id_generator` where `biz_type` = 101 结果：\nid\tcurrent_max_id\tstep\tversion\tbiz_type 1\t0\t100\t0\t101 4.不够用的话，更新之后重新 SELECT 即可。\nUPDATE sequence_id_generator SET current_max_id = 0+100, version=version+1 WHERE version = 0 AND `biz_type` = 101 SELECT `current_max_id`, `step`,`version` FROM `sequence_id_generator` where `biz_type` = 101 结果：\nid\tcurrent_max_id\tstep\tversion\tbiz_type 1\t100\t100\t1\t101 相比于数据库主键自增的方式，数据库的号段模式对于数据库的访问次数更少，数据库压力更小。\n另外，为了避免单点问题，你可以从使用主从模式来提高可用性。\n数据库号段模式的优缺点:\n优点 ：ID 有序递增、存储消耗空间小 缺点 ：存在数据库单点问题（可以使用数据库集群解决，不过增加了复杂度）、ID 没有具体业务含义、安全问题（比如根据订单 ID 的递增规律就能推算出每天的订单量，商业机密啊！ ） NoSQL # 一般情况下，NoSQL 方案使用 Redis 多一些。我们通过 Redis 的 incr 命令即可实现对 id 原子顺序递增。\n127.0.0.1:6379\u0026gt; set sequence_id_biz_type 1 OK 127.0.0.1:6379\u0026gt; incr sequence_id_biz_type (integer) 2 127.0.0.1:6379\u0026gt; get sequence_id_biz_type \u0026#34;2\u0026#34; 为了提高可用性和并发，我们可以使用 Redis Cluster。Redis Cluster 是 Redis 官方提供的 Redis 集群解决方案（3.0+版本）。\n除了 Redis Cluster 之外，你也可以使用开源的 Redis 集群方案Codis （大规模集群比如上百个节点的时候比较推荐）。\n除了高可用和并发之外，我们知道 Redis 基于内存，我们需要持久化数据，避免重启机器或者机器故障后数据丢失。Redis 支持两种不同的持久化方式：快照（snapshotting，RDB）、只追加文件（append-only file, AOF）。 并且，Redis 4.0 开始支持 RDB 和 AOF 的混合持久化（默认关闭，可以通过配置项 aof-use-rdb-preamble 开启）。\n关于 Redis 持久化，我这里就不过多介绍。不了解这部分内容的小伙伴，可以看看 JavaGuide 对于 Redis 知识点的总结。\nRedis 方案的优缺点：\n优点 ： 性能不错并且生成的 ID 是有序递增的 缺点 ： 和数据库主键自增方案的缺点类似 除了 Redis 之外，MongoDB ObjectId 经常也会被拿来当做分布式 ID 的解决方案。\nMongoDB ObjectId 一共需要 12 个字节存储：\n0~3：时间戳 3~6： 代表机器 ID 7~8：机器进程 ID 9~11 ：自增值 MongoDB 方案的优缺点：\n优点 ： 性能不错并且生成的 ID 是有序递增的 缺点 ： 需要解决重复 ID 问题（当机器时间不对的情况下，可能导致会产生重复 ID） 、有安全性问题（ID 生成有规律性） 算法 # UUID # UUID 是 Universally Unique Identifier（通用唯一标识符） 的缩写。UUID 包含 32 个 16 进制数字（8-4-4-4-12）。\nJDK 就提供了现成的生成 UUID 的方法，一行代码就行了。\n//输出示例：cb4a9ede-fa5e-4585-b9bb-d60bce986eaa UUID.randomUUID() RFC 4122 中关于 UUID 的示例是这样的：\n我们这里重点关注一下这个 Version(版本)，不同的版本对应的 UUID 的生成规则是不同的。\n5 种不同的 Version(版本)值分别对应的含义（参考维基百科对于 UUID 的介绍）：\n版本 1 : UUID 是根据时间和节点 ID（通常是 MAC 地址）生成； 版本 2 : UUID 是根据标识符（通常是组或用户 ID）、时间和节点 ID 生成； 版本 3、版本 5 : 版本 5 - 确定性 UUID 通过散列（hashing）名字空间（namespace）标识符和名称生成； 版本 4 : UUID 使用随机性或伪随机性生成。 下面是 Version 1 版本下生成的 UUID 的示例：\nJDK 中通过 UUID 的 randomUUID() 方法生成的 UUID 的版本默认为 4。\nUUID uuid = UUID.randomUUID(); int version = uuid.version();// 4 另外，Variant(变体)也有 4 种不同的值，这种值分别对应不同的含义。这里就不介绍了，貌似平时也不怎么需要关注。\n需要用到的时候，去看看维基百科对于 UUID 的 Variant(变体) 相关的介绍即可。\n从上面的介绍中可以看出，UUID 可以保证唯一性，因为其生成规则包括 MAC 地址、时间戳、名字空间（Namespace）、随机或伪随机数、时序等元素，计算机基于这些规则生成的 UUID 是肯定不会重复的。\n虽然，UUID 可以做到全局唯一性，但是，我们一般很少会使用它。\n比如使用 UUID 作为 MySQL 数据库主键的时候就非常不合适：\n数据库主键要尽量越短越好，而 UUID 的消耗的存储空间比较大（32 个字符串，128 位）。 UUID 是无顺序的，InnoDB 引擎下，数据库主键的无序性会严重影响数据库性能。 最后，我们再简单分析一下 UUID 的优缺点 （面试的时候可能会被问到的哦！） :\n优点 ：生成速度比较快、简单易用 缺点 ： 存储消耗空间大（32 个字符串，128 位） 、 不安全（基于 MAC 地址生成 UUID 的算法会造成 MAC 地址泄露)、无序（非自增）、没有具体业务含义、需要解决重复 ID 问题（当机器时间不对的情况下，可能导致会产生重复 ID） Snowflake(雪花算法) # Snowflake 是 Twitter 开源的分布式 ID 生成算法。Snowflake 由 64 bit 的二进制数字组成，这 64bit 的二进制被分成了几部分，每一部分存储的数据都有特定的含义：\n第 0 位： 符号位（标识正负），始终为 0，没有用，不用管。 第 1~41 位 ：一共 41 位，用来表示时间戳，单位是毫秒，可以支撑 2 ^41 毫秒（约 69 年） 第 42~52 位 ：一共 10 位，一般来说，前 5 位表示机房 ID，后 5 位表示机器 ID（实际项目中可以根据实际情况调整）。这样就可以区分不同集群/机房的节点。 第 53~64 位 ：一共 12 位，用来表示序列号。 序列号为自增值，代表单台机器每毫秒能够产生的最大 ID 数(2^12 = 4096),也就是说单台机器每毫秒最多可以生成 4096 个 唯一 ID。 如果你想要使用 Snowflake 算法的话，一般不需要你自己再造轮子。有很多基于 Snowflake 算法的开源实现比如美团 的 Leaf、百度的 UidGenerator，并且这些开源实现对原有的 Snowflake 算法进行了优化。\n另外，在实际项目中，我们一般也会对 Snowflake 算法进行改造，最常见的就是在 Snowflake 算法生成的 ID 中加入业务类型信息。\n我们再来看看 Snowflake 算法的优缺点 ：\n优点 ：生成速度比较快、生成的 ID 有序递增、比较灵活（可以对 Snowflake 算法进行简单的改造比如加入业务 ID） 缺点 ： 需要解决重复 ID 问题（依赖时间，当机器时间不对的情况下，可能导致会产生重复 ID）。 开源框架 # UidGenerator(百度) # UidGenerator 是百度开源的一款基于 Snowflake(雪花算法)的唯一 ID 生成器。\n不过，UidGenerator 对 Snowflake(雪花算法)进行了改进，生成的唯一 ID 组成如下。\n可以看出，和原始 Snowflake(雪花算法)生成的唯一 ID 的组成不太一样。并且，上面这些参数我们都可以自定义。\nUidGenerator 官方文档中的介绍如下：\n自 18 年后，UidGenerator 就基本没有再维护了，我这里也不过多介绍。想要进一步了解的朋友，可以看看 UidGenerator 的官方介绍。\nLeaf(美团) # Leaf 是美团开源的一个分布式 ID 解决方案 。这个项目的名字 Leaf（树叶） 起源于德国哲学家、数学家莱布尼茨的一句话： “There are no two identical leaves in the world”（世界上没有两片相同的树叶） 。这名字起得真心挺不错的，有点文艺青年那味了！\nLeaf 提供了 号段模式 和 Snowflake(雪花算法) 这两种模式来生成分布式 ID。并且，它支持双号段，还解决了雪花 ID 系统时钟回拨问题。不过，时钟问题的解决需要弱依赖于 Zookeeper 。\nLeaf 的诞生主要是为了解决美团各个业务线生成分布式 ID 的方法多种多样以及不可靠的问题。\nLeaf 对原有的号段模式进行改进，比如它这里增加了双号段避免获取 DB 在获取号段的时候阻塞请求获取 ID 的线程。简单来说，就是我一个号段还没用完之前，我自己就主动提前去获取下一个号段（图片来自于美团官方文章：《Leaf——美团点评分布式 ID 生成系统》）。\n根据项目 README 介绍，在 4C8G VM 基础上，通过公司 RPC 方式调用，QPS 压测结果近 5w/s，TP999 1ms。\nTinyid(滴滴) # Tinyid 是滴滴开源的一款基于数据库号段模式的唯一 ID 生成器。\n数据库号段模式的原理我们在上面已经介绍过了。Tinyid 有哪些亮点呢？\n为了搞清楚这个问题，我们先来看看基于数据库号段模式的简单架构方案。（图片来自于 Tinyid 的官方 wiki:《Tinyid 原理介绍》）\n在这种架构模式下，我们通过 HTTP 请求向发号器服务申请唯一 ID。负载均衡 router 会把我们的请求送往其中的一台 tinyid-server。\n这种方案有什么问题呢？在我看来（Tinyid 官方 wiki 也有介绍到），主要由下面这 2 个问题：\n获取新号段的情况下，程序获取唯一 ID 的速度比较慢。 需要保证 DB 高可用，这个是比较麻烦且耗费资源的。 除此之外，HTTP 调用也存在网络开销。\nTinyid 的原理比较简单，其架构如下图所示：\n相比于基于数据库号段模式的简单架构方案，Tinyid 方案主要做了下面这些优化：\n双号段缓存 ：为了避免在获取新号段的情况下，程序获取唯一 ID 的速度比较慢。 Tinyid 中的号段在用到一定程度的时候，就会去异步加载下一个号段，保证内存中始终有可用号段。 增加多 db 支持 ：支持多个 DB，并且，每个 DB 都能生成唯一 ID，提高了可用性。 增加 tinyid-client ：纯本地操作，无 HTTP 请求消耗，性能和可用性都有很大提升。 Tinyid 的优缺点这里就不分析了，结合数据库号段模式的优缺点和 Tinyid 的原理就能知道。\n总结 # 通过这篇文章，我基本上已经把最常见的分布式 ID 生成方案都总结了一波。\n除了上面介绍的方式之外，像 ZooKeeper 这类中间件也可以帮助我们生成唯一 ID。没有银弹，一定要结合实际项目来选择最适合自己的方案。\n"},{"id":95,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/ly01ly_api-gateway/","title":"api网关","section":"分布式系统","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n什么是网关？有什么用？ # 微服务背景下，一个系统被拆分为多个服务，但是像安全认证，流量控制，日志，监控等功能是每个服务都需要的，没有网关的话，我们就需要在每个服务中单独实现，这使得我们做了很多重复的事情并且没有一个全局的视图来统一管理这些功能。\n一般情况下，网关可以为我们提供请求转发、安全认证（身份/权限认证）、流量控制、负载均衡、降级熔断、日志、监控等功能。\n上面介绍了这么多功能，实际上，网关主要做了一件事情：请求过滤 。\n有哪些常见的网关系统？ # Netflix Zuul # Zuul 是 Netflix 开发的一款提供动态路由、监控、弹性、安全的网关服务。\nZuul 主要通过过滤器（类似于 AOP）来过滤请求，从而实现网关必备的各种功能。\n我们可以自定义过滤器来处理请求，并且，Zuul 生态本身就有很多现成的过滤器供我们使用。就比如限流可以直接用国外朋友写的 spring-cloud-zuul-ratelimit (这里只是举例说明，一般是配合 hystrix 来做限流)：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-zuul\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.marcosbarbero.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-zuul-ratelimit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.0.RELEASE\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Zuul 1.x 基于同步 IO，性能较差。Zuul 2.x 基于 Netty 实现了异步 IO，性能得到了大幅改进。\nGithub 地址 ： https://github.com/Netflix/zuul 官方 Wiki ： https://github.com/Netflix/zuul/wiki Spring Cloud Gateway # SpringCloud Gateway 属于 Spring Cloud 生态系统中的网关，其诞生的目标是为了替代老牌网关 **Zuul **。准确点来说，应该是 Zuul 1.x。SpringCloud Gateway 起步要比 Zuul 2.x 更早。\n为了提升网关的性能，SpringCloud Gateway 基于 Spring WebFlux 。Spring WebFlux 使用 Reactor 库来实现响应式编程模型，底层基于 Netty 实现异步 IO。\nSpring Cloud Gateway 的目标，不仅提供统一的路由方式，并且基于 Filter 链的方式提供了网关基本的功能，例如：安全，监控/指标，和限流。\nSpring Cloud Gateway 和 Zuul 2.x 的差别不大，也是通过过滤器来处理请求。不过，目前更加推荐使用 Spring Cloud Gateway 而非 Zuul，Spring Cloud 生态对其支持更加友好。\nGithub 地址 ： https://github.com/spring-cloud/spring-cloud-gateway 官网 ： https://spring.io/projects/spring-cloud-gateway Kong # Kong 是一款基于 OpenResty 的高性能、云原生、可扩展的网关系统。\nOpenResty 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关。\nKong 提供了插件机制来扩展其功能。比如、在服务上启用 Zipkin 插件\n$ curl -X POST http://kong:8001/services/{service}/plugins \\ --data \u0026#34;name=zipkin\u0026#34; \\ --data \u0026#34;config.http_endpoint=http://your.zipkin.collector:9411/api/v2/spans\u0026#34; \\ --data \u0026#34;config.sample_ratio=0.001\u0026#34; Github 地址： https://github.com/Kong/kong 官网地址 ： https://konghq.com/kong APISIX # APISIX 是一款基于 Nginx 和 etcd 的高性能、云原生、可扩展的网关系统。\netcd是使用 Go 语言开发的一个开源的、高可用的分布式 key-value 存储系统，使用 Raft 协议做分布式共识。\n与传统 API 网关相比，APISIX 具有动态路由和插件热加载，特别适合微服务系统下的 API 管理。并且，APISIX 与 SkyWalking（分布式链路追踪系统）、Zipkin（分布式链路追踪系统）、Prometheus（监控系统） 等 DevOps 生态工具对接都十分方便。\n作为 NGINX 和 Kong 的替代项目，APISIX 目前已经是 Apache 顶级开源项目，并且是最快毕业的国产开源项目。国内目前已经有很多知名企业（比如金山、有赞、爱奇艺、腾讯、贝壳）使用 APISIX 处理核心的业务流量。\n根据官网介绍：“APISIX 已经生产可用，功能、性能、架构全面优于 Kong”。\nGithub 地址 ：https://github.com/apache/apisix 官网地址： https://apisix.apache.org/zh/ 相关阅读：\n有了 NGINX 和 Kong，为什么还需要 Apache APISIX APISIX 技术博客 APISIX 用户案例 Shenyu # Shenyu 是一款基于 WebFlux 的可扩展、高性能、响应式网关，Apache 顶级开源项目。\nShenyu 通过插件扩展功能，插件是 ShenYu 的灵魂，并且插件也是可扩展和热插拔的。不同的插件实现不同的功能。Shenyu 自带了诸如限流、熔断、转发 、重写、重定向、和路由监控等插件。\nGithub 地址： https://github.com/apache/incubator-shenyu 官网地址 ： https://shenyu.apache.org/ "},{"id":96,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/base/raft-algorithm/","title":"raft算法","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n1 背景 # 当今的数据中心和应用程序在高度动态的环境中运行，为了应对高度动态的环境，它们通过额外的服务器进行横向扩展，并且根据需求进行扩展和收缩。同时，服务器和网络故障也很常见。\n因此，系统必须在正常操作期间处理服务器的上下线。它们必须对变故做出反应并在几秒钟内自动适应；对客户来说的话，明显的中断通常是不可接受的。\n幸运的是，分布式共识可以帮助应对这些挑战。\n1.1 拜占庭将军 # 在介绍共识算法之前，先介绍一个简化版拜占庭将军的例子来帮助理解共识算法。\n假设多位拜占庭将军中没有叛军，信使的信息可靠但有可能被暗杀的情况下，将军们如何达成是否要进攻的一致性决定？\n解决方案大致可以理解成：先在所有的将军中选出一个大将军，用来做出所有的决定。\n举例如下：假如现在一共有 3 个将军 A，B 和 C，每个将军都有一个随机时间的倒计时器，倒计时一结束，这个将军就把自己当成大将军候选人，然后派信使传递选举投票的信息给将军 B 和 C，如果将军 B 和 C 还没有把自己当作候选人（自己的倒计时还没有结束），并且没有把选举票投给其他人，它们就会把票投给将军 A，信使回到将军 A 时，将军 A 知道自己收到了足够的票数，成为大将军。在有了大将军之后，是否需要进攻就由大将军 A 决定，然后再去派信使通知另外两个将军，自己已经成为了大将军。如果一段时间还没收到将军 B 和 C 的回复（信使可能会被暗杀），那就再重派一个信使，直到收到回复。\n1.2 共识算法 # 共识是可容错系统中的一个基本问题：即使面对故障，服务器也可以在共享状态上达成一致。\n共识算法允许一组节点像一个整体一样一起工作，即使其中的一些节点出现故障也能够继续工作下去，其正确性主要是源于复制状态机的性质：一组Server的状态机计算相同状态的副本，即使有一部分的Server宕机了它们仍然能够继续运行。\n图-1 复制状态机架构 一般通过使用复制日志来实现复制状态机。每个Server存储着一份包括命令序列的日志文件，状态机会按顺序执行这些命令。因为每个日志包含相同的命令，并且顺序也相同，所以每个状态机处理相同的命令序列。由于状态机是确定性的，所以处理相同的状态，得到相同的输出。\n因此共识算法的工作就是保持复制日志的一致性。服务器上的共识模块从客户端接收命令并将它们添加到日志中。它与其他服务器上的共识模块通信，以确保即使某些服务器发生故障。每个日志最终包含相同顺序的请求。一旦命令被正确地复制，它们就被称为已提交。每个服务器的状态机按照日志顺序处理已提交的命令，并将输出返回给客户端，因此，这些服务器形成了一个单一的、高度可靠的状态机。\n适用于实际系统的共识算法通常具有以下特性：\n安全。确保在非拜占庭条件（也就是上文中提到的简易版拜占庭）下的安全性，包括网络延迟、分区、包丢失、复制和重新排序。 高可用。只要大多数服务器都是可操作的，并且可以相互通信，也可以与客户端进行通信，那么这些服务器就可以看作完全功能可用的。因此，一个典型的由五台服务器组成的集群可以容忍任何两台服务器端故障。假设服务器因停止而发生故障；它们稍后可能会从稳定存储上的状态中恢复并重新加入集群。 一致性不依赖时序。错误的时钟和极端的消息延迟，在最坏的情况下也只会造成可用性问题，而不会产生一致性问题。 在集群中大多数服务器响应，命令就可以完成，不会被少数运行缓慢的服务器来影响整体系统性能。 2 基础 # 2.1 节点类型 # 一个 Raft 集群包括若干服务器，以典型的 5 服务器集群举例。在任意的时间，每个服务器一定会处于以下三个状态中的一个：\nLeader：负责发起心跳，响应客户端，创建日志，同步日志。 Candidate：Leader 选举过程中的临时角色，由 Follower 转化而来，发起投票参与竞选。 Follower：接受 Leader 的心跳和日志同步数据，投票给 Candidate。 在正常的情况下，只有一个服务器是 Leader，剩下的服务器是 Follower。Follower 是被动的，它们不会发送任何请求，只是响应来自 Leader 和 Candidate 的请求。\n图-2：服务器的状态 2.2 任期 # 图-3：任期 如图 3 所示，raft 算法将时间划分为任意长度的任期（term），任期用连续的数字表示，看作当前 term 号。每一个任期的开始都是一次选举，在选举开始时，一个或多个 Candidate 会尝试成为 Leader。如果一个 Candidate 赢得了选举，它就会在该任期内担任 Leader。如果没有选出 Leader，将会开启另一个任期，并立刻开始下一次选举。raft 算法保证在给定的一个任期最少要有一个 Leader。\n每个节点都会存储当前的 term 号，当服务器之间进行通信时会交换当前的 term 号；如果有服务器发现自己的 term 号比其他人小，那么他会更新到较大的 term 值。如果一个 Candidate 或者 Leader 发现自己的 term 过期了，他会立即退回成 Follower。如果一台服务器收到的请求的 term 号是过期的，那么它会拒绝此次请求。\n2.3 日志 # entry：每一个事件成为 entry，只有 Leader 可以创建 entry。entry 的内容为\u0026lt;term,index,cmd\u0026gt;其中 cmd 是可以应用到状态机的操作。 log：由 entry 构成的数组，每一个 entry 都有一个表明自己在 log 中的 index。只有 Leader 才可以改变其他节点的 log。entry 总是先被 Leader 添加到自己的 log 数组中，然后再发起共识请求，获得同意后才会被 Leader 提交给状态机。Follower 只能从 Leader 获取新日志和当前的 commitIndex，然后把对应的 entry 应用到自己的状态机中。 3 领导人选举 # raft 使用心跳机制来触发 Leader 的选举。\n如果一台服务器能够收到来自 Leader 或者 Candidate 的有效信息，那么它会一直保持为 Follower 状态，并且刷新自己的 electionElapsed，重新计时。\nLeader 会向所有的 Follower 周期性发送心跳来保证自己的 Leader 地位。如果一个 Follower 在一个周期内没有收到心跳信息，就叫做选举超时，然后它就会认为此时没有可用的 Leader，并且开始进行一次选举以选出一个新的 Leader。\n为了开始新的选举，Follower 会自增自己的 term 号并且转换状态为 Candidate。然后他会向所有节点发起 RequestVoteRPC 请求， Candidate 的状态会持续到以下情况发生：\n赢得选举 其他节点赢得选举 一轮选举结束，无人胜出 赢得选举的条件是：一个 Candidate 在一个任期内收到了来自集群内的多数选票（N/2+1），就可以成为 Leader。\n在 Candidate 等待选票的时候，它可能收到其他节点声明自己是 Leader 的心跳，此时有两种情况：\n该 Leader 的 term 号大于等于自己的 term 号，说明对方已经成为 Leader，则自己回退为 Follower。 该 Leader 的 term 号小于自己的 term 号，那么会拒绝该请求并让该节点更新 term。 由于可能同一时刻出现多个 Candidate，导致没有 Candidate 获得大多数选票，如果没有其他手段来重新分配选票的话，那么可能会无限重复下去。\nraft 使用了随机的选举超时时间来避免上述情况。每一个 Candidate 在发起选举后，都会随机化一个新的枚举超时时间，这种机制使得各个服务器能够分散开来，在大多数情况下只有一个服务器会率先超时；它会在其他服务器超时之前赢得选举。\n4 日志复制 # 一旦选出了 Leader，它就开始接受客户端的请求。每一个客户端的请求都包含一条需要被复制状态机（Replicated State Mechine）执行的命令。\nLeader 收到客户端请求后，会生成一个 entry，包含\u0026lt;index,term,cmd\u0026gt;，再将这个 entry 添加到自己的日志末尾后，向所有的节点广播该 entry，要求其他服务器复制这条 entry。\n如果 Follower 接受该 entry，则会将 entry 添加到自己的日志后面，同时返回给 Leader 同意。\n如果 Leader 收到了多数的成功响应，Leader 会将这个 entry 应用到自己的状态机中，之后可以成为这个 entry 是 committed 的，并且向客户端返回执行结果。\nraft 保证以下两个性质：\n在两个日志里，有两个 entry 拥有相同的 index 和 term，那么它们一定有相同的 cmd 在两个日志里，有两个 entry 拥有相同的 index 和 term，那么它们前面的 entry 也一定相同 通过“仅有 Leader 可以生存 entry”来保证第一个性质，第二个性质需要一致性检查来进行保证。\n一般情况下，Leader 和 Follower 的日志保持一致，然后，Leader 的崩溃会导致日志不一样，这样一致性检查会产生失败。Leader 通过强制 Follower 复制自己的日志来处理日志的不一致。这就意味着，在 Follower 上的冲突日志会被领导者的日志覆盖。\n为了使得 Follower 的日志和自己的日志一致，Leader 需要找到 Follower 与它日志一致的地方，然后删除 Follower 在该位置之后的日志，接着把这之后的日志发送给 Follower。\nLeader 给每一个Follower 维护了一个 nextIndex，它表示 Leader 将要发送给该追随者的下一条日志条目的索引。当一个 Leader 开始掌权时，它会将 nextIndex 初始化为它的最新的日志条目索引数+1。如果一个 Follower 的日志和 Leader 的不一致，AppendEntries 一致性检查会在下一次 AppendEntries RPC 时返回失败。在失败之后，Leader 会将 nextIndex 递减然后重试 AppendEntries RPC。最终 nextIndex 会达到一个 Leader 和 Follower 日志一致的地方。这时，AppendEntries 会返回成功，Follower 中冲突的日志条目都被移除了，并且添加所缺少的上了 Leader 的日志条目。一旦 AppendEntries 返回成功，Follower 和 Leader 的日志就一致了，这样的状态会保持到该任期结束。\n5 安全性 # 5.1 选举限制 # Leader 需要保证自己存储全部已经提交的日志条目。这样才可以使日志条目只有一个流向：从 Leader 流向 Follower，Leader 永远不会覆盖已经存在的日志条目。\n每个 Candidate 发送 RequestVoteRPC 时，都会带上最后一个 entry 的信息。所有节点收到投票信息时，会对该 entry 进行比较，如果发现自己的更新，则拒绝投票给该 Candidate。\n判断日志新旧的方式：如果两个日志的 term 不同，term 大的更新；如果 term 相同，更长的 index 更新。\n5.2 节点崩溃 # 如果 Leader 崩溃，集群中的节点在 electionTimeout 时间内没有收到 Leader 的心跳信息就会触发新一轮的选主，在选主期间整个集群对外是不可用的。\n如果 Follower 和 Candidate 崩溃，处理方式会简单很多。之后发送给它的 RequestVoteRPC 和 AppendEntriesRPC 会失败。由于 raft 的所有请求都是幂等的，所以失败的话会无限的重试。如果崩溃恢复后，就可以收到新的请求，然后选择追加或者拒绝 entry。\n5.3 时间与可用性 # raft 的要求之一就是安全性不依赖于时间：系统不能仅仅因为一些事件发生的比预想的快一些或者慢一些就产生错误。为了保证上述要求，最好能满足以下的时间条件：\nbroadcastTime \u0026lt;\u0026lt; electionTimeout \u0026lt;\u0026lt; MTBF broadcastTime：向其他节点并发发送消息的平均响应时间； electionTimeout：选举超时时间； MTBF(mean time between failures)：单台机器的平均健康时间； broadcastTime应该比electionTimeout小一个数量级，为的是使Leader能够持续发送心跳信息（heartbeat）来阻止Follower开始选举；\nelectionTimeout也要比MTBF小几个数量级，为的是使得系统稳定运行。当Leader崩溃时，大约会在整个electionTimeout的时间内不可用；我们希望这种情况仅占全部时间的很小一部分。\n由于broadcastTime和MTBF是由系统决定的属性，因此需要决定electionTimeout的时间。\n一般来说，broadcastTime 一般为 0.5～20ms，electionTimeout 可以设置为 10～500ms，MTBF 一般为一两个月。\n6 参考 # https://tanxinyu.work/raft/ https://github.com/OneSizeFitsQuorum/raft-thesis-zh_cn/blob/master/raft-thesis-zh_cn.md https://github.com/ongardie/dissertation/blob/master/stanford.pdf https://knowledge-sharing.gitbooks.io/raft/content/chapter5.html "},{"id":97,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/base/paxos-algorithm/","title":"paxos算法","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n背景 # Paxos 算法是 Leslie Lamport（莱斯利·兰伯特）在 1990 年提出了一种分布式系统 共识 算法。这也是第一个被证明完备的共识算法（前提是不存在拜占庭将军问题，也就是没有恶意节点）。\n为了介绍 Paxos 算法，兰伯特专门写了一篇幽默风趣的论文。在这篇论文中，他虚拟了一个叫做 Paxos 的希腊城邦来更形象化地介绍 Paxos 算法。\n不过，审稿人并不认可这篇论文的幽默。于是，他们就给兰伯特说：“如果你想要成功发表这篇论文的话，必须删除所有 Paxos 相关的故事背景”。兰伯特一听就不开心了：“我凭什么修改啊，你们这些审稿人就是缺乏幽默细胞，发不了就不发了呗！”。\n于是乎，提出 Paxos 算法的那篇论文在当时并没有被成功发表。\n直到 1998 年，系统研究中心 (Systems Research Center，SRC）的两个技术研究员需要找一些合适的分布式算法来服务他们正在构建的分布式系统，Paxos 算法刚好可以解决他们的部分需求。因此，兰伯特就把论文发给了他们。在看了论文之后，这俩大佬觉得论文还是挺不错的。于是，兰伯特在 1998 年重新发表论文 《The Part-Time Parliament》。\n论文发表之后，各路学者直呼看不懂，言语中还略显调侃之意。这谁忍得了，在 2001 年的时候，兰伯特专门又写了一篇 《Paxos Made Simple》 的论文来简化对 Paxos 的介绍，主要讲述两阶段共识协议部分，顺便还不忘嘲讽一下这群学者。\n《Paxos Made Simple》这篇论文就 14 页，相比于 《The Part-Time Parliament》的 33 页精简了不少。最关键的是这篇论文的摘要就一句话：\nThe Paxos algorithm, when presented in plain English, is very simple.\n翻译过来的意思大概就是：当我用无修饰的英文来描述时，Paxos 算法真心简单！\n有没有感觉到来自兰伯特大佬满满地嘲讽的味道？\n介绍 # Paxos 算法是第一个被证明完备的分布式系统共识算法。共识算法的作用是让分布式系统中的多个节点之间对某个提案（Proposal）达成一致的看法。提案的含义在分布式系统中十分宽泛，像哪一个节点是 Leader 节点、多个事件发生的顺序等等都可以是一个提案。\n兰伯特当时提出的 Paxos 算法主要包含 2 个部分:\nBasic Paxos 算法 ： 描述的是多节点之间如何就某个值(提案 Value)达成共识。 Multi-Paxos 思想 ： 描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。Multi-Paxos 说白了就是执行多次 Basic Paxos ，核心还是 Basic Paxos 。 由于 Paxos 算法在国际上被公认的非常难以理解和实现，因此不断有人尝试简化这一算法。到了 2013 年才诞生了一个比 Paxos 算法更易理解和实现的共识算法—Raft 算法 。更具体点来说，Raft 是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现。\n针对没有恶意节点的情况，除了 Raft 算法之外，当前最常用的一些共识算法比如 ZAB 协议 、 Fast Paxos 算法都是基于 Paxos 算法改进的。\n针对存在恶意节点的情况，一般使用的是 工作量证明（POW，Proof-of-Work） 、 权益证明（PoS，Proof-of-Stake ） 等共识算法。这类共识算法最典型的应用就是区块链，就比如说前段时间以太坊官方宣布其共识机制正在从工作量证明(PoW)转变为权益证明(PoS)。\n区块链系统使用的共识算法需要解决的核心问题是 拜占庭将军问题 ，这和我们日常接触到的 ZooKeeper、Etcd、Consul 等分布式中间件不太一样。\n下面我们来对 Paxos 算法的定义做一个总结：\nPaxos 算法是兰伯特在 1990 年提出了一种分布式系统共识算法。 兰伯特当时提出的 Paxos 算法主要包含 2 个部分: Basic Paxos 算法和 Multi-Paxos 思想。 Raft 算法、ZAB 协议、 Fast Paxos 算法都是基于 Paxos 算法改进而来。 Basic Paxos 算法 # Basic Paxos 中存在 3 个重要的角色：\n提议者（Proposer）：也可以叫做协调者（coordinator），提议者负责接受客户端的请求并发起提案。提案信息通常包括提案编号 (Proposal ID) 和提议的值 (Value)。 接受者（Acceptor）：也可以叫做投票员（voter），负责对提议者的提案进行投票，同时需要记住自己的投票历史； 学习者（Learner）：如果有超过半数接受者就某个提议达成了共识，那么学习者就需要接受这个提议，并就该提议作出运算，然后将运算结果返回给客户端。 为了减少实现该算法所需的节点数，一个节点可以身兼多个角色。并且，一个提案被选定需要被半数以上的 Acceptor 接受。这样的话，Basic Paxos 算法还具备容错性，在少于一半的节点出现故障时，集群仍能正常工作。\nMulti Paxos 思想 # Basic Paxos 算法的仅能就单个值达成共识，为了能够对一系列的值达成共识，我们需要用到 Basic Paxos 思想。\n⚠️注意 ： Multi-Paxos 只是一种思想，这种思想的核心就是通过多个 Basic Paxos 实例就一系列值达成共识。也就是说，Basic Paxos 是 Multi-Paxos 思想的核心，Multi-Paxos 就是多执行几次 Basic Paxos。\n由于兰伯特提到的 Multi-Paxos 思想缺少代码实现的必要细节(比如怎么选举领导者)，所以在理解和实现上比较困难。\n不过，也不需要担心，我们并不需要自己实现基于 Multi-Paxos 思想的共识算法，业界已经有了比较出名的实现。像 Raft 算法就是 Multi-Paxos 的一个变种，其简化了 Multi-Paxos 的思想，变得更容易被理解以及工程实现，实际项目中可以优先考虑 Raft 算法。\n参考 # https://zh.wikipedia.org/wiki/Paxos 分布式系统中的一致性与共识算法：http://www.xuyasong.com/?p=1970 "},{"id":98,"href":"/zh/docs/technology/Review/java_guide/lydly_distributed_system/base/cap_base-theorem/","title":"CAP\u0026BASE 理论","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n经历过技术面试的小伙伴想必对 CAP \u0026amp; BASE 这个两个理论已经再熟悉不过了！\n我当年参加面试的时候，不夸张地说，只要问到分布式相关的内容，面试官几乎是必定会问这两个分布式相关的理论。一是因为这两个分布式基础理论是学习分布式知识的必备前置基础，二是因为很多面试官自己比较熟悉这两个理论（方便提问）。\n我们非常有必要将这两个理论搞懂，并且能够用自己的理解给别人讲出来。\nCAP 理论 # CAP 理论/定理起源于 2000 年，由加州大学伯克利分校的 Eric Brewer 教授在分布式计算原理研讨会（PODC）上提出，因此 CAP 定理又被称作 布鲁尔定理（Brewer’s theorem）\n2 年后，麻省理工学院的 Seth Gilbert 和 Nancy Lynch 发表了布鲁尔猜想的证明，CAP 理论正式成为分布式领域的定理。\n简介 # [kənˈsɪstənsi] consistency 一致性\n[əˌveɪlə'bɪləti] availability 可用性 ,\n[pɑːˈtɪʃn] 分割 [ˈtɒlərəns] 容忍, CAP 也就是 Consistency（一致性）、Availability（可用性）、Partition Tolerance（分区容错性） 这三个单词首字母组合。\nCAP 理论的提出者布鲁尔在提出 CAP 猜想的时候，并没有详细定义 Consistency、Availability、Partition Tolerance 三个单词的明确定义。\n因此，对于 CAP 的民间解读有很多，一般比较被大家推荐的是下面 👇 这种版本的解读。\n在理论计算机科学中，CAP 定理（CAP theorem）指出对于一个分布式系统来说，当设计读写操作时，只能同时满足以下三点中的两个：\n一致性（Consistency） : 所有节点访问同一份最新的数据副本 可用性（Availability）: 非故障的节点在合理的时间内返回合理的响应（不是错误或者超时的响应）。 分区容错性（Partition Tolerance） : 分布式系统出现网络分区的时候，仍然能够对外提供服务。 什么是网络分区？\n分布式系统中，多个节点之前的网络本来是连通的，但是因为某些故障（比如部分节点网络出了问题）某些节点之间不连通了，整个网络就分成了几块区域，这就叫 网络分区。\n不是所谓的“3 选 2” # 大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在 CAP 理论诞生 12 年之后，CAP 之父也在 2012 年重写了之前的论文。\n当发生网络分区的时候，如果我们要继续服务（也就是P)，那么强一致性和可用性只能 2 选 1。也就是说当网络分区之后 P 是前提，决定了 P 之后才有 C 和 A 的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。\n简而言之就是：CAP 理论中分区容错性 P 是一定要满足的，在此基础上，只能满足可用性 A 或者一致性 C。\n因此，分布式系统理论上不可能选择 CA 架构，只能选择 CP 或者 AP 架构。 比如 ZooKeeper、HBase 就是 CP 架构，Cassandra、Eureka 就是 AP 架构，Nacos 不仅支持 CP 架构也支持 AP 架构。\nP，分区容错性，就是一定要保证能提供服务\n为啥不可能选择 CA 架构呢？ 举个例子：若系统出现“分区”，系统中的某个节点在进行写操作。为了保证 C， 必须要禁止其他节点的读写操作，这就和 A 发生冲突了。如果为了保证 A，其他节点的读写操作正常的话，那就和 C 发生冲突了。\n选择 CP 还是 AP 的关键在于当前的业务场景，没有定论，比如对于需要确保强一致性的场景如银行一般会选择保证 CP 。\n另外，需要补充说明的一点是： 如果网络分区正常的话（系统在绝大部分时候所处的状态），也就说不需要保证 P 的时候，C 和 A 能够同时保证。\nCAP 实际应用案例 # 我这里以注册中心来探讨一下 CAP 的实际应用。考虑到很多小伙伴不知道注册中心是干嘛的，这里简单以 Dubbo 为例说一说。\n下图是 Dubbo 的架构图。注册中心 Registry 在其中扮演了什么角色呢？提供了什么服务呢？\n注册中心负责服务地址的注册与查找，相当于目录服务，服务提供者和消费者只在启动时与注册中心交互，注册中心不转发请求，压力较小。\n](https://camo.gith\n常见的可以作为注册中心的组件有：ZooKeeper、Eureka、Nacos\u0026hellip;。\nZooKeeper 保证的是 CP。 任何时刻对 ZooKeeper 的读请求都能得到一致性的结果，但是， ZooKeeper 不保证每次请求的可用性比如在 Leader 选举过程中或者半数以上的机器不可用的时候服务就是不可用的。 Eureka 保证的则是 AP。 Eureka 在设计的时候就是优先保证 A （可用性）。在 Eureka 中不存在什么 Leader 节点，每个节点都是一样的、平等的。因此 Eureka 不会像 ZooKeeper 那样出现选举过程中或者半数以上的机器不可用的时候服务就是不可用的情况。 Eureka 保证即使大部分节点挂掉也不会影响正常提供服务，只要有一个节点是可用的就行了。只不过这个节点上的数据可能并不是最新的。 Nacos 不仅支持 CP 也支持 AP。 总结 # 在进行分布式系统设计和开发时，我们不应该仅仅局限在 CAP 问题上，还要关注系统的扩展性、可用性等等\n在系统发生“分区”的情况下，CAP 理论只能满足 CP 或者 AP。要注意的是，这里的前提是系统发生了“分区”\n如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。\n总结：如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。\n推荐阅读 # CAP 定理简化 （英文，有趣的案例） 神一样的 CAP 理论被应用在何方 （中文，列举了很多实际的例子） 请停止呼叫数据库 CP 或 AP （英文，带给你不一样的思考） BASE 理论 # BASE 理论起源于 2008 年， 由 eBay 的架构师 Dan Pritchett 在 ACM 上发表。\n简介 # BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE 理论是对 CAP 中一致性 C 和可用性 A 权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于 CAP 定理逐步演化而来的，它大大降低了我们对系统的要求。\nBASE 理论的核心思想 # 即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。\n也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。\nBASE 理论本质上是对 CAP 的延伸和补充，更具体地说，是对 CAP 中 AP 方案的一个补充。\n为什么这样说呢？\nCAP 理论这节我们也说过了：\n如果系统没有发生“分区”的话，节点间的网络连接通信正常的话，也就不存在 P 了。这个时候，我们就可以同时保证 C 和 A 了。因此，如果系统发生“分区”，我们要考虑选择 CP 还是 AP。如果系统没有发生“分区”的话，我们要思考如何保证 CA 。\n因此，AP 方案只是在系统发生分区的时候放弃一致性，而不是永远放弃一致性。在分区故障恢复后，系统应该达到最终一致性。这一点其实就是 BASE 理论延伸的地方。\nBASE 理论三要素 # 基本可用 # 基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。\n什么叫允许损失部分可用性呢？\n响应时间上的损失: 正常情况下，处理用户请求需要 0.5s 返回结果，但是由于系统出现故障，处理用户请求的时间变为 3 s。 系统功能上的损失：正常情况下，用户可以使用系统的全部功能，但是由于系统访问量突然剧增，系统的部分非核心功能无法使用。 软状态 # 软状态指允许系统中的数据存在中间状态（CAP 理论中的数据不一致），并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时。\n最终一致性 # 最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。\n分布式一致性的 3 种级别：\n强一致性 ：系统写入了什么，读出来的就是什么。 弱一致性 ：不一定可以读取到最新写入的值，也不保证多少时间之后读取到的数据是最新的，只是会尽量保证某个时刻达到数据一致的状态。 最终一致性 ：弱一致性的升级版，系统会保证在一定时间内达到数据一致的状态。 业界比较推崇是最终一致性级别，但是某些对数据一致要求十分严格的场景比如银行转账还是要保证强一致性。\n那实现最终一致性的具体方式是什么呢? 《分布式协议与算法实战》 中是这样介绍：\n读时修复 : 在读取数据时，检测数据的不一致，进行修复。比如 Cassandra 的 Read Repair 实现，具体来说，在向 Cassandra 系统查询数据的时候，如果检测到不同节点的副本数据不一致，系统就自动修复数据。 写时修复 : 在写入数据，检测数据的不一致时，进行修复。比如 Cassandra 的 Hinted Handoff 实现。具体来说，Cassandra 集群的节点之间远程写数据的时候，如果写失败 就将数据缓存下来，然后定时重传，修复数据的不一致性。 异步修复 : 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复。 比较推荐 写时修复，这种方式对性能消耗比较低。\n总结 # ACID 是数据库事务完整性的理论，CAP 是分布式系统设计理论，BASE 是 CAP 理论中 AP 方案的延伸。\n"},{"id":99,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/MyBatis/principle/mybatis-principle3/","title":"Mybatis原理系列(3)","section":"原理","content":" 转载自https://www.jianshu.com/p/4e268828db48（添加小部分笔记）感谢作者!\n还没看完\n在上篇文章中，我们讲解了MyBatis的启动流程，以及启动过程中涉及到的组件，在本篇文中，我们继续探索SqlSession,SqlSessionFactory,SqlSessionFactoryBuilder的关系。SqlSession作为MyBatis的核心组件，可以说MyBatis的所有操作都是围绕SqlSession来展开的。对SqlSession理解透彻，才能全面掌握MyBatis。\n1. SqlSession初识 # SqlSession在一开始就介绍过是高级接口，类似于JDBC操作的connection对象，它包装了数据库连接，通过这个接口我们可以实现增删改查，提交/回滚事物，关闭连接，获取代理类等操作。SqlSession是个接口，其默认实现是DefaultSqlSession。SqlSession是线程不安全的，每个线程都会有自己唯一的SqlSession，不同线程间调用同一个SqlSession会出现问题，因此在使用完后需要close掉。\nSqlSession的方法\n2. SqlSession的创建 # SqlSessionFactoryBuilder的build()方法使用建造者模式创建了SqlSessionFactory接口对象，SqlSessionFactory接口的默认实现是DefaultSqlSessionFactory。SqlSessionFactory使用实例工厂模式来创建SqlSession对象。SqlSession,SqlSessionFactory,SqlSessionFactoryBuilder的关系如下(图画得有点丑\u0026hellip;)：\n类图\nDefaultSqlSessionFactory中openSession是有两种方法一种是openSessionFromDataSource，另一种是openSessionFromConnection。这两种是什么区别呢？从字面意义上将，一种是从数据源中获取SqlSession对象，一种是由已有连接获取SqlSession。SqlSession实际是对数据库连接的一层包装，数据库连接是个珍贵的资源，如果频繁的创建销毁将会影响吞吐量，因此使用数据库连接池化技术就可以复用数据库连接了。因此openSessionFromDataSource会从数据库连接池中获取一个连接，然后包装成一个SqlSession对像。openSessionFromConnection则是直接包装已有的连接并返回SqlSession对像。\nopenSessionFromDataSource 主要经历了以下几步：\n从获取configuration中获取Environment对象，Environment包含了数据库配置 从Environment获取DataSource数据源 从DataSource数据源中获取Connection连接对象 从DataSource数据源中获取TransactionFactory事物工厂 从TransactionFactory中创建事物Transaction对象 创建Executor对象 包装configuration和Executor对象成DefaultSqlSession对象 private SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; try { final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(\u0026#34;Error opening session. Cause: \u0026#34; + e, e); } finally { ErrorContext.instance().reset(); } } private SqlSession openSessionFromConnection(ExecutorType execType, Connection connection) { try { boolean autoCommit; try { autoCommit = connection.getAutoCommit(); } catch (SQLException e) { // Failover to true, as most poor drivers // or databases won\u0026#39;t support transactions autoCommit = true; } final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); final Transaction tx = transactionFactory.newTransaction(connection); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { throw ExceptionFactory.wrapException(\u0026#34;Error opening session. Cause: \u0026#34; + e, e); } finally { ErrorContext.instance().reset(); } } 3. SqlSession的使用 # SqlSession 获取成功后，我们就可以使用其中的方法了，比如直接使用SqlSession发送sql语句，或者通过mapper映射文件的方式来使用，在上两篇文章中我们都是通过mapper映射文件来使用的，接下来就介绍第一种，直接使用SqlSession发送sql语句。\npublic static void main(String[] args){ try { // 1. 读取配置 InputStream inputStream = Resources.getResourceAsStream(\u0026#34;mybatis-config.xml\u0026#34;); // 2. 获取SqlSessionFactory对象 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // 3. 获取SqlSession对象 SqlSession sqlSession = sqlSessionFactory.openSession(); // 4. 执行sql TTestUser user = sqlSession.selectOne(\u0026#34;com.example.demo.dao.TTestUserMapper.selectByPrimaryKey\u0026#34;, 13L); log.info(\u0026#34;user = [{}]\u0026#34;, JSONUtil.toJsonStr(user)); // 5. 关闭连接 sqlSession.close(); inputStream.close(); } catch (Exception e){ log.error(\u0026#34;errMsg = [{}]\u0026#34;, e.getMessage(), e); } } 其中com.example.demo.dao.TTestUserMapper.selectByPrimaryKey指定了TTestUserMapper中selectByPrimaryKey这个方法，在对应的mapper/TTestUserMapper.xml我们定义了id一致的sql语句\n\u0026lt;select id=\u0026#34;selectByPrimaryKey\u0026#34; parameterType=\u0026#34;java.lang.Long\u0026#34; resultMap=\u0026#34;BaseResultMap\u0026#34;\u0026gt; select \u0026lt;include refid=\u0026#34;Base_Column_List\u0026#34; /\u0026gt; from t_test_user where id = #{id,jdbcType=BIGINT} \u0026lt;/select\u0026gt; Mybatis会在一开始加载的时候将每个标签中的sql语句包装成MappedStatement对象，并以类全路径名+方法名为key，MappedStatement为value缓存在内存中。在执行对应的方法时，就会根据这个唯一路径找到TTestUserMapper.xml这条sql语句并且执行返回结果。\n4. SqlSession的执行原理 # 4. 1 SqlSession的selectOne的执行原理 # SqlSession的selectOne代码如下，其实是调用selectList()方法获取第一条数据的。其中参数statement就是statement的id，parameter就是参数。\npublic \u0026lt;T\u0026gt; T selectOne(String statement, Object parameter) { List\u0026lt;T\u0026gt; list = this.selectList(statement, parameter); if (list.size() == 1) { return list.get(0); } else if (list.size() \u0026gt; 1) { throw new TooManyResultsException(\u0026#34;Expected one result (or null) to be returned by selectOne(), but found: \u0026#34; + list.size()); } else { return null; } } RowBounds 对象是分页对象，主要拼接sql中的start,limit条件。并且可以看到两个重要步骤：\n从configuration的成员变量mappedStatements中获取MappedStatement对象。mappedStatements是Map\u0026lt;String, MappedStatement\u0026gt;类型的缓存结构，其中key就是mapper接口全类名+方法名，MappedStatement就是对标签中配置的sql一个包装 使用executor成员变量来执行查询并且指定结果处理器，并且返回结果。Executor也是mybatis的一个重要的组件。sql的执行都是由Executor对象来操作的。 public \u0026lt;E\u0026gt; List\u0026lt;E\u0026gt; selectList(String statement, Object parameter, RowBounds rowBounds) { List var5; try { MappedStatement ms = this.configuration.getMappedStatement(statement); var5 = this.executor.query(ms, this.wrapCollection(parameter), rowBounds, Executor.NO_RESULT_HANDLER); } catch (Exception var9) { throw ExceptionFactory.wrapException(\u0026#34;Error querying database. Cause: \u0026#34; + var9, var9); } finally { ErrorContext.instance().reset(); } return var5; } MappedStatement对象的具体内容和Executor对象的类型，我们将在其它文章中详述。\n4. 2 SqlSession的通过mapper对象使用的执行原理 # 在启动流程那篇文章中，我们大致了解了sqlSession.getMapper返回的其实是个代理类MapperProxy，然后调mapper接口的方法其实都是调用MapperProxy的invoke方法，进而调用MapperMethod的execute方法。\npublic static void main(String[] args) { try { // 1. 读取配置 InputStream inputStream = Resources.getResourceAsStream(\u0026#34;mybatis-config.xml\u0026#34;); // 2. 创建SqlSessionFactory工厂 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // 3. 获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); // 4. 获取Mapper TTestUserMapper userMapper = sqlSession.getMapper(TTestUserMapper.class); // 5. 执行接口方法 TTestUser userInfo = userMapper.selectByPrimaryKey(16L); System.out.println(\u0026#34;userInfo = \u0026#34; + JSONUtil.toJsonStr(userInfo)); // 6. 提交事物 sqlSession.commit(); // 7. 关闭资源 sqlSession.close(); inputStream.close(); } catch (Exception e){ log.error(e.getMessage(), e); } } MapperMethod的execute方法中使用命令模式进行增删改查操作，其实也是调用了sqlSession的增删改查方法。\npublic Object execute(SqlSession sqlSession, Object[] args) { Object result; switch (command.getType()) { case INSERT: { Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); break; } case UPDATE: { Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); break; } case DELETE: { Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); break; } case SELECT: if (method.returnsVoid() \u0026amp;\u0026amp; method.hasResultHandler()) { executeWithResultHandler(sqlSession, args); result = null; } else if (method.returnsMany()) { result = executeForMany(sqlSession, args); } else if (method.returnsMap()) { result = executeForMap(sqlSession, args); } else if (method.returnsCursor()) { result = executeForCursor(sqlSession, args); } else { Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); if (method.returnsOptional() \u0026amp;\u0026amp; (result == null || !method.getReturnType().equals(result.getClass()))) { result = Optional.ofNullable(result); } } break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(\u0026#34;Unknown execution method for: \u0026#34; + command.getName()); } if (result == null \u0026amp;\u0026amp; method.getReturnType().isPrimitive() \u0026amp;\u0026amp; !method.returnsVoid()) { throw new BindingException(\u0026#34;Mapper method \u0026#39;\u0026#34; + command.getName() + \u0026#34; attempted to return null from a method with a primitive return type (\u0026#34; + method.getReturnType() + \u0026#34;).\u0026#34;); } return result; } 总结 # 在这篇文章中我们详细介绍了SqlSession的作用，创建过程，使用方法，以及执行原理等，对SqlSession已经有了比较全面的了解。其中涉及到的Executor对象，MappedStatement对象，ResultHandler我们将在其它文章中讲解。欢迎在评论区中讨论指正，一起进步。\n"},{"id":100,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/MyBatis/principle/mybatis-principle2/","title":"Mybatis原理系列(2)","section":"原理","content":" 转载自https://www.jianshu.com/p/7d6b891180a3（添加小部分笔记）感谢作者!\n在上篇文章中，我们举了一个例子如何使用MyBatis，但是对其中dao层，entity层，mapper层间的关系不得而知，从此篇文章开始，笔者将从MyBatis的启动流程着手，真正的开始研究MyBatis源码了。\n1. MyBatis启动代码示例 # 在上篇文章中，介绍了MyBatis的相关配置和各层代码编写，本文将以下代码展开描述和介绍MyBatis的启动流程，并简略的介绍各个模块的作用，各个模块的细节部分将在其它文章中呈现。\n回顾下上文中使用mybatis的部分代码，包括七步。每步虽然都是一行代码，但是隐藏了很多细节。接下来我们将围绕这起步展开了解。\n@Slf4j public class MyBatisBootStrap { public static void main(String[] args) { try { // 1. 读取配置 InputStream inputStream = Resources.getResourceAsStream(\u0026#34;mybatis-config.xml\u0026#34;); // 2. 创建SqlSessionFactory工厂 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // 3. 获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); // 4. 获取Mapper TTestUserMapper userMapper = sqlSession.getMapper(TTestUserMapper.class); // 5. 执行接口方法 TTestUser userInfo = userMapper.selectByPrimaryKey(16L); System.out.println(\u0026#34;userInfo = \u0026#34; + JSONUtil.toJsonStr(userInfo)); // 6. 提交事物 sqlSession.commit(); // 7. 关闭资源 sqlSession.close(); inputStream.close(); } catch (Exception e){ log.error(e.getMessage(), e); } } } 2. 读取配置 # // 1. 读取配置 InputStream inputStream = Resources.getResourceAsStream(\u0026#34;mybatis-config.xml\u0026#34;); 在mybatis-config.xml中我们配置了属性，环境，映射文件路径等，其实不仅可以配置以上内容，还可以配置插件，反射工厂，类型处理器等等其它内容。在启动流程中的第一步我们就需要读取这个配置文件，并获取一个输入流为下一步解析配置文件作准备。\nmybatis-config.xml 内容如下\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE configuration PUBLIC \u0026#34;-//mybatis.org//DTD Config 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-config.dtd\u0026#34;\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!--一些重要的全局配置--\u0026gt; \u0026lt;settings\u0026gt; \u0026lt;setting name=\u0026#34;cacheEnabled\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;lazyLoadingEnabled\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;multipleResultSetsEnabled\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;useColumnLabel\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;useGeneratedKeys\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;autoMappingBehavior\u0026#34; value=\u0026#34;PARTIAL\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;autoMappingUnknownColumnBehavior\u0026#34; value=\u0026#34;WARNING\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;defaultExecutorType\u0026#34; value=\u0026#34;SIMPLE\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;defaultStatementTimeout\u0026#34; value=\u0026#34;25\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;defaultFetchSize\u0026#34; value=\u0026#34;100\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;safeRowBoundsEnabled\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;mapUnderscoreToCamelCase\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;localCacheScope\u0026#34; value=\u0026#34;STATEMENT\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;jdbcTypeForNull\u0026#34; value=\u0026#34;OTHER\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;lazyLoadTriggerMethods\u0026#34; value=\u0026#34;equals,clone,hashCode,toString\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;logImpl\u0026#34; value=\u0026#34;STDOUT_LOGGING\u0026#34; /\u0026gt;--\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;environments default=\u0026#34;development\u0026#34;\u0026gt; \u0026lt;environment id=\u0026#34;development\u0026#34;\u0026gt; \u0026lt;transactionManager type=\u0026#34;JDBC\u0026#34;/\u0026gt; \u0026lt;dataSource type=\u0026#34;POOLED\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;driver\u0026#34; value=\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;url\u0026#34; value=\u0026#34;jdbc:mysql://10.255.0.50:3306/volvo_bev?useUnicode=true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;username\u0026#34; value=\u0026#34;appdev\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;password\u0026#34; value=\u0026#34;FEGwo3EzsdDYS9ooYKGCjRQepkwG\u0026#34;/\u0026gt; \u0026lt;/dataSource\u0026gt; \u0026lt;/environment\u0026gt; \u0026lt;/environments\u0026gt; \u0026lt;mappers\u0026gt; \u0026lt;!--这边可以使用package和resource两种方式加载mapper--\u0026gt; \u0026lt;!--\u0026lt;package name=\u0026#34;包名\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;mapper resource=\u0026#34;./mappers/SysUserMapper.xml\u0026#34;/\u0026gt; \u0026lt;package name=\u0026#34;com.example.demo.dao\u0026#34;/\u0026gt; --\u0026gt; \u0026lt;mapper resource=\u0026#34;./mapper/TTestUserMapper.xml\u0026#34;/\u0026gt; \u0026lt;/mappers\u0026gt; \u0026lt;/configuration\u0026gt; 3. 创建SqlSessionFactory工厂 # SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); 我们在学习Java的设计模式时，会学到工厂模式，工厂模式又分为简单工厂模式，工厂方法模式，抽象工厂模式等等。工厂模式就是为了创建对象提供接口，并将创建对象的具体细节屏蔽起来，从而可以提高灵活性。\npublic interface SqlSessionFactory { SqlSession openSession(); SqlSession openSession(boolean autoCommit); SqlSession openSession(Connection connection); SqlSession openSession(TransactionIsolationLevel level); SqlSession openSession(ExecutorType execType); SqlSession openSession(ExecutorType execType, boolean autoCommit); SqlSession openSession(ExecutorType execType, TransactionIsolationLevel level); SqlSession openSession(ExecutorType execType, Connection connection); Configuration getConfiguration(); } 由此可知SqlSessionFactory工厂是为了创建一个对象而生的，其产出的对象就是SqlSession对象。SqlSession是MyBatis面向数据库的高级接口，其提供了执行查询sql，更新sql，提交事物，回滚事物，**获取映射代理类(也就是Mapper)**等等方法。\n在此笔者列出了主要方法，一些重载的方法就过滤掉了。\npublic interface SqlSession extends Closeable { /** * 查询一个结果对象 **/ \u0026lt;T\u0026gt; T selectOne(String statement, Object parameter); /** * 查询一个结果集合 **/ \u0026lt;E\u0026gt; List\u0026lt;E\u0026gt; selectList(String statement, Object parameter, RowBounds rowBounds); /** * 查询一个map **/ \u0026lt;K, V\u0026gt; Map\u0026lt;K, V\u0026gt; selectMap(String statement, Object parameter, String mapKey, RowBounds rowBounds); /** * 查询游标 **/ \u0026lt;T\u0026gt; Cursor\u0026lt;T\u0026gt; selectCursor(String statement, Object parameter, RowBounds rowBounds); void select(String statement, Object parameter, RowBounds rowBounds, ResultHandler handler); /** * 插入 **/ int insert(String statement, Object parameter); /** * 修改 **/ int update(String statement, Object parameter); /** * 删除 **/ int delete(String statement, Object parameter); /** * 提交事物 **/ void commit(boolean force); /** * 回滚事物 **/ void rollback(boolean force); List\u0026lt;BatchResult\u0026gt; flushStatements(); void close(); void clearCache(); Configuration getConfiguration(); /** * 获取映射代理类 **/ \u0026lt;T\u0026gt; T getMapper(Class\u0026lt;T\u0026gt; type); /** * 获取数据库连接 **/ Connection getConnection(); } 回到开始，SqlSessionFactory工厂是怎么创建的出来的呢？SqlSessionFactoryBuilder就是创建者，以Builder结尾我们很容易想到了Java设计模式中的建造者模式，一个对象的创建是由众多复杂对象组成的，建造者模式就是一个创建复杂对象的选择，它与工厂模式相比，建造者模式更加关注零件装配的顺序。\npublic class SqlSessionFactoryBuilder { public SqlSessionFactory build(InputStream inputStream, String environment, Properties properties) { try { XMLConfigBuilder parser = new XMLConfigBuilder(inputStream, environment, properties); return build(parser.parse()); } catch (Exception e) { throw ExceptionFactory.wrapException(\u0026#34;Error building SqlSession.\u0026#34;, e); } finally { ErrorContext.instance().reset(); try { inputStream.close(); } catch (IOException e) { // Intentionally ignore. Prefer previous error. } } } } 其中XMLConfigBuilder就是解析mybatis-config.xml中每个标签的内容，parse()方法返回的就是一个Configuration对象.Configuration也是MyBatis中一个很重要的组件，包括插件，对象工厂，反射工厂，映射文件，类型解析器等等都存储在Configuration对象中。\npublic Configuration parse() { if (parsed) { throw new BuilderException(\u0026#34;Each XMLConfigBuilder can only be used once.\u0026#34;); } parsed = true; parseConfiguration(parser.evalNode(\u0026#34;/configuration\u0026#34;)); return configuration; } private void parseConfiguration(XNode root) { try { // issue #117 read properties first // 解析properties节点 propertiesElement(root.evalNode(\u0026#34;properties\u0026#34;)); Properties settings = settingsAsProperties(root.evalNode(\u0026#34;settings\u0026#34;)); loadCustomVfs(settings); loadCustomLogImpl(settings); typeAliasesElement(root.evalNode(\u0026#34;typeAliases\u0026#34;)); pluginElement(root.evalNode(\u0026#34;plugins\u0026#34;)); objectFactoryElement(root.evalNode(\u0026#34;objectFactory\u0026#34;)); objectWrapperFactoryElement(root.evalNode(\u0026#34;objectWrapperFactory\u0026#34;)); reflectorFactoryElement(root.evalNode(\u0026#34;reflectorFactory\u0026#34;)); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 environmentsElement(root.evalNode(\u0026#34;environments\u0026#34;)); databaseIdProviderElement(root.evalNode(\u0026#34;databaseIdProvider\u0026#34;)); typeHandlerElement(root.evalNode(\u0026#34;typeHandlers\u0026#34;)); mapperElement(root.evalNode(\u0026#34;mappers\u0026#34;)); } catch (Exception e) { throw new BuilderException(\u0026#34;Error parsing SQL Mapper Configuration. Cause: \u0026#34; + e, e); } } 在获取到Configuration对象后，SqlSessionFactoryBuilder就会创建一个DefaultSqlSessionFactory对象，DefaultSqlSessionFactory是SqlSessionFactory的一个默认实现，还有一个实现是SqlSessionManager。\npublic SqlSessionFactory build(Configuration config) { return new DefaultSqlSessionFactory(config); } 4. 获取sqlSession # // 3. 获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); 在前面我们讲到，sqlSession是操作数据库的高级接口，我们操作数据库都是通过这个接口操作的。获取sqlSession有两种方式，一种是从数据源中获取的，还有一种是从连接中获取。\n貌似默认是从数据源获取\n获取到的都是DefaultSqlSession对象，也就是sqlSession的默认实现。\n注意，过程中有个Executor\u0026mdash;执行器\nprivate SqlSession openSessionFromDataSource(ExecutorType execType, TransactionIsolationLevel level, boolean autoCommit) { Transaction tx = null; try { final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); tx = transactionFactory.newTransaction(environment.getDataSource(), level, autoCommit); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { closeTransaction(tx); // may have fetched a connection so lets call close() throw ExceptionFactory.wrapException(\u0026#34;Error opening session. Cause: \u0026#34; + e, e); } finally { ErrorContext.instance().reset(); } } private SqlSession openSessionFromConnection(ExecutorType execType, Connection connection) { try { boolean autoCommit; try { autoCommit = connection.getAutoCommit(); } catch (SQLException e) { // Failover to true, as most poor drivers // or databases won\u0026#39;t support transactions autoCommit = true; } final Environment environment = configuration.getEnvironment(); final TransactionFactory transactionFactory = getTransactionFactoryFromEnvironment(environment); final Transaction tx = transactionFactory.newTransaction(connection); final Executor executor = configuration.newExecutor(tx, execType); return new DefaultSqlSession(configuration, executor, autoCommit); } catch (Exception e) { throw ExceptionFactory.wrapException(\u0026#34;Error opening session. Cause: \u0026#34; + e, e); } finally { ErrorContext.instance().reset(); } } 获取SqlSession步骤\n5. 获取Mapper代理类 # 在上一步获取到sqlSession后，我们接下来就获取到了mapper代理类。\n// 4. 获取Mapper TTestUserMapper userMapper = sqlSession.getMapper(TTestUserMapper.class); 这个getMapper方法，我们看看DefaultSqlSession是怎么做的\nDefaultSqlSession 的 getMapper 方法\npublic \u0026lt;T\u0026gt; T getMapper(Class\u0026lt;T\u0026gt; type) { return this.configuration.getMapper(type, this); } Configuration 的 getMapper 方法\npublic \u0026lt;T\u0026gt; T getMapper(Class\u0026lt;T\u0026gt; type, SqlSession sqlSession) { return this.mapperRegistry.getMapper(type, sqlSession); } MapperRegistry 中有个getMapper方法，实际上是从成员变量knownMappers中获取的，这个knownMappers是个key-value形式的缓存，key是mapper接口的class对象，value是MapperProxyFactory代理工厂，这个工厂就是用来创建MapperProxy代理类的。\npublic class MapperRegistry { private final Configuration config; private final Map\u0026lt;Class\u0026lt;?\u0026gt;, MapperProxyFactory\u0026lt;?\u0026gt;\u0026gt; knownMappers = new HashMap(); public MapperRegistry(Configuration config) { this.config = config; } public \u0026lt;T\u0026gt; T getMapper(Class\u0026lt;T\u0026gt; type, SqlSession sqlSession) { MapperProxyFactory\u0026lt;T\u0026gt; mapperProxyFactory = (MapperProxyFactory)this.knownMappers.get(type); if (mapperProxyFactory == null) { throw new BindingException(\u0026#34;Type \u0026#34; + type + \u0026#34; is not known to the MapperRegistry.\u0026#34;); } else { try { return mapperProxyFactory.newInstance(sqlSession); } catch (Exception var5) { throw new BindingException(\u0026#34;Error getting mapper instance. Cause: \u0026#34; + var5, var5); } } } } 如果对java动态代理了解的同学就知道，Proxy.newProxyInstance()方法可以创建出一个目标对象一个代理对象。由此可知每次调用getMapper方法都会创建出一个代理类出来。\npublic class MapperProxyFactory\u0026lt;T\u0026gt; { private final Class\u0026lt;T\u0026gt; mapperInterface; private final Map\u0026lt;Method, MapperMethod\u0026gt; methodCache = new ConcurrentHashMap(); public MapperProxyFactory(Class\u0026lt;T\u0026gt; mapperInterface) { this.mapperInterface = mapperInterface; } public Class\u0026lt;T\u0026gt; getMapperInterface() { return this.mapperInterface; } public Map\u0026lt;Method, MapperMethod\u0026gt; getMethodCache() { return this.methodCache; } protected T newInstance(MapperProxy\u0026lt;T\u0026gt; mapperProxy) { return Proxy.newProxyInstance(this.mapperInterface.getClassLoader(), new Class[]{this.mapperInterface}, mapperProxy); } public T newInstance(SqlSession sqlSession) { MapperProxy\u0026lt;T\u0026gt; mapperProxy = new MapperProxy(sqlSession, this.mapperInterface, this.methodCache); return this.newInstance(mapperProxy); } } 回到上面，那这个MapperProxyFactory是怎么加载到MapperRegistry的knownMappers缓存中的呢？\n在上面的Configuration类的parseConfiguration方法中，我们会解析 mappers标签，mapperElement方法就会解析mapper接口。\nprivate void parseConfiguration(XNode root) { try { // issue #117 read properties first // 解析properties节点 propertiesElement(root.evalNode(\u0026#34;properties\u0026#34;)); Properties settings = settingsAsProperties(root.evalNode(\u0026#34;settings\u0026#34;)); loadCustomVfs(settings); loadCustomLogImpl(settings); typeAliasesElement(root.evalNode(\u0026#34;typeAliases\u0026#34;)); pluginElement(root.evalNode(\u0026#34;plugins\u0026#34;)); objectFactoryElement(root.evalNode(\u0026#34;objectFactory\u0026#34;)); objectWrapperFactoryElement(root.evalNode(\u0026#34;objectWrapperFactory\u0026#34;)); reflectorFactoryElement(root.evalNode(\u0026#34;reflectorFactory\u0026#34;)); settingsElement(settings); // read it after objectFactory and objectWrapperFactory issue #631 environmentsElement(root.evalNode(\u0026#34;environments\u0026#34;)); databaseIdProviderElement(root.evalNode(\u0026#34;databaseIdProvider\u0026#34;)); typeHandlerElement(root.evalNode(\u0026#34;typeHandlers\u0026#34;)); mapperElement(root.evalNode(\u0026#34;mappers\u0026#34;)); } catch (Exception e) { throw new BuilderException(\u0026#34;Error parsing SQL Mapper Configuration. Cause: \u0026#34; + e, e); } } private void mapperElement(XNode parent) throws Exception { if (parent != null) { for (XNode child : parent.getChildren()) { if (\u0026#34;package\u0026#34;.equals(child.getName())) { String mapperPackage = child.getStringAttribute(\u0026#34;name\u0026#34;); configuration.addMappers(mapperPackage); } else { String resource = child.getStringAttribute(\u0026#34;resource\u0026#34;); String url = child.getStringAttribute(\u0026#34;url\u0026#34;); String mapperClass = child.getStringAttribute(\u0026#34;class\u0026#34;); if (resource != null \u0026amp;\u0026amp; url == null \u0026amp;\u0026amp; mapperClass == null) { ErrorContext.instance().resource(resource); InputStream inputStream = Resources.getResourceAsStream(resource); XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, resource, configuration.getSqlFragments()); mapperParser.parse(); } else if (resource == null \u0026amp;\u0026amp; url != null \u0026amp;\u0026amp; mapperClass == null) { ErrorContext.instance().resource(url); InputStream inputStream = Resources.getUrlAsStream(url); XMLMapperBuilder mapperParser = new XMLMapperBuilder(inputStream, configuration, url, configuration.getSqlFragments()); mapperParser.parse(); } else if (resource == null \u0026amp;\u0026amp; url == null \u0026amp;\u0026amp; mapperClass != null) { Class\u0026lt;?\u0026gt; mapperInterface = Resources.classForName(mapperClass); configuration.addMapper(mapperInterface); } else { throw new BuilderException(\u0026#34;A mapper element may only specify a url, resource or class, but not more than one.\u0026#34;); } } } } } 解析完后，就将这个mapper接口加到 mapperRegistry中，\nconfiguration.addMapper(mapperInterface); Configuration的addMapper方法\npublic \u0026lt;T\u0026gt; void addMapper(Class\u0026lt;T\u0026gt; type) { mapperRegistry.addMapper(type); } 最后还是加载到了MapperRegistry的knownMappers中去了\npublic \u0026lt;T\u0026gt; void addMapper(Class\u0026lt;T\u0026gt; type) { if (type.isInterface()) { if (hasMapper(type)) { throw new BindingException(\u0026#34;Type \u0026#34; + type + \u0026#34; is already known to the MapperRegistry.\u0026#34;); } boolean loadCompleted = false; try { knownMappers.put(type, new MapperProxyFactory\u0026lt;\u0026gt;(type)); // It\u0026#39;s important that the type is added before the parser is run // otherwise the binding may automatically be attempted by the // mapper parser. If the type is already known, it won\u0026#39;t try. MapperAnnotationBuilder parser = new MapperAnnotationBuilder(config, type); parser.parse(); loadCompleted = true; } finally { if (!loadCompleted) { knownMappers.remove(type); } } } } 获取mapper代理类过程\n6. 执行mapper接口方法 # // 5. 执行接口方法 TTestUser userInfo = userMapper.selectByPrimaryKey(16L); selectByPrimaryKey是TTestUserMapper接口中定义的一个方法，但是我们没有编写TTestUserMapper接口的的实现类，那么Mybatis是怎么帮我们执行的呢？前面讲到，获取mapper对象时，是会获取到一个MapperProxyFactory工厂类，并创建一个MapperProxy代理类，在执行Mapper接口的方法时，会调用MapperProxy的invoke方法。\n@Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { try { if (Object.class.equals(method.getDeclaringClass())) { return method.invoke(this, args); } else { return cachedInvoker(method).invoke(proxy, method, args, sqlSession); } } catch (Throwable t) { throw ExceptionUtil.unwrapThrowable(t); } } 如果是Object的方法就直接执行，否则执行cachedInvoker(method).invoke(proxy, method, args, sqlSession); 这行代码，到这里，想必有部分同学已经头晕了吧。怎么又来了个invoke方法。 cachedInvoker 是返回缓存的MapperMethodInvoker对象，MapperMethodInvoker的invoke方法会执行MapperMethod的execute方法。\npublic class MapperMethod { private final SqlCommand command; private final MethodSignature method; public MapperMethod(Class\u0026lt;?\u0026gt; mapperInterface, Method method, Configuration config) { this.command = new SqlCommand(config, mapperInterface, method); this.method = new MethodSignature(config, mapperInterface, method); } public Object execute(SqlSession sqlSession, Object[] args) { Object result; switch (command.getType()) { case INSERT: { Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.insert(command.getName(), param)); break; } case UPDATE: { Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.update(command.getName(), param)); break; } case DELETE: { Object param = method.convertArgsToSqlCommandParam(args); result = rowCountResult(sqlSession.delete(command.getName(), param)); break; } case SELECT: if (method.returnsVoid() \u0026amp;\u0026amp; method.hasResultHandler()) { executeWithResultHandler(sqlSession, args); result = null; } else if (method.returnsMany()) { result = executeForMany(sqlSession, args); } else if (method.returnsMap()) { result = executeForMap(sqlSession, args); } else if (method.returnsCursor()) { result = executeForCursor(sqlSession, args); } else { Object param = method.convertArgsToSqlCommandParam(args); result = sqlSession.selectOne(command.getName(), param); if (method.returnsOptional() \u0026amp;\u0026amp; (result == null || !method.getReturnType().equals(result.getClass()))) { result = Optional.ofNullable(result); } } break; case FLUSH: result = sqlSession.flushStatements(); break; default: throw new BindingException(\u0026#34;Unknown execution method for: \u0026#34; + command.getName()); } if (result == null \u0026amp;\u0026amp; method.getReturnType().isPrimitive() \u0026amp;\u0026amp; !method.returnsVoid()) { throw new BindingException(\u0026#34;Mapper method \u0026#39;\u0026#34; + command.getName() + \u0026#34; attempted to return null from a method with a primitive return type (\u0026#34; + method.getReturnType() + \u0026#34;).\u0026#34;); } return result; } } 然后根据执行的接口找到mapper.xml中配置的sql，并处理参数，然后执行返回结果处理结果等步骤。\n7. 提交事务 # // 6. 提交事务 sqlSession.commit(); 事务就是将若干数据库操作看成一个单元，要么全部成功，要么全部失败，如果失败了，则会执行执行回滚操作，恢复到开始执行的数据库状态。\n8. 关闭资源 # // 7. 关闭资源 sqlSession.close(); inputStream.close(); sqlSession是种共用资源，用完了要返回到池子中，以供其它地方使用。\n9. 总结 # 至此我们已经大致了解了Mybatis启动时的大致流程，很多细节都还没有详细介绍，这是因为涉及到的层面又深又广，如果在一篇文章中介绍，反而会让读者如置云里雾里，不知所云。因此，在接下来我将每个模块的详细介绍。如果文章有什么错误或者需要改进的，希望同学们指出来，希望对大家有帮助。\n"},{"id":101,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/MyBatis/principle/mybatis-principle1/","title":"Mybatis原理系列(1)","section":"原理","content":" 转载自https://www.jianshu.com/p/ada025f97a07（添加小部分笔记）感谢作者!\n作为Java码农，无论在面试中，还是在工作中都会遇到MyBatis的相关问题。笔者从大学开始就接触MyBatis，到现在为止都是会用，知道怎么配置，怎么编写xml，但是不知道Mybatis核心原理，一遇到问题就复制错误信息百度解决。为了改变这种境地，鼓起勇气开始下定决心阅读MyBatis源码，并开始记录阅读过程，希望和大家分享。\n1. 初识MyBatis # 还记得当初接触MyBatis时，觉得要配置很多，而且sql要单独写在xml中，相比Hibernate来说简直不太友好，直到后来出现了复杂的业务需求，需要编写相应的复杂的sql，此时用Hibernate反而更加麻烦了，用MyBatis是真香了。因此笔者对MyBatis的第一印象就是将业务关注的sql和java代码进行了解耦，在业务复杂变化的时候，相应的数据库操作需要相应进行修改，如果通过java代码构建操作数据逻辑，这不断变动的需求对程序员的耐心是极大的考验。如果将sql统一的维护在一个文件里，java代码用接口定义，在需求变动时，只用改相应的sql，从而减少了修改量，提高开发效率。以上也是经常在面试中经常问到的Hibernate和MyBatis间的区别一点。\n切到正题，Mybatis是什么呢？\nMybatis SQL 映射框架使得一个面向对象构建的应用程序去访问一个关系型数据库变得更容易。MyBatis使用XML描述符或注解将对象与存储过程或SQL语句耦合。与对象关系映射工具相比，简单性是MyBatis数据映射器的最大优势。\n以上是Mybatis的官方解释，其中“映射”，“面向对象”，“关系型”，“xml”等等都是Mybatis的关键词，也是我们了解了Mybatis原理后，会恍然大悟的地方。笔者现在不详述这些概念，在最后总结的时候再进行详述。我们只要知道Mybatis为我们操作数据库提供了很大的便捷。\n2. 源码下载 # 这里建议使用maven即可，在pom.xml添加以下依赖\n\u0026lt;dependencies\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.32\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.mybatis\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.6\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--这里还添加了一些辅助的依赖--\u0026gt; \u0026lt;!--lombok--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.8\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--日志模块--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.17.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 然后在ExternalLibraries 的mybatis:3.5.6里找到，就能看到目录结构 ，随便找一个进去 idea右上角会出现DownloadSource之类的字样 ，点击即可\n我们首先要从github上下载源码，仓库地址，然后在IDEA中clone代码\n在打开中的IDEA中，选择vsc -\u0026gt; get from version control -\u0026gt; 复制刚才的地址\nimage.png\n点击clone即可\nimage.png\n经过漫长的等待后，代码会全部下载下来，项目结果如下，框起来的就是我们要关注的核心代码了。\nimage.png\n每个包就是MyBatis的一个模块，每个包的作用如下：\n3. 一个简单的栗子 # 不知道现在还有没有同学知道怎么使用原生的JDBC进行数据库操作，现在框架太方便了，为我们考虑了很多，也隐藏了很多细节，因此会让我们处于一个云里雾里的境地，为什么这么设计，这样设计解决了什么问题，我们是不得而知的，为了了解其中奥秘，还是需要我们从头开始了解。\n接下来笔者将以两个栗子来分别讲讲如何用原生的JDBC操作数据库，以及如何使用MyBatis框架来实现相同的功能，并比较两者的区别。\n首先创建数据库 test\n3.1 创建表 # 在此我们建了两张表，一张是t_test_user用户信息主表，一张是t_test_user_info用户信息副表，两张表通过member_id进行关联。\nDROP TABLE IF EXISTS `t_test_user`; CREATE TABLE `t_test_user` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;id\u0026#39;, `member_id` bigint(20) NOT NULL COMMENT \u0026#39;会员id\u0026#39;, `real_name` varchar(255) CHARACTER SET utf8 DEFAULT NULL COMMENT \u0026#39;真实姓名\u0026#39;, `nickname` varchar(255) CHARACTER SET utf8mb4 DEFAULT NULL COMMENT \u0026#39;会员昵称\u0026#39;, `date_create` datetime DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;创建时间\u0026#39;, `date_update` datetime DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \u0026#39;更新时间\u0026#39;, `deleted` bigint(20) DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;删除标识，0未删除，时间戳-删除时间\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=42013 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=\u0026#39;测试表\u0026#39;; DROP TABLE IF EXISTS `t_test_user_info`; CREATE TABLE `t_test_user_info` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT \u0026#39;id\u0026#39;, `member_id` bigint(20) NOT NULL COMMENT \u0026#39;会员id\u0026#39;, `member_phone` varchar(50) CHARACTER SET utf8mb4 DEFAULT NULL COMMENT \u0026#39;电话\u0026#39;, `member_province` varchar(50) CHARACTER SET utf8mb4 DEFAULT NULL COMMENT \u0026#39;省\u0026#39;, `member_city` varchar(50) CHARACTER SET utf8mb4 DEFAULT NULL COMMENT \u0026#39;市\u0026#39;, `member_county` varchar(50) CHARACTER SET utf8mb4 DEFAULT NULL COMMENT \u0026#39;区\u0026#39;, `date_create` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;创建时间\u0026#39;, `date_update` datetime NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT \u0026#39;更新时间\u0026#39;, `deleted` bigint(20) NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;删除标识，0未删除，时间戳-删除时间\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=17 DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci COMMENT=\u0026#39;用户信息测试表\u0026#39;; 3.2 使用Java JDBC进行操作数据库 # JDBC(Java Database Connectivity，简称JDBC）是Java中用来规范客户端程序如何来访问数据库的应用程序接口，提供了诸如查询和更新数据库中数据的方法。使用JDBC操作数据库，一般包含7步，代码如下。\npublic class JDBCTest { /** * 数据库地址 替换成本地的地址 */ private static final String url = \u0026#34;jdbc:mysql://localhost:3306/test?useUnicode=true\u0026#34;; /** * 数据库用户名 */ private static final String username = \u0026#34;test\u0026#34;; /** * 密码 */ private static final String password = \u0026#34;test\u0026#34;; public static void main(String[] args) { try { // 1. 加载数据库驱动 Class.forName(\u0026#34;com.mysql.jdbc.Driver\u0026#34;); // 2. 获得连接 Connection connection = DriverManager.getConnection(url, username, password); // 3. 创建sql语句 String sql = \u0026#34;select * from t_test_user\u0026#34;; Statement statement = connection.createStatement(); // 4. 执行sql ResultSet result = statement.executeQuery(sql); // 5. 处理结果 while(result.next()){ System.out.println(\u0026#34;result = \u0026#34; + result.getString(1)); } // 6. 关闭连接 result.close(); connection.close(); } catch (Exception e){ System.out.println(e); } } } 3.3 使用Mybatis进行操作数据库 # 3.3.1 新增mybatis-config.xml配置 # 在路径src/main/resources/mybatis-config.xml新增配置，配置内容如下\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE configuration PUBLIC \u0026#34;-//mybatis.org//DTD Config 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-config.dtd\u0026#34;\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;!--一些重要的全局配置--\u0026gt; \u0026lt;settings\u0026gt; \u0026lt;setting name=\u0026#34;cacheEnabled\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;lazyLoadingEnabled\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;multipleResultSetsEnabled\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;useColumnLabel\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;useGeneratedKeys\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;autoMappingBehavior\u0026#34; value=\u0026#34;PARTIAL\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;autoMappingUnknownColumnBehavior\u0026#34; value=\u0026#34;WARNING\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;defaultExecutorType\u0026#34; value=\u0026#34;SIMPLE\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;defaultStatementTimeout\u0026#34; value=\u0026#34;25\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;defaultFetchSize\u0026#34; value=\u0026#34;100\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;safeRowBoundsEnabled\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;mapUnderscoreToCamelCase\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;localCacheScope\u0026#34; value=\u0026#34;STATEMENT\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;jdbcTypeForNull\u0026#34; value=\u0026#34;OTHER\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;lazyLoadTriggerMethods\u0026#34; value=\u0026#34;equals,clone,hashCode,toString\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--\u0026lt;setting name=\u0026#34;logImpl\u0026#34; value=\u0026#34;STDOUT_LOGGING\u0026#34; /\u0026gt;--\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;environments default=\u0026#34;development\u0026#34;\u0026gt; \u0026lt;environment id=\u0026#34;development\u0026#34;\u0026gt; \u0026lt;transactionManager type=\u0026#34;JDBC\u0026#34;/\u0026gt; \u0026lt;dataSource type=\u0026#34;POOLED\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;driver\u0026#34; value=\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;url\u0026#34; value=\u0026#34;jdbc:mysql://localhost:3306/test?useUnicode=true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;username\u0026#34; value=\u0026#34;root\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;password\u0026#34; value=\u0026#34;123456\u0026#34;/\u0026gt; \u0026lt;/dataSource\u0026gt; \u0026lt;/environment\u0026gt; \u0026lt;/environments\u0026gt; \u0026lt;mappers\u0026gt; \u0026lt;!--这边可以使用package或resource两种方式加载mapper--\u0026gt; \u0026lt;!--\u0026lt;package name=\u0026#34;包名\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;!--如果这里使用了包名, 那么resource下 的Mapper.xml文件的层级,一定要和Mapper类的全类名一样,即com/example/demo/dao/TTestUserMapper.xml--\u0026gt; \u0026lt;!--\u0026lt;mapper resource=\u0026#34;具体的Mapper.xml地址\u0026#34; /\u0026gt;--\u0026gt; \u0026lt;mapper resource=\u0026#34;mapper/TTestUserMapper.xml\u0026#34; /\u0026gt; \u0026lt;!--\u0026lt;package name=\u0026#34;com.example.demo.dao\u0026#34;/\u0026gt;--\u0026gt; \u0026lt;/mappers\u0026gt; \u0026lt;/configuration\u0026gt; 3.3.2 新增mapper接口 # 新增src/main/java/com/example/demo/dao/TTestUserMapper.java 接口\npackage com.example.demo.dao; import com.example.demo.entity.TTestUser; import org.apache.ibatis.annotations.Mapper; import java.util.List; @Mapper public interface TTestUserMapper { /** * This method was generated by MyBatis Generator. * This method corresponds to the database table t_test_user * * @mbggenerated */ int deleteByPrimaryKey(Long id); /** * This method was generated by MyBatis Generator. * This method corresponds to the database table t_test_user * * @mbggenerated */ int insert(TTestUser record); /** * This method was generated by MyBatis Generator. * This method corresponds to the database table t_test_user * * @mbggenerated */ int insertSelective(TTestUser record); int batchInsert(List\u0026lt;TTestUser\u0026gt; records); /** * This method was generated by MyBatis Generator. * This method corresponds to the database table t_test_user * * @mbggenerated */ TTestUser selectByPrimaryKey(Long id); /** * This method was generated by MyBatis Generator. * This method corresponds to the database table t_test_user * * @mbggenerated */ int updateByPrimaryKeySelective(TTestUser record); /** * This method was generated by MyBatis Generator. * This method corresponds to the database table t_test_user * * @mbggenerated */ int updateByPrimaryKey(TTestUser record); } 3.3.3 新增映射配置文件 # src/main/resources/mapper/TTestUserMapper.xml 新增映射配置文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;com.example.demo.dao.TTestUserMapper\u0026#34;\u0026gt; \u0026lt;resultMap id=\u0026#34;BaseResultMap\u0026#34; type=\u0026#34;com.example.demo.entity.TTestUser\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; \u0026lt;id column=\u0026#34;id\u0026#34; jdbcType=\u0026#34;BIGINT\u0026#34; property=\u0026#34;id\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;member_id\u0026#34; jdbcType=\u0026#34;BIGINT\u0026#34; property=\u0026#34;memberId\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;real_name\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; property=\u0026#34;realName\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;nickname\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; property=\u0026#34;nickname\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;date_create\u0026#34; jdbcType=\u0026#34;TIMESTAMP\u0026#34; property=\u0026#34;dateCreate\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;date_update\u0026#34; jdbcType=\u0026#34;TIMESTAMP\u0026#34; property=\u0026#34;dateUpdate\u0026#34; /\u0026gt; \u0026lt;result column=\u0026#34;deleted\u0026#34; jdbcType=\u0026#34;BIGINT\u0026#34; property=\u0026#34;deleted\u0026#34; /\u0026gt; \u0026lt;/resultMap\u0026gt; \u0026lt;sql id=\u0026#34;Base_Column_List\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; id, member_id, real_name, nickname, date_create, date_update, deleted \u0026lt;/sql\u0026gt; \u0026lt;select id=\u0026#34;selectByPrimaryKey\u0026#34; parameterType=\u0026#34;java.lang.Long\u0026#34; resultMap=\u0026#34;BaseResultMap\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; select \u0026lt;include refid=\u0026#34;Base_Column_List\u0026#34; /\u0026gt; from t_test_user where id = #{id,jdbcType=BIGINT} \u0026lt;/select\u0026gt; \u0026lt;delete id=\u0026#34;deleteByPrimaryKey\u0026#34; parameterType=\u0026#34;java.lang.Long\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; delete from t_test_user where id = #{id,jdbcType=BIGINT} \u0026lt;/delete\u0026gt; \u0026lt;insert id=\u0026#34;insert\u0026#34; parameterType=\u0026#34;com.example.demo.entity.TTestUser\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; insert into t_test_user (id, member_id, real_name, nickname, date_create, date_update, deleted) values (#{id,jdbcType=BIGINT}, #{memberId,jdbcType=BIGINT}, #{realName,jdbcType=VARCHAR}, #{nickname,jdbcType=VARCHAR}, #{dateCreate,jdbcType=TIMESTAMP}, #{dateUpdate,jdbcType=TIMESTAMP}, #{deleted,jdbcType=BIGINT}) \u0026lt;/insert\u0026gt; \u0026lt;insert id=\u0026#34;insertSelective\u0026#34; parameterType=\u0026#34;com.example.demo.entity.TTestUser\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; insert into t_test_user \u0026lt;trim prefix=\u0026#34;(\u0026#34; suffix=\u0026#34;)\u0026#34; suffixOverrides=\u0026#34;,\u0026#34;\u0026gt; \u0026lt;if test=\u0026#34;id != null\u0026#34;\u0026gt; id, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;memberId != null\u0026#34;\u0026gt; member_id, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;realName != null\u0026#34;\u0026gt; real_name, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;nickname != null\u0026#34;\u0026gt; nickname, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;dateCreate != null\u0026#34;\u0026gt; date_create, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;dateUpdate != null\u0026#34;\u0026gt; date_update, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;deleted != null\u0026#34;\u0026gt; deleted, \u0026lt;/if\u0026gt; \u0026lt;/trim\u0026gt; \u0026lt;trim prefix=\u0026#34;values (\u0026#34; suffix=\u0026#34;)\u0026#34; suffixOverrides=\u0026#34;,\u0026#34;\u0026gt; \u0026lt;if test=\u0026#34;id != null\u0026#34;\u0026gt; #{id,jdbcType=BIGINT}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;memberId != null\u0026#34;\u0026gt; #{memberId,jdbcType=BIGINT}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;realName != null\u0026#34;\u0026gt; #{realName,jdbcType=VARCHAR}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;nickname != null\u0026#34;\u0026gt; #{nickname,jdbcType=VARCHAR}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;dateCreate != null\u0026#34;\u0026gt; #{dateCreate,jdbcType=TIMESTAMP}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;dateUpdate != null\u0026#34;\u0026gt; #{dateUpdate,jdbcType=TIMESTAMP}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;deleted != null\u0026#34;\u0026gt; #{deleted,jdbcType=BIGINT}, \u0026lt;/if\u0026gt; \u0026lt;/trim\u0026gt; \u0026lt;/insert\u0026gt; \u0026lt;update id=\u0026#34;updateByPrimaryKeySelective\u0026#34; parameterType=\u0026#34;com.example.demo.entity.TTestUser\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; update t_test_user \u0026lt;set\u0026gt; \u0026lt;if test=\u0026#34;memberId != null\u0026#34;\u0026gt; member_id = #{memberId,jdbcType=BIGINT}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;realName != null\u0026#34;\u0026gt; real_name = #{realName,jdbcType=VARCHAR}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;nickname != null\u0026#34;\u0026gt; nickname = #{nickname,jdbcType=VARCHAR}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;dateCreate != null\u0026#34;\u0026gt; date_create = #{dateCreate,jdbcType=TIMESTAMP}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;dateUpdate != null\u0026#34;\u0026gt; date_update = #{dateUpdate,jdbcType=TIMESTAMP}, \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;deleted != null\u0026#34;\u0026gt; deleted = #{deleted,jdbcType=BIGINT}, \u0026lt;/if\u0026gt; \u0026lt;/set\u0026gt; where id = #{id,jdbcType=BIGINT} \u0026lt;/update\u0026gt; \u0026lt;update id=\u0026#34;updateByPrimaryKey\u0026#34; parameterType=\u0026#34;com.example.demo.entity.TTestUser\u0026#34;\u0026gt; \u0026lt;!-- WARNING - @mbggenerated This element is automatically generated by MyBatis Generator, do not modify. --\u0026gt; update t_test_user set member_id = #{memberId,jdbcType=BIGINT}, real_name = #{realName,jdbcType=VARCHAR}, nickname = #{nickname,jdbcType=VARCHAR}, date_create = #{dateCreate,jdbcType=TIMESTAMP}, date_update = #{dateUpdate,jdbcType=TIMESTAMP}, deleted = #{deleted,jdbcType=BIGINT} where id = #{id,jdbcType=BIGINT} \u0026lt;/update\u0026gt; \u0026lt;/mapper\u0026gt; 3.3.5 新增实体类 # package com.example.demo.entity; import java.io.Serializable; import java.util.Date; public class TTestUser implements Serializable { /** * This field was generated by MyBatis Generator. * This field corresponds to the database column t_test_user.id * * @mbggenerated */ private Long id; /** * This field was generated by MyBatis Generator. * This field corresponds to the database column t_test_user.member_id * * @mbggenerated */ private Long memberId; /** * This field was generated by MyBatis Generator. * This field corresponds to the database column t_test_user.real_name * * @mbggenerated */ private String realName; /** * This field was generated by MyBatis Generator. * This field corresponds to the database column t_test_user.nickname * * @mbggenerated */ private String nickname; /** * This field was generated by MyBatis Generator. * This field corresponds to the database column t_test_user.date_create * * @mbggenerated */ private Date dateCreate; /** * This field was generated by MyBatis Generator. * This field corresponds to the database column t_test_user.date_update * * @mbggenerated */ private Date dateUpdate; /** * This field was generated by MyBatis Generator. * This field corresponds to the database column t_test_user.deleted * * @mbggenerated */ private Long deleted; /** * This field was generated by MyBatis Generator. * This field corresponds to the database table t_test_user * * @mbggenerated */ private static final long serialVersionUID = 1L; /** * This method was generated by MyBatis Generator. * This method returns the value of the database column t_test_user.id * * @return the value of t_test_user.id * * @mbggenerated */ public Long getId() { return id; } /** * This method was generated by MyBatis Generator. * This method sets the value of the database column t_test_user.id * * @param id the value for t_test_user.id * * @mbggenerated */ public void setId(Long id) { this.id = id; } /** * This method was generated by MyBatis Generator. * This method returns the value of the database column t_test_user.member_id * * @return the value of t_test_user.member_id * * @mbggenerated */ public Long getMemberId() { return memberId; } /** * This method was generated by MyBatis Generator. * This method sets the value of the database column t_test_user.member_id * * @param memberId the value for t_test_user.member_id * * @mbggenerated */ public void setMemberId(Long memberId) { this.memberId = memberId; } /** * This method was generated by MyBatis Generator. * This method returns the value of the database column t_test_user.real_name * * @return the value of t_test_user.real_name * * @mbggenerated */ public String getRealName() { return realName; } /** * This method was generated by MyBatis Generator. * This method sets the value of the database column t_test_user.real_name * * @param realName the value for t_test_user.real_name * * @mbggenerated */ public void setRealName(String realName) { this.realName = realName == null ? null : realName.trim(); } /** * This method was generated by MyBatis Generator. * This method returns the value of the database column t_test_user.nickname * * @return the value of t_test_user.nickname * * @mbggenerated */ public String getNickname() { return nickname; } /** * This method was generated by MyBatis Generator. * This method sets the value of the database column t_test_user.nickname * * @param nickname the value for t_test_user.nickname * * @mbggenerated */ public void setNickname(String nickname) { this.nickname = nickname == null ? null : nickname.trim(); } /** * This method was generated by MyBatis Generator. * This method returns the value of the database column t_test_user.date_create * * @return the value of t_test_user.date_create * * @mbggenerated */ public Date getDateCreate() { return dateCreate; } /** * This method was generated by MyBatis Generator. * This method sets the value of the database column t_test_user.date_create * * @param dateCreate the value for t_test_user.date_create * * @mbggenerated */ public void setDateCreate(Date dateCreate) { this.dateCreate = dateCreate; } /** * This method was generated by MyBatis Generator. * This method returns the value of the database column t_test_user.date_update * * @return the value of t_test_user.date_update * * @mbggenerated */ public Date getDateUpdate() { return dateUpdate; } /** * This method was generated by MyBatis Generator. * This method sets the value of the database column t_test_user.date_update * * @param dateUpdate the value for t_test_user.date_update * * @mbggenerated */ public void setDateUpdate(Date dateUpdate) { this.dateUpdate = dateUpdate; } /** * This method was generated by MyBatis Generator. * This method returns the value of the database column t_test_user.deleted * * @return the value of t_test_user.deleted * * @mbggenerated */ public Long getDeleted() { return deleted; } /** * This method was generated by MyBatis Generator. * This method sets the value of the database column t_test_user.deleted * * @param deleted the value for t_test_user.deleted * * @mbggenerated */ public void setDeleted(Long deleted) { this.deleted = deleted; } /** * This method was generated by MyBatis Generator. * This method corresponds to the database table t_test_user * * @mbggenerated */ @Override public String toString() { StringBuilder sb = new StringBuilder(); sb.append(getClass().getSimpleName()); sb.append(\u0026#34; [\u0026#34;); sb.append(\u0026#34;Hash = \u0026#34;).append(hashCode()); sb.append(\u0026#34;, id=\u0026#34;).append(id); sb.append(\u0026#34;, memberId=\u0026#34;).append(memberId); sb.append(\u0026#34;, realName=\u0026#34;).append(realName); sb.append(\u0026#34;, nickname=\u0026#34;).append(nickname); sb.append(\u0026#34;, dateCreate=\u0026#34;).append(dateCreate); sb.append(\u0026#34;, dateUpdate=\u0026#34;).append(dateUpdate); sb.append(\u0026#34;, deleted=\u0026#34;).append(deleted); sb.append(\u0026#34;, serialVersionUID=\u0026#34;).append(serialVersionUID); sb.append(\u0026#34;]\u0026#34;); return sb.toString(); } } 3.3.6 执行查询 # @Slf4j public class MyBatisBootStrap { public static void main(String[] args) { try { // 1. 读取配置 InputStream inputStream = Resources.getResourceAsStream(\u0026#34;mybatis-config.xml\u0026#34;); // 2. 创建SqlSessionFactory工厂 SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); // 3. 获取sqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); // 4. 获取Mapper TTestUserMapper userMapper = sqlSession.getMapper(TTestUserMapper.class); // 5. 执行接口方法 TTestUser userInfo = userMapper.selectByPrimaryKey(16L); System.out.println(\u0026#34;userInfo = \u0026#34; + JSONUtil.toJsonStr(userInfo)); // 6. 提交事务 sqlSession.commit(); // 7. 关闭资源 sqlSession.close(); inputStream.close(); } catch (Exception e){ log.error(e.getMessage(), e); } } } 3.4 区别 # 发现没有在写MyBatis的时候，新增了dao, mapper.xml, entity, mybatis-config.xml等很多东西，工作量反而增大了。但是dao, mapper.xml, entity都是可以根据插件mybatis-generator生成的，我们也不用一一去创建，而且我们没有涉及到原生JDBC中加载驱动，创建连接，处理结果集，关闭连接等等这些操作，这些都是MyBatis帮我们做了，我们只用关心提供的查询接口和sql编写即可。\n如果使用原生的JDBC进行数据库操作，我们需要关心如何加载驱动，如何获取连接关闭连接，如何获取结果集等等与业务无关的地方，而MyBatis通过**“映射”这个核心概念将sql和java接口关联起来，我们调用java接口就相当于可以直接执行sql**，并且将结果映射为java pojo对象，这也是我们开头说的**“映射”，“面向对象的”**的原因了。\n4. 总结 # 这篇文章简单的介绍了下MyBatis的基本概念，并提供了简单的栗子，接下来几篇文章打算写下Mybatis的启动流程，让我们更好的了解下mybatis的各模块协作。\n"},{"id":102,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/MyBatis/MyBatis-interview/","title":"Mybatis面试","section":"MyBatis","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n部分疑问参考自 https://blog.csdn.net/Gherbirthday0916 感谢作者!\n#{} 和 ${} 的区别是什么？ # 注：这道题是面试官面试我同事的。\n答：\n${}是 Properties 文件中的变量占位符，它可以用于标签属性值和 sql 内部，属于静态文本替换，比如${driver}会被静态替换为com.mysql.jdbc. Driver。 #{}是 sql 的参数占位符，MyBatis 会将 sql 中的#{}替换为? 号，在 sql 执行前会使用 PreparedStatement 的参数设置方法，按序给 sql 的? 号占位符设置参数值，比如 ps.setInt(0, parameterValue)，#{item.name} 的取值方式为使用反射从参数对象中获取 item 对象的 name 属性值，相当于 param.getItem().getName()。 [这里用到了反射] 在底层构造完整SQL语句时，MyBatis的两种传参方式所采取的方式不同。#{Parameter}采用预编译的方式构造SQL，避免了 SQL注入 的产生。而**${Parameter}采用拼接的方式构造SQL，在对用户输入过滤不严格**的前提下，此处很可能存在SQL注入\nxml 映射文件中，除了常见的 select、insert、update、delete 标签之外，还有哪些标签？ # 注：这道题是京东面试官面试我时问的。\n答：还有很多其他的标签， \u0026lt;resultMap\u0026gt; 、 \u0026lt;parameterMap\u0026gt; 、 \u0026lt;sql\u0026gt; 、 \u0026lt;include\u0026gt; 、 \u0026lt;selectKey\u0026gt; ，加上动态 sql 的 9 个标签， trim|where|set|foreach|if|choose|when|otherwise|bind 等，其中 \u0026lt;sql\u0026gt; 为 sql 片段标签，通过 \u0026lt;include\u0026gt; 标签引入 sql 片段， \u0026lt;selectKey\u0026gt; 为不支持自增的主键生成策略标签。\nset标签，是update用的\n\u0026lt;update id=\u0026#34;updateUserById\u0026#34; parameterType=\u0026#34;user\u0026#34;\u0026gt; update user \u0026lt;set\u0026gt; \u0026lt;if test=\u0026#34;uid!=null\u0026#34;\u0026gt; uid=#{uid} \u0026lt;/if\u0026gt; \u0026lt;/set\u0026gt; \u0026lt;/update\u0026gt; ResultMap（结果集映射）：\n假设我们的数据库字段和映射pojo类的属性字段不一致，那么查询结果，不一致的字段值会为null\n这时可以使用Mybatis ResultMap 结果集映射\nparameterMap（参数类型映射）：\n很少使用，基本都用parameterType替代\nparameterMap标签可以用来定义参数组，可以为参数组指定ID、参数类型\n例如有一个bean是这样的：\npublic class ArgBean { private String name; private int age; // 忽略 getter 和 setter } 下面使用 \u0026lt;parameterMap\u0026gt; 将参数 ArgBean 对象进行映射\n\u0026lt;parameterMap id=\u0026#34;PARAM_MAP\u0026#34; type=\u0026#34;com.hxstrive.mybatis.parameter.demo2.ArgBean\u0026#34;\u0026gt; \u0026lt;parameter property=\u0026#34;age\u0026#34; javaType=\u0026#34;integer\u0026#34; /\u0026gt; \u0026lt;parameter property=\u0026#34;name\u0026#34; javaType=\u0026#34;String\u0026#34; /\u0026gt; \u0026lt;/parameterMap\u0026gt; sql（sql片段标签）/include（片段插入标签）： 重复的SQL预计永远不可避免，\u0026lt;sql\u0026gt;标签就是用来解决这个问题的\n其中 \u0026lt;sql\u0026gt; 为 sql 片段标签，通过 \u0026lt;include\u0026gt; 标签引入 sql 片段\n例如：\n\u0026lt;mapper namespace=\u0026#34;com.klza.dao.UserMapper\u0026#34;\u0026gt; \u0026lt;sql id=\u0026#34;sqlUserParameter\u0026#34;\u0026gt;id,username,password\u0026lt;/sql\u0026gt; \u0026lt;select id=\u0026#34;getUserList\u0026#34; resultType=\u0026#34;user\u0026#34;\u0026gt; select \u0026lt;include refid=\u0026#34;sqlUserParameter\u0026#34;/\u0026gt; from test.user \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; \u0026lt;selectKey\u0026gt; 为不支持自增的主键生成策略标签\n\u0026lt;insert id=\u0026#34;insert\u0026#34; parameterType=\u0026#34;com.pinyougou.pojo.TbGoods\u0026#34; \u0026gt; \u0026lt;selectKey resultType=\u0026#34;java.lang.Long\u0026#34; order=\u0026#34;AFTER\u0026#34; keyProperty=\u0026#34;id\u0026#34;\u0026gt; SELECT LAST_INSERT_ID() AS id \u0026lt;/selectKey\u0026gt; insert into tb_goods (id, seller_id ) values (#{id,jdbcType=BIGINT}, #{sellerId,jdbcType=VARCHAR} \u0026lt;/insert\u0026gt; Dao 接口的工作原理是什么？Dao 接口里的方法，参数不同时，方法能重载吗？ # 注：这道题也是京东面试官面试我被问的。\n答：最佳实践中，通常一个 xml 映射文件，都会写一个 Dao 接口与之对应。Dao 接口就是人们常说的 Mapper 接口，接口的全限名，就是映射文件中的 namespace 的值，接口的方法名，就是映射文件中 MappedStatement 的 id 值，接口方法内的参数，就是传递给 sql 的参数。 Mapper 接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为 key 值，可唯一定位一个 MappedStatement ，举例： com.mybatis3.mappers. StudentDao.findStudentById ，可以唯一找到 namespace 为 com.mybatis3.mappers. StudentDao 下面 id = findStudentById 的 MappedStatement 。在 MyBatis 中，每一个 \u0026lt;select\u0026gt; 、 \u0026lt;insert\u0026gt; 、 \u0026lt;update\u0026gt; 、 \u0026lt;delete\u0026gt; 标签，都会被解析为一个 MappedStatement 对象。\nDao 接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。\nDao 接口里的方法可以重载，但是 Mybatis 的 xml 里面的 ID 不允许重复。\nMybatis 版本 3.3.0，亲测如下：\n/** * Mapper接口里面方法重载 */ public interface StuMapper { List\u0026lt;Student\u0026gt; getAllStu(); List\u0026lt;Student\u0026gt; getAllStu(@Param(\u0026#34;id\u0026#34;) Integer id); } 然后在 StuMapper.xml 中利用 Mybatis 的动态 sql 就可以实现。\n\u0026lt;select id=\u0026#34;getAllStu\u0026#34; resultType=\u0026#34;com.pojo.Student\u0026#34;\u0026gt; select * from student \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;id != null\u0026#34;\u0026gt; id = #{id} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; \u0026lt;/select\u0026gt; 能正常运行，并能得到相应的结果，这样就实现了在 Dao 接口中写重载方法。\nMybatis 的 Dao 接口可以有多个重载方法，但是多个接口对应的映射必须只有一个，否则启动会报错。\n相关 issue ：更正：Dao 接口里的方法可以重载，但是 Mybatis 的 xml 里面的 ID 不允许重复！。\nDao 接口的工作原理是 JDK 动态代理，MyBatis 运行时会使用 JDK 动态代理为 Dao 接口生成代理 proxy 对象，代理对象 proxy 会拦截接口方法，转而执行 MappedStatement 所代表的 sql，然后将 sql 执行结果返回。\n补充 ：\nDao 接口方法可以重载，但是需要满足以下条件：\n仅有一个无参方法和一个有参方法 (多个参数)的方法中，参数数量必须(和xml中的)一致。且使用相同的 @Param ，或者使用 param1 这种 测试如下 ：\nPersonDao.java Person queryById(); Person queryById(@Param(\u0026#34;id\u0026#34;) Long id); Person queryById(@Param(\u0026#34;id\u0026#34;) Long id, @Param(\u0026#34;name\u0026#34;) String name); PersonMapper.xml \u0026lt;select id=\u0026#34;queryById\u0026#34; resultMap=\u0026#34;PersonMap\u0026#34;\u0026gt; select id, name, age, address from person \u0026lt;where\u0026gt; \u0026lt;if test=\u0026#34;id != null\u0026#34;\u0026gt; id = #{id} \u0026lt;/if\u0026gt; \u0026lt;if test=\u0026#34;name != null and name != \u0026#39;\u0026#39;\u0026#34;\u0026gt; and name = #{name} \u0026lt;/if\u0026gt; \u0026lt;/where\u0026gt; limit 1 \u0026lt;/select\u0026gt; org.apache.ibatis.scripting.xmltags. DynamicContext. ContextAccessor#getProperty 方法用于获取 \u0026lt;if\u0026gt; 标签中的条件值\nContextAccessor 这个修饰符为默认（同一个包内）\npublic Object getProperty(Map context, Object target, Object name) { Map map = (Map) target; Object result = map.get(name); if (map.containsKey(name) || result != null) { return result; } Object parameterObject = map.get(PARAMETER_OBJECT_KEY); if (parameterObject instanceof Map) { return ((Map)parameterObject).get(name); } return null; } parameterObject 为 map，存放的是 Dao 接口中参数相关信息。\n((Map)parameterObject).get(name) 方法如下\npublic V get(Object key) { if (!super.containsKey(key)) { throw new BindingException(\u0026#34;Parameter \u0026#39;\u0026#34; + key + \u0026#34;\u0026#39; not found. Available parameters are \u0026#34; + keySet()); } return super.get(key); } queryById()方法执行时，parameterObject为 null，getProperty方法返回 null 值，\u0026lt;if\u0026gt;标签获取的所有条件值都为 null，所有条件不成立，动态 sql 可以正常执行。 queryById(1L)方法执行时，parameterObject为 map，包含了**id和param1**两个 key 值。当获取\u0026lt;if\u0026gt;标签中name的属性值时，进入((Map)parameterObject).get(name)方法中，map 中 key 不包含name，所以抛出异常。 queryById(1L,\u0026quot;1\u0026quot;)方法执行时，parameterObject中包含id,param1,name,param2四个 key 值，id和name属性都可以获取到，动态 sql 正常执行。 也就是说，if的test一定是会进行判断的(除非整个parameterObject为null)。但是如果这里面的param 不存在，那么就会抛异常 (BindingException)\nMyBatis 是如何进行分页的？分页插件的原理是什么？ # 注：我出的。\n答：(1) MyBatis 使用 RowBounds 对象进行分页，它是针对 ResultSet 结果集执行的内存分页，而非物理分页；\n//使用 //Mapper中 List\u0026lt;User\u0026gt; getUserListLimit(RowBounds rowBounds); //Mapper.xml定义 （不变） \u0026lt;select id=\u0026#34;getUserListLimit\u0026#34; resultType=\u0026#34;user\u0026#34;\u0026gt; select * from test.user \u0026lt;/select\u0026gt; //使用, 将会从index为0的记录开始，取两条记录 List\u0026lt;User\u0026gt; userListLimit = userMapper.getUserListLimit(new RowBounds(0, 2)); for (User user : userListLimit) { System.out.println(user); } 想要使用mybatis日志，只要加上日志模块的依赖即可\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/ch.qos.logback/logback-classic --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;ch.qos.logback\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;logback-classic\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 查看上面的日志可以发现，实际查找的是全部的数据(没有使用物理分页)\n14:28:14.938 [main] DEBUG org.mybatis.example.BlogMapper.selectBlog - ==\u0026gt; Preparing: select * from Blog 14:28:14.996 [main] DEBUG org.mybatis.example.BlogMapper.selectBlog - ==\u0026gt; Parameters: [Blog{id=2, name=\u0026#39;n2\u0026#39;, age=20}, Blog{id=3, name=\u0026#39;n3\u0026#39;, age=30}] (2) 可以在 sql 内直接书写带有物理分页的参数来完成物理分页功能\n(3) 也可以使用分页插件来完成物理分页\n分页插件的基本原理是使用 MyBatis 提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的 sql，然后重写 sql，根据 dialect 方言，添加对应的物理分页语句和物理分页参数。\n举例： select _ from student ，拦截 sql 后重写为： select t._ from （select \\* from student）t limit 0，10\n分页插件的使用\n接下来介绍PageHelper插件的使用：\n第一步，引入依赖：\n\u0026lt;!-- https://mvnrepository.com/artifact/com.github.pagehelper/pagehelper --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.github.pagehelper\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;pagehelper\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 第二步，mybatis-config.xml配置拦截器：\n\u0026lt;!-- 配置pageHelper拦截器 --\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin interceptor=\u0026#34;com.github.pagehelper.PageInterceptor\u0026#34;/\u0026gt; \u0026lt;/plugins\u0026gt; 第三步，代码编写：\nMapper接口类：\nList\u0026lt;User\u0026gt; getUserListByPageHelper(); Mapper.xml：\n\u0026lt;select id=\u0026#34;getUserListByPageHelper\u0026#34; resultType=\u0026#34;user\u0026#34;\u0026gt; select * from test.user \u0026lt;/select\u0026gt; 测试程序：\n// 开启分页功能 int pageNum = 1; // 当前页码 int pageSize = 2; // 每页的记录数 PageHelper.startPage(pageNum, pageSize); List\u0026lt;User\u0026gt; userListByPageHelper = userMapper.getUserListByPageHelper(); userListByPageHelper.forEach(System.out::println); 第四步：获取pageInfo信息：\npageHelper真正强大的地方在于它的pageInfo功能，它可以为我们提供详细的分页数据：\n例如：\n// 开启分页功能 int pageNum = 2; // 当前页码 int pageSize = 5; // 每页的记录数 PageHelper.startPage(pageNum, pageSize); List\u0026lt;User\u0026gt; userListByPageHelper = userMapper.getUserListByPageHelper(); // 设置导航的卡片数为3 PageInfo\u0026lt;User\u0026gt; userPageInfo = new PageInfo\u0026lt;\u0026gt;(userListByPageHelper, 3); System.out.println(userPageInfo); /* * PageInfo{pageNum=2, pageSize=5, size=5, startRow=6, endRow=10, total=1004, pages=201, * list=Page{count=true, pageNum=2, pageSize=5, startRow=5, endRow=10, total=1004, pages=201, reasonable=false, pageSizeZero=false} * [User(id=6, username=Cheng Zhennan, password=Jx3SLGXeS4), User(id=7, username=Thelma Hernandez, password=VxVO6dEgym), User(id=8, username=Emma Wood, password=XljUnUrnFZ), User(id=9, username=Kikuchi Akina, password=IgditeatR7), User(id=10, username=Miura Kenta, password=2CbmTGczZv)], * prePage=1, nextPage=3, isFirstPage=false, isLastPage=false, hasPreviousPage=true, hasNextPage=true, navigatePages=3, navigateFirstPage=1, navigateLastPage=3, navigatepageNums=[1, 2, 3]} 简述 MyBatis 的插件运行原理，以及如何编写一个插件。 # 注：我出的。\n答：MyBatis 仅可以编写针对 ParameterHandler 、 ResultSetHandler 、 StatementHandler 、 Executor 这 4 种接口的插件，MyBatis 使用 JDK 的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这 4 种接口对象的方法时，就会进入拦截方法，具体就是 InvocationHandler 的 invoke() 方法，当然，只会拦截那些你指定需要拦截的方法。\n实现 MyBatis 的 Interceptor 接口并复写 intercept() 方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。\nMyBatis 执行批量插入，能返回数据库主键列表吗？ # 注：我出的。\n答：能，JDBC 都能，MyBatis 当然也能。\nMyBatis 动态 sql 是做什么的？都有哪些动态 sql？能简述一下动态 sql 的执行原理不？ # 注：我出的。\n答：MyBatis 动态 sql 可以让我们在 xml 映射文件内，以标签的形式编写动态 sql，完成逻辑判断和动态拼接 sql 的功能。其执行原理为，使用 OGNL 从 sql 参数对象中计算表达式的值，根据表达式的值动态拼接 sql，以此来完成动态 sql 的功能。\nMyBatis 提供了 9 种动态 sql 标签:\n\u0026lt;if\u0026gt;\u0026lt;/if\u0026gt; \u0026lt;where\u0026gt;\u0026lt;/where\u0026gt;(trim,set) \u0026lt;choose\u0026gt;\u0026lt;/choose\u0026gt;（when, otherwise） \u0026lt;foreach\u0026gt;\u0026lt;/foreach\u0026gt; \u0026lt;bind/\u0026gt; 关于 MyBatis 动态 SQL 的详细介绍，请看这篇文章：Mybatis 系列全解（八）：Mybatis 的 9 大动态 SQL 标签你知道几个？ 。\n关于这些动态 SQL 的具体使用方法，请看这篇文章：Mybatis【13】\u0026ndash; Mybatis 动态 sql 标签怎么使用？\nMyBatis 是如何将 sql 执行结果封装为目标对象并返回的？都有哪些映射形式？ # 注：我出的。\n答：第一种是使用 \u0026lt;resultMap\u0026gt; 标签，逐一定义列名和对象属性名之间的映射关系。第二种是使用 sql 列的别名功能，将列别名书写为对象属性名，比如 T_NAME AS NAME，对象属性名一般是 name，小写，但是列名不区分大小写，MyBatis 会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成 T_NAME AS NaMe，MyBatis 一样可以正常工作。\n有了列名与属性名的映射关系后，MyBatis 通过反射创建对象，同时使用反射 给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。\nMyBatis 能执行一对一、一对多的关联查询吗？都有哪些实现方式，以及它们之间的区别。【实际没有用过】 # 注：我出的。\n答：能，MyBatis 不仅可以执行一对一、一对多的关联查询，还可以执行多对一，多对多的关联查询，多对一查询，其实就是一对一查询，只需要把 selectOne() 修改为 selectList() 即可；多对多查询，其实就是一对多查询，只需要把 selectOne() 修改为 selectList() 即可。\n关联对象查询，有两种实现方式，一种是单独发送一个 sql 去查询关联对象，赋给主对象，然后返回主对象。另一种是使用嵌套查询，嵌套查询的含义为使用 join 查询，一部分列是 A 对象的属性值，另外一部分列是关联对象 B 的属性值，好处是只发一个 sql 查询，就可以把主对象和其关联对象查出来。\n那么问题来了，join 查询出来 100 条记录，如何确定主对象是 5 个，而不是 100 个？其去重复的原理是 \u0026lt;resultMap\u0026gt; 标签内的 \u0026lt;id\u0026gt; 子标签，指定了唯一确定一条记录的 id 列，MyBatis 根据 \u0026lt;id\u0026gt; 列值来完成 100 条记录的去重复功能， \u0026lt;id\u0026gt; 可以有多个，代表了联合主键的语意。\n同样主对象的关联对象，也是根据这个原理去重复的，尽管一般情况下，只有主对象会有重复记录，关联对象一般不会重复。\n举例：下面 join 查询出来 6 条记录，一、二列是 Teacher 对象列，第三列为 Student 对象列，MyBatis 去重复处理后，结果为 1 个老师 6 个学生，而不是 6 个老师 6 个学生。\nt_id t_name s_id 1 teacher 38 1 teacher 39 1 teacher 40 1 teacher 41 1 teacher 42 1 teacher 43 MyBatis 是否支持延迟加载？如果支持，它的实现原理是什么？ # 注：我出的。\n答：MyBatis 仅支持 association 关联对象和 collection 关联集合对象的延迟加载，association 指的就是一对一，collection 指的就是一对多查询。在 MyBatis 配置文件中，可以配置是否启用延迟加载 lazyLoadingEnabled=true|false。\n它的原理是，使用 CGLIB 创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用 a.getB().getName() ，拦截器 invoke() 方法发现 a.getB() 是 null 值，那么就会单独发送事先保存好的查询关联 B 对象的 sql，把 B 查询上来，然后调用 a.setB(b)，于是 a 的对象 b 属性就有值了，接着完成 a.getB().getName() 方法的调用。这就是延迟加载的基本原理。\n当然了，不光是 MyBatis，几乎所有的包括 Hibernate，支持延迟加载的原理都是一样的。\nMyBatis 的 xml 映射文件中，不同的 xml 映射文件，id 是否可以重复？ # 注：我出的。\n答：不同的 xml 映射文件，如果配置了 namespace，那么 id 可以重复；如果没有配置 namespace，那么 id 不能重复；毕竟 namespace 不是必须的，只是最佳实践而已。\n原因就是 namespace+id 是作为 Map\u0026lt;String, MappedStatement\u0026gt; 的 key 使用的，如果没有 namespace，就剩下 id，那么，id 重复会导致数据互相覆盖。有了 namespace，自然 id 就可以重复，namespace 不同，namespace+id 自然也就不同。\nMyBatis 中如何执行批处理？ # 注：我出的。\n答：使用 BatchExecutor 完成批处理。\nMyBatis 都有哪些 Executor 执行器？它们之间的区别是什么？ # 注：我出的\n答：MyBatis 有三种基本的 Executor 执行器：\nSimpleExecutor： 每执行一次 update 或 select，就开启一个 Statement 对象，用完立刻关闭 Statement 对象。 ReuseExecutor： 执行 update 或 select，以 sql 作为 key 查找 Statement 对象，存在就使用，不存在就创建，用完后，不关闭 Statement 对象，而是放置于 Map\u0026lt;String, Statement\u0026gt;内，供下一次使用。简言之，就是重复使用 Statement 对象。 BatchExecutor ：执行 update（没有 select，JDBC 批处理不支持 select），将所有 sql 都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个 Statement 对象，每个 Statement 对象都是 addBatch()完毕后，等待逐一执行 executeBatch()批处理。与 JDBC 批处理相同。 作用范围：Executor 的这些特点，都严格限制在 SqlSession 生命周期范围内。\nMyBatis 中如何指定使用哪一种 Executor 执行器？ # 注：我出的\n答：在 MyBatis 配置文件中，可以指定默认的 ExecutorType 执行器类型，也可以手动给 DefaultSqlSessionFactory 的创建 SqlSession 的方法传递 ExecutorType 类型参数。\nMyBatis 是否可以映射 Enum 枚举类？ # 注：我出的\n答：MyBatis 可以映射枚举类，不单可以映射枚举类，MyBatis 可以映射任何对象到表的一列上。映射方式为自定义一个 TypeHandler ，实现 TypeHandler 的 setParameter() 和 getResult() 接口方法。 TypeHandler 有两个作用：\n一是完成从 javaType 至 jdbcType 的转换； 二是完成 jdbcType 至 javaType 的转换，体现为 setParameter() 和 getResult() 两个方法，分别代表设置 sql 问号占位符参数和获取列查询结果。 MyBatis 映射文件中，如果 A 标签通过 include 引用了 B 标签的内容，请问，B 标签能否定义在 A 标签的后面，还是说必须定义在 A 标签的前面？ # 注：我出的\n答：虽然 MyBatis 解析 xml 映射文件是按照顺序解析的，但是，被引用的 B 标签依然可以定义在任何地方，MyBatis 都可以正确识别。\n原理是，MyBatis 解析 A 标签，发现 A 标签引用了 B 标签，但是 B 标签尚未解析到，尚不存在，此时，MyBatis 会将 A 标签标记为未解析状态，然后继续解析余下的标签，包含 B 标签，待所有标签解析完毕，MyBatis 会重新解析那些被标记为未解析的标签，此时再解析 A 标签时，B 标签已经存在，A 标签也就可以正常解析完成了。\n简述 MyBatis 的 xml 映射文件和 MyBatis 内部数据结构之间的映射关系？[不懂] # 注：我出的\n答：MyBatis 将所有 xml 配置信息都封装到 All-In-One 重量级对象 Configuration 内部。在 xml 映射文件中， \u0026lt;parameterMap\u0026gt; 标签会被解析为 ParameterMap 对象，其每个子元素会被解析为 ParameterMapping 对象。 \u0026lt;resultMap\u0026gt; 标签会被解析为 ResultMap 对象，其每个子元素会被解析为 ResultMapping 对象。每一个 \u0026lt;select\u0026gt;、\u0026lt;insert\u0026gt;、\u0026lt;update\u0026gt;、\u0026lt;delete\u0026gt; 标签均会被解析为 MappedStatement 对象，标签内的 sql 会被解析为 BoundSql 对象。\n为什么说 MyBatis 是半自动 ORM 映射工具？它与全自动的区别在哪里？ # 注：我出的\n答：Hibernate 属于全自动 ORM 映射工具，使用 Hibernate 查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。而 MyBatis 在查询关联对象或关联集合对象时，需要手动编写 sql 来完成，所以，称之为半自动 ORM 映射工具。\n面试题看似都很简单，但是想要能正确回答上来，必定是研究过源码且深入的人，而不是仅会使用的人或者用的很熟的人，以上所有面试题及其答案所涉及的内容，在我的 MyBatis 系列博客中都有详细讲解和原理分析。\n"},{"id":103,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/conditional_on_class/","title":"ConditionalOnClass实践","section":"框架","content":" 两个测试方向 # 方向1：两个maven项目 # 详见git上的 conditional_on_class_main 项目以及 conditional_on_class2 项目\n基础maven项目 conditional_on_class2\npom文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/project\u0026gt; java类\npackage com; public class LyReferenceImpl { public String sayWord() { return \u0026#34;hello one\u0026#34;; } } 简单的SpringBoot项目 conditional_on_class_main\n\u0026lt;!--pom文件--\u0026gt; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_main\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.8\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--把1配置的bean引用进来--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;!-- 默认会将conditional_on_class_2 打包进去,现在会配置SayExist 如果放开注释,那么会配置SayNotExist--\u0026gt; \u0026lt;!--\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_2\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt;--\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;jvmArguments\u0026gt;-Dfile.encoding=UTF-8\u0026lt;/jvmArguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;repackage\u0026lt;/goal\u0026gt;\u0026lt;!--可以把依赖的包都打包到生成的Jar包中 --\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; //两个配置类 //配置类1 package com.config; import com.service.ISay; import com.service.SayExist; import org.springframework.boot.autoconfigure.condition.ConditionalOnClass; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration //不要放在方法里面,否则会报错\u0026#34;java.lang.ArrayStoreException: sun.reflect.annotation.TypeNotPresentExceptionProxy\u0026#34; @ConditionalOnClass(value = com.LyReferenceImpl.class) public class ExistConfiguration { @Bean public ISay getISay1(){ return new SayExist(); } } //配置类2 package com.config; import com.service.ISay; import com.service.SayNotExist; import org.springframework.boot.autoconfigure.condition.ConditionalOnMissingClass; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; @Configuration @ConditionalOnMissingClass(\u0026#34;com.LyReferenceImpl\u0026#34;) public class NotExistConfiguration { @Bean public ISay getISay1(){ return new SayNotExist(); } } 方向2：3个maven项目(建议用这个理解) # 注意，这里可能还漏了一个问题，那就是 这个conditional_on_class1 的configuration之所以能够被自动装配，是因为和 conditional_on_class_main1的Application类是同一个包，所以不用特殊处理。如果是其他包名的话，那么是需要用到spring boot的自动装配机制的：在conditional_on_class1 工程的 resources 包下创建META-INF/spring.factories，并写上Config类的全类名\n详见 git上的 conditional_on_class_main1, conditional_on_class1 项目以及 conditional_on_class2 项目\n基础 conditional_on_class2\npom文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/project\u0026gt; java类\npackage com; public class LyReferenceImpl { public String sayWord() { return \u0026#34;hello one\u0026#34;; } } 以LyReferenceImpl.class存不存在，决定创建哪个bean\nconditional_on_class_1 pom.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_1\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--引入被引用的类，只在编译期存在--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;optional\u0026gt;true\u0026lt;/optional\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.springframework.boot/spring-boot-autoconfigure --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-autoconfigure\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.8\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; //根据是否存在class_2中的类，进行自动装配 @Configuration //不要放在方法里面,否则会报错\u0026#34;java.lang.ArrayStoreException: sun.reflect.annotation.TypeNotPresentExceptionProxy\u0026#34; @ConditionalOnClass(value = com.LyReferenceImpl.class) public class ExistConfiguration { @Bean public LyEntity lyEntity1(){ return new LyEntity(\u0026#34;存在\u0026#34;); } } @Configuration @ConditionalOnMissingClass(\u0026#34;com.LyReferenceImpl\u0026#34;) public class NotExistConfiguration { @Bean public LyEntity lyEntity1(){ return new LyEntity(\u0026#34;不存在\u0026#34;); } } //基础类 public class LyEntity { private String name; private Integer age; public LyEntity(String name) { this.name = name; } public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; } } 使用 在_main项目中 pom.xml\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.8\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--把1配置的bean引用进来--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_1\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--加上这个则会提示存在--\u0026gt; \u0026lt;!-- \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.example\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;conditional_on_class_2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 如果不存在class_2中的类，则提示不存在；如果存在则提示存在\n@SpringBootApplication @RestController public class MyApplication { @Autowired private LyEntity lyEntity; @RequestMapping(\u0026#34;hello\u0026#34;) public String hello(){ return lyEntity.getName(); } public static void main(String[] args) { SpringApplication.run(MyApplication.class,args); } } "},{"id":104,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/ly05ly_springboot-auto-assembly/","title":"SpringBoot自动装配原理","section":"框架","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n每次问到 Spring Boot， 面试官非常喜欢问这个问题：“讲述一下 SpringBoot 自动装配原理？”。\n我觉得我们可以从以下几个方面回答：\n什么是 SpringBoot 自动装配？ SpringBoot 是如何实现自动装配的？如何实现按需加载？ 如何实现一个 Starter？ 篇幅问题，这篇文章并没有深入，小伙伴们也可以直接使用 debug 的方式去看看 SpringBoot 自动装配部分的源代码。\n前言 # 使用过 Spring 的小伙伴，一定有被 XML 配置统治的恐惧。即使 Spring 后面引入了基于注解的配置，我们在开启某些 Spring 特性或者引入第三方依赖的时候，还是需要用 XML 或 Java 进行显式配置。\n举个例子。没有 Spring Boot 的时候，我们写一个 RestFul Web 服务，还首先需要进行如下配置。\n@Configuration public class RESTConfiguration { @Bean public View jsonTemplate() { MappingJackson2JsonView view = new MappingJackson2JsonView(); view.setPrettyPrint(true); return view; } @Bean public ViewResolver viewResolver() { return new BeanNameViewResolver(); } } spring-servlet.xml \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:context=\u0026#34;http://www.springframework.org/schema/context\u0026#34; xmlns:mvc=\u0026#34;http://www.springframework.org/schema/mvc\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context/ http://www.springframework.org/schema/context/spring-context.xsd http://www.springframework.org/schema/mvc/ http://www.springframework.org/schema/mvc/spring-mvc.xsd\u0026#34;\u0026gt; \u0026lt;context:component-scan base-package=\u0026#34;com.howtodoinjava.demo\u0026#34; /\u0026gt; \u0026lt;mvc:annotation-driven /\u0026gt; \u0026lt;!-- JSON Support --\u0026gt; \u0026lt;bean name=\u0026#34;viewResolver\u0026#34; class=\u0026#34;org.springframework.web.servlet.view.BeanNameViewResolver\u0026#34;/\u0026gt; \u0026lt;bean name=\u0026#34;jsonTemplate\u0026#34; class=\u0026#34;org.springframework.web.servlet.view.json.MappingJackson2JsonView\u0026#34;/\u0026gt; \u0026lt;/beans\u0026gt; 但是，Spring Boot 项目，我们只需要添加相关依赖，无需配置，通过启动下面的 main 方法即可。\n@SpringBootApplication public class DemoApplication { public static void main(String[] args) { SpringApplication.run(DemoApplication.class, args); } } 并且，我们通过 Spring Boot 的全局配置文件 application.properties或application.yml即可对项目进行设置比如更换端口号，配置 JPA 属性等等。\n为什么 Spring Boot 使用起来这么酸爽呢？ 这得益于其自动装配。自动装配可以说是 Spring Boot 的核心，那究竟什么是自动装配呢？\n什么是 SpringBoot 自动装配？ # 我们现在提到自动装配的时候，一般会和 Spring Boot 联系在一起。但是，实际上 Spring Framework 早就实现了这个功能。Spring Boot 只是在其基础上，通过 SPI 的方式，做了进一步优化。\nSpringBoot 定义了一套接口规范，这套规范规定：SpringBoot 在启动时会扫描外部引用 jar 包中的META-INF/spring.factories文件，将文件中配置的类型信息加载到 Spring 容器（此处涉及到 JVM 类加载机制与 Spring 的容器知识），并执行类中定义的各种操作。对于外部 jar 来说，只需要按照 SpringBoot 定义的标准，就能将自己的功能装置进 SpringBoot。\n没有 Spring Boot 的情况下，如果我们需要引入第三方依赖，需要手动配置，非常麻烦。但是，Spring Boot 中，我们直接引入一个 starter 即可。比如你想要在项目中使用 redis 的话，直接在项目中引入对应的 starter 即可。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-redis\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 引入 starter 之后，我们通过少量注解和一些简单的配置就能使用第三方组件提供的功能了。\n在我看来，自动装配可以简单理解为：通过注解或者一些简单的配置就能在 Spring Boot 的帮助下实现某块功能。\nSpringBoot 是如何实现自动装配的？ # 我们先看一下 SpringBoot 的核心注解 SpringBootApplication 。\n@Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited \u0026lt;1.\u0026gt;@SpringBootConfiguration \u0026lt;2.\u0026gt;@ComponentScan \u0026lt;3.\u0026gt;@EnableAutoConfiguration public @interface SpringBootApplication { } @Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Configuration //实际上它也是一个配置类 public @interface SpringBootConfiguration { } 大概可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。根据 SpringBoot 官网，这三个注解的作用分别是：\n@EnableAutoConfiguration：启用 SpringBoot 的自动配置机制\n@Configuration：允许在上下文中注册额外的 bean 或导入其他配置类\n@ComponentScan： 扫描被@Component (@Service,@Controller)注解的 bean，注解默认会扫描启动类所在的包下所有的类 ，可以自定义不扫描某些 bean。如下图所示，容器中将排除TypeExcludeFilter和AutoConfigurationExcludeFilter。\n@EnableAutoConfiguration 是实现自动装配的重要注解，我们以这个注解入手。\n@EnableAutoConfiguration:实现自动装配的核心注解 # EnableAutoConfiguration 只是一个简单地注解，自动装配核心功能的实现实际是通过 **AutoConfigurationImportSelector**类。\n@Target({ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @AutoConfigurationPackage //作用：将main包下的所有组件注册到容器中 @Import({AutoConfigurationImportSelector.class}) //加载自动装配类 xxxAutoconfiguration public @interface EnableAutoConfiguration { String ENABLED_OVERRIDE_PROPERTY = \u0026#34;spring.boot.enableautoconfiguration\u0026#34;; Class\u0026lt;?\u0026gt;[] exclude() default {}; String[] excludeName() default {}; } 我们现在重点分析下**AutoConfigurationImportSelector** 类到底做了什么？\nAutoConfigurationImportSelector:加载自动装配类 # AutoConfigurationImportSelector类的继承体系如下：\npublic class AutoConfigurationImportSelector implements DeferredImportSelector, BeanClassLoaderAware, ResourceLoaderAware, BeanFactoryAware, EnvironmentAware, Ordered { } public interface DeferredImportSelector extends ImportSelector { } public interface ImportSelector { String[] selectImports(AnnotationMetadata var1); } 可以看出，AutoConfigurationImportSelector 类实现了 ImportSelector接口，也就实现了这个接口中的 selectImports方法，该方法主要用于获取所有符合条件的类的全限定类名，这些类需要被加载到 IoC 容器中。\nprivate static final String[] NO_IMPORTS = new String[0]; public String[] selectImports(AnnotationMetadata annotationMetadata) { // \u0026lt;1\u0026gt;.判断自动装配开关是否打开 if (!this.isEnabled(annotationMetadata)) { return NO_IMPORTS; } else { //\u0026lt;2\u0026gt;.获取所有需要装配的bean AutoConfigurationMetadata autoConfigurationMetadata = AutoConfigurationMetadataLoader.loadMetadata(this.beanClassLoader); AutoConfigurationImportSelector.AutoConfigurationEntry autoConfigurationEntry = this.getAutoConfigurationEntry(autoConfigurationMetadata, annotationMetadata); return StringUtils.toStringArray(autoConfigurationEntry.getConfigurations()); } } 这里我们需要重点关注一下getAutoConfigurationEntry()方法，这个方法主要负责加载自动配置类的。\n该方法调用链如下：\n现在我们结合getAutoConfigurationEntry()的源码来详细分析一下：\nprivate static final AutoConfigurationEntry EMPTY_ENTRY = new AutoConfigurationEntry(); AutoConfigurationEntry getAutoConfigurationEntry(AutoConfigurationMetadata autoConfigurationMetadata, AnnotationMetadata annotationMetadata) { //\u0026lt;1\u0026gt;. if (!this.isEnabled(annotationMetadata)) { return EMPTY_ENTRY; } else { //\u0026lt;2\u0026gt;. AnnotationAttributes attributes = this.getAttributes(annotationMetadata); //\u0026lt;3\u0026gt;. List\u0026lt;String\u0026gt; configurations = this.getCandidateConfigurations(annotationMetadata, attributes); //\u0026lt;4\u0026gt;. configurations = this.removeDuplicates(configurations); Set\u0026lt;String\u0026gt; exclusions = this.getExclusions(annotationMetadata, attributes); this.checkExcludedClasses(configurations, exclusions); configurations.removeAll(exclusions); configurations = this.filter(configurations, autoConfigurationMetadata); this.fireAutoConfigurationImportEvents(configurations, exclusions); return new AutoConfigurationImportSelector.AutoConfigurationEntry(configurations, exclusions); } } 第 1 步:\n判断自动装配开关是否打开。默认spring.boot.enableautoconfiguration=true，可在 application.properties 或 application.yml 中设置\n第 2 步 ：\n用于获取EnableAutoConfiguration注解中的 exclude 和 excludeName。\n第 3 步\n获取需要自动装配的所有配置类，读取META-INF/spring.factories\nspring-boot/spring-boot-project/spring-boot-autoconfigure/src/main/resources/META-INF/spring.factories 从下图可以看到这个文件的配置内容都被我们读取到了。XXXAutoConfiguration的作用就是按需加载组件。\n[\n不光是这个依赖下的META-INF/spring.factories被读取到，所有 Spring Boot Starter 下的META-INF/spring.factories都会被读取到。\n所以，你可以清楚滴看到， druid 数据库连接池的 Spring Boot Starter 就创建了META-INF/spring.factories文件。\n如果，我们自己要创建一个 Spring Boot Starter，这一步是必不可少的。\n第 4 步 ：\n到这里可能面试官会问你:“spring.factories中这么多配置，每次启动都要全部加载么？”。\n很明显，这是不现实的。我们 debug 到后面你会发现，configurations 的值变小了。\n因为，这一步有经历了一遍筛选，@ConditionalOnXXX 中的所有条件都满足，该类才会生效。\n@Configuration // 检查相关的类：RabbitTemplate 和 Channel是否存在 // 存在才会加载 @ConditionalOnClass({ RabbitTemplate.class, Channel.class }) @EnableConfigurationProperties(RabbitProperties.class) @Import(RabbitAnnotationDrivenConfiguration.class) public class RabbitAutoConfiguration { } 有兴趣的童鞋可以详细了解下 Spring Boot 提供的条件注解\n@ConditionalOnBean：当容器里有指定 Bean 的条件下 @ConditionalOnMissingBean：当容器里没有指定 Bean 的情况下 @ConditionalOnSingleCandidate：当指定 Bean 在容器中只有一个，或者虽然有多个但是指定首选 Bean @ConditionalOnClass：当类路径下有指定类的条件下(这个极其重要) @ConditionalOnMissingClass：当类路径下没有指定类的条件下 @ConditionalOnProperty：指定的属性是否有指定的值 @ConditionalOnResource：类路径是否有指定的值 @ConditionalOnExpression：基于 SpEL 表达式作为判断条件 @ConditionalOnJava：基于 Java 版本作为判断条件 @ConditionalOnJndi：在 JNDI 存在的条件下差在指定的位置 @ConditionalOnNotWebApplication：当前项目不是 Web 项目的条件下 @ConditionalOnWebApplication：当前项目是 Web 项 目的条件下 如何实现一个 Starter # 光说不练假把式，现在就来撸一个 starter，实现自定义线程池\n第一步，创建threadpool-spring-boot-starter工程\n第二步，引入 Spring Boot 相关依赖\n[\n第三步，创建ThreadPoolAutoConfiguration\n第四步，在threadpool-spring-boot-starter工程的 resources 包下创建META-INF/spring.factories文件\n最后新建工程引入threadpool-spring-boot-starter\n测试通过！！！\n[\n总结 # Spring Boot 通过**@EnableAutoConfiguration开启自动装配，通过 SpringFactoriesLoader 最终加载META-INF/spring.factories中的自动配置类实现自动装配**，自动配置类其实就是通过@Conditional按需加载的配置类，想要其生效必须引入**spring-boot-starter-xxx包实现起步依赖**\n"},{"id":105,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/ly04ly_spring-design-patterns/","title":"spring 设计模式","section":"框架","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n“JDK 中用到了哪些设计模式? Spring 中用到了哪些设计模式? ”这两个问题，在面试中比较常见。\n我在网上搜索了一下关于 Spring 中设计模式的讲解几乎都是千篇一律，而且大部分都年代久远。所以，花了几天时间自己总结了一下。\n由于我的个人能力有限，文中如有任何错误各位都可以指出。另外，文章篇幅有限，对于设计模式以及一些源码的解读我只是一笔带过，这篇文章的主要目的是回顾一下 Spring 中的设计模式。\n控制反转(IoC)和依赖注入(DI) # IoC(Inversion of Control,控制反转) 是 Spring 中一个非常非常重要的概念，它不是什么技术，而是一种解耦的设计思想。IoC 的主要目的是借助于“第三方”(Spring 中的 IoC 容器) 实现具有依赖关系的对象之间的解耦(IOC 容器管理对象，你只管使用即可)，从而降低代码之间的耦合度。\nIoC 是一个原则，而不是一个模式，以下模式（但不限于）实现了 IoC 原则。\nSpring IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。 IoC 容器负责创建对象，将对象连接在一起，配置这些对象，并从创建中处理这些对象的整个生命周期，直到它们被完全销毁。\n在实际项目中一个 Service 类如果有几百甚至上千个类作为它的底层，我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IOC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。\n关于 Spring IOC 的理解，推荐看这一下知乎的一个回答：https://www.zhihu.com/question/23277575/answer/169698662 ，非常不错。\n控制反转怎么理解呢? 举个例子：\u0026quot;对象 a 依赖了对象 b，当对象 a 需要使用 对象 b 的时候必须自己去创建。但是当系统引入了 IOC 容器后， 对象 a 和对象 b 之前就失去了直接的联系。这个时候，当对象 a 需要使用 对象 b 的时候， 我们可以指定 IOC 容器去创建一个对象 b 注入到对象 a 中\u0026quot;。 对象 a 获得依赖对象 b 的过程,由主动行为变为了被动行为，控制权反转，这就是控制反转名字的由来。\nDI(Dependecy Inject,依赖注入)是实现控制反转的一种设计模式，依赖注入就是将实例变量传入到一个对象中去。\n工厂设计模式 # //方式一 spring3.1后,XmlBeanFactory被弃用 XmlBeanFactory factory = new XmlBeanFactory (new ClassPathResource(\u0026#34;beans.xml\u0026#34;)); User bean = factory.getBean(User.class); System.out.println(bean); /* //方式二 ApplicationContext context=new ClassPathXmlApplicationContext(\u0026#34;beans.xml\u0026#34;); User bean = context.getBean(User.class); System.out.println(bean);*/ Spring 使用工厂模式可以通过 BeanFactory 或 ApplicationContext 创建 bean 对象。\nApplicationContext继承了ListableBeanFactory，ListableBeanFactory继承了BeanFactory\n两者对比：\nBeanFactory ：延迟注入(使用到某个 bean 的时候才会注入),相比于ApplicationContext 来说会占用更少的内存，程序启动速度更快。 ApplicationContext ：容器启动的时候，不管你用没用到，一次性创建所有 bean 。BeanFactory 仅提供了最基本的依赖注入支持，ApplicationContext 扩展了 BeanFactory ,除了有BeanFactory的功能还有额外更多功能，所以一般开发人员使用ApplicationContext会更多。 ApplicationContext 的三个实现类：\nClassPathXmlApplication：把上下文文件当成类路径资源。 FileSystemXmlApplication：从文件系统中的 XML 文件载入上下文定义信息。 XmlWebApplicationContext：从 Web 系统中的 XML 文件载入上下文定义信息。 Example:\nimport org.springframework.context.ApplicationContext; import org.springframework.context.support.FileSystemXmlApplicationContext; public class App { public static void main(String[] args) { ApplicationContext context = new FileSystemXmlApplicationContext( \u0026#34;C:/work/IOC Containers/springframework.applicationcontext/src/main/resources/bean-factory-config.xml\u0026#34;); HelloApplicationContext obj = (HelloApplicationContext) context.getBean(\u0026#34;helloApplicationContext\u0026#34;); obj.getMsg(); } } 单例设计模式 # 在我们的系统中，有一些对象其实我们只需要一个，比如说：线程池、缓存、对话框、注册表、日志对象、充当打印机、显卡等设备驱动程序的对象。事实上，这一类对象只能有一个实例，如果制造出多个实例就可能会导致一些问题的产生，比如：程序的行为异常、资源使用过量、或者不一致性的结果。\n使用单例模式的好处 :\n对于频繁使用的对象，可以省略创建对象所花费的时间，这对于那些重量级对象而言，是非常可观的一笔系统开销； 由于 new 操作的次数减少，因而对系统内存的使用频率也会降低，这将减轻 GC 压力，缩短 GC 停顿时间。 Spring 中 bean 的默认作用域就是 singleton(单例)的。 除了 singleton 作用域，Spring 中 bean 还有下面几种作用域：\nprototype : 每次获取都会创建一个新的 bean 实例。也就是说，连续 getBean() 两次，得到的是不同的 Bean 实例。 request （仅 Web 应用可用）: 每一次 HTTP 请求都会产生一个新的 bean（请求 bean），该 bean 仅在当前 HTTP request 内有效。 session （仅 Web 应用可用） : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean（会话 bean），该 bean 仅在当前 HTTP session 内有效。 application/global-session （仅 Web 应用可用）： 每个 Web 应用在启动时创建一个 Bean（应用 Bean），，该 bean 仅在当前应用启动时间内有效。 websocket （仅 Web 应用可用）：每一次 WebSocket 会话产生一个新的 bean。 Spring 通过 ConcurrentHashMap 实现单例注册表的特殊方式实现单例模式。\nSpring 实现单例的核心代码如下：\n// 通过 ConcurrentHashMap（线程安全） 实现单例注册表 private final Map\u0026lt;String, Object\u0026gt; singletonObjects = new ConcurrentHashMap\u0026lt;String, Object\u0026gt;(64); public Object getSingleton(String beanName, ObjectFactory\u0026lt;?\u0026gt; singletonFactory) { Assert.notNull(beanName, \u0026#34;\u0026#39;beanName\u0026#39; must not be null\u0026#34;); synchronized (this.singletonObjects) { // 检查缓存中是否存在实例 Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null) { //...省略了很多代码 try { singletonObject = singletonFactory.getObject(); } //...省略了很多代码 // 如果实例对象在不存在，我们注册到单例注册表中。 addSingleton(beanName, singletonObject); } return (singletonObject != NULL_OBJECT ? singletonObject : null); } } //将对象添加到单例注册表 protected void addSingleton(String beanName, Object singletonObject) { synchronized (this.singletonObjects) { this.singletonObjects.put(beanName, (singletonObject != null ? singletonObject : NULL_OBJECT)); } } } 单例 Bean 存在线程安全问题吗？\n大部分时候我们并没有在项目中使用多线程，所以很少有人会关注这个问题。单例 Bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候是存在资源竞争的。\n常见的有两种解决办法：\n在 Bean 中尽量避免定义可变的成员变量。 在类中定义一个 ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 不过，大部分 Bean 实际都是无状态（没有实例变量）的（比如 Dao、Service），这种情况下， Bean 是线程安全的。\n代理设计模式 # 代理模式在 AOP 中的应用 # AOP(Aspect-Oriented Programming，面向切面编程) 能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。\nSpring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用 JDK Proxy 去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示：\n当然，你也可以使用 AspectJ ,Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。\n使用 AOP 之后我们可以把一些通用功能抽象出来，在需要用到的地方直接使用即可，这样大大简化了代码量。我们需要增加新功能时也方便，这样也提高了系统扩展性。日志功能、事务管理等等场景都用到了 AOP 。\nSpring AOP 和 AspectJ AOP 有什么区别? # Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。\nSpring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单，\n如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比 Spring AOP 快很多。\n模板方法 # 模板方法模式是一种行为设计模式，它定义一个操作中的算法的骨架，而将一些步骤延迟到子类中。 模板方法使得子类可以不改变一个算法的结构即可重定义该算法的某些特定步骤的实现方式。\npublic abstract class Template { //这是我们的模板方法 public final void TemplateMethod(){ PrimitiveOperation1(); PrimitiveOperation2(); PrimitiveOperation3(); } protected void PrimitiveOperation1(){ //当前类实现 } //被子类实现的方法 protected abstract void PrimitiveOperation2(); protected abstract void PrimitiveOperation3(); } public class TemplateImpl extends Template { @Override public void PrimitiveOperation2() { //当前类实现 } @Override public void PrimitiveOperation3() { //当前类实现 } } Spring 中 JdbcTemplate、HibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。一般情况下，我们都是使用继承的方式来实现模板模式，但是 Spring 并没有使用这种方式，而是使用 Callback 模式与模板方法模式配合，既达到了代码复用的效果，同时增加了灵活性。\n什么是Callback模式\n//纯模板方法模式 public abstract class JdbcTemplate { public final Object execute（String sql）{ Connection con=null; Statement stmt=null; try { con=getConnection（）; stmt=con.createStatement（）; Object retValue=executeWithStatement（stmt,sql）; return retValue; } catch（SQLException e）{ ... } finally { closeStatement（stmt）; releaseConnection（con）; } } protected abstract Object executeWithStatement（Statement stmt, String sql）; } Callback类\npublic interface StatementCallback{ Object doWithStatement（Statement stmt）; } 使用\n//结合Callback模式 public class JdbcTemplate { //主要是传入了一个类 public final Object execute（StatementCallback callback）{ Connection con=null; Statement stmt=null; try { con=getConnection（）; stmt=con.createStatement（）; Object retValue=callback.doWithStatement（stmt）; return retValue; } catch（SQLException e）{ ... } finally { closeStatement（stmt）; releaseConnection（con）; } } ...//其它方法定义 } JdbcTemplate jdbcTemplate=...; final String sql=...; StatementCallback callback=new StatementCallback(){ public Object=doWithStatement(Statement stmt){ return ...; } } jdbcTemplate.execute(callback); 观察者模式 # 观察者模式是一种对象行为型模式。它表示的是一种对象与对象之间具有依赖关系，当一个对象发生改变的时候，这个对象所依赖的对象也会做出反应。Spring 事件驱动模型就是观察者模式很经典的一个应用。Spring 事件驱动模型非常有用，在很多场景都可以解耦我们的代码。比如我们每次添加商品的时候都需要重新更新商品索引，这个时候就可以利用观察者模式来解决这个问题。\nSpring 事件驱动模型中的三种角色 # 事件角色：是一种属性（物品）\n事件监听者：（注册到事件发布者上）\n事件发布者：当发布的时候，通知指定的监听者（观察者）\n事件角色 # ApplicationEvent (org.springframework.context包下)充当事件的角色,这是一个抽象类，它继承了java.util.EventObject并实现了 java.io.Serializable接口。\nSpring 中默认存在以下事件，他们都是对 ApplicationContextEvent 的实现(继承自ApplicationContextEvent)：\nContextStartedEvent：ApplicationContext 启动后触发的事件; ContextStoppedEvent：ApplicationContext 停止后触发的事件; ContextRefreshedEvent：ApplicationContext 初始化或刷新完成后触发的事件; ContextClosedEvent：ApplicationContext 关闭后触发的事件。 事件监听者角色 # ApplicationListener 充当了事件监听者角色，它是一个接口，里面只定义了一个 onApplicationEvent（）方法来处理ApplicationEvent。ApplicationListener接口类源码如下，可以看出接口定义看出接口中的事件只要实现了 ApplicationEvent就可以了。所以，在 Spring 中我们只要实现 ApplicationListener 接口的 onApplicationEvent() 方法即可完成监听事件\n注意代码，E extends ApplicationEvent , 对某类事件进行监听\npackage org.springframework.context; import java.util.EventListener; @FunctionalInterface public interface ApplicationListener\u0026lt;E extends ApplicationEvent\u0026gt; extends EventListener { void onApplicationEvent(E var1); } 事件发布者角色 # ApplicationEventPublisher 充当了事件的发布者，它也是一个接口。\n@FunctionalInterface public interface ApplicationEventPublisher { default void publishEvent(ApplicationEvent event) { this.publishEvent((Object)event); } void publishEvent(Object var1); } ApplicationEventPublisher 接口的publishEvent（）这个方法在AbstractApplicationContext类中被实现(这个类继承了ApplicationEventPublisher)，阅读这个方法的实现，你会发现实际上事件真正是通过ApplicationEventMulticaster来广播出去的。具体内容过多，就不在这里分析了，后面可能会单独写一篇文章提到。\nSpring 的事件流程总结 # 定义一个事件: 实现一个继承自 ApplicationEvent，并且写相应的构造函数； 定义一个事件监听者：实现 ApplicationListener 接口，重写 onApplicationEvent() 方法； 使用事件发布者发布消息: 可以通过 ApplicationEventPublisher 的 publishEvent() 方法发布消息。 Example:\n// 定义一个事件,继承自ApplicationEvent并且写相应的构造函数 public class DemoEvent extends ApplicationEvent{ private static final long serialVersionUID = 1L; private String message; public DemoEvent(Object source,String message){ super(source); this.message = message; } public String getMessage() { return message; } } // 定义一个事件监听者,实现ApplicationListener接口，重写 onApplicationEvent() 方法； @Component public class DemoListener implements ApplicationListener\u0026lt;DemoEvent\u0026gt;{ //使用onApplicationEvent接收消息 @Override public void onApplicationEvent(DemoEvent event) { String msg = event.getMessage(); System.out.println(\u0026#34;接收到的信息是：\u0026#34;+msg); } } // 发布事件，可以通过ApplicationEventPublisher 的 publishEvent() 方法发布消息。 @Component public class DemoPublisher { @Autowired ApplicationContext applicationContext; public void publish(String message){ //发布事件 applicationContext.publishEvent(new DemoEvent(this, message)); } } 当调用 DemoPublisher 的 publish() 方法的时候，比如 demoPublisher.publish(\u0026quot;你好\u0026quot;) ，控制台就会打印出:接收到的信息是：你好 。\n适配器模式 # 适配器模式(Adapter Pattern) 将一个接口转换成客户希望的另一个接口，适配器模式使接口不兼容的那些类可以一起工作。\nSpring AOP 中的适配器模式 # 我们知道 Spring AOP 的实现是基于代理模式，但是 Spring AOP 的增强或通知(Advice)使用到了适配器模式，与之相关的接口是AdvisorAdapter 。\nAdvice 常用的类型有：BeforeAdvice（目标方法调用前,前置通知）、AfterAdvice（目标方法调用后,后置通知）、AfterReturningAdvice(目标方法执行结束后，return 之前)等等。每个类型 Advice（通知）都有对应的拦截器:MethodBeforeAdviceInterceptor、AfterReturningAdviceInterceptor、ThrowsAdviceInterceptor 等等。\nSpring 预定义的通知要通过对应的适配器，适配成 MethodInterceptor 接口(方法拦截器)类型的对象（如：MethodBeforeAdviceAdapter 通过调用 getInterceptor 方法，将 MethodBeforeAdvice 适配成 MethodBeforeAdviceInterceptor ）。\nSpring MVC 中的适配器模式 # 在 Spring MVC 中，DispatcherServlet 根据请求信息调用 HandlerMapping，解析请求对应的 Handler。解析到对应的 Handler（也就是我们平常说的 Controller 控制器）后，开始由HandlerAdapter 适配器处理。HandlerAdapter 作为期望接口，具体的适配器实现类用于对目标类进行适配，Controller 作为需要适配的类。\n为什么要在 Spring MVC 中使用适配器模式？\nSpring MVC 中的 Controller 种类众多，不同类型的 Controller 通过不同的方法来对请求进行处理。如果不利用适配器模式的话，DispatcherServlet 直接获取对应类型的 Controller，需要的自行来判断，像下面这段代码一样：\nif(mappedHandler.getHandler() instanceof MultiActionController){ ((MultiActionController)mappedHandler.getHandler()).xxx }else if(mappedHandler.getHandler() instanceof XXX){ ... }else if(...){ ... } 假如我们再增加一个 Controller类型就要在上面代码中再加入一行 判断语句，这种形式就使得程序难以维护，也违反了设计模式中的开闭原则 – 对扩展开放，对修改关闭。\n装饰者模式 # 装饰者模式可以动态地给对象添加一些额外的属性或行为。相比于使用继承，装饰者模式更加灵活。简单点儿说就是当我们需要修改原有的功能，但我们又不愿直接去修改原有的代码时，设计一个 Decorator 套在原有代码外面。其实在 JDK 中就有很多地方用到了装饰者模式，比如 InputStream家族，InputStream 类下有 FileInputStream (读取文件)、BufferedInputStream (增加缓存,使读取文件速度大大提升)等子类都在不修改InputStream 代码的情况下扩展了它的功能。\nSpring 中配置 DataSource 的时候，DataSource 可能是不同的数据库和数据源。我们能否根据客户的需求在少修改原有类的代码下动态切换不同的数据源？这个时候就要用到装饰者模式(这一点我自己还没太理解具体原理)。Spring 中用到的包装器模式在类名上含有 Wrapper或者 Decorator。这些类基本上都是动态地给一个对象添加一些额外的职责\n总结 # Spring 框架中用到了哪些设计模式？\n工厂设计模式 : Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 :Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 \u0026hellip;\u0026hellip; 参考 # 《Spring 技术内幕》 https://blog.eduonix.com/java-programming-2/learn-design-patterns-used-spring-framework/ http://blog.yeamin.top/2018/03/27/单例模式-Spring单例实现原理分析/ https://www.tutorialsteacher.com/ioc/inversion-of-control https://design-patterns.readthedocs.io/zh_CN/latest/behavioral_patterns/observer.html https://juejin.im/post/5a8eb261f265da4e9e307230 https://juejin.im/post/5ba28986f265da0abc2b6084 "},{"id":106,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/ly03ly_spring-transaction/","title":"Spring事务详情","section":"框架","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n前段时间答应读者的 Spring 事务 分析总结终于来了。这部分内容比较重要，不论是对于工作还是面试，但是网上比较好的参考资料比较少。\n什么是事务？ # 事务是逻辑上的一组操作，要么都执行，要么都不执行。\n相信大家应该都能背上面这句话了，下面我结合我们日常的真实开发来谈一谈。\n我们系统的每个业务方法可能包括了多个原子性的数据库操作，比如下面的 savePerson() 方法中就有两个原子性的数据库操作。这些原子性的数据库操作是有依赖的，它们要么都执行，要不就都不执行。\npublic void savePerson() { personDao.save(person); personDetailDao.save(personDetail); } 另外，需要格外注意的是：事务能否生效数据库引擎是否支持事务是关键。比如常用的 MySQL 数据库默认使用支持事务的 innodb引擎。但是，如果把数据库引擎变为 myisam，那么程序也就不再支持事务了！\n事务最经典也经常被拿出来说例子就是转账了。假如小明要给小红转账 1000 元，这个转账会涉及到两个关键操作就是：\n将小明的余额减少 1000 元。 将小红的余额增加 1000 元。 万一在这两个操作之间突然出现错误比如银行系统崩溃或者网络故障，导致小明余额减少而小红的余额没有增加，这样就不对了。事务就是保证这两个关键操作要么都成功，要么都要失败。\npublic class OrdersService { private AccountDao accountDao; public void setOrdersDao(AccountDao accountDao) { this.accountDao = accountDao; } @Transactional(propagation = Propagation.REQUIRED, isolation = Isolation.DEFAULT, readOnly = false, timeout = -1) public void accountMoney() { //小红账户多1000 accountDao.addMoney(1000,xiaohong); //模拟突然出现的异常，比如银行中可能为突然停电等等 //如果没有配置事务管理的话会造成，小红账户多了1000而小明账户没有少钱 int i = 10 / 0; //小王账户少1000 accountDao.reduceMoney(1000,xiaoming); } } 另外，数据库事务的 ACID 四大特性是事务的基础，下面简单来了解一下。\n事务的特性（ACID）了解么? AID -\u0026gt; C # 原子性（Atomicity）： 一个事务（transaction）中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。事务在执行过程中发生错误，会被回滚（Rollback）到事务开始前的状态，就像这个事务从来没有执行过一样。即，事务不可分割、不可约简。 一致性（Consistency）： 在事务开始之前和事务结束以后，数据库的完整性没有被破坏。这表示写入的资料必须完全符合所有的预设约束、触发器、级联回滚等。 隔离性（Isolation）： 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致。事务隔离分为不同级别，包括未提交读（Read uncommitted）、提交读（read committed）、可重复读（repeatable read）和串行化（Serializable）。 持久性（Durability）: 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。 参考 ：https://zh.wikipedia.org/wiki/ACID 。\n详谈 Spring 对事务的支持 # ⚠️ 再提醒一次：你的程序是否支持事务首先取决于数据库 ，比如使用 MySQL 的话，如果你选择的是 innodb 引擎，那么恭喜你，是可以支持事务的。但是，如果你的 MySQL 数据库使用的是 myisam 引擎的话，那不好意思，从根上就是不支持事务的。\n这里再多提一下一个非常重要的知识点： MySQL 怎么保证原子性的？\n我们知道如果想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在 MySQL 中，恢复机制是通过 回滚日志（undo log） 实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作。如果执行过程中遇到异常的话，我们直接利用 回滚日志 中的信息将数据回滚到修改之前的样子即可！并且，回滚日志会先于数据持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚之前未完成的事务。\nSpring 支持两种方式的事务管理 # 编程式事务管理 # 通过 **TransactionTemplate或者TransactionManager**手动管理事务，实际应用中很少使用，但是对于你理解 Spring 事务管理原理有帮助。\n使用TransactionTemplate 进行编程式事务管理的示例代码如下：\n@Autowired private TransactionTemplate transactionTemplate; public void testTransaction() { transactionTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) { try { // .... 业务代码 } catch (Exception e){ //回滚 transactionStatus.setRollbackOnly(); } } }); } 使用 TransactionManager 进行编程式事务管理的示例代码如下：\n@Autowired private PlatformTransactionManager transactionManager; public void testTransaction() { TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition()); try { // .... 业务代码 transactionManager.commit(status); } catch (Exception e) { transactionManager.rollback(status); } } 声明式事务管理 # 推荐使用（代码侵入性最小），实际是通过 AOP 实现（基于@Transactional 的全注解方式使用最多）。\n使用 @Transactional注解进行事务管理的示例代码如下：\n@Transactional(propagation = Propagation.REQUIRED) public void aMethod { //do something B b = new B(); C c = new C(); b.bMethod(); c.cMethod(); } Spring 事务管理接口介绍 # Spring 框架中，事务管理相关最重要的 3 个接口如下：\nPlatformTransactionManager： （平台）事务管理器，Spring 事务策略的核心。 TransactionDefinition： 事务定义信息(事务隔离级别、传播行为、超时、只读、回滚规则)。 TransactionStatus： 事务运行状态。 我们可以把 PlatformTransactionManager 接口可以被看作是事务上层的管理者，而 TransactionDefinition 和 TransactionStatus 这两个接口可以看作是事务的描述。\nPlatformTransactionManager 会根据 TransactionDefinition 的定义比如事务超时时间、隔离级别、传播行为等来进行事务管理 ，而 TransactionStatus 接口则提供了一些方法来获取事务相应的状态比如是否新事务、是否可以回滚等等。\nPlatformTransactionManager:事务管理接口 # Spring 并不直接管理事务，而是提供了多种事务管理器 。Spring 事务管理器的接口是： PlatformTransactionManager 。\n通过这个接口，Spring 为各个平台如：JDBC(DataSourceTransactionManager)、Hibernate(HibernateTransactionManager)、JPA(JpaTransactionManager)等都提供了对应的事务管理器，但是具体的实现就是各个平台自己的事情了。\nPlatformTransactionManager 接口的具体实现如下:\nPlatformTransactionManager接口中定义了三个方法：\npackage org.springframework.transaction; import org.springframework.lang.Nullable; public interface PlatformTransactionManager { //获得事务 TransactionStatus getTransaction(@Nullable TransactionDefinition var1) throws TransactionException; //提交事务 void commit(TransactionStatus var1) throws TransactionException; //回滚事务 void rollback(TransactionStatus var1) throws TransactionException; } 这里多插一嘴。为什么要定义或者说抽象出来PlatformTransactionManager这个接口呢？\n主要是因为要将事务管理行为抽象出来，然后不同的平台去实现它，这样我们可以保证提供给外部的行为(提供给程序员)不变，方便我们扩展。\n我前段时间在我的知识星球分享过：“为什么我们要用接口？” 。\n《设计模式》（GOF 那本）这本书在很多年前都提到过说要基于接口而非实现编程，你真的知道为什么要基于接口编程么？\n纵观开源框架和项目的源码，接口是它们不可或缺的重要组成部分。要理解为什么要用接口，首先要搞懂接口提供了什么功能。我们可以把接口理解为提供了一系列功能列表的约定，接口本身不提供功能，它只定义行为。但是谁要用，就要先实现我，遵守我的约定，然后再自己去实现我定义的要实现的功能。\n举个例子，我上个项目有发送短信的需求，为此，我们定了一个接口，接口只有两个方法:\n1.发送短信 2.处理发送结果的方法。\n刚开始我们用的是阿里云短信服务，然后我们实现这个接口完成了一个阿里云短信的服务。后来，我们突然又换到了别的短信服务平台，我们这个时候只需要再实现这个接口即可。这样保证了我们提供给外部的行为不变。几乎不需要改变什么代码，我们就轻松完成了需求的转变，提高了代码的灵活性和可扩展性。\n什么时候用接口？当你要实现的功能模块设计抽象行为的时候，比如发送短信的服务，图床的存储服务等等。\nTransactionDefinition:事务属性 # 事务管理器接口 PlatformTransactionManager 通过 getTransaction(TransactionDefinition definition) 方法来得到一个事务，这个方法里面的参数是 TransactionDefinition 类 ，这个类就定义了一些基本的事务属性。\n什么是事务属性呢？ 事务属性可以理解成事务的一些基本配置，描述了事务策略如何应用到方法上。\n事务属性包含了 5 个方面：\n隔离级别 传播行为 回滚规则 是否只读 事务超时 TransactionDefinition 接口中定义了 5 个方法以及一些表示事务属性的常量比如隔离级别、传播行为等等。\npackage org.springframework.transaction; import org.springframework.lang.Nullable; public interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; int ISOLATION_DEFAULT = -1; int ISOLATION_READ_UNCOMMITTED = 1; int ISOLATION_READ_COMMITTED = 2; int ISOLATION_REPEATABLE_READ = 4; int ISOLATION_SERIALIZABLE = 8; int TIMEOUT_DEFAULT = -1; // 返回事务的传播行为，默认值为 REQUIRED。 int getPropagationBehavior(); //返回事务的隔离级别，默认值是 DEFAULT int getIsolationLevel(); // 返回事务的超时时间，默认值为-1。如果超过该时间限制但事务还没有完成，则自动回滚事务。 int getTimeout(); // 返回是否为只读事务，默认值为 false boolean isReadOnly(); @Nullable String getName(); } TransactionStatus:事务状态 # TransactionStatus接口用来记录事务的状态 该接口定义了一组方法,用来获取或判断 事务的相应状态信息。\nPlatformTransactionManager.getTransaction(…)方法返回一个 TransactionStatus 对象。\nTransactionStatus 接口内容如下：\npublic interface TransactionStatus{ boolean isNewTransaction(); // 是否是新的事务 boolean hasSavepoint(); // 是否有恢复点 void setRollbackOnly(); // 设置为只回滚 boolean isRollbackOnly(); // 是否为只回滚 boolean isCompleted; // 是否已完成 } 事务属性详解 # 实际业务开发中，大家一般都是使用 @Transactional 注解来开启事务，但很多人并不清楚这个注解里面的参数是什么意思，有什么用。为了更好的在项目中使用事务管理，强烈推荐好好阅读一下下面的内容。\n事务传播行为 # 事务传播行为是为了解决业务层方法之间互相调用的事务问题。\n当事务方法被另一个事务方法调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。\n举个例子：我们在 A 类的aMethod()方法中调用了 B 类的 bMethod() 方法。这个时候就涉及到业务层方法之间互相调用的事务问题。如果我们的 bMethod()如果发生异常需要回滚，如何配置事务传播行为才能让 aMethod()也跟着回滚呢？这个时候就需要事务传播行为的知识了，如果你不知道的话一定要好好看一下。\n@Service Class A { @Autowired B b; @Transactional(propagation = Propagation.xxx) public void aMethod { //do something b.bMethod(); } } @Service Class B { @Transactional(propagation = Propagation.xxx) public void bMethod { //do something } } 在TransactionDefinition定义中包括了如下几个表示传播行为的常量：\npublic interface TransactionDefinition { int PROPAGATION_REQUIRED = 0; int PROPAGATION_SUPPORTS = 1; int PROPAGATION_MANDATORY = 2; int PROPAGATION_REQUIRES_NEW = 3; int PROPAGATION_NOT_SUPPORTED = 4; int PROPAGATION_NEVER = 5; int PROPAGATION_NESTED = 6; ...... } 不过，为了方便使用，Spring 相应地定义了一个枚举类：Propagation\npackage org.springframework.transaction.annotation; import org.springframework.transaction.TransactionDefinition; public enum Propagation { REQUIRED(TransactionDefinition.PROPAGATION_REQUIRED), SUPPORTS(TransactionDefinition.PROPAGATION_SUPPORTS), MANDATORY(TransactionDefinition.PROPAGATION_MANDATORY), REQUIRES_NEW(TransactionDefinition.PROPAGATION_REQUIRES_NEW), NOT_SUPPORTED(TransactionDefinition.PROPAGATION_NOT_SUPPORTED), NEVER(TransactionDefinition.PROPAGATION_NEVER), NESTED(TransactionDefinition.PROPAGATION_NESTED); private final int value; Propagation(int value) { this.value = value; } public int value() { return this.value; } } 正确的事务传播行为可能的值如下 ：\n1.TransactionDefinition.PROPAGATION_REQUIRED\n使用的最多的一个事务传播行为，我们平时经常使用的@Transactional注解默认使用就是这个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。也就是说：\n如果外部方法没有开启事务的话，Propagation.REQUIRED修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。 如果外部方法开启事务并且被Propagation.REQUIRED的话，所有Propagation.REQUIRED修饰的内部方法和外部方法均属于同一事务 ，只要一个方法回滚，整个事务均回滚。 举个例子：如果我们上面的aMethod()和bMethod()使用的都是PROPAGATION_REQUIRED传播行为的话，两者使用的就是同一个事务，只要其中一个方法回滚，整个事务均回滚。\n@Service Class A { @Autowired B b; @Transactional(propagation = Propagation.REQUIRED) public void aMethod { //do something b.bMethod(); } } @Service Class B { @Transactional(propagation = Propagation.REQUIRED) public void bMethod { //do something } } 2.TransactionDefinition.PROPAGATION_REQUIRES_NEW\n创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。\n举个例子：如果我们上面的bMethod()使用PROPAGATION_REQUIRES_NEW事务传播行为修饰，aMethod还是用PROPAGATION_REQUIRED修饰的话。如果aMethod()发生异常回滚，bMethod()不会跟着回滚，因为 bMethod()开启了独立的事务。但是，如果 bMethod()抛出了未被捕获的异常并且这个异常满足事务回滚规则的话,aMethod()同样也会回滚，因为这个异常被 aMethod()的事务管理机制检测到了。\n@Service Class A { @Autowired B b; @Transactional(propagation = Propagation.REQUIRED) public void aMethod { //do something b.bMethod(); } } @Service Class B { @Transactional(propagation = Propagation.REQUIRES_NEW) public void bMethod { //do something } } 3.TransactionDefinition.PROPAGATION_NESTED:\n如果当前存在事务，就在嵌套事务内执行；如果当前没有事务，就执行与TransactionDefinition.PROPAGATION_REQUIRED类似的操作。也就是说：\n在外部方法开启事务的情况下,在内部开启一个新的事务，作为嵌套事务存在。 如果外部方法无事务，则单独开启一个事务，与 PROPAGATION_REQUIRED 类似。 这里还是简单举个例子：如果 bMethod() 回滚的话，aMethod()也会回滚。\n如果aMethod()回滚的话，bMethod()也会回滚\n@Service Class A { @Autowired B b; @Transactional(propagation = Propagation.REQUIRED) public void aMethod { //do something b.bMethod(); } } @Service Class B { @Transactional(propagation = Propagation.NESTED) public void bMethod { //do something } } 4.TransactionDefinition.PROPAGATION_MANDATORY\n如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）\n这个使用的很少，就不举例子来说了。\n若是错误的配置以下 3 种事务传播行为，事务将不会发生回滚，这里不对照案例讲解了，使用的很少。\nTransactionDefinition.PROPAGATION_SUPPORTS: 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED: 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER: 以非事务方式运行，如果当前存在事务，则抛出异常。 更多关于事务传播行为的内容请看这篇文章：《太难了~面试官让我结合案例讲讲自己对 Spring 事务传播行为的理解。》\n事务隔离级别 # TransactionDefinition 接口中定义了五个表示隔离级别的常量：\npublic interface TransactionDefinition { ...... int ISOLATION_DEFAULT = -1; int ISOLATION_READ_UNCOMMITTED = 1; int ISOLATION_READ_COMMITTED = 2; int ISOLATION_REPEATABLE_READ = 4; int ISOLATION_SERIALIZABLE = 8; ...... } 和事务传播行为那块一样，为了方便使用，Spring 也相应地定义了一个枚举类：Isolation\npublic enum Isolation { DEFAULT(TransactionDefinition.ISOLATION_DEFAULT), READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED), READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED), REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ), SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); private final int value; Isolation(int value) { this.value = value; } public int value() { return this.value; } } 下面我依次对每一种事务隔离级别进行介绍：\nTransactionDefinition.ISOLATION_DEFAULT :使用后端数据库默认的隔离级别，MySQL 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED :最低的隔离级别，使用这个隔离级别很少，因为它允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED : 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ : 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE : 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 相关阅读：MySQL事务隔离级别详解。\n事务超时属性 # 所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒，默认值为-1，这表示事务的超时时间取决于底层事务系统或者没有超时时间。\n事务只读属性 # package org.springframework.transaction; import org.springframework.lang.Nullable; public interface TransactionDefinition { ...... // 返回是否为只读事务，默认值为 false boolean isReadOnly(); } 对于只有读取数据查询的事务，可以指定事务类型为 readonly，即只读事务。只读事务不涉及数据的修改，数据库会提供一些优化手段，适合用在有多条数据库查询操作的方法中。\n很多人就会疑问了，为什么我一个数据查询操作还要启用事务支持呢？\n拿 MySQL 的 innodb 举例子，根据官网 https://dev.mysql.com/doc/refman/5.7/en/innodb-autocommit-commit-rollback.html 描述：\nMySQL 默认对每一个新建立的连接都启用了autocommit模式。在该模式下，每一个发送到 MySQL 服务器的sql语句都会在一个单独的事务中进行处理，执行结束后会自动提交事务，并开启一个新的事务（才会开启一个新的事务）。\n但是，如果你给方法加上了Transactional注解的话，这个方法执行的所有sql会被放在一个事务中。如果声明了只读事务的话，数据库就会去优化它的执行，并不会带来其他的什么收益。\n如果不加Transactional，每条sql会开启一个单独的事务，中间被其它事务改了数据，都会实时读取到最新值。\n分享一下关于事务只读属性，其他人的解答：\n如果你一次执行单条查询语句，则没有必要启用事务支持，数据库默认支持 SQL 执行期间的读一致性； 如果你一次执行多条查询语句，例如统计查询，报表查询，在这种场景下，多条查询 SQL 必须保证整体的读一致性，否则，在前条 SQL 查询之后，后条 SQL 查询之前，数据被其他用户改变，则该次整体的统计查询将会出现读数据不一致的状态，此时，应该启用事务支持（避免多次查询结果不一致） 事务回滚规则 # 这些规则定义了哪些异常会导致事务回滚而哪些不会。默认情况下，事务只有遇到运行期异常（RuntimeException 的子类）时才会回滚，Error 也会导致事务回滚，但是，在遇到检查型（Checked）异常时不会回滚。[ **这里说的是默认情况下 **]\n如果你想要回滚你定义的特定的异常类型的话，可以这样：\n@Transactional(rollbackFor= MyException.class) @Transactional 注解使用详解 # @Transactional 的作用范围 # 方法 ：推荐将注解使用于方法上，不过需要注意的是：该注解只能应用到 public 方法上，否则不生效。 类 ：如果这个注解使用在类上的话，表明该注解对该类中所有的 public 方法都生效。 接口 ：不推荐在接口上使用。 @Transactional 的常用配置参数 # @Transactional注解源码如下，里面包含了基本事务属性的配置：\n@Target({ElementType.TYPE, ElementType.METHOD}) @Retention(RetentionPolicy.RUNTIME) @Inherited @Documented public @interface Transactional { @AliasFor(\u0026#34;transactionManager\u0026#34;) String value() default \u0026#34;\u0026#34;; @AliasFor(\u0026#34;value\u0026#34;) String transactionManager() default \u0026#34;\u0026#34;; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default TransactionDefinition.TIMEOUT_DEFAULT; boolean readOnly() default false; Class\u0026lt;? extends Throwable\u0026gt;[] rollbackFor() default {}; String[] rollbackForClassName() default {}; Class\u0026lt;? extends Throwable\u0026gt;[] noRollbackFor() default {}; String[] noRollbackForClassName() default {}; } @Transactional 的常用配置参数总结（只列出了 5 个我平时比较常用的）：\n属性名 说明 propagation 事务的传播行为，默认值为 REQUIRED，可选的值在上面介绍过 isolation 事务的隔离级别，默认值采用 DEFAULT，可选的值在上面介绍过 timeout 事务的超时时间，默认值为-1（不会超时）。如果超过该时间限制但事务还没有完成，则自动回滚事务。 readOnly 指定事务是否为只读事务，默认值为 false。 rollbackFor 用于指定能够触发事务回滚的异常类型，并且可以指定多个异常类型。 @Transactional 事务注解原理 # 面试中在问 AOP 的时候可能会被问到的一个问题。简单说下吧！\n我们知道，@Transactional 的工作机制是基于 AOP 实现的，AOP 又是使用动态代理实现的。如果目标对象实现了接口，默认情况下会采用 JDK 的动态代理，如果目标对象没有实现了接口,会使用 CGLIB 动态代理。\n多提一嘴：createAopProxy() 方法 决定了是使用 JDK 还是 Cglib 来做动态代理，源码如下：\npublic class DefaultAopProxyFactory implements AopProxyFactory, Serializable { @Override public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException { if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) { Class\u0026lt;?\u0026gt; targetClass = config.getTargetClass(); if (targetClass == null) { throw new AopConfigException(\u0026#34;TargetSource cannot determine target class: \u0026#34; + \u0026#34;Either an interface or a target is required for proxy creation.\u0026#34;); } if (targetClass.isInterface() || Proxy.isProxyClass(targetClass)) { return new JdkDynamicAopProxy(config); } return new ObjenesisCglibAopProxy(config); } else { return new JdkDynamicAopProxy(config); } } ....... } 如果一个类或者一个类中的 public 方法上被标注@Transactional 注解的话，Spring 容器就会在启动的时候为其创建一个代理类，在调用被@Transactional 注解的 public 方法的时候，实际调用的是，TransactionInterceptor 类中的 invoke()方法。这个方法的作用就是在目标方法之前开启事务，方法执行过程中如果遇到异常的时候回滚事务，方法调用完成之后提交事务。\nTransactionInterceptor 类中的 invoke()方法内部实际调用的是 TransactionAspectSupport 类的 invokeWithinTransaction()方法。由于新版本的 Spring 对这部分重写很大，而且用到了很多响应式编程的知识，这里就不列源码了。\nSpring AOP 自调用问题 # 若同一类中的其他没有 @Transactional 注解的方法内部调用有 @Transactional 注解的方法，有@Transactional 注解的方法的事务会失效。\n这是由于Spring AOP代理的原因造成的，因为只有当 @Transactional 注解的方法在类以外被调用的时候，Spring 事务管理才生效。\n因为这里使用了this，而内部调用(使用this)第二个方法其实是使用了原始类，而非代理类\nMyService 类中的method1()调用method2()就会导致method2()的事务失效。\n@Service public class MyService { private void method1() { method2(); //...... } @Transactional public void method2() { //...... } } 解决办法就是避免同一类中自调用或者使用 AspectJ 取代 Spring AOP 代理。\n@Transactional 的使用注意事项总结 # @Transactional 注解只有作用到 public 方法上事务才生效，不推荐在接口上使用； 避免同一个类中调用 @Transactional 注解的方法，这样会导致事务失效； 正确的设置 @Transactional 的 rollbackFor 和 propagation 属性，否则事务可能会回滚失败; 被 @Transactional 注解的方法所在的类必须被 Spring 管理，否则不生效； 底层使用的数据库必须支持事务机制，否则不生效； \u0026hellip;\u0026hellip; 参考 # [总结]Spring 事务管理中@Transactional 的参数:http://www.mobabel.net/spring 事务管理中 transactional 的参数/ Spring 官方文档：https://docs.spring.io/spring/docs/4.2.x/spring-framework-reference/html/transaction.html 《Spring5 高级编程》 透彻的掌握 Spring 中@transactional 的使用: https://www.ibm.com/developerworks/cn/java/j-master-spring-transactional-use/index.html Spring 事务的传播特性：https://github.com/love-somnus/Spring/wiki/Spring 事务的传播特性 Spring 事务传播行为详解 ：https://segmentfault.com/a/1190000013341344 全面分析 Spring 的编程式事务管理及声明式事务管理：https://www.ibm.com/developerworks/cn/education/opensource/os-cn-spring-trans/index.html "},{"id":107,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/ly02ly_spring-annotations/","title":"Spring/SpringBoot常用注解","section":"框架","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n0.前言 # 可以毫不夸张地说，这篇文章介绍的 Spring/SpringBoot 常用注解基本已经涵盖你工作中遇到的大部分常用的场景。对于每一个注解我都说了具体用法，掌握搞懂，使用 SpringBoot 来开发项目基本没啥大问题了！\n为什么要写这篇文章？\n最近看到网上有一篇关于 SpringBoot 常用注解的文章被转载的比较多，我看了文章内容之后属实觉得质量有点低，并且有点会误导没有太多实际使用经验的人（这些人又占据了大多数）。所以，自己索性花了大概 两天时间简单总结一下了。\n因为我个人的能力和精力有限，如果有任何不对或者需要完善的地方，请帮忙指出！Guide 哥感激不尽！\n1. @SpringBootApplication # 这里先单独拎出@SpringBootApplication 注解说一下，虽然我们一般不会主动去使用它。\nGuide 哥：这个注解是 Spring Boot 项目的基石，创建 SpringBoot 项目之后会默认在主类加上。\n@SpringBootApplication public class SpringSecurityJwtGuideApplication { public static void main(java.lang.String[] args) { SpringApplication.run(SpringSecurityJwtGuideApplication.class, args); } } 我们可以把 @SpringBootApplication看作是 @Configuration、@EnableAutoConfiguration、@ComponentScan 注解的集合。\npackage org.springframework.boot.autoconfigure; @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Inherited @SpringBootConfiguration @EnableAutoConfiguration @ComponentScan(excludeFilters = { @Filter(type = FilterType.CUSTOM, classes = TypeExcludeFilter.class), @Filter(type = FilterType.CUSTOM, classes = AutoConfigurationExcludeFilter.class) }) public @interface SpringBootApplication { ...... } package org.springframework.boot; @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @Documented @Configuration public @interface SpringBootConfiguration { } 根据 SpringBoot 官网，这三个注解的作用分别是：\n@EnableAutoConfiguration：启用 SpringBoot 的自动配置机制 (这个是必须的，另外两个可以不要) @ComponentScan： 扫描被@Component (@Repository,@Service,@Controller)注解的 bean，注解默认会扫描该类所在的包下所有的类。 @Configuration：允许在 Spring 上下文中注册额外的 bean 或导入其他配置类 2. Spring Bean 相关 # 2.1. @Autowired # 自动导入对象到类中，被注入进的类同样要被 Spring 容器管理比如：Service 类注入到 Controller 类中。\n@Service public class UserService { ...... } @RestController @RequestMapping(\u0026#34;/users\u0026#34;) public class UserController { @Autowired private UserService userService; ...... } 2.2. @Component,@Repository,@Service, @Controller # 我们一般使用 @Autowired 注解让 Spring 容器帮我们自动装配 bean。要想把类标识成可用于 @Autowired 注解自动装配的 bean 的类,可以采用以下注解实现：\n@Component ：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用于接受用户请求并调用 Service 层返回数据给前端页面。 2.3. @RestController # @RestController注解是@Controller和@ResponseBody的合集,表示这是个控制器 bean,并且是将函数的返回值直接填入 HTTP 响应体中,是 REST 风格的控制器。\nGuide 哥：现在都是前后端分离，说实话我已经很久没有用过@Controller。如果你的项目太老了的话，就当我没说。\n单独使用 @Controller 不加 @ResponseBody的话一般是用在要返回一个视图的情况，这种情况属于比较传统的 Spring MVC 的应用，对应于前后端不分离的情况。@Controller +@ResponseBody 返回 JSON 或 XML 形式数据\n关于@RestController 和 @Controller的对比，请看这篇文章：@RestController vs @Controller。\n2.4. @Scope # 声明 Spring Bean 的作用域，使用方法:\n@Bean @Scope(\u0026#34;singleton\u0026#34;) public Person personSingleton() { return new Person(); } 四种常见的 Spring Bean 的作用域：\nsingleton : 唯一 bean 实例，Spring 中的 bean 默认都是单例的。 prototype : 每次请求都会创建一个新的 bean 实例。 request : 每一次 HTTP 请求都会产生一个新的 bean，该 bean 仅在当前 HTTP request 内有效。 session : 每一个 HTTP Session 会产生一个新的 bean，该 bean 仅在当前 HTTP session 内有效。 2.5. @Configuration # 一般用来声明配置类，可以使用 @Component注解替代，不过使用@Configuration注解声明配置类更加语义化。(还有就是配置第三方库)\n@Configuration public class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } } 3. 处理常见的 HTTP 请求类型 # 5 种常见的请求类型:\nGET ：请求从服务器获取特定资源。举个例子：GET /users（获取所有学生） POST ：在服务器上创建一个新的资源。举个例子：POST /users（创建学生） PUT ：更新服务器上的资源（客户端提供更新后的整个资源）。举个例子：PUT /users/12（更新编号为 12 的学生） DELETE ：从服务器删除特定的资源。举个例子：DELETE /users/12（删除编号为 12 的学生） PATCH ：更新服务器上的资源（客户端提供更改的属性，可以看做作是部分更新），使用的比较少，这里就不举例子了。 3.1. GET 请求 # @GetMapping(\u0026#34;users\u0026#34;)` 等价于`@RequestMapping(value=\u0026#34;/users\u0026#34;,method=RequestMethod.GET) @GetMapping(\u0026#34;/users\u0026#34;) public ResponseEntity\u0026lt;List\u0026lt;User\u0026gt;\u0026gt; getAllUsers() { return userRepository.findAll(); } 3.2. POST 请求 # @PostMapping(\u0026#34;users\u0026#34;)` 等价于`@RequestMapping(value=\u0026#34;/users\u0026#34;,method=RequestMethod.POST) 关于@RequestBody注解的使用，在下面的“前后端传值”这块会讲到。\n@PostMapping(\u0026#34;/users\u0026#34;) public ResponseEntity\u0026lt;User\u0026gt; createUser(@Valid @RequestBody UserCreateRequest userCreateRequest) { return userRespository.save(userCreateRequest); } 3.3. PUT 请求 # @PutMapping(\u0026#34;/users/{userId}\u0026#34;)` 等价于`@RequestMapping(value=\u0026#34;/users/{userId}\u0026#34;,method=RequestMethod.PUT) @PutMapping(\u0026#34;/users/{userId}\u0026#34;) public ResponseEntity\u0026lt;User\u0026gt; updateUser(@PathVariable(value = \u0026#34;userId\u0026#34;) Long userId, @Valid @RequestBody UserUpdateRequest userUpdateRequest) { ...... } 3.4. DELETE 请求 # @DeleteMapping(\u0026#34;/users/{userId}\u0026#34;)`等价于`@RequestMapping(value=\u0026#34;/users/{userId}\u0026#34;,method=RequestMethod.DELETE) @DeleteMapping(\u0026#34;/users/{userId}\u0026#34;) public ResponseEntity deleteUser(@PathVariable(value = \u0026#34;userId\u0026#34;) Long userId){ ...... } 3.5. PATCH 请求 # 一般实际项目中，我们都是 PUT 不够用了之后才用 PATCH 请求去更新数据。\n@PatchMapping(\u0026#34;/profile\u0026#34;) public ResponseEntity updateStudent(@RequestBody StudentUpdateRequest studentUpdateRequest) { studentRepository.updateDetail(studentUpdateRequest); return ResponseEntity.ok().build(); } 4. 前后端传值 # 掌握前后端传值的正确姿势，是你开始 CRUD 的第一步！\n4.1. @PathVariable 和 @RequestParam # @PathVariable用于获取路径参数，@RequestParam用于获取查询参数。\n举个简单的例子：\n@GetMapping(\u0026#34;/klasses/{klassId}/teachers\u0026#34;) public List\u0026lt;Teacher\u0026gt; getKlassRelatedTeachers( @PathVariable(\u0026#34;klassId\u0026#34;) Long klassId, @RequestParam(value = \u0026#34;type\u0026#34;, required = false) String type ) { ... } 如果我们请求的 url 是：/klasses/123456/teachers?type=web\n那么我们服务获取到的数据就是：klassId=123456,type=web。\n4.2. @RequestBody # 用于读取 Request 请求（可能是 POST,PUT,DELETE,GET 请求）的 body 部分并且Content-Type 为 application/json 格式的数据，接收到数据之后会自动将数据绑定到 Java 对象上去。系统会使用HttpMessageConverter或者自定义的HttpMessageConverter将请求的 body 中的 json 字符串转换为 java 对象。\n我用一个简单的例子来给演示一下基本使用！\n我们有一个注册的接口：\n@PostMapping(\u0026#34;/sign-up\u0026#34;) public ResponseEntity signUp(@RequestBody @Valid UserRegisterRequest userRegisterRequest) { userService.save(userRegisterRequest); return ResponseEntity.ok().build(); } UserRegisterRequest对象：\n@Data @AllArgsConstructor @NoArgsConstructor public class UserRegisterRequest { @NotBlank private String userName; @NotBlank private String password; @NotBlank private String fullName; } 我们发送 post 请求到这个接口，并且 body 携带 JSON 数据：\n{\u0026#34;userName\u0026#34;:\u0026#34;coder\u0026#34;,\u0026#34;fullName\u0026#34;:\u0026#34;shuangkou\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;123456\u0026#34;} 这样我们的后端就可以直接把 json 格式的数据映射到我们的 UserRegisterRequest 类上。\n👉 需要注意的是：一个请求方法只可以有一个@RequestBody，但是可以有多个@RequestParam和@PathVariable。 如果你的方法必须要用两个 @RequestBody来接受数据的话，大概率是你的数据库设计或者系统设计出问题了！\n5. 读取配置信息 # 很多时候我们需要将一些常用的配置信息比如阿里云 oss、发送短信、微信认证的相关配置信息等等放到配置文件中。\n下面我们来看一下 Spring 为我们提供了哪些方式帮助我们从配置文件中读取这些配置信息。\n我们的数据源application.yml内容如下：\nwuhan2020: 2020年初武汉爆发了新型冠状病毒，疫情严重，但是，我相信一切都会过去！武汉加油！中国加油！ my-profile: name: Guide哥 email: koushuangbwcx@163.com library: location: 湖北武汉加油中国加油 books: - name: 天才基本法 description: 二十二岁的林朝夕在父亲确诊阿尔茨海默病这天，得知自己暗恋多年的校园男神裴之即将出国深造的消息——对方考取的学校，恰是父亲当年为她放弃的那所。 - name: 时间的秩序 description: 为什么我们记得过去，而非未来？时间“流逝”意味着什么？是我们存在于时间之内，还是时间存在于我们之中？卡洛·罗韦利用诗意的文字，邀请我们思考这一亘古难题——时间的本质。 - name: 了不起的我 description: 如何养成一个新习惯？如何让心智变得更成熟？如何拥有高质量的关系？ 如何走出人生的艰难时刻？ 5.1. @Value(常用) # 使用 @Value(\u0026quot;${property}\u0026quot;) 读取比较简单的配置信息：\n@Value(\u0026#34;${wuhan2020}\u0026#34;) String wuhan2020; 5.2. @ConfigurationProperties(常用) # 通过@ConfigurationProperties读取配置信息并与 bean 绑定。\n@Component @ConfigurationProperties(prefix = \u0026#34;library\u0026#34;) class LibraryProperties { @NotEmpty private String location; private List\u0026lt;Book\u0026gt; books; @Setter @Getter @ToString static class Book { String name; String description; } 省略getter/setter ...... } 你可以像使用普通的 Spring bean 一样，将其注入到类中使用。\n5.3. @PropertySource（不常用） # @PropertySource读取指定 properties 文件\n@Component @PropertySource(\u0026#34;classpath:website.properties\u0026#34;) class WebSite { @Value(\u0026#34;${url}\u0026#34;) private String url; 省略getter/setter ...... } 更多内容请查看我的这篇文章：《10 分钟搞定 SpringBoot 如何优雅读取配置文件？》 。\n6. 参数校验 # 数据的校验的重要性就不用说了，即使在前端对数据进行校验的情况下，我们还是要对传入后端的数据再进行一遍校验，避免用户绕过浏览器直接通过一些 HTTP 工具直接向后端请求一些违法数据。\nJSR(Java Specification Requests） 是一套 JavaBean 参数校验的标准，它定义了很多常用的校验注解，我们可以直接将这些注解加在我们 JavaBean 的属性上面，这样就可以在需要校验的时候进行校验了，非常方便！\n校验的时候我们实际用的是 Hibernate Validator 框架。Hibernate Validator 是 Hibernate 团队最初的数据校验框架，Hibernate Validator 4.x 是 Bean Validation 1.0（JSR 303）的参考实现，Hibernate Validator 5.x 是 Bean Validation 1.1（JSR 349）的参考实现，目前最新版的 Hibernate Validator 6.x 是 Bean Validation 2.0（JSR 380）的参考实现。\nSpringBoot 项目的 spring-boot-starter-web 依赖中已经有 hibernate-validator 包，不需要引用相关依赖。如下图所示（通过 idea 插件—Maven Helper 生成）：\n注：更新版本的 spring-boot-starter-web 依赖中不再有 hibernate-validator 包（如2.3.11.RELEASE），需要自己引入 spring-boot-starter-validation 依赖。\n非 SpringBoot 项目需要自行引入相关依赖包，这里不多做讲解，具体可以查看我的这篇文章：《如何在 Spring/Spring Boot 中做参数校验？你需要了解的都在这里！》。\n👉 需要注意的是： 所有的注解，推荐使用 JSR 注解，即javax.validation.constraints，而不是org.hibernate.validator.constraints\n6.1. 一些常用的字段验证的注解 # @NotEmpty 被注释的字符串的不能为 null 也不能为空 @NotBlank 被注释的字符串非 null，并且必须包含一个非空白字符 @Null 被注释的元素必须为 null @NotNull 被注释的元素必须不为 null @AssertTrue 被注释的元素必须为 true @AssertFalse 被注释的元素必须为 false @Pattern(regex=,flag=)被注释的元素必须符合指定的正则表达式 @Email 被注释的元素必须是 Email 格式。 @Min(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @Max(value)被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @DecimalMin(value)被注释的元素必须是一个数字，其值必须大于等于指定的最小值 @DecimalMax(value) 被注释的元素必须是一个数字，其值必须小于等于指定的最大值 @Size(max=, min=)被注释的元素的大小必须在指定的范围内 @Digits(integer, fraction)被注释的元素必须是一个数字，其值必须在可接受的范围内 @Past被注释的元素必须是一个过去的日期 @Future 被注释的元素必须是一个将来的日期 \u0026hellip;\u0026hellip; 6.2. 验证请求体(RequestBody) # @Data @AllArgsConstructor @NoArgsConstructor public class Person { @NotNull(message = \u0026#34;classId 不能为空\u0026#34;) private String classId; @Size(max = 33) @NotNull(message = \u0026#34;name 不能为空\u0026#34;) private String name; @Pattern(regexp = \u0026#34;((^Man$|^Woman$|^UGM$))\u0026#34;, message = \u0026#34;sex 值不在可选范围\u0026#34;) @NotNull(message = \u0026#34;sex 不能为空\u0026#34;) private String sex; @Email(message = \u0026#34;email 格式不正确\u0026#34;) @NotNull(message = \u0026#34;email 不能为空\u0026#34;) private String email; } 我们在需要验证的参数上加上了@Valid注解，如果验证失败，它将抛出MethodArgumentNotValidException。\n@RestController @RequestMapping(\u0026#34;/api\u0026#34;) public class PersonController { @PostMapping(\u0026#34;/person\u0026#34;) public ResponseEntity\u0026lt;Person\u0026gt; getPerson(@RequestBody @Valid Person person) { return ResponseEntity.ok().body(person); } } 6.3. 验证请求参数(Path Variables 和 Request Parameters) # 一定一定不要忘记在类上加上 @Validated 注解了，这个参数可以告诉 Spring 去校验方法参数。\n@RestController @RequestMapping(\u0026#34;/api\u0026#34;) @Validated public class PersonController { @GetMapping(\u0026#34;/person/{id}\u0026#34;) public ResponseEntity\u0026lt;Integer\u0026gt; getPersonByID(@Valid @PathVariable(\u0026#34;id\u0026#34;) @Max(value = 5,message = \u0026#34;超过 id 的范围了\u0026#34;) Integer id) { return ResponseEntity.ok().body(id); } } 更多关于如何在 Spring 项目中进行参数校验的内容，请看《如何在 Spring/Spring Boot 中做参数校验？你需要了解的都在这里！》这篇文章。\n7. 全局处理 Controller 层异常 # 介绍一下我们 Spring 项目必备的全局处理 Controller 层异常。\n相关注解：\n@ControllerAdvice :注解定义全局异常处理类 @ExceptionHandler :注解声明异常处理方法 如何使用呢？拿我们在第 5 节参数校验这块来举例子。如果方法参数不对的话就会抛出MethodArgumentNotValidException，我们来处理这个异常。\n@ControllerAdvice @ResponseBody public class GlobalExceptionHandler { /** * 请求参数异常处理 */ @ExceptionHandler(MethodArgumentNotValidException.class) public ResponseEntity\u0026lt;?\u0026gt; handleMethodArgumentNotValidException(MethodArgumentNotValidException ex, HttpServletRequest request) { ...... } } 更多关于 Spring Boot 异常处理的内容，请看我的这两篇文章：\nSpringBoot 处理异常的几种常见姿势 使用枚举简单封装一个优雅的 Spring Boot 全局异常处理！ 8. JPA 相关 # 8.1. 创建表 # @Entity声明一个类对应一个数据库实体。\n@Table 设置表名\n@Entity @Table(name = \u0026#34;role\u0026#34;) public class Role { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String description; 省略getter/setter...... } 8.2. 创建主键 # @Id ：声明一个字段为主键。\n使用@Id声明之后，我们还需要定义主键的生成策略。我们可以使用 @GeneratedValue 指定主键生成策略。\n1.通过 @GeneratedValue直接使用 JPA 内置提供的四种主键生成策略来指定主键生成策略。\n@Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; JPA 使用枚举定义了 4 种常见的主键生成策略，如下：\nGuide 哥：枚举替代常量的一种用法\npublic enum GenerationType { /** * 使用一个特定的数据库表格来保存主键 * 持久化引擎通过关系数据库的一张特定的表格来生成主键, */ TABLE, /** *在某些数据库中,不支持主键自增长,比如Oracle、PostgreSQL其提供了一种叫做\u0026#34;序列(sequence)\u0026#34;的机制生成主键 */ SEQUENCE, /** * 主键自增长 */ IDENTITY, /** *把主键生成策略交给持久化引擎(persistence engine), *持久化引擎会根据数据库在以上三种主键生成 策略中选择其中一种 */ AUTO } @GeneratedValue`注解默认使用的策略是`GenerationType.AUTO public @interface GeneratedValue { GenerationType strategy() default AUTO; String generator() default \u0026#34;\u0026#34;; } 一般使用 MySQL 数据库的话，使用GenerationType.IDENTITY策略比较普遍一点（分布式系统的话需要另外考虑使用分布式 ID）。\n2.通过 @GenericGenerator声明一个主键策略，然后 @GeneratedValue使用这个策略\n@Id @GeneratedValue(generator = \u0026#34;IdentityIdGenerator\u0026#34;) @GenericGenerator(name = \u0026#34;IdentityIdGenerator\u0026#34;, strategy = \u0026#34;identity\u0026#34;) private Long id; 等价于：\n@Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; jpa 提供的主键生成策略有如下几种：\npublic class DefaultIdentifierGeneratorFactory implements MutableIdentifierGeneratorFactory, Serializable, ServiceRegistryAwareService { @SuppressWarnings(\u0026#34;deprecation\u0026#34;) public DefaultIdentifierGeneratorFactory() { register( \u0026#34;uuid2\u0026#34;, UUIDGenerator.class ); register( \u0026#34;guid\u0026#34;, GUIDGenerator.class );\t// can be done with UUIDGenerator + strategy register( \u0026#34;uuid\u0026#34;, UUIDHexGenerator.class );\t// \u0026#34;deprecated\u0026#34; for new use register( \u0026#34;uuid.hex\u0026#34;, UUIDHexGenerator.class ); // uuid.hex is deprecated register( \u0026#34;assigned\u0026#34;, Assigned.class ); register( \u0026#34;identity\u0026#34;, IdentityGenerator.class ); register( \u0026#34;select\u0026#34;, SelectGenerator.class ); register( \u0026#34;sequence\u0026#34;, SequenceStyleGenerator.class ); register( \u0026#34;seqhilo\u0026#34;, SequenceHiLoGenerator.class ); register( \u0026#34;increment\u0026#34;, IncrementGenerator.class ); register( \u0026#34;foreign\u0026#34;, ForeignGenerator.class ); register( \u0026#34;sequence-identity\u0026#34;, SequenceIdentityGenerator.class ); register( \u0026#34;enhanced-sequence\u0026#34;, SequenceStyleGenerator.class ); register( \u0026#34;enhanced-table\u0026#34;, TableGenerator.class ); } public void register(String strategy, Class generatorClass) { LOG.debugf( \u0026#34;Registering IdentifierGenerator strategy [%s] -\u0026gt; [%s]\u0026#34;, strategy, generatorClass.getName() ); final Class previous = generatorStrategyToClassNameMap.put( strategy, generatorClass ); if ( previous != null ) { LOG.debugf( \u0026#34; - overriding [%s]\u0026#34;, previous.getName() ); } } } 8.3. 设置字段类型 # @Column 声明字段。\n示例：\n设置属性 userName 对应的数据库字段名为 user_name，长度为 32，非空\n@Column(name = \u0026#34;user_name\u0026#34;, nullable = false, length=32) private String userName; 设置字段类型并且加默认值，这个还是挺常用的。\n@Column(columnDefinition = \u0026#34;tinyint(1) default 1\u0026#34;) private Boolean enabled; 8.4. 指定不持久化特定字段 # @Transient ：声明不需要与数据库映射的字段，在保存的时候不需要保存进数据库 。\n如果我们想让secrect 这个字段不被持久化，可以使用 @Transient关键字声明。\n@Entity(name=\u0026#34;USER\u0026#34;) public class User { ...... @Transient private String secrect; // not persistent because of @Transient } 除了 @Transient关键字声明， 还可以采用下面几种方法：\nstatic String secrect; // not persistent because of static final String secrect = \u0026#34;Satish\u0026#34;; // not persistent because of final transient String secrect; // not persistent because of transient 一般使用注解的方式比较多。\n8.5. 声明大字段 # @Lob:声明某个字段为大字段。\n@Lob private String content; 更详细的声明：\n@Lob //指定 Lob 类型数据的获取策略， FetchType.EAGER 表示非延迟加载，而 FetchType.LAZY 表示延迟加载 ； @Basic(fetch = FetchType.EAGER) //columnDefinition 属性指定数据表对应的 Lob 字段类型 @Column(name = \u0026#34;content\u0026#34;, columnDefinition = \u0026#34;LONGTEXT NOT NULL\u0026#34;) private String content; 8.6. 创建枚举类型的字段 # 可以使用枚举类型的字段，不过枚举字段要用@Enumerated注解修饰。\npublic enum Gender { MALE(\u0026#34;男性\u0026#34;), FEMALE(\u0026#34;女性\u0026#34;); private String value; Gender(String str){ value=str; } } @Entity @Table(name = \u0026#34;role\u0026#34;) public class Role { @Id @GeneratedValue(strategy = GenerationType.IDENTITY) private Long id; private String name; private String description; @Enumerated(EnumType.STRING) private Gender gender; 省略getter/setter...... } 数据库里面对应存储的是 MALE/FEMALE。\n8.7. 增加审计功能 # 只要继承了 AbstractAuditBase的类都会默认加上下面四个字段。\n@Data @AllArgsConstructor @NoArgsConstructor @MappedSuperclass @EntityListeners(value = AuditingEntityListener.class) public abstract class AbstractAuditBase { @CreatedDate @Column(updatable = false) @JsonIgnore private Instant createdAt; @LastModifiedDate @JsonIgnore private Instant updatedAt; @CreatedBy @Column(updatable = false) @JsonIgnore private String createdBy; @LastModifiedBy @JsonIgnore private String updatedBy; } 我们对应的审计功能对应地配置类可能是下面这样的（Spring Security 项目）:\n@Configuration @EnableJpaAuditing public class AuditSecurityConfiguration { @Bean AuditorAware\u0026lt;String\u0026gt; auditorAware() { return () -\u0026gt; Optional.ofNullable(SecurityContextHolder.getContext()) .map(SecurityContext::getAuthentication) .filter(Authentication::isAuthenticated) .map(Authentication::getName); } } 简单介绍一下上面涉及到的一些注解：\n@CreatedDate: 表示该字段为创建时间字段，在这个实体被 insert 的时候，会设置值\n@CreatedBy :表示该字段为创建人，在这个实体被 insert 的时候，会设置值\n@LastModifiedDate、@LastModifiedBy同理。\n@EnableJpaAuditing：开启 JPA 审计功能。\n8.8. 删除/修改数据 # @Modifying 注解提示 JPA 该操作是修改操作,注意还要配合@Transactional注解使用。\n@Repository public interface UserRepository extends JpaRepository\u0026lt;User, Integer\u0026gt; { @Modifying @Transactional(rollbackFor = Exception.class) void deleteByUserName(String userName); } 8.9. 关联关系 # @OneToOne 声明一对一关系 @OneToMany 声明一对多关系 @ManyToOne 声明多对一关系 @ManyToMany 声明多对多关系 更多关于 Spring Boot JPA 的文章请看我的这篇文章：一文搞懂如何在 Spring Boot 正确中使用 JPA 。\n9. 事务 @Transactional # 在要开启事务的方法上使用@Transactional注解即可!\n@Transactional(rollbackFor = Exception.class) public void save() { ...... } 我们知道 Exception 分为运行时异常 RuntimeException 和非运行时异常。在@Transactional注解中如果不配置rollbackFor属性,那么事务只会在遇到RuntimeException的时候才会回滚,加上rollbackFor=Exception.class,可以让事务在遇到非运行时异常时也回滚。\n@Transactional 注解一般可以作用在类或者方法上。\n作用于类：当把@Transactional 注解放在类上时，表示所有该类的 public 方法都配置相同的事务属性信息。 作用于方法：当类配置了@Transactional，方法也配置了@Transactional，方法的事务会覆盖 类的事务配置信息。 更多关于 Spring 事务的内容请查看我的这篇文章：可能是最漂亮的 Spring 事务管理详解 。\n10. json 数据处理 # 10.1. 过滤 json 数据 # @JsonIgnoreProperties 作用在类上用于过滤掉特定字段不返回或者不解析。\n//生成json时将userRoles属性过滤 @JsonIgnoreProperties({\u0026#34;userRoles\u0026#34;}) public class User { private String userName; private String fullName; private String password; private List\u0026lt;UserRole\u0026gt; userRoles = new ArrayList\u0026lt;\u0026gt;(); } @JsonIgnore一般用于类的属性上，作用和上面的@JsonIgnoreProperties 一样。\npublic class User { private String userName; private String fullName; private String password; //生成json时将userRoles属性过滤 @JsonIgnore private List\u0026lt;UserRole\u0026gt; userRoles = new ArrayList\u0026lt;\u0026gt;(); } 10.2. 格式化 json 数据 # @JsonFormat一般用来格式化 json 数据。\n比如：\n@JsonFormat(shape=JsonFormat.Shape.STRING, pattern=\u0026#34;yyyy-MM-dd\u0026#39;T\u0026#39;HH:mm:ss.SSS\u0026#39;Z\u0026#39;\u0026#34;, timezone=\u0026#34;GMT\u0026#34;) private Date date; 10.3. 扁平化对象 # @Getter @Setter @ToString public class Account { private Location location; private PersonInfo personInfo; @Getter @Setter @ToString public static class Location { private String provinceName; private String countyName; } @Getter @Setter @ToString public static class PersonInfo { private String userName; private String fullName; } } 未扁平化之前：\n{ \u0026#34;location\u0026#34;: { \u0026#34;provinceName\u0026#34;:\u0026#34;湖北\u0026#34;, \u0026#34;countyName\u0026#34;:\u0026#34;武汉\u0026#34; }, \u0026#34;personInfo\u0026#34;: { \u0026#34;userName\u0026#34;: \u0026#34;coder1234\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;shaungkou\u0026#34; } } 使用@JsonUnwrapped **扁平对象(外面那层没了)**之后：\n@Getter @Setter @ToString public class Account { @JsonUnwrapped private Location location; @JsonUnwrapped private PersonInfo personInfo; ...... } { \u0026#34;provinceName\u0026#34;:\u0026#34;湖北\u0026#34;, \u0026#34;countyName\u0026#34;:\u0026#34;武汉\u0026#34;, \u0026#34;userName\u0026#34;: \u0026#34;coder1234\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;shaungkou\u0026#34; } 11. 测试相关 # @ActiveProfiles一般作用于测试类上， 用于声明生效的 Spring 配置文件。\n@SpringBootTest(webEnvironment = RANDOM_PORT) @ActiveProfiles(\u0026#34;test\u0026#34;) @Slf4j public abstract class TestBase { ...... } @Test声明一个方法为测试方法\n@Transactional被声明的测试方法的数据会回滚，避免污染测试数据。\n@WithMockUser Spring Security 提供的，用来模拟一个真实用户，并且可以赋予权限。\n@Test @Transactional @WithMockUser(username = \u0026#34;user-id-18163138155\u0026#34;, authorities = \u0026#34;ROLE_TEACHER\u0026#34;) void should_import_student_success() throws Exception { ...... } 暂时总结到这里吧！虽然花了挺长时间才写完，不过可能还是会一些常用的注解的被漏掉，所以，我将文章也同步到了 Github 上去，Github 地址： 欢迎完善！\n本文已经收录进我的 75K Star 的 Java 开源项目 JavaGuide：https://github.com/Snailclimb/JavaGuide。\n"},{"id":108,"href":"/zh/docs/technology/Review/java_guide/lybly_framework/ly01ly_spring-knowledge-and-questions-summary/","title":"spring 常见面试题总结","section":"框架","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n这篇文章主要是想通过一些问题，加深大家对于 Spring 的理解，所以不会涉及太多的代码！\n下面的很多问题我自己在使用 Spring 的过程中也并没有注意，自己也是临时查阅了很多资料和书籍补上的。网上也有一些很多关于 Spring 常见问题/面试题整理的文章，我感觉大部分都是互相 copy，而且很多问题也不是很好，有些回答也存在问题。所以，自己花了一周的业余时间整理了一下，希望对大家有帮助。\nSpring 基础 # 什么是 Spring 框架? # Spring 是一款开源的轻量级 Java 开发框架，旨在提高开发人员的开发效率以及系统的可维护性。\n我们一般说 Spring 框架指的都是 Spring Framework，它是很多模块的集合，使用这些模块可以很方便地协助我们进行开发，比如说 Spring 支持 IoC（Inversion of Control:控制反转） 和 AOP(Aspect-Oriented Programming:面向切面编程)、可以很方便地对数据库进行访问、可以很方便地集成第三方组件（电子邮件，任务，调度，缓存等等）、对单元测试支持比较好、支持 RESTful Java 应用程序的开发。\n[\nSpring 最核心的思想就是不重新造轮子，开箱即用，提高开发效率。\nSpring 翻译过来就是春天的意思，可见其目标和使命就是为 Java 程序员带来春天啊！感动！\n🤐 多提一嘴 ： 语言的流行通常需要一个杀手级的应用，Spring 就是 Java 生态的一个杀手级的应用框架。\nSpring 提供的核心功能主要是 IoC 和 AOP。学习 Spring ，一定要把 IoC 和 AOP 的核心思想搞懂！\nSpring 官网：https://spring.io/ Github 地址： https://github.com/spring-projects/spring-framework Spring 包含的模块有哪些？ # Spring4.x 版本 ：\nSpring5.x 版本 ：\nSpring5.x 版本中 Web 模块的 Sertlet (应该是Servlet 吧)组件已经被废弃掉，同时增加了用于异步响应式处理的 WebFlux 组件。\nSpring 各个模块的依赖关系如下： Core Container # Spring 框架的核心模块，也可以说是基础模块，主要提供 IoC 依赖注入功能的支持。Spring 其他所有的功能基本都需要依赖于该模块，我们从上面那张 Spring 各个模块的依赖关系图就可以看出来。\nspring-core ：Spring 框架基本的核心工具类。 spring-beans ：提供对 bean 的创建、配置和管理等功能的支持。 spring-context ：提供对国际化、事件传播、资源加载等功能的支持。 spring-expression ：提供对表达式语言（Spring Expression Language） SpEL 的支持，只依赖于 core 模块，不依赖于其他模块，可以单独使用。 AOP # spring-aspects ：该模块为与 AspectJ 的集成提供支持。 spring-aop ：提供了面向切面的编程实现。 spring-instrument ：提供了为 JVM 添加代理（agent）的功能。 具体来讲，它为 Tomcat 提供了一个织入代理，能够为 Tomcat 传递类文 件，就像这些文件是被类加载器加载的一样。没有理解也没关系，这个模块的使用场景非常有限。 Data Access/Integration # spring-jdbc ：提供了对数据库访问的抽象 JDBC。不同的数据库都有自己独立的 API 用于操作数据库，而 Java 程序只需要和 JDBC API 交互，这样就屏蔽了数据库的影响。 spring-tx ：提供对事务的支持。 spring-orm ： 提供对 Hibernate、JPA 、iBatis 等 ORM 框架的支持。 spring-oxm ：提供一个抽象层支撑 OXM(Object-to-XML-Mapping)，例如：JAXB、Castor、XMLBeans、JiBX 和 XStream 等。 spring-jms : 消息服务。自 Spring Framework 4.1 以后，它还提供了对 spring-messaging 模块的继承。 Spring Web # spring-web ：对 Web 功能的实现提供一些最基础的支持。 spring-webmvc ： 提供对 Spring MVC 的实现。 spring-websocket ： 提供了对 WebSocket 的支持，WebSocket 可以让客户端和服务端进行双向通信。 spring-webflux ：提供对 WebFlux 的支持。WebFlux 是 Spring Framework 5.0 中引入的新的响应式框架。与 Spring MVC 不同，它不需要 Servlet API，是完全异步。 Messaging # spring-messaging 是从 Spring4.0 开始新加入的一个模块，主要职责是为 Spring 框架集成一些基础的报文传送应用。\nSpring Test # Spring 团队提倡测试驱动开发（TDD）。有了控制反转 (IoC)的帮助，单元测试和集成测试变得更简单。\nSpring 的测试模块对 JUnit（单元测试框架）、TestNG（类似 JUnit）、Mockito（主要用来 Mock 对象）、PowerMock（解决 Mockito 的问题比如无法模拟 final, static， private 方法）等等常用的测试框架支持的都比较好。\nSpring,Spring MVC,Spring Boot 之间什么关系? # 很多人对 Spring,Spring MVC,Spring Boot 这三者傻傻分不清楚！这里简单介绍一下这三者，其实很简单，没有什么高深的东西。\nSpring 包含了多个功能模块（上面刚刚提到过），其中最重要的是 Spring-Core（主要提供 IoC 依赖注入功能的支持） 模块， Spring 中的其他模块（比如 Spring MVC）的功能实现基本都需要依赖于该模块。\n下图对应的是 Spring4.x 版本。目前最新的 5.x 版本中 Web 模块的 Portlet 组件已经被废弃掉，同时增加了用于异步响应式处理的 WebFlux 组件。\nSpring MVC 是 Spring 中的一个很重要的模块，主要赋予 Spring 快速构建 MVC 架构的 Web 程序的能力。MVC 是模型(Model)、视图(View)、控制器(Controller)的简写，其核心思想是通过将业务逻辑、数据、显示分离来组织代码。\n使用 Spring 进行开发各种配置过于麻烦比如开启某些 Spring 特性时，需要用 XML 或 Java 进行显式配置。于是，Spring Boot 诞生了！\nSpring 旨在简化 J2EE 企业应用程序开发。Spring Boot 旨在简化 Spring 开发（减少配置文件，开箱即用！）。\nSpring Boot 只是简化了配置，如果你需要构建 MVC 架构的 Web 程序，你还是需要使用 Spring MVC 作为 MVC 框架，只是说 Spring Boot 帮你简化了 Spring MVC 的很多配置，真正做到开箱即用！\nSpring IoC # 谈谈自己对于 Spring IoC 的了解 # IoC（Inversion of Control:控制反转） 是一种设计思想，而不是一个具体的技术实现。IoC 的思想就是将原本在程序中手动创建对象的控制权，交由 Spring 框架来管理。不过， IoC 并非 Spring 特有，在其他语言中也有应用。\n为什么叫控制反转？\n控制 ：指的是对象创建（实例化、管理）的权力 反转 ：控制权交给外部环境（Spring 框架、IoC 容器） 将对象之间的相互依赖关系交给 IoC 容器来管理，并由 IoC 容器完成对象的注入。这样可以很大程度上简化应用的开发，把应用从复杂的依赖关系中解放出来。 IoC 容器就像是一个工厂一样，当我们需要创建一个对象的时候，只需要配置好配置文件/注解即可，完全不用考虑对象是如何被创建出来的。\n在实际项目中一个 Service 类可能依赖了很多其他的类，假如我们需要实例化这个 Service，你可能要每次都要搞清这个 Service 所有底层类的构造函数，这可能会把人逼疯。如果利用 IoC 的话，你只需要配置好，然后在需要的地方引用就行了，这大大增加了项目的可维护性且降低了开发难度。\n在 Spring 中， IoC 容器是 Spring 用来实现 IoC 的载体， IoC 容器实际上就是个 Map（key，value），Map 中存放的是各种对象。\nSpring 时代我们一般通过 XML 文件来配置 Bean，后来开发人员觉得 XML 文件来配置不太好，于是 SpringBoot 注解配置就慢慢开始流行起来。\n相关阅读：\nIoC 源码阅读 面试被问了几百遍的 IoC 和 AOP ，还在傻傻搞不清楚？ 什么是 Spring Bean？ # 简单来说，Bean 代指的就是那些被 IoC 容器所管理的对象。\n我们需要告诉 IoC 容器帮助我们管理哪些对象，这个是通过配置元数据来定义的。配置元数据可以是 XML 文件、注解或者 Java 配置类。\n\u0026lt;!-- Constructor-arg with \u0026#39;value\u0026#39; attribute --\u0026gt; \u0026lt;bean id=\u0026#34;...\u0026#34; class=\u0026#34;...\u0026#34;\u0026gt; \u0026lt;constructor-arg value=\u0026#34;...\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; 下图简单地展示了 IoC 容器如何使用配置元数据来管理对象。\norg.springframework.beans和 org.springframework.context 这两个包是 IoC 实现的基础，如果想要研究 IoC 相关的源码的话，可以去看看\n将一个类声明为 Bean 的注解有哪些? # @Component ：通用的注解，可标注任意类为 Spring 组件。如果一个 Bean 不知道属于哪个层，可以使用@Component 注解标注。 @Repository : 对应持久层即 Dao 层，主要用于数据库相关操作。 @Service : 对应服务层，主要涉及一些复杂的逻辑，需要用到 Dao 层。 @Controller : 对应 Spring MVC 控制层，主要用户接受用户请求并调用 Service 层返回数据给前端页面。 @Component 和 @Bean 的区别是什么？ # @Component 注解作用于类，而@Bean注解作用于方法。 @Component通常是通过类路径扫描来自动侦测以及自动装配到 Spring 容器中（我们可以使用 @ComponentScan 注解定义要扫描的路径从中找出标识了需要装配的类自动装配到 Spring 的 bean 容器中）。@Bean 注解通常是我们在标有该注解的方法中定义产生这个 bean,@Bean告诉了 Spring 这是某个类的实例，当我需要用它的时候还给我。 @Bean 注解比 @Component 注解的自定义性更强，而且很多地方我们只能通过 @Bean 注解来注册 bean。比如当我们引用第三方库中的类需要装配到 Spring容器时，则只能通过 @Bean来实现。 @Bean注解使用示例：\n@Configuration public class AppConfig { @Bean public TransferService transferService() { return new TransferServiceImpl(); } } 上面的代码相当于下面的 xml 配置\n\u0026lt;beans\u0026gt; \u0026lt;bean id=\u0026#34;transferService\u0026#34; class=\u0026#34;com.acme.TransferServiceImpl\u0026#34;/\u0026gt; \u0026lt;/beans\u0026gt; 下面这个例子是通过 @Component 无法实现的。（带有逻辑）\n@Bean public OneService getService(status) { case (status) { when 1: return new serviceImpl1(); when 2: return new serviceImpl2(); when 3: return new serviceImpl3(); } } 注入 Bean 的注解有哪些？ # Spring 内置的 @Autowired 以及 JDK 内置的 @Resource 和 @Inject 都可以用于注入 Bean。\nAnnotaion Package Source @Autowired org.springframework.bean.factory Spring 2.5+ @Resource javax.annotation Java JSR-250 @Inject javax.inject Java JSR-330 @Autowired 和@Resource使用的比较多一些。\n@Autowired 和 @Resource 的区别是什么？ # Autowired 属于 Spring 内置的注解，默认的注入方式为byType（根据类型进行匹配），也就是说会优先根据接口类型去匹配并注入 Bean （接口的实现类）。\n这会有什么问题呢？ 当一个接口存在多个实现类的话，byType这种方式就无法正确注入对象了，因为这个时候 Spring 会同时找到多个满足条件的选择，默认情况下它自己不知道选择哪一个。\n这种情况下，注入方式会变为 byName（根据名称进行匹配），这个名称通常就是类名（首字母小写）。就比如说下面代码中的 smsService 就是我这里所说的名称，这样应该比较好理解了吧。\n// smsService 就是我们上面所说的名称 @Autowired private SmsService smsService; 举个例子，SmsService 接口有两个实现类: SmsServiceImpl1和 SmsServiceImpl2，且它们都已经被 Spring 容器所管理。\n// 报错，byName 和 byType 都无法匹配到 bean @Autowired private SmsService smsService; // 正确注入 SmsServiceImpl1 对象对应的 bean @Autowired private SmsService smsServiceImpl1; // 正确注入 SmsServiceImpl1 对象对应的 bean // smsServiceImpl1 就是我们上面所说的名称 @Autowired @Qualifier(value = \u0026#34;smsServiceImpl1\u0026#34;) private SmsService smsService; 我们还是建议通过 @Qualifier 注解来显式指定名称而不是依赖变量的名称。\n@Resource属于 JDK 提供的注解，默认注入方式为 byName。如果无法通过名称匹配到对应的 Bean 的话，注入方式会变为byType。\n@Resource 有两个比较重要且日常开发常用的属性：name（名称）、type（类型）。\npublic @interface Resource { String name() default \u0026#34;\u0026#34;; Class\u0026lt;?\u0026gt; type() default Object.class; } 如果仅指定 name 属性则注入方式为byName，如果仅指定type属性则注入方式为byType，如果同时指定name 和type属性（不建议这么做）则注入方式为byType+byName。\n// 报错，byName 和 byType 都无法匹配到 bean @Resource private SmsService smsService; // 正确注入 SmsServiceImpl1 对象对应的 bean @Resource private SmsService smsServiceImpl1; // 正确注入 SmsServiceImpl1 对象对应的 bean（比较推荐这种方式） @Resource(name = \u0026#34;smsServiceImpl1\u0026#34;) private SmsService smsService; 简单总结一下：\n@Autowired 是 Spring 提供的注解，@Resource 是 JDK 提供的注解。 Autowired 默认的注入方式为**byType（根据类型进行匹配）**，@Resource默认注入方式为 byName（根据名称进行匹配）。 当一个接口存在多个实现类的情况下，@Autowired 和@Resource都需要通过名称才能正确匹配到对应的 Bean。Autowired 可以通过 @Qualifier 注解来显式指定名称，@Resource可以通过 name 属性来显式指定名称。 Bean 的作用域有哪些? # Spring 中 Bean 的作用域通常有下面几种：\nsingleton : IoC 容器中只有唯一的 bean 实例。Spring 中的 bean 默认都是单例的，是对单例设计模式的应用。 prototype : 每次获取都会创建一个新的 bean 实例。也就是说，连续 getBean() 两次，得到的是不同的 Bean 实例。 request （仅 Web 应用可用）: 每一次 HTTP 请求都会产生一个新的 bean（请求 bean），该 bean 仅在当前 HTTP request 内有效。 session （仅 Web 应用可用） : 每一次来自新 session 的 HTTP 请求都会产生一个新的 bean（会话 bean），该 bean 仅在当前 HTTP session 内有效。 application/global-session （仅 Web 应用可用）： 每个 Web 应用在启动时创建一个 Bean（应用 Bean），该 bean 仅在当前应用启动时间内有效。 websocket （仅 Web 应用可用）：每一次 WebSocket 会话产生一个新的 bean。 如何配置 bean 的作用域呢？\nxml 方式：\n\u0026lt;bean id=\u0026#34;...\u0026#34; class=\u0026#34;...\u0026#34; scope=\u0026#34;singleton\u0026#34;\u0026gt;\u0026lt;/bean\u0026gt; 注解方式：\n@Bean @Scope(value = ConfigurableBeanFactory.SCOPE_PROTOTYPE) public Person personPrototype() { return new Person(); } 单例 Bean 的线程安全问题了解吗？ # 大部分时候我们并没有在项目中使用多线程，所以很少有人会关注这个问题。单例 Bean 存在线程问题，主要是因为当多个线程操作同一个对象的时候是存在资源竞争的。\n常见的有两种解决办法：\n在 Bean 中尽量避免定义可变的成员变量。 在类中定义一个 ThreadLocal 成员变量，将需要的可变成员变量保存在 ThreadLocal 中（推荐的一种方式）。 不过，大部分 Bean 实际都是无状态（没有实例变量）的（比如 Dao、Service），这种情况下， Bean 是线程安全的。\nBean 的生命周期了解么? # 下面的内容整理自：https://yemengying.com/2016/07/14/spring-bean-life-cycle/ ，除了这篇文章，再推荐一篇很不错的文章 ：https://www.cnblogs.com/zrtqsk/p/3735273.html 。\nBean 容器找到配置文件中 Spring Bean 的定义。 Bean 容器利用 Java Reflection API 创建一个 Bean 的实例。【反射】 如果涉及到一些属性值 利用 set()方法设置一些属性值。 aware 英[əˈweə(r)] adj. 意识到的,发觉,发现`\n如果 Bean 实现了 BeanNameAware 接口，调用 setBeanName()方法，传入 Bean 的名字。 如果 Bean 实现了 BeanClassLoaderAware 接口，调用 setBeanClassLoader()方法，传入 ClassLoader对象的实例。 如果 Bean 实现了 BeanFactoryAware 接口，调用 setBeanFactory()方法，传入 BeanFactory对象的实例。 与上面的类似，如果实现了其他 *.Aware接口，就调用相应的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessBeforeInitialization() 方法 如果 Bean 实现了**InitializingBean接口，执行afterPropertiesSet()**方法。 如果 Bean 在配置文件中的定义包含 init-method 属性，执行指定的方法。 如果有和加载这个 Bean 的 Spring 容器相关的 BeanPostProcessor 对象，执行postProcessAfterInitialization() 方法 当要销毁 Bean 的时候，如果 Bean 实现了 DisposableBean 接口，执行 destroy() 方法。 当要销毁 Bean 的时候，如果 Bean 在配置文件中的定义包含 destroy-method 属性，执行指定的方法。 图示：\n与之比较类似的中文版本:\nSpring AoP # 谈谈自己对于 AOP 的了解 # aspect 英[ˈæspekt] 方位 n.\noriented 英[ˈɔːrientɪd] 朝向 v.\nAOP(Aspect-Oriented Programming:面向切面编程)能够将那些与业务无关，却为业务模块所共同调用的逻辑或责任（例如事务处理、日志管理、权限控制等）封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可拓展性和可维护性。\nSpring AOP 就是基于动态代理的，如果要代理的对象，实现了某个接口，那么 Spring AOP 会使用 JDK Proxy，去创建代理对象，而对于没有实现接口的对象，就无法使用 JDK Proxy 去进行代理了，这时候 Spring AOP 会使用 Cglib 生成一个被代理对象的子类来作为代理，如下图所示：\n当然你也可以使用 AspectJ ！Spring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。\nAOP 切面编程设计到的一些专业术语：\n术语 含义 目标(Target) 被通知的对象 代理(Proxy) 向目标对象应用通知之后创建的代理对象 连接点(JoinPoint) 目标对象的所属类中，定义的所有方法均为连接点 切入点(Pointcut) 被切面拦截 / 增强的连接点（切入点一定是连接点，连接点不一定是切入点） 通知(Advice) 增强的逻辑 / 代码，也即拦截到目标对象的连接点之后要做的事情 切面(Aspect) 切入点(Pointcut)+通知(Advice) Weaving(织入) 将通知应用到目标对象，进而生成代理对象的过程动作 Spring AOP 和 AspectJ AOP 有什么区别？ # Spring AOP 属于运行时增强，而 AspectJ 是编译时增强。 Spring AOP 基于代理(Proxying)，而 AspectJ 基于字节码操作(Bytecode Manipulation)。\nSpring AOP 已经集成了 AspectJ ，AspectJ 应该算的上是 Java 生态系统中最完整的 AOP 框架了。AspectJ 相比于 Spring AOP 功能更加强大，但是 Spring AOP 相对来说更简单，\n如果我们的切面比较少，那么两者性能差异不大。但是，当切面太多的话，最好选择 AspectJ ，它比 Spring AOP 快很多。\nAspectJ 定义的通知类型有哪些？ # Before（前置通知）：目标对象的方法调用之前触发 After （后置通知）：目标对象的方法调用之后触发 AfterReturning（返回通知）：目标对象的方法调用完成，在返回结果值之后触发 AfterThrowing（异常通知） ：目标对象的方法运行中抛出 / 触发异常后触发。AfterReturning 和 AfterThrowing 两者互斥。如果方法调用成功无异常，则会有返回值；如果方法抛出了异常，则不会有返回值。 Around （环绕通知）：编程式控制目标对象的方法调用。环绕通知是所有通知类型中可操作范围最大的一种，因为它可以直接拿到目标对象，以及要执行的方法，所以环绕通知可以任意的在目标对象的方法调用前后搞事，甚至不调用目标对象的方法 多个切面的执行顺序如何控制？ # 1、通常使用**@Order 注解**直接定义切面顺序\n// 值越小优先级越高 @Order(3) @Component @Aspect public class LoggingAspect implements Ordered { 2、实现Ordered 接口重写 getOrder 方法。\n@Component @Aspect public class LoggingAspect implements Ordered { // .... @Override public int getOrder() { // 返回值越小优先级越高 return 1; } } Spring MVC # 说说自己对于 Spring MVC 了解? # MVC 是模型(Model)、视图(View)、控制器(Controller)的简写，其核心思想是通过将业务逻辑、数据、显示分离来组织代码。\n网上有很多人说 MVC 不是设计模式，只是软件设计规范，我个人更倾向于 MVC 同样是众多设计模式中的一种。java-design-patterns 项目中就有关于 MVC 的相关介绍。\n想要真正理解 Spring MVC，我们先来看看 Model 1 和 Model 2 这两个没有 Spring MVC 的时代。\nModel 1 时代\n很多学 Java 后端比较晚的朋友可能并没有接触过 Model 1 时代下的 JavaWeb 应用开发。在 Model1 模式下，整个 Web 应用几乎全部用 JSP 页面组成，只用少量的 JavaBean 来处理数据库连接、访问等操作。\n这个模式下 JSP 即是控制层（Controller）又是表现层（View）。显而易见，这种模式存在很多问题。比如控制逻辑和表现逻辑混杂在一起，导致代码重用率极低；再比如前端和后端相互依赖，难以进行测试维护并且开发效率极低。\nModel 2 时代\n学过 Servlet 并做过相关 Demo 的朋友应该了解“Java Bean(Model)+ JSP（View）+Servlet（Controller） ”这种开发模式，这就是早期的 JavaWeb MVC 开发模式。\nModel:系统涉及的数据，也就是 dao 和 bean。 View：展示模型中的数据，只是用来展示。 Controller：处理用户请求都发送给 Servlet，返回数据给 JSP 并展示给用户。 Model2 模式下还存在很多问题，Model2 的抽象和封装程度还远远不够，使用 Model2 进行开发时不可避免地会重复造轮子，这就大大降低了程序的可维护性和复用性。\n于是，很多 JavaWeb 开发相关的 MVC 框架应运而生比如 Struts2，但是 Struts2 比较笨重。\nSpring MVC 时代\n随着 Spring 轻量级开发框架的流行，Spring 生态圈出现了 Spring MVC 框架， Spring MVC 是当前最优秀的 MVC 框架。相比于 Struts2 ， Spring MVC 使用更加简单和方便，开发效率更高，并且 Spring MVC 运行速度更快。\nMVC 是一种设计模式，Spring MVC 是一款很优秀的 MVC 框架。Spring MVC 可以帮助我们进行更简洁的 Web 层的开发，并且它天生与 Spring 框架集成。Spring MVC 下我们一般把后端项目分为 Service 层（处理业务）、Dao 层（数据库操作）、Entity 层（实体类）、Controller 层(控制层，返回数据给前台页面)。\nSpring MVC 的核心组件有哪些？ # 记住了下面这些组件，也就记住了 SpringMVC 的工作原理。\nDispatcherServlet ：核心的中央处理器，负责接收请求、分发，并给予客户端响应。 HandlerMapping ：处理器映射器，根据 uri 去匹配查找能处理的 Handler ，并会将请求涉及到的拦截器和 Handler 一起封装。 HandlerAdapter ：处理器适配器，根据 HandlerMapping 找到的 Handler ，适配执行对应的 Handler； Handler ：请求处理器，处理实际请求的处理器。 ViewResolver ：视图解析器，根据 Handler 返回的逻辑视图 / 视图，解析并渲染真正的视图，并传递给 DispatcherServlet 响应客户端 SpringMVC 工作原理了解吗? # Spring MVC 原理如下图所示：\nSpringMVC 工作原理的图解我没有自己画，直接图省事在网上找了一个非常清晰直观的，原出处不明。\n流程说明（重要）：\n客户端（浏览器）发送请求， DispatcherServlet拦截请求。 DispatcherServlet 根据请求信息调用 HandlerMapping 。HandlerMapping 根据 uri 去匹配查找能处理的 Handler（也就是我们平常说的 Controller 控制器） ，并会将请求涉及到的拦截器和 Handler 一起封装。 DispatcherServlet 调用 **HandlerAdapter**适配执行 Handler 。 Handler 完成对用户请求的处理后，会返回一个 ModelAndView 对象给DispatcherServlet，ModelAndView 顾名思义，包含了数据模型以及相应的视图的信息。Model 是返回的数据对象，View 是个逻辑上的 View。 ViewResolver 会根据逻辑 View 查找实际的 View。 DispaterServlet 把返回的 Model 传给 View（视图渲染）。 把 View 返回给请求者（浏览器）\n统一异常处理怎么做？ # 推荐使用注解的方式统一异常处理，具体会使用到 @ControllerAdvice + @ExceptionHandler 这两个注解 。\n@ControllerAdvice @ResponseBody public class GlobalExceptionHandler { @ExceptionHandler(BaseException.class) public ResponseEntity\u0026lt;?\u0026gt; handleAppException(BaseException ex, HttpServletRequest request) { //...... } @ExceptionHandler(value = ResourceNotFoundException.class) public ResponseEntity\u0026lt;ErrorReponse\u0026gt; handleResourceNotFoundException(ResourceNotFoundException ex, HttpServletRequest request) { //...... } } 这种异常处理方式下，会给所有或者指定的 Controller 织入异常处理的逻辑（AOP），当 Controller 中的方法抛出异常的时候，由被@ExceptionHandler 注解修饰的方法进行处理。\nExceptionHandlerMethodResolver 中 getMappedMethod 方法决定了异常具体被哪个被 @ExceptionHandler 注解修饰的方法处理异常。【这个是框架里的源码，不是自己写的】\n@Nullable private Method getMappedMethod(Class\u0026lt;? extends Throwable\u0026gt; exceptionType) { List\u0026lt;Class\u0026lt;? extends Throwable\u0026gt;\u0026gt; matches = new ArrayList\u0026lt;\u0026gt;(); //找到可以处理的所有异常信息。mappedMethods 中存放了异常和处理异常的方法的对应关系 for (Class\u0026lt;? extends Throwable\u0026gt; mappedException : this.mappedMethods.keySet()) { if (mappedException.isAssignableFrom(exceptionType)) { matches.add(mappedException); } } // 不为空说明有方法处理异常 if (!matches.isEmpty()) { // 按照匹配程度从小到大排序 matches.sort(new ExceptionDepthComparator(exceptionType)); // 返回处理异常的方法 return this.mappedMethods.get(matches.get(0)); } else { return null; } } 从源代码看出： getMappedMethod()会首先找到可以匹配处理异常的所有方法信息，然后对其进行从小到大的排序，最后取最小的那一个匹配的方法(即匹配度最高的那个)。\nSpring 框架中用到了哪些设计模式？ # 关于下面这些设计模式的详细介绍，可以看我写的 Spring 中的设计模式详解 这篇文章。\n工厂设计模式 : Spring 使用工厂模式通过 BeanFactory、ApplicationContext 创建 bean 对象。 代理设计模式 : Spring AOP 功能的实现。 单例设计模式 : Spring 中的 Bean 默认都是单例的。 模板方法模式 : Spring 中 jdbcTemplate、hibernateTemplate 等以 Template 结尾的对数据库操作的类，它们就使用到了模板模式。 包装器设计模式 : 我们的项目需要连接多个数据库，而且不同的客户在每次访问中根据需要会去访问不同的数据库。这种模式让我们可以根据客户的需求能够动态切换不同的数据源。 观察者模式: Spring 事件驱动模型就是观察者模式很经典的一个应用。 适配器模式 : Spring AOP 的增强或通知(Advice)使用到了适配器模式、spring MVC 中也是用到了适配器模式适配Controller。 \u0026hellip;\u0026hellip; Spring 事务 # 关于 Spring 事务的详细介绍，可以看我写的 Spring 事务详解 这篇文章。\nSpring 管理事务的方式有几种？ # 编程式事务 ： 在代码中硬编码(不推荐使用) : 通过 **TransactionTemplate**或者 TransactionManager 手动管理事务，实际应用中很少使用，但是对于你理解 Spring 事务管理原理有帮助。 声明式事务 ： 在 XML 配置文件中配置或者直接基于注解（推荐使用） : 实际是通过 AOP 实现（基于@**Transactional** 的全注解方式使用最多） Spring事务失效的几种情况（非javaguide） # 1.spring事务实现方式及原理 # Spring 事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring 是无法提供事务功能的。真正的数据库层的事务提交和回滚是在binlog提交之后进行提交的 通过 redo log 来重做， undo log来回滚。\n一般我们在程序里面使用的都是在方法上面加@Transactional 注解，这种属于声明式事务。\n声明式事务本质是通过 AOP 功能，对方法前后进行拦截，将事务处理的功能编织到拦截的方法中，也就是在目标方法开始之前加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。\n2.数据库本身不支持事务 # 这里以 MySQL 为例，其 MyISAM 引擎是不支持事务操作的，InnoDB 才是支持事务的引擎，一般要支持事务都会使用 InnoDB\n3.当前类的调用 # @Service public class UserServiceImpl implements UserService { public void update(User user) { updateUser(user); } @Transactional(rollbackFor = Exception.class) public void updateUser(User user) { // update user } } 复制代码 上面的这种情况下是不会有事务管理操作的。\n通过看声明式事务的原理可知，spring使用的是AOP切面的方式，本质上使用的是动态代理来达到事务管理的目的，当前类调用的方法上面加@Transactional 这个是没有任何作用的，因为调用这个方法的是this.\nOK， 我们在看下面的一种例子。\n@Service public class UserServiceImpl implements UserService { @Transactional(rollbackFor = Exception.class) public void update(User user) { updateUser(user); } @Transactional(propagation = Propagation.REQUIRES_NEW) public void updateUser(User user) { // update user } } 复制代码 这次在 update 方法上加了 @Transactional，updateUser 加了 REQUIRES_NEW 新开启一个事务，那么新开的事务管用么？\n答案是：不管用！\n因为它们发生了自身调用，就调该类自己的方法，而没有经过 Spring 的代理类，默认只有在外部调用事务才会生效，这也是老生常谈的经典问题了。\n4.方法不是public的 # @Service public class UserServiceImpl implements UserService { @Transactional(rollbackFor = Exception.class) private void updateUser(User user) { // update user } } 复制代码 private 方法是不会被spring代理的，因此是不会有事务产生的，这种做法是无效的。\n5.没有被spring管理 # //@Service public class UserServiceImpl implements UserService { @Transactional(rollbackFor = Exception.class) public void updateUser(User user) { // update user } } 复制代码 没有被spring管理的bean， spring连代理对象都无法生成，当然无效咯。\n6.配置的事务传播性有问题 # @Service public class UserServiceImpl implements UserService { @Transactional(propagation = Propagation.NOT_SUPPORTED) public void update(User user) { // update user } } 复制代码 回顾一下spring的事务传播行为\nSpring 事务的传播行为说的是，当多个事务同时存在的时候， Spring 如何处理这些事务的行为。\nPROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。 PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行 PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。 PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。 PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。 PROPAGATION_NEVER： 以非事务方式执行，如果当前存在事务，则抛出异常。 PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按 REQUIRED 属性执行 当传播行为设置了PROPAGATION_NOT_SUPPORTED，PROPAGATION_NEVER，PROPAGATION_SUPPORTS这三种时，就有可能存在事务不生效\n7.异常被你 \u0026ldquo;抓住\u0026quot;了 # @Service public class UserServiceImpl implements UserService { @Transactional(rollbackFor = Exception.class) public void update(User user) { try{ // update user }catch(Execption e){ log.error(\u0026#34;异常\u0026#34;,e) } } } 复制代码 异常被抓了，这样子代理类就没办法知道你到底有没有错误，需不需要回滚，所以这种情况也是没办法回滚的哦。\n8.接口层声明式事务使用cglib代理 # 注意，这是个前后关系，说的是：如果在接口层使用了声明式事务，结果用的是cglib代理，那么事务就不会生效\npublic interface UserService { @Transactional(rollbackFor = Exception.class) public void update(User user) } 复制代码 @Service public class UserServiceImpl implements UserService { public void update(User user) { // update user } } 复制代码 通过元素的 \u0026ldquo;proxy-target-class\u0026rdquo; 属性值来控制是基于接口的还是基于类的代理被创建。如果 \u0026ldquo;proxy-target-class\u0026rdquo; 属值被设置为 \u0026ldquo;true\u0026rdquo;，那么基于类的代理将起作用（这时需要CGLIB库cglib.jar在CLASSPATH中）。如果 \u0026ldquo;proxy-target-class\u0026rdquo; 属值被设置为 \u0026ldquo;false\u0026rdquo; 或者这个属性被省略，那么标准的JDK基于接口的代理将起作用\n注解@Transactional cglib与java动态代理最大区别是代理目标对象不用实现接口,那么注解要是写到接口方法上，要是使用cglib代理，这时注解事务就失效了，为了保持兼容注解最好都写到实现类方法上。\n9.rollbackFor异常指定错误 # @Service public class UserServiceImpl implements UserService { @Transactional public void update(User user) { // update user } } 复制代码 上面这种没有指定回滚异常，这个时候默认的回滚异常是RuntimeException ，如果出现其他异常那么就不会回滚事务\nSpring 事务中哪几种事务传播行为? # 事务传播行为是为了解决业务层方法之间互相调用的事务问题。\n当事务方法被另一个事务方法(也可能非事务)调用时，必须指定事务应该如何传播。例如：方法可能继续在现有事务中运行，也可能开启一个新事务，并在自己的事务中运行。\n注意几点，下面这个值都是内方法上的注解的值，且两个方法必须属于不同类\n@Service public class MyClassServiceImpl extends ServiceImpl\u0026lt;MyClassMapper, MyClass\u0026gt; implements MyClassService { @Autowired private UserService userService; //外方法 @Override public void methodOuter() throws Exception { //新增一条记录 MyClass myClass=new MyClass(); myClass.setName(\u0026#34;class_name\u0026#34;); this.saveOrUpdate(myClass); //调用内方法 userService.methodInner(); //抛出异常 //throw new Exception(\u0026#34;hello\u0026#34;); } } @Service public class UserServiceImpl extends ServiceImpl\u0026lt;UserMapper, User\u0026gt; implements UserService { //内方法 @Transactional( rollbackFor = Exception.class ,propagation = Propagation.REQUIRED ) @Override public void methodInner() throws Exception { //新增一条记录 User user = new User(); user.setName(\u0026#34;outer_name\u0026#34;); this.saveOrUpdate(user); //抛出异常 //throw new Exception(\u0026#34;hello\u0026#34;); } } 正确的事务传播行为可能的值如下:\n注：如果外方法不存在事务，则内外方法完全独立，自己(方法内)抛异常不影响另一方法\n1.TransactionDefinition.PROPAGATION_REQUIRED\n使用的最多的一个事务传播行为，我们平时经常使用的@Transactional注解默认使用就是这个事务传播行为。如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。\n如果外方法存在事务，则不论 外方法或内方法抛出异常，都会导致外内所在事务（同一个）回滚\n2.TransactionDefinition.PROPAGATION_REQUIRES_NEW\n创建一个新的事务，如果当前存在事务，则把当前事务挂起。也就是说不管外部方法是否开启事务，Propagation.REQUIRES_NEW修饰的内部方法会新开启自己的事务，且开启的事务相互独立，互不干扰。\n如果外方法存在事务，如果仅内方法抛异常，会导致外方法回滚；如果仅外方法抛异常，则不会回滚内方法\n3.TransactionDefinition.PROPAGATION_NESTED\n如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。\n如果外方法存在事务，（效果和1一样）， 不论 外方法或内方法抛出异常，都会导致外内所在事务（和1唯一不同的是，他们是不同事务）回滚\n4.TransactionDefinition.PROPAGATION_MANDATORY\n如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。（mandatory：强制性）\n这个使用的很少。\n如果外方法存在事务，（效果和1一样）， 不论 外方法或内方法抛出异常，都会导致外内所在事务（和1唯一不同的是，如果外方法不存在事务，调用该方法前就直接抛异常）回滚\n若是错误的配置以下 3 种事务传播行为，事务将不会发生回滚：\nTransactionDefinition.PROPAGATION_SUPPORTS: 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED: 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER: 以非事务方式运行，如果当前存在事务，则抛出异常。 Spring 事务中的隔离级别有哪几种? # //这个注解应该是用来修改session级别的隔离级别\n和事务传播行为这块一样，为了方便使用，Spring 也相应地定义了一个枚举类：Isolation\npublic enum Isolation { DEFAULT(TransactionDefinition.ISOLATION_DEFAULT), READ_UNCOMMITTED(TransactionDefinition.ISOLATION_READ_UNCOMMITTED), READ_COMMITTED(TransactionDefinition.ISOLATION_READ_COMMITTED), REPEATABLE_READ(TransactionDefinition.ISOLATION_REPEATABLE_READ), SERIALIZABLE(TransactionDefinition.ISOLATION_SERIALIZABLE); private final int value; Isolation(int value) { this.value = value; } public int value() { return this.value; } } 下面我依次对每一种事务隔离级别进行介绍：\nTransactionDefinition.ISOLATION_DEFAULT :使用后端数据库默认的隔离级别，MySQL 默认采用的 REPEATABLE_READ 隔离级别 Oracle 默认采用的 READ_COMMITTED 隔离级别. TransactionDefinition.ISOLATION_READ_UNCOMMITTED :最低的隔离级别，使用这个隔离级别很少，因为它允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED : 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 TransactionDefinition.ISOLATION_REPEATABLE_READ : 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 TransactionDefinition.ISOLATION_SERIALIZABLE : 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 注意，这个注解的使用方法，下面写了两个方法分别模拟两个不同的线程操作（供不同的controller使用）\n@Transactional( isolation = Isolation.READ_COMMITTED ) public User isolation1() { //读取userid=1的值 User byId = this.getById(1L); return byId; } @Transactional public User isolation2() throws InterruptedException { //10s后修改 TimeUnit.SECONDS.sleep(10); User user=new User(); user.setId(1L); user.setName(\u0026#34;1被修改了\u0026#34;); this.saveOrUpdate(user); //10s后提交 TimeUnit.SECONDS.sleep(10); return null; } 当isolation1为读已提交时，只要isolation2方法没有执行完毕（没有提交），那么isolation1只会读取到未修改的值； 当isolation1为读为提交时，即使isolation2方法没有执行完毕（没有提交），那么isolation1也会立马读取到最新的值； @Transactional(rollbackFor = Exception.class)注解了解吗？ # Exception 分为运行时异常 RuntimeException 和非运行时异常。事务管理对于企业应用来说是至关重要的，即使出现异常情况，它也可以保证数据的一致性。\n当 @Transactional 注解作用于类上时，该类的所有 public 方法将都具有该类型的事务属性，同时，我们也可以在方法级别使用该标注来覆盖类级别的定义。如果类或者方法加了这个注解，那么这个类里面的方法抛出异常，就会回滚，数据库里面的数据也会回滚。\n在 @Transactional 注解中如果不配置rollbackFor属性,那么事务只会在遇到**RuntimeException的时候才会回滚，加上 rollbackFor=Exception.class,可以让事务在遇到非运行时异常时**也回滚。\nSpring Data JPA # JPA 重要的是实战，这里仅对小部分知识点进行总结。\n如何使用 JPA 在数据库中非持久化一个字段？ # 假如我们有下面一个类：\n@Entity(name=\u0026#34;USER\u0026#34;) public class User { @Id @GeneratedValue(strategy = GenerationType.AUTO) @Column(name = \u0026#34;ID\u0026#34;) private Long id; @Column(name=\u0026#34;USER_NAME\u0026#34;) private String userName; @Column(name=\u0026#34;PASSWORD\u0026#34;) private String password; private String secrect; } 如果我们想让secrect 这个字段不被持久化，也就是不被数据库存储怎么办？我们可以采用下面几种方法：\nstatic String transient1; // not persistent because of static final String transient2 = \u0026#34;Satish\u0026#34;; // not persistent because of final transient String transient3; // not persistent because of transient @Transient String transient4; // not persistent because of @Transient 一般使用后面两种方式比较多，我个人使用注解的方式比较多。\nJPA 的审计功能是做什么的？有什么用？ # 审计功能主要是帮助我们记录数据库操作的具体行为比如某条记录是谁创建的、什么时间创建的、最后修改人是谁、最后修改时间是什么时候。\n@Data @AllArgsConstructor @NoArgsConstructor @MappedSuperclass @EntityListeners(value = AuditingEntityListener.class) public abstract class AbstractAuditBase { @CreatedDate @Column(updatable = false) @JsonIgnore private Instant createdAt; @LastModifiedDate @JsonIgnore private Instant updatedAt; @CreatedBy @Column(updatable = false) @JsonIgnore private String createdBy; @LastModifiedBy @JsonIgnore private String updatedBy; } @CreatedDate: 表示该字段为创建时间字段，在这个实体被 insert 的时候，会设置值\n@CreatedBy :表示该字段为创建人，在这个实体被 insert 的时候，会设置值\n@LastModifiedDate、@LastModifiedBy同理。\n实体之间的关联关系注解有哪些？ # @OneToOne : 一对一。 @ManyToMany ：多对多。 @OneToMany : 一对多。 @ManyToOne ：多对一。 利用 @ManyToOne 和 @OneToMany 也可以表达多对多的关联关系。\nSpring Security # Spring Security 重要的是实战，这里仅对小部分知识点进行总结。\n有哪些控制请求访问权限的方法？ # permitAll() ：无条件允许任何形式访问，不管你登录还是没有登录。 anonymous() ：允许匿名访问，也就是没有登录才可以访问。 denyAll() ：无条件决绝任何形式的访问。 authenticated()：只允许已认证的用户访问。 fullyAuthenticated() ：只允许已经登录或者通过 remember-me 登录的用户访问。 hasRole(String) : 只允许指定的角色访问。 hasAnyRole(String)\t: 指定一个或者多个角色，满足其一的用户即可访问。 hasAuthority(String) ：只允许具有指定权限的用户访问 hasAnyAuthority(String) ：指定一个或者多个权限，满足其一的用户即可访问。 hasIpAddress(String) : 只允许指定 ip 的用户访问。 hasRole 和 hasAuthority 有区别吗？ # 可以看看松哥的这篇文章：Spring Security 中的 hasRole 和 hasAuthority 有区别吗？，介绍的比较详细。\n如何对密码进行加密？ # 如果我们需要保存密码这类敏感数据到数据库的话，需要先加密再保存。\nSpring Security 提供了多种加密算法的实现，开箱即用，非常方便。这些加密算法实现类的父类是 PasswordEncoder ，如果你想要自己实现一个加密算法的话，也需要继承 PasswordEncoder。\nPasswordEncoder 接口一共也就 3 个必须实现的方法。\npublic interface PasswordEncoder { // 加密也就是对原始密码进行编码 String encode(CharSequence var1); // 比对原始密码和数据库中保存的密码 boolean matches(CharSequence var1, String var2); // 判断加密密码是否需要再次进行加密，默认返回 false default boolean upgradeEncoding(String encodedPassword) { return false; } } 官方推荐使用基于 bcrypt 强哈希函数的加密算法实现类。\n如何优雅更换系统使用的加密算法？ # 如果我们在开发过程中，突然发现现有的加密算法无法满足我们的需求，需要更换成另外一个加密算法，这个时候应该怎么办呢？\n推荐的做法是通过 DelegatingPasswordEncoder 兼容多种不同的密码加密方案，以适应不同的业务需求。\n从名字也能看出来，DelegatingPasswordEncoder 其实就是一个代理类，并非是一种全新的加密算法，它做的事情就是代理上面提到的加密算法实现类。在 Spring Security 5.0之后，默认就是基于 DelegatingPasswordEncoder 进行密码加密的。\n参考 # 《Spring 技术内幕》 《从零开始深入学习 Spring》：https://juejin.cn/book/6857911863016390663 http://www.cnblogs.com/wmyskxz/p/8820371.html https://www.journaldev.com/2696/spring-interview-questions-and-answers https://www.edureka.co/blog/interview-questions/spring-interview-questions/ https://www.cnblogs.com/clwydjgs/p/9317849.html https://howtodoinjava.com/interview-questions/top-spring-interview-questions-with-answers/ http://www.tomaszezula.com/2014/02/09/spring-series-part-5-component-vs-bean/ https://stackoverflow.com/questions/34172888/difference-between-bean-and-autowired "},{"id":109,"href":"/zh/docs/technology/Review/java_guide/lyaly_dev_tools/git/","title":"git","section":"开发工具","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n版本控制 # 什么是版本控制 # 版本控制是一种记录一个或若干文件内容变化，以便将来查阅特定版本修订情况的系统。 除了项目源代码，你还可以对任何类型的文件进行版本控制。\n为什么要版本控制 # 有了它你就可以将某个文件回溯到之前的状态，甚至将整个项目都回退到过去某个时间点的状态，你可以比较文件的变化细节，查出最后是谁修改了哪个地方，从而找出导致怪异问题出现的原因，又是谁在何时报告了某个功能缺陷等等。\n本地版本控制系统 # 许多人习惯用复制整个项目目录的方式来保存不同的版本，或许还会改名加上备份时间以示区别。 这么做唯一的好处就是简单，但是特别容易犯错。 有时候会混淆所在的工作目录，一不小心会写错文件或者覆盖意想外的文件。\n为了解决这个问题，人们很久以前就开发了许多种本地版本控制系统，大多都是采用某种简单的数据库来记录文件的历次更新差异。\n集中化的版本控制系统 # 接下来人们又遇到一个问题，如何让在不同系统上的开发者协同工作？ 于是，集中化的版本控制系统（Centralized Version Control Systems，简称 CVCS）应运而生。\n集中化的版本控制系统都有一个单一的集中管理的服务器，保存所有文件的修订版本，而协同工作的人们都通过客户端连到这台服务器，取出最新的文件或者提交更新。\n这么做虽然解决了本地版本控制系统无法让在不同系统上的开发者协同工作的诟病，但也还是存在下面的问题：\n单点故障： 中央服务器宕机，则其他人无法使用；如果中心数据库磁盘损坏又没有进行备份，你将丢失所有数据。本地版本控制系统也存在类似问题，只要整个项目的历史记录被保存在单一位置，就有丢失所有历史更新记录的风险。 必须联网才能工作： 受网络状况、带宽影响。 分布式版本控制系统 # 于是分布式版本控制系统（Distributed Version Control System，简称 DVCS）面世了。 Git 就是一个典型的分布式版本控制系统。\n这类系统，客户端并不只提取最新版本的文件快照，而是把代码仓库完整地镜像下来。 这么一来，任何一处协同工作用的服务器发生故障，事后都可以用任何一个镜像出来的本地仓库恢复。 因为每一次的克隆操作，实际上都是一次对代码仓库的完整备份。\n分布式版本控制系统可以不用联网就可以工作，因为每个人的电脑上都是完整的版本库，当你修改了某个文件后，你只需要将自己的修改推送给别人就可以了。但是，在实际使用分布式版本控制系统的时候，很少会直接进行推送修改，而是使用一台充当“中央服务器”的东西。这个服务器的作用仅仅是用来方便“交换”大家的修改，没有它大家也一样干活，只是交换修改不方便而已。\n分布式版本控制系统的优势不单是不必联网这么简单，后面我们还会看到 Git 极其强大的分支管理等功能。\n认识 Git # Git 简史 # Linux 内核项目组当时使用分布式版本控制系统 BitKeeper 来管理和维护代码。但是，后来开发 BitKeeper 的商业公司同 Linux 内核开源社区的合作关系结束，他们收回了 Linux 内核社区免费使用 BitKeeper 的权力。 Linux 开源社区（特别是 Linux 的缔造者 Linus Torvalds）基于使用 BitKeeper 时的经验教训，开发出自己的版本系统，而且对新的版本控制系统做了很多改进。\nGit 与其他版本管理系统的主要区别 # Git 在保存和对待各种信息的时候与其它版本控制系统有很大差异，尽管操作起来的命令形式非常相近，理解这些差异将有助于防止你使用中的困惑。\n下面我们主要说一个关于 Git 与其他版本管理系统的主要差别：对待数据的方式。\nGit采用的是直接记录快照的方式，而非差异比较。我后面会详细介绍这两种方式的差别。\n大部分版本控制系统（CVS、Subversion、Perforce、Bazaar 等等）都是以文件变更列表的方式存储信息，这类系统将它们保存的信息看作是一组基本文件和每个文件随时间逐步累积的差异。\n具体原理如下图所示，理解起来其实很简单，每当我们提交更新一个文件之后，系统都会记录这个文件做了哪些更新，以增量符号Δ(Delta)表示。\n我们怎样才能得到一个文件的最终版本呢？\n很简单，高中数学的基本知识，我们只需要将这些原文件和这些增加进行相加就行了。\n这种方式有什么问题呢？\n比如我们的增量特别特别多的话，如果我们要得到最终的文件是不是会耗费时间和性能。\nGit 不按照以上方式对待或保存数据。 反之，Git 更像是把数据看作是对小型文件系统的一组快照。 每次你提交更新，或在 Git 中保存项目状态时，它主要对当时的全部文件制作一个快照并保存这个快照的索引。 为了高效，如果文件没有修改，Git 不再重新存储该文件，而是只保留一个链接指向之前存储的文件。 Git 对待数据更像是一个 快照流。\nGit 的三种状态 # Git 有三种状态，你的文件可能处于其中之一：\n已提交（committed）：数据已经安全的保存在本地数据库中。 已修改（modified）：已修改表示修改了文件，但还没保存到数据库中。 已暂存（staged）：表示对一个已修改文件的当前版本做了标记，使之包含在下次提交的快照中。 由此引入 Git 项目的三个工作区域的概念：Git 仓库(.git directory)、工作目录(Working Directory) 以及 暂存区域(Staging Area) 。\n基本的 Git 工作流程如下：\n在工作目录中修改文件。 暂存文件，将文件的快照放入暂存区域。 提交更新，找到暂存区域的文件，将快照永久性存储到 Git 仓库目录。 Git 使用快速入门 # 获取 Git 仓库 # 有两种取得 Git 项目仓库的方法。\n在现有目录中初始化仓库: 进入项目目录运行 git init 命令,该命令将创建一个名为 .git 的子目录。 从一个服务器克隆一个现有的 Git 仓库: git clone [url] 自定义本地仓库的名字: git clone [url] directoryname 记录每次更新到仓库 # 检测当前文件状态 : git status 提出更改（把它们添加到暂存区）：git add filename (针对特定文件)、git add *(所有文件)、git add *.txt（支持通配符，所有 .txt 文件） 忽略文件：.gitignore 文件 提交更新: git commit -m \u0026quot;代码提交信息\u0026quot; （每次准备提交前，先用 git status 看下，是不是都已暂存起来了， 然后再运行提交命令 git commit） 跳过使用暂存区域更新的方式 : git commit -a -m \u0026quot;代码提交信息\u0026quot;。 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。 移除文件 ：git rm filename （从暂存区域移除，然后提交。） 对文件重命名 ：git mv README.md README(这个命令相当于mv README.md README、git rm README.md、git add README 这三条命令的集合) 一个好的 Git 提交消息 # 一个好的 Git 提交消息如下：\n标题行：用这一行来描述和解释你的这次提交 主体部分可以是很少的几行，来加入更多的细节来解释提交，最好是能给出一些相关的背景或者解释这个提交能修复和解决什么问题。 主体部分当然也可以有几段，但是一定要注意换行和句子不要太长。因为这样在使用 \u0026#34;git log\u0026#34; 的时候会有缩进比较好看。 提交的标题行描述应该尽量的清晰和尽量的一句话概括。这样就方便相关的 Git 日志查看工具显示和其他人的阅读。\n推送改动到远程仓库 # 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：git remote add origin \u0026lt;server\u0026gt; ,比如我们要让本地的一个仓库和 Github 上创建的一个仓库关联可以这样**git remote add origin https://github.com/Snailclimb/test.git**\n将这些改动提交到远端仓库：git push origin master (可以把 master 换成你想要推送的任何分支)\n如此你就能够将你的改动推送到所添加的服务器上去了。\n远程仓库的移除与重命名 # 将 test 重命名为 test1：git remote rename test test1 移除远程仓库 test1:git remote rm test1 查看提交历史 # 在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。git log 会按提交时间列出所有的更新，最近的更新排在最上面。\n可以添加一些参数来查看自己希望看到的内容：\n只看某个人的提交记录：\ngit log --author=bob 撤销操作 # 有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 --amend 选项的提交命令尝试重新提交：\ngit commit --amend 取消暂存的文件\ngit reset filename 撤消对文件的修改:\ngit checkout -- filename 假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：\ngit fetch origin git reset --hard origin/master 分支 # 分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认”的分支。在其他分支上进行开发，完成后再将它们合并到主分支上。\n我们通常在开发新功能、修复一个紧急 bug 等等时候会选择创建分支。单分支开发好还是多分支开发好，还是要看具体场景来说。\n创建一个名字叫做 test 的分支\ngit branch test 切换当前分支到 test（当你切换分支的时候，Git 会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。 Git 会自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样）\ngit checkout test 你也可以直接这样创建分支并切换过去(上面两条命令的合写)\ngit checkout -b feature_x 切换到主分支\ngit checkout master 合并分支(可能会有冲突)\ngit merge test 把新建的分支删掉\ngit branch -d feature_x 将分支推送到远端仓库（推送成功后其他人可见）：\ngit push origin 推荐 # 在线演示学习工具：\n「补充，来自issue729」Learn Git Branching https://oschina.gitee.io/learn-git-branching/ 。该网站可以方便的演示基本的git操作，讲解得明明白白。每一个基本命令的作用和结果。\n推荐阅读：\nGit - 简明指南 图解Git 猴子都能懂得Git入门 https://git-scm.com/book/en/v2 Generating a new SSH key and adding it to the ssh-agent 一个好的 Git 提交消息，出自 Linus 之手 "},{"id":110,"href":"/zh/docs/technology/Review/java_guide/lyaly_dev_tools/maven/","title":"maven","section":"开发工具","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n这部分内容主要根据 Maven 官方文档整理，做了对应的删减，主要保留比较重要的部分，不涉及实战，主要是一些重要概念的介绍。\nMaven 介绍 # Maven 官方文档是这样介绍的 Maven 的：\nApache Maven is a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project\u0026rsquo;s build, reporting and documentation from a central piece of information.\nApache Maven 的本质是一个软件项目管理和理解工具。基于项目对象模型 (Project Object Model，POM) 的概念，Maven 可以从一条中心信息 管理项目的构建、报告和文档。\n什么是 POM？ 每一个 Maven 工程都有一个 pom.xml 文件，位于根目录中，包含项目构建生命周期的详细信息。通过 pom.xml 文件，我们可以定义项目的坐标、项目依赖、项目信息、插件信息等等配置。\n对于开发者来说，Maven 的主要作用主要有 3 个：\n项目构建 ：提供标准的、跨平台的自动化项目构建方式。 依赖管理 ：方便快捷的管理项目依赖的资源（jar 包），避免资源间的版本冲突问题。 统一开发结构 ：提供标准的、统一的项目结构。 关于 Maven 的基本使用这里就不介绍了，建议看看官网的 5 分钟上手 Maven 的教程：Maven in 5 Minutes 。\nMaven 坐标 # 项目中依赖的第三方库以及插件可统称为构件。每一个构件都可以使用 Maven 坐标 唯一标识，坐标元素包括：\ngoupId(必须): 定义了当前 Maven 项目隶属的组织或公司。groupId 一般分为多段，通常情况下，第一段为域，第二段为公司名称。域又分为 org、com、cn 等，其中 org 为非营利组织，com 为商业组织，cn 表示中国。以 apache 开源社区的 tomcat 项目为例，这个项目的 groupId 是 org.apache，它的域是 org（因为 tomcat 是非营利项目），公司名称是 apache，artifactId 是 tomcat。 artifactId(必须)：定义了当前 Maven 项目的名称，项目的唯一的标识符，对应项目根目录的名称。 version(必须)： 定义了 Maven 项目当前所处版本。 packaging（可选）：定义了 Maven 项目的打包方式（比如 jar，war\u0026hellip;），默认使用 jar。 classifier(可选)：常用于区分从同一 POM 构建的具有不同内容的构件，可以是任意的字符串，附加在版本号之后。 只要你提供正确的坐标，就能从 Maven 仓库中找到相应的构件供我们使用。\n举个例子（引入阿里巴巴开源的 EasyExcel） ：\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;easyexcel\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 你可以在 https://mvnrepository.com/ 这个网站上找到几乎所有可用的构件，如果你的项目使用的是 Maven 作为构建工具，那这个网站你一定会经常接触。\nMaven 依赖 # 如果使用 Maven 构建产生的构件（例如 Jar 文件）被其他的项目引用，那么该构件就是其他项目的依赖。\n依赖配置 # 配置信息示例 ：\n\u0026lt;project\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;...\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;...\u0026lt;/scope\u0026gt; \u0026lt;optional\u0026gt;...\u0026lt;/optional\u0026gt; \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;groupId\u0026gt;...\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;...\u0026lt;/artifactId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 配置说明 ：\ndependencies： 一个 pom.xml 文件中只能存在一个这样的标签，是用来管理依赖的总标签。 dependency：包含在 dependencies 标签中，可以有多个，每一个表示项目的一个依赖。 groupId,artifactId,version(必要)：依赖的基本坐标，对于任何一个依赖来说，基本坐标是最重要的，Maven 根据坐标才能找到需要的依赖。我们在上面解释过这些元素的具体意思，这里就不重复提了。 type(可选)：依赖的类型，对应于项目坐标定义的 packaging。大部分情况下，该元素不必声明，其默认值是 jar。 scope(可选)：依赖的范围，默认值是 compile。 optional(可选)： 标记依赖是否可选 exclusions(可选)：用来排除传递性依赖,例如 jar 包冲突 依赖范围 # classpath 用于指定 .class 文件存放的位置，类加载器会从该路径中加载所需的 .class 文件到内存中。\nMaven 在编译、执行测试、实际运行有着三套不同的 classpath：\n编译 classpath ：编译主代码有效 测试 classpath ：编译、运行测试代码有效 运行 classpath ：项目运行时有效 Maven 的依赖范围如下：\ncompile：编译依赖范围（默认），使用此依赖范围对于编译、测试、运行三种都有效，即在编译、测试和运行的时候都要使用该依赖 Jar 包。 test：测试依赖范围，从字面意思就可以知道此依赖范围只能用于测试，而在编译和运行项目时无法使用此类依赖，典型的是 JUnit，它只用于编译 测试代码和运行测试代码的时候才需要。 provided ：此依赖范围，对于编译和测试有效，而对运行时无效。比如 servlet-api.jar 在 Tomcat 中已经提供了，我们只需要的是编译期提供而已。 runtime：运行时依赖范围，对于测试和运行有效，但是在编译主代码时无效，典型的就是 JDBC 驱动实现。 system：系统依赖范围，使用 system 范围的依赖时必须通过 systemPath 元素显示地指定依赖文件的路径，不依赖 Maven 仓库解析，所以可能会造成建构的不可移植。 传递依赖性 # 依赖冲突 # 1、对于 Maven 而言，同一个 groupId 同一个 artifactId 下，只能使用一个 version。\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;in.hocg.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.48\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- 只会使用 1.0.49 这个版本的依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;in.hocg.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.49\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 若相同类型但版本不同的依赖存在于同一个 pom 文件，只会引入后一个声明的依赖。\n2、项目的两个依赖同时引入了某个依赖。\n举个例子，项目存在下面这样的依赖关系：\n依赖链路一：A -\u0026gt; B -\u0026gt; C -\u0026gt; X(1.0) 依赖链路二：A -\u0026gt; D -\u0026gt; X(2.0) 这两条依赖路径上有两个版本的 X，为了避免依赖重复，Maven 只会选择其中的一个进行解析。\n哪个版本的 X 会被 Maven 解析使用呢?\nMaven 在遇到这种问题的时候，会遵循 路径最短优先 和 声明顺序优先 两大原则。解决这个问题的过程也被称为 Maven 依赖调解 。\n路径最短优先\n依赖链路一：A -\u0026gt; B -\u0026gt; C -\u0026gt; X(1.0) // dist = 3 依赖链路二：A -\u0026gt; D -\u0026gt; X(2.0) // dist = 2 依赖链路二的路径最短，因此，X(2.0)会被解析使用。\n不过，你也可以发现。路径最短优先原则并不是通用的，像下面这种路径长度相等的情况就不能单单通过其解决了：\n依赖链路一：A -\u0026gt; B -\u0026gt; X(1.0) // dist = 3 依赖链路二：A -\u0026gt; D -\u0026gt; X(2.0) // dist = 2 因此，Maven 又定义了声明顺序优先原则。\n依赖调解第一原则不能解决所有问题，比如这样的依赖关系：A-\u0026gt;B-\u0026gt;Y(1.0)、A-\u0026gt; C-\u0026gt;Y(2.0)，Y(1.0)和 Y(2.0)的依赖路径长度是一样的，都为 2。Maven 定义了依赖调解的第二原则：\n声明顺序优先\n在依赖路径长度相等的前提下，在 pom.xml 中依赖声明的顺序决定了谁会被解析使用，顺序最前的那个依赖优胜。该例中，如果 B 的依赖声明在 D 之前，那么 X (1.0)就会被解析使用。\n\u0026lt;!-- A pom.xml --\u0026gt; \u0026lt;dependencies\u0026gt; ... dependency B ... dependency D \u0026lt;/dependencies\u0026gt; 排除依赖 # 单纯依赖 Maven 来进行依赖调解，在很多情况下是不适用的，需要我们手动排除依赖。\n举个例子，当前项目存在下面这样的依赖关系：\n依赖链路一：A -\u0026gt; B -\u0026gt; C -\u0026gt; X(1.5) // dist = 3 依赖链路二：A -\u0026gt; D -\u0026gt; X(1.0) // dist = 2 根据路径最短优先原则，X(1.0) 会被解析使用，也就是说实际用的是 1.0 版本的 X。\n但是！！！这会一些问题：如果 D 依赖用到了 1.5 版本的 X 中才有的一个类，运行项目就会报NoClassDefFoundError错误。如果 D 依赖用到了 1.5 版本的 X 中才有的一个方法，运行项目就会报NoSuchMethodError错误。\n现在知道为什么你的 Maven 项目总是会报**NoClassDefFoundError和NoSuchMethodError**错误了吧？\n如何解决呢？ 我们可以通过exclusive标签手动将 X(1.0) 给排除。\n\u0026lt;dependencyB\u0026gt; ...... \u0026lt;exclusions\u0026gt; \u0026lt;exclusion\u0026gt; \u0026lt;artifactId\u0026gt;x\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;org.apache.x\u0026lt;/groupId\u0026gt; \u0026lt;/exclusion\u0026gt; \u0026lt;/exclusions\u0026gt; \u0026lt;/dependency\u0026gt; 一般我们在解决依赖冲突的时候，都会优先保留版本较高的。这是因为大部分 jar 在升级的时候都会做到向下兼容。\n如果高版本修改了低版本的一些类或者方法的话，这个时候就能直接保留高版本了，而是应该考虑优化上层依赖，比如升级上层依赖的版本。\n还是上面的例子：\n依赖链路一：A -\u0026gt; B -\u0026gt; C -\u0026gt; X(1.5) // dist = 3 依赖链路二：A -\u0026gt; D -\u0026gt; X(1.0) // dist = 2 我们保留了 1.5 版本的 X，但是这个版本的 X 删除了 1.0 版本中的某些类。这个时候，我们可以考虑升级 D 的版本到一个 X 兼容的版本。\nMaven 仓库 # 在 Maven 世界中，任何一个依赖、插件或者项目构建的输出，都可以称为 构件 。\n坐标和依赖是构件在 Maven 世界中的逻辑表示方式，构件的物理表示方式是文件，Maven 通过仓库来统一管理这些文件。 任何一个构件都有一组坐标唯一标识。有了仓库之后，无需手动引入构件，我们直接给定构件的坐标即可在 Maven 仓库中找到该构件。\nMaven 仓库分为：\n本地仓库 ：运行 Maven 的计算机上的一个目录，它缓存远程下载的构件并包含尚未发布的临时构件。settings.xml 文件中可以看到 Maven 的本地仓库路径配置，默认本地仓库路径是在 ${user.home}/.m2/repository。 远程仓库 ：官方或者其他组织维护的 Maven 仓库。 Maven 远程仓库可以分为：\n中央仓库 ：这个仓库是由 Maven 社区来维护的，里面存放了绝大多数开源软件的包，并且是作为 Maven 的默认配置，不需要开发者额外配置。另外为了方便查询，还提供了一个查询地址，开发者可以通过这个地址更快的搜索需要构件的坐标。 私服 ：私服是一种特殊的远程 Maven 仓库，它是架设在局域网内的仓库服务，私服一般被配置为互联网远程仓库的镜像，供局域网内的 Maven 用户使用。 其他的公共仓库 ：有一些公共仓库是未来加速访问（比如阿里云 Maven 镜像仓库）或者部分构件不存在于中央仓库中。 Maven 依赖包寻找顺序：\n先去本地仓库找寻，有的话，直接使用。 本地仓库没有找到的话，会去远程仓库找寻，下载包到本地仓库。 远程仓库没有找到的话，会报错。 Maven 生命周期 # Maven 的生命周期就是为了对所有的构建过程进行抽象和统一，包含了项目的清理、初始化、编译、测试、打包、集成测试、验证、部署和站点生成等几乎所有构建步骤。\nMaven 定义了 3 个生命周期META-INF/plexus/components.xml：\ndefault 生命周期 clean生命周期 site生命周期 这些生命周期是相互独立的，每个生命周期包含多个阶段(phase)。并且，这些阶段是有序的，也就是说，后面的阶段依赖于前面的阶段。当执行某个阶段的时候，会先执行它前面的阶段。\n执行 Maven 生命周期的命令格式如下：\nmvn 阶段 [阶段2] ...[阶段n] default 生命周期 # default生命周期是在没有任何关联插件的情况下定义的，是 Maven 的主要生命周期，用于构建应用程序，共包含 23 个阶段。\n\u0026lt;phases\u0026gt; \u0026lt;!-- 验证项目是否正确，并且所有必要的信息可用于完成构建过程 --\u0026gt; \u0026lt;phase\u0026gt;validate\u0026lt;/phase\u0026gt; \u0026lt;!-- 建立初始化状态，例如设置属性 --\u0026gt; \u0026lt;phase\u0026gt;initialize\u0026lt;/phase\u0026gt; \u0026lt;!-- 生成要包含在编译阶段的源代码 --\u0026gt; \u0026lt;phase\u0026gt;generate-sources\u0026lt;/phase\u0026gt; \u0026lt;!-- 处理源代码 --\u0026gt; \u0026lt;phase\u0026gt;process-sources\u0026lt;/phase\u0026gt; \u0026lt;!-- 生成要包含在包中的资源 --\u0026gt; \u0026lt;phase\u0026gt;generate-resources\u0026lt;/phase\u0026gt; \u0026lt;!-- 将资源复制并处理到目标目录中，为打包阶段做好准备。 --\u0026gt; \u0026lt;phase\u0026gt;process-resources\u0026lt;/phase\u0026gt; \u0026lt;!-- 编译项目的源代码 --\u0026gt; \u0026lt;phase\u0026gt;compile\u0026lt;/phase\u0026gt; \u0026lt;!-- 对编译生成的文件进行后处理，例如对 Java 类进行字节码增强/优化 --\u0026gt; \u0026lt;phase\u0026gt;process-classes\u0026lt;/phase\u0026gt; \u0026lt;!-- 生成要包含在编译阶段的任何测试源代码 --\u0026gt; \u0026lt;phase\u0026gt;generate-test-sources\u0026lt;/phase\u0026gt; \u0026lt;!-- 处理测试源代码 --\u0026gt; \u0026lt;phase\u0026gt;process-test-sources\u0026lt;/phase\u0026gt; \u0026lt;!-- 生成要包含在编译阶段的测试源代码 --\u0026gt; \u0026lt;phase\u0026gt;generate-test-resources\u0026lt;/phase\u0026gt; \u0026lt;!-- 处理从测试代码文件编译生成的文件 --\u0026gt; \u0026lt;phase\u0026gt;process-test-resources\u0026lt;/phase\u0026gt; \u0026lt;!-- 编译测试源代码 --\u0026gt; \u0026lt;phase\u0026gt;test-compile\u0026lt;/phase\u0026gt; \u0026lt;!-- 处理从测试代码文件编译生成的文件 --\u0026gt; \u0026lt;phase\u0026gt;process-test-classes\u0026lt;/phase\u0026gt; \u0026lt;!-- 使用合适的单元测试框架（Junit 就是其中之一）运行测试 --\u0026gt; \u0026lt;phase\u0026gt;test\u0026lt;/phase\u0026gt; \u0026lt;!-- 在实际打包之前，执行任何的必要的操作为打包做准备 --\u0026gt; \u0026lt;phase\u0026gt;prepare-package\u0026lt;/phase\u0026gt; \u0026lt;!-- 获取已编译的代码并将其打包成可分发的格式，例如 JAR、WAR 或 EAR 文件 --\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;!-- 在执行集成测试之前执行所需的操作。 例如，设置所需的环境 --\u0026gt; \u0026lt;phase\u0026gt;pre-integration-test\u0026lt;/phase\u0026gt; \u0026lt;!-- 处理并在必要时部署软件包到集成测试可以运行的环境 --\u0026gt; \u0026lt;phase\u0026gt;integration-test\u0026lt;/phase\u0026gt; \u0026lt;!-- 执行集成测试后执行所需的操作。 例如，清理环境 --\u0026gt; \u0026lt;phase\u0026gt;post-integration-test\u0026lt;/phase\u0026gt; \u0026lt;!-- 运行任何检查以验证打的包是否有效并符合质量标准。 --\u0026gt; \u0026lt;phase\u0026gt;verify\u0026lt;/phase\u0026gt; \u0026lt;!-- 将包安装到本地仓库中，可以作为本地其他项目的依赖 --\u0026gt; \u0026lt;phase\u0026gt;install\u0026lt;/phase\u0026gt; \u0026lt;!-- 将最终的项目包复制到远程仓库中与其他开发者和项目共享 --\u0026gt; \u0026lt;phase\u0026gt;deploy\u0026lt;/phase\u0026gt; \u0026lt;/phases\u0026gt; 根据前面提到的阶段间依赖关系理论，当我们执行 mvn test命令的时候，会执行从 validate 到 test 的所有阶段，这也就解释了为什么执行测试的时候，项目的代码能够自动编译。\nclean 生命周期 # clean 生命周期的目的是清理项目，共包含 3 个阶段：\npre-clean clean post-clean \u0026lt;phases\u0026gt; \u0026lt;!-- 执行一些需要在clean之前完成的工作 --\u0026gt; \u0026lt;phase\u0026gt;pre-clean\u0026lt;/phase\u0026gt; \u0026lt;!-- 移除所有上一次构建生成的文件 --\u0026gt; \u0026lt;phase\u0026gt;clean\u0026lt;/phase\u0026gt; \u0026lt;!-- 执行一些需要在clean之后立刻完成的工作 --\u0026gt; \u0026lt;phase\u0026gt;post-clean\u0026lt;/phase\u0026gt; \u0026lt;/phases\u0026gt; \u0026lt;default-phases\u0026gt; \u0026lt;clean\u0026gt; org.apache.maven.plugins:maven-clean-plugin:2.5:clean \u0026lt;/clean\u0026gt; \u0026lt;/default-phases\u0026gt; 根据前面提到的阶段间依赖关系理论，当我们执行 mvn clean 的时候，会执行 clean 生命周期中的 pre-clean 和 clean 阶段。\nsite 生命周期 # site 生命周期的目的是建立和发布项目站点，共包含 4 个阶段：\npre-site site post-site site-deploy \u0026lt;phases\u0026gt; \u0026lt;!-- 执行一些需要在生成站点文档之前完成的工作 --\u0026gt; \u0026lt;phase\u0026gt;pre-site\u0026lt;/phase\u0026gt; \u0026lt;!-- 生成项目的站点文档作 --\u0026gt; \u0026lt;phase\u0026gt;site\u0026lt;/phase\u0026gt; \u0026lt;!-- 执行一些需要在生成站点文档之后完成的工作，并且为部署做准备 --\u0026gt; \u0026lt;phase\u0026gt;post-site\u0026lt;/phase\u0026gt; \u0026lt;!-- 将生成的站点文档部署到特定的服务器上 --\u0026gt; \u0026lt;phase\u0026gt;site-deploy\u0026lt;/phase\u0026gt; \u0026lt;/phases\u0026gt; \u0026lt;default-phases\u0026gt; \u0026lt;site\u0026gt; org.apache.maven.plugins:maven-site-plugin:3.3:site \u0026lt;/site\u0026gt; \u0026lt;site-deploy\u0026gt; org.apache.maven.plugins:maven-site-plugin:3.3:deploy \u0026lt;/site-deploy\u0026gt; \u0026lt;/default-phases\u0026gt; Maven 能够基于 pom.xml 所包含的信息，自动生成一个友好的站点，方便团队交流和发布项目信息。\nMaven 插件 # Maven 本质上是一个插件执行框架，所有的执行过程，都是由一个一个插件独立完成的。像咱们日常使用到的 install、clean、deploy 等命令，其实底层都是一个一个的 Maven 插件。关于 Maven 的核心插件可以参考官方的这篇文档：https://maven.apache.org/plugins/index.html 。\n除了 Maven 自带的插件之外，还有一些三方提供的插件比如单测覆盖率插件 jacoco-maven-plugin、帮助开发检测代码中不合规范的地方的插件 maven-checkstyle-plugin、分析代码质量的 sonar-maven-plugin。并且，我们还可以自定义插件来满足自己的需求。\njacoco-maven-plugin 使用示例：\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.jacoco\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jacoco-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.8.8\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;prepare-agent\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;generate-code-coverage-report\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;report\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 你可以将 Maven 插件理解为一组任务的集合，用户可以通过命令行直接运行指定插件的任务，也可以将插件任务挂载到构建生命周期，随着生命周期运行。\nMaven 插件被分为下面两种类型：\nBuild plugins ：在构建时执行。 Reporting plugins：在网站生成过程中执行。 Maven 多模块管理 # 多模块管理简单地来说就是将一个项目分为多个模块，每个模块只负责单一的功能实现。直观的表现就是一个 Maven 项目中不止有一个 pom.xml 文件，会在不同的目录中有多个 pom.xml 文件，进而实现多模块管理。\n多模块管理除了可以更加便于项目开发和管理，还有如下好处：\n降低代码之间的耦合性（从类级别的耦合提升到 jar 包级别的耦合）； 减少重复，提升复用性； 每个模块都可以是自解释的（通过模块名或者模块文档）； 模块还规范了代码边界的划分，开发者很容易通过模块确定自己所负责的内容。 多模块管理下，会有一个父模块，其他的都是子模块。父模块通常只有一个 pom.xml，没有其他内容。父模块的 pom.xml 一般只定义了各个依赖的版本号、包含哪些子模块以及插件有哪些。不过，要注意的是，如果依赖只在某个子项目中使用，则可以在子项目的 pom.xml 中直接引入，防止父 pom 的过于臃肿。\n如下图所示，Dubbo 项目就被分成了多个子模块比如 dubbo-common（公共逻辑模块）、dubbo-remoting（远程通讯模块）、dubbo-rpc（远程调用模块）。\n文章推荐 # 安全同学讲 Maven 间接依赖场景的仲裁机制 - 阿里开发者 - 2022 高效使用 Java 构建工具｜ Maven 篇 - 阿里开发者 - 2022 安全同学讲 Maven 重打包的故事 - 阿里开发者 - 2022 参考 # 《Maven 实战》 Introduction to Repositories - Maven 官方文档：https://maven.apache.org/guides/introduction/introduction-to-repositories.html Introduction to the Build Lifecycle - Maven 官方文档：https://maven.apache.org/guides/introduction/introduction-to-the-lifecycle.html#Lifecycle_Reference Maven 依赖范围：http://www.mvnbook.com/maven-dependency.html 解决 maven 依赖冲突，这篇就够了！：https://www.cnblogs.com/qdhxhz/p/16363532.html Multi-Module Project with Maven：https://www.baeldung.com/maven-multi-module "},{"id":111,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly030301lyatomicpre/","title":"Atomic预备知识","section":"并发","content":" Java实现CAS的原理[非javaguide] # i++是非线程安全的，因为i++不是原子操作；可以使用synchronized和CAS实现加锁\nsynchronized是悲观锁，一旦获得锁，其他线程进入后就会阻塞等待锁；而CAS是乐观锁，执行时不会加锁，假设没有冲突，如果因为冲突失败了就重试，直到成功\n乐观锁和悲观锁\n这是一种分类方式 悲观锁，总是认为每次访问共享资源会发生冲突，所以必须对每次数据操作加锁，以保证临界区的程序同一时间只能有一个线程在执行 乐观锁，又称**“无锁”**，假设对共享资源访问没有冲突，线程可以不停的执行，无需加锁无需等待；一旦发生冲突，通常是使用一种称为CAS的技术保证线程执行安全 无锁没有锁的存在，因此不可能发生死锁，即乐观锁天生免疫死锁 乐观锁用于**“读多写少”的环境，避免加锁频繁影响性能；悲观锁用于“写多读少”，避免频繁失败及重试**影响性能 CAS概念，即CompareAndSwap ，比较和交换，CAS中，有三个值（概念上）\nV：要更新的变量(var)；E：期望值（expected）；N：新值（new） 判断V是否等于E，如果等于，将V的值设置为N；如果不等，说明已经有其它线程更新了V，则当前线程放弃更新，什么都不做。 一般来说，预期值E本质上指的是“旧值”（判断是否修改了）\n如果有一个多个线程共享的变量i原本等于5，我现在在线程A中，想把它设置为新的值6; 我们使用CAS来做这个事情； （首先要把原来的值5在线程中保存起来） 接下来是原子操作：首先我们用（现在的i）去与5对比，发现它等于5，说明没有被其它线程改过，那我就把它设置为新的值6，此次CAS成功，i的值被设置成了6； 如果不等于5，说明i被其它线程改过了（比如现在i的值为2），那么我就什么也不做，此次CAS失败，i的值仍然为2。 其中i为V，5为E，6为N\nCAS是一种原子操作，它是一种系统原语，是一条CPU原子指令，从CPU层面保证它的原子性（不可能出现说，判断了对比了i为5之后，正准备更新它的值，此时该值被其他线程改了）\n当多个线程同时使用CAS操作一个变量时，只有一个会胜出，并成功更新，其余均会失败，但失败的线程并不会被挂起，仅是被告知失败，并且允许再次尝试，当然也允许失败的线程放弃操作。\nJava实现CAS的原理 - Unsafe类\n在Java中，如果一个方法是native的，那Java就不负责具体实现它，而是交给底层的JVM使用c或者c++去实现\nJava中有一个Unsafe类，在sun.misc包中，里面有一些native方法，其中包括：\nboolean compareAndSwapObject(Object o, long offset,Object expected, Object x); boolean compareAndSwapInt(Object o, long offset,int expected,int x); boolean compareAndSwapLong(Object o, long offset,long expected,long x); //------\u0026gt;AtomicInteger.class public class AtomicInteger extends Number implements java.io.Serializable { private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } }\nUnsafe中对CAS的实现是C++写的，它的具体实现和操作系统、CPU都有关系。Linux的X86中主要通过cmpxchgl这个指令在CPU级完成CAS操作，如果是多处理器则必须使用lock指令加锁\nUnsafe类中还有park(线程挂起)和unpark(线程恢复)，LockSupport底层则调用了该方法；还有支持反射操作的allocateInstance()\n原子操作- AtomicInteger类源码简析 JDK提供了一些原子操作的类，在java.util.concurrent.atomic包下面，JDK11中有如下17个类 包括 原子更新基本类型，原子更新数组，原子更新引用，原子更新字段(属性)\n其中，AtomicInteger类的getAndAdd(int data)\npublic final int getAndAdd(int delta) { return unsafe.getAndAddInt(this, valueOffset, delta); } //unsafe字段 private static final jdk.internal.misc.Unsafe U = jdk.internal.misc.Unsafe.getUnsafe(); //上面方法实际调用 @HotSpotIntrinsicCandidate public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!weakCompareAndSetInt(o, offset, v, v + delta)); return v; } //对于offset，这是一个对象偏移量，用于获取某个字段相对Java对象的起始地址的偏移量 /* 一个java对象可以看成是一段内存，各个字段都得按照一定的顺序放在这段内存里，同时考虑到对齐要求，可能这些字段不是连续放置的， 用这个方法能准确地告诉你某个字段相对于对象的起始内存地址的字节偏移量，因为是相对偏移量，所以它其实跟某个具体对象又没什么太大关系，跟class的定义和虚拟机的内存模型的实现细节更相关。 */ public class AtomicInteger extends Number implements java.io.Serializable { private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } } 再重新看这段代码\n@HotSpotIntrinsicCandidate public final int getAndAddInt(Object o, long offset, int delta) { int v; do { v = getIntVolatile(o, offset); } while (!weakCompareAndSetInt(o, offset, v, v + delta)); return v; } 这里声明了v，即要返回的值，即不论如何都会返回原来的值(更新成功前的值)，然后新的值为v+delta\n使用do-while保证所有循环至少执行一遍\n循环体的条件是一个CAS方法：\npublic final boolean weakCompareAndSetInt(Object o, long offset, int expected, int x) { return compareAndSetInt(o, offset, expected, x); } public final native boolean compareAndSetInt(Object o, long offset, int expected, int x); 最终调用了native方法：compareAndSetInt方法\n为甚么要经过一层weakCompareAndSetInt，在JDK 8及之前的版本，这两个方法是一样的。\n而在JDK 9开始，这两个方法上面增加了@HotSpotIntrinsicCandidate注解。这个注解允许HotSpot VM自己来写汇编或IR编译器来实现该方法以提供性能。也就是说虽然外面看到的在JDK9中weakCompareAndSet和compareAndSet底层依旧是调用了一样的代码，但是不排除HotSpot VM会手动来实现weakCompareAndSet真正含义的功能的可能性。\n简单来说，weakCompareAndSet操作仅保留了volatile自身变量的特性，而除去了happens-before规则带来的内存语义。也就是说，weakCompareAndSet**无法保证处理操作目标的volatile变量外的其他变量的执行顺序( 编译器和处理器为了优化程序性能而对指令序列进行重新排序 )，同时也无法保证这些变量的可见性。**这在一定程度上可以提高性能。（没看懂）\nCAS如果旧值V不等于预期值E，它就会更新失败。说明旧的值发生了变化。那我们当然需要返回的是被其他线程改变之后的旧值了，因此放在了do循环体内\nCAS实现原子操作的三大问题\nABA问题\n就是一个值原来是A，变成了B，又变回了A。这个时候使用CAS是检查不出变化的，但实际上却被更新了两次\n在变量前面追加上版本号或者时间戳。从JDK 1.5开始，JDK的atomic包里提供了一个类AtomicStampedReference类来解决ABA问题\nAtomicStampedReference类的compareAndSet方法的作用是首先检查当前引用是否等于预期引用，并且检查当前标志是否等于预期标志，如果二者都相等，才使用CAS设置为新的值和标志。\npublic boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) { Pair\u0026lt;V\u0026gt; current = pair; return expectedReference == current.reference \u0026amp;\u0026amp; expectedStamp == current.stamp \u0026amp;\u0026amp; ((newReference == current.reference \u0026amp;\u0026amp; newStamp == current.stamp) || casPair(current, Pair.of(newReference, newStamp))); } 循环时间长开销大\nCAS多与自旋结合，如果自旋CAS长时间不成功，则会占用大量CPU资源，解决思路是让JVM支持处理器提供的pause指令\npause指令能让自旋失败时cpu睡眠一小段时间再继续自旋，从而使得读操作的频率低很多,为解决内存顺序冲突而导致的CPU流水线重排的代价也会小很多。\n限制次数（如果可以放弃操作的话）\n只能保证一个共享变量的原子操作\n使用JDK 1.5开始就提供的AtomicReference类保证对象之间的原子性，把多个变量放到一个对象里面进行CAS操作； 使用锁。锁内的临界区代码可以保证只有当前线程能操作。 AtomicInteger的使用[非javaguide] # //AtomicInteger类常用方法(下面的自增，都使用了CAS，是同步安全的) ublic final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 ------ //使用如下 class AtomicIntegerTest { private AtomicInteger count = new AtomicInteger(); //使用AtomicInteger之后，不需要对该方法加锁，也可以实现线程安全。 public void increment() { count.incrementAndGet(); } public int getCount() { return count.get(); } } 浅谈AtomicInteger实现原理[非javaguide] # 位于Java.util.concurrent.atomic包下，对int封装，提供原子性的访问和更新操作，其原子性操作的实现基于CAS（CompareAndSet）\nCAS，比较并交换，Java并发中lock-free机制的基础，调用Sun的Unsafe的CompareAndSwapInt完成，为native方法，基于CPU的CAS指令来实现的，即无阻塞；且为CAS原语\nCAS：三个参数，1. 当前内存值V 2.旧的预期值 3.即将更新的值，当且仅当预期值A和内存值相同时，将内存值改为 8 并返回true；否则返回false 在JAVA中,CAS通过调用C++库实现，由C++库再去调用CPU指令集。\nCAS确定\nABA　问题 如果期间发生了 A -\u0026gt; B -\u0026gt; A 的更新，仅仅判断数值是 A，可能导致不合理的修改操作；为此，提供了AtomicStampedReference 工具类，为引用建立类似版本号ｓｔａｍｐ的方式\n循环时间长，开销大。CAS适用于竞争情况短暂的情况，有需要的时候要限制自旋次数，以免过度消耗CPU\n只能保证一个共享变量的原子操作 对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁；或者取巧一下，比如 i = 2 , j = a ，合并后为 ij = 2a ，然后cas操作2a\nJava1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作，例子如下： 如图，它是同时更新了两个变量，而这两个变量都在新的对象上，所以就能解决多个共享变量的问题，即“将问题转换成，如果变量更新了，则更换一个对象”\nAtomicInteger原理浅析\n一些公共属性：\npublic class AtomicInteger extends Number implements java.io.Serializable { private static final long serialVersionUID = 6214790243416807050L; // setup to use Unsafe.compareAndSwapInt for updates private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; } AtomicInteger，根据valueOffset代表的该变量值，在内存中的偏移地址，从而获取数据；且value用volatile修饰，保证多线程之间的可见性\npublic final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1); } //unsafe.getAndAddInt public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4));//先获取var1对象的偏移量为var2的内存地址上的值【现在的实际值】 //如果此刻还是var5，+1并赋值，否则重新获取 return var5; } 假设线程1和线程2通过getIntVolatile拿到value的值都为1，线程1被挂起，线程2继续执行 （这里是非原子的哦） 线程2在compareAndSwapInt操作中由于预期值和内存值都为1，因此成功将内存值更新为2 线程1继续执行，在compareAndSwapInt操作中，预期值是1，而当前的内存值为2，CAS操作失败，什么都不做，返回false 线程1重新通过getIntVolatile拿到最新的value为2，再进行一次compareAndSwapInt操作，这次操作成功，内存值更新为3 原子操作的实现原理\nJava中的CAS操作正是利用了处理器提供的CMPXCHG指令实现的。自旋CAS实现的基本思路就是循环进行CAS操作直到操作成功为止。 在CAS中有三个操作数：分别是内存地址（在Java中可以简单理解为变量的内存地址，用V表示，要获取实时值）、旧的预期值（用A表示,[操作之前保存的]）和新值（用B表示）。CAS指令执行时，当且仅当V符合旧的预期值A时，处理器才会用新值B更新V的值，否则他就不执行更新，但无论是否更新了V的值，都会返回V的旧值。(这里说的三个值，指的是逻辑概念，而不是实际概念) "},{"id":112,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0611lymysql-high-performance-optimization-specification-recommendations/","title":"MySQL高性能优化规范建议总结","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n索引优化相关\nin 代替 or not exist 代替 not in 数据库命名规范 # 所有数据库对象名称必须使用小写字母并用下划线分割 所有数据库对象名称禁止使用 MySQL 保留关键字（如果表名中包含关键字查询时，需要将其用单引号括起来） 数据库对象的命名要能做到见名识意，并且最好不要超过 32 个字符 临时库表必须以 tmp_ 为前缀并以日期为后缀，备份表必须以 bak_ 为前缀并以日期 (时间戳) 为后缀 所有存储相同数据的列名和列类型必须一致（一般作为关联列，如果查询时关联列类型不一致会自动进行数据类型隐式转换，会造成列上的索引失效，导致查询效率降低） 数据库基本设计规范 # 所有表必须使用InnoDB存储引擎 # 没有特殊要求（即 InnoDB 无法满足的功能如：列存储，存储空间数据等）的情况下，所有表必须使用 InnoDB 存储引擎（MySQL5.5 之前默认使用 Myisam，5.6 以后默认的为 InnoDB）。 InnoDB 支持事务，支持行级锁，更好的恢复性，高并发下性能更好 数据库和表的字符集统一使用UTF-8 # 兼容性更好，统一字符集可以避免由于字符集转换产生的乱码，不同的字符集进行比较前需要进行转换会造成索引失效，如果数据库中有存储 emoji 表情的需要，字符集需要采用 utf8mb4 字符集。\n参考文章：\nMySQL 字符集不一致导致索引失效的一个真实案例open in new window [MySQL 字符集详解 所有表和字段都需要添加注释 # 使用 comment 从句添加表和列的备注，从一开始就进行数据字典的维护\n尽量控制单表数据量的大小，建议控制在500万以内 # 500 万并不是 MySQL 数据库的限制，过大会造成修改表结构，备份，恢复都会有很大的问题。\n可以用历史数据归档（应用于日志数据），**分库分表（应用于业务数据）**等手段来控制数据量大小\n谨慎使用MySQL分区表 # 分区表在物理上表现为多个文件，在逻辑上表现为一个表；\n谨慎选择分区键，跨分区查询效率可能更低；\n建议采用物理分表的方式管理大数据。\n经常一起使用的列放到一个表中 # 避免更多的关联操作。\n禁止在表中建立预留字段 # 预留字段的命名很难做到见名识义。 预留字段无法确认存储的数据类型，所以无法选择合适的类型。 对预留字段类型的修改，会对表进行锁定 禁止在数据库中存储文本（比如图片）这类大的二进制数据 # 在数据库中存储文件会严重影响数据库性能，消耗过多存储空间。\n文件（比如图片）这类大的二进制数据通常存储于文件服务器，数据库只存储文件地址信息。\n不要被数据库范式所束缚 # 一般来说，设计关系数据库时需要满足第三范式，但为了满足第三范式，我们可能会拆分出多张表。而在进行查询时需要对多张表进行关联查询，有时为了提高查询效率，会降低范式的要求，在表中保存一定的冗余信息，也叫做反范式。但要注意反范式一定要适度。\n禁止在线上做数据库压力测试 # 禁止从开发环境、测试环境，直接连接生产环境数据库 # 安全隐患极大，要对生产环境抱有敬畏之心！\n数据库字段设计规范 # 优先选择符合存储需要的最小数据类型 # Byte：字节\n存储字节越小，占用也就空间越小，性能也越好。\na.某些字符串可以转换成数字类型存储比如可以将 IP 地址转换成整型数据。\n数字是连续的，性能更好，占用空间也更小。\nMySQL 提供了两个方法来处理 ip 地址\nINET_ATON() ： 把 ip 转为无符号整型 (4-8 位) INET_NTOA() :把整型的 ip 转为地址 插入数据前，先用 INET_ATON() 把 ip 地址转为整型，显示数据时，使用 INET_NTOA() 把整型的 ip 地址转为地址显示即可。\nb.对于非负型的数据 (如自增 ID,整型 IP，年龄) 来说,要优先使用无符号整型来存储。\n无符号相对于有符号可以多出一倍的存储空间\nSIGNED INT -2147483648~2147483647 UNSIGNED INT 0~4294967295 c.小数值类型（比如年龄、状态表示如 0/1）优先使用 TINYINT 类型。\n避免使用TEXT、BLOB数据类型，最常见的TEXT类型可以存储64k的数据 # a. 建议把 BLOB 或是 TEXT 列分离到单独的扩展表中。\nMySQL 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。而且对于这种数据，MySQL 还是要进行二次查询，会使 sql 性能变得很差，但是不是说一定不能使用这样的数据类型。\n如果一定要使用，建议把 BLOB 或是 TEXT 列分离到单独的扩展表中，查询时一定不要使用 select *而只需要取出必要的列，不需要 TEXT 列的数据时不要对该列进行查询。\n2、TEXT 或 BLOB 类型只能使用前缀索引\n因为 MySQL 对索引字段长度是有限制的，所以 TEXT 类型只能使用前缀索引，并且 TEXT 列上是不能有默认值的\n避免使用ENUM类型 # 原因：\n修改 ENUM 值需要使用 ALTER 语句； ENUM 类型的 ORDER BY 操作效率低，需要额外操作； ENUM 数据类型存在一些限制比如建议不要使用数值作为 ENUM 的枚举值。 相关阅读：是否推荐使用 MySQL 的 enum 类型？ - 架构文摘 - 知乎open in new window\n尽可能把所有的列定义为NOT NULL # 除非有特别的原因使用 NULL 值，应该总是让字段保持 NOT NULL。\n索引 NULL 列需要额外的空间来保存，所以要占用更多的空间；\n该位指示了该行数据中是否有NULL值，有则使用1来表示，该部分占的字节大小大概为1字节，比如当NULL标志位为06时，06转换为二进制为110，表示第二列和第三列为NULL。\n进行比较和计算时要对 NULL 值做特别的处理。\n相关阅读：技术分享 | MySQL 默认值选型（是空，还是 NULL）open in new window 。\n使用TIMESTAMP(4个字节)或DATETIME类型（8个字节）存储时间 # TIMESTAMP 存储的时间范围 1970-01-01 00:00:01 ~ 2038-01-19-03:14:07\nTIMESTAMP 占用 4 字节和 INT 相同，但比 INT 可读性高\n超出 TIMESTAMP 取值范围的使用 DATETIME 类型存储\n反之， 经常会有人用字符串存储日期型的数据**（不正确的做法）**\n缺点 1：无法用日期函数进行计算和比较 缺点 2：用字符串存储日期要占用更多的空间 同财务相关的金额类数据必须使用decimal类型 # 非精准浮点 ：float,double 精准浮点 ：decimal decimal 类型为精准浮点数，在计算时不会丢失精度。占用空间由定义的宽度决定，每 4 个字节可以存储 9 位数字，并且小数点要占用一个字节。并且，decimal 可用于存储比 bigint 更大的整型数据\n不过， 由于 decimal 需要额外的空间和计算开销，应该尽量只在需要对数据进行精确计算时才使用 decimal 。\n单表不要包含过多字段 # 如果一个表包含过多字段的话，可以考虑将其分解成多个表，必要时增加中间表进行关联。\n索引设计规范 # 限制每张表上的索引数量，建议单张表索引不超过5个 # 索引并不是越多越好！索引可以提高效率同样可以降低效率。\n索引可以增加查询效率，但同样也会降低插入和更新的效率，甚至有些情况下会降低查询效率。\n因为 MySQL 优化器在选择如何优化查询时，会根据统一信息，对每一个可以用到的索引来进行评估，以生成出一个最好的执行计划，如果同时有很多个索引都可以用于查询，就会增加 MySQL 优化器生成执行计划的时间，同样会降低查询性能。\n禁止使用全文索引 # 全文索引不适用于 OLTP 场景。\nOn-Line Transaction Processing联机事务处理过程(OLTP)，也称为面向交易的处理过程\n禁止给表中的每一列都建立单独的索引 # 5.6 版本之前，一个 sql 只能使用到一个表中的一个索引，5.6 以后，虽然有了合并索引的优化方式，但是还是远远没有使用一个联合索引的查询方式好\n尽量使用联合索引\n每个InnoDB表必须有个主键 # InnoDB 是一种索引组织表：数据的存储的逻辑顺序和索引的顺序是相同的。每个表都可以有多个索引，但是表的存储顺序只能有一种。 InnoDB 是按照主键索引的顺序来组织表的 不要使用更新频繁的列作为主键，不适用(使用)多列主键（相当于联合索引） 不要使用 UUID,MD5,HASH,字符串列作为主键（无法保证数据的顺序增长） 主键建议使用自增 ID 值 常见索引列建议 # 出现在 SELECT、UPDATE、DELETE 语句的 WHERE 从句中的列\n包含在 ORDER BY、GROUP BY、DISTINCT 中的字段\n并不要将符合 1 和 2 中的字段的列都建立一个索引， 通常将 1、2 中的字段建立联合索引效果更好\n多表 join 的关联列\n如何选择索引列的顺序 # 建立索引的目的是：希望通过索引进行数据查找，减少随机 IO，增加查询性能 ，索引能过滤出越少的数据，则从磁盘中读入的数据也就越少。\n区分度最高的放在联合索引的最左侧（区分度=列中不同值的数量/列的总行数） 尽量把字段长度小的列放在联合索引的最左侧（因为字段长度越小，一页能存储的数据量越大，IO 性能也就越好） 使用最频繁的列放到联合索引的左侧（这样可以比较少的建立一些索引） 避免建立冗余索引和重复索引（增加了查询优化器生成执行计划的时间） # 重复索引示例：primary key(id)、index(id)、unique index(id) 冗余索引示例：index(a,b,c)、index(a,b)、index(a) 对于频繁的查询有优先考虑使用覆盖索引 # 覆盖索引：就是包含了所有查询字段 (where,select,order by,group by 包含的字段) 的索引\n覆盖索引的好处：\n避免 InnoDB 表进行索引的二次查询: InnoDB 是以聚集索引的顺序来存储的，对于 InnoDB 来说，二级索引在叶子节点中所保存的是行的主键信息，如果是用二级索引查询数据的话，在查找到相应的键值后，还要通过主键进行二次查询才能获取我们真实所需要的数据。而在覆盖索引中，二级索引的键值中可以获取所有的数据，避免了对主键的二次查询 ，减少了 IO 操作，提升了查询效率。 可以把随机 IO 变成顺序 IO 加快查询效率: 由于覆盖索引是按键值的顺序存储的，对于 IO 密集型的范围查找来说，对比随机从磁盘读取每一行的数据 IO 要少的多，因此利用覆盖索引在访问时也可以把磁盘的随机读取的 IO 转变成索引查找的顺序 IO。 索引SET规范 # 尽量避免使用外键约束\n不建议使用外键约束（foreign key），但一定要在表与表之间的关联键上建立索引 外键可用于保证数据的参照完整性，但建议在业务端实现 外键会影响父表和子表的写操作从而降低性能 数据库SQL开发规范 # 优化对性能影响较大的SQL语句 # 要找到最需要优化的 SQL 语句。要么是使用最频繁的语句，要么是优化后提高最明显的语句，可以通过查询 MySQL 的慢查询日志来发现需要进行优化的 SQL 语句；\n充分利用表上已经存在的索引 # 避免使用双%号的查询条件。如：a like '%123%'，（如果无前置%,只有后置%，是可以用到列上的索引的）\n一个 SQL 只能利用到复合索引中的一列进行范围查询。如：有 a,b,c 列的联合索引，在查询条件中有 a 列的范围查询，则在 b,c 列上的索引将不会被用到。\nhttps://blog.csdn.net/qq_33589510/article/details/123038988\n(a=1 b=1 c=1) (a=1 b=2 c=1) (a=1 b=2 c=3)\n(a=2 b=2 c=3) (a=2 b=2 c=5) (a=2 b=5 c=1) (a=2 b=5 c=2)\n(a=3 b=0 c=1) (a=3 b=3 c=5) (a=3 b=8 c=6)\n假设有一条SQL为select a,b,c from table where a = 2 and b \u0026gt;1 and c = 2，那么索引c就用不到了，因为有可能b查找后c是无序的了\n在定义联合索引时，如果 a 列要用到范围查找的话，就要把 a 列放到联合索引的右侧，使用 left join 或 not exists 来优化 not in 操作，因为 not in 也通常会使用索引失效。\n这个的意思是，如果有两个列，b，a都是索引\nSELECT * FROM table WHERE a \u0026gt; 1 and b = 2;\n对于上面这句，如果建立(a，b)，那么只有a会用得到。而如果建立(b，a)，则都能用上 （如果没有b= 2，那么（b，a）索引就用不上了）\n禁止使用SELECT * 必须使用SELECT \u0026lt;字段列表\u0026gt; 查询 # SELECT * 消耗更多的 CPU 和 IO 以网络带宽资源 SELECT * 无法使用覆盖索引 SELECT \u0026lt;字段列表\u0026gt; 可减少表结构变更带来的影响 禁止使用不含字段列表的INSERT语句 # 如：\ninsert into t values (\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;); 应使用：\ninsert into t(c1,c2,c3) values (\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;); 建议使用预编译语句进行数据库操作 # （这里应该是针对jdbc，不是指mybatis的情况）\n例子：\nMySQL执行预编译分为如三步：\n执行预编译语句，例如：prepare myfun from \u0026lsquo;select * from t_book where bid=?\u0026rsquo;\n设置变量，例如：set @str=\u0026lsquo;b1\u0026rsquo;\n执行语句，例如：execute myfun using @str\n如果需要再次执行myfun，那么就不再需要第一步，即不需要再编译语句了：\n设置变量，例如：set @str=\u0026lsquo;b2\u0026rsquo; 执行语句，例如：execute myfun using @str 通过查看MySQL日志可以看到执行的过程：\n使用Statement执行预编译\n**使用Statement执行预编译就是把上面的SQL语句执行一次。 **\nConnection con = JdbcUtils.getConnection(); Statement stmt = con.createStatement(); stmt.executeUpdate(\u0026#34;prepare myfun from \u0026#39;select * from t_book where bid=?\u0026#39;\u0026#34;); stmt.executeUpdate(\u0026#34;set @str=\u0026#39;b1\u0026#39;\u0026#34;); ResultSet rs = stmt.executeQuery(\u0026#34;execute myfun using @str\u0026#34;); while(rs.next()) { System.out.print(rs.getString(1) + \u0026#34;, \u0026#34;); System.out.print(rs.getString(2) + \u0026#34;, \u0026#34;); System.out.print(rs.getString(3) + \u0026#34;, \u0026#34;); System.out.println(rs.getString(4)); } stmt.executeUpdate(\u0026#34;set @str=\u0026#39;b2\u0026#39;\u0026#34;); rs = stmt.executeQuery(\u0026#34;execute myfun using @str\u0026#34;); while(rs.next()) { System.out.print(rs.getString(1) + \u0026#34;, \u0026#34;); System.out.print(rs.getString(2) + \u0026#34;, \u0026#34;); System.out.print(rs.getString(3) + \u0026#34;, \u0026#34;); System.out.println(rs.getString(4)); } rs.close(); stmt.close(); con.close(); useServerPrepStmts参数\n默认使用PreparedStatement是不能执行预编译的，这需要在url中给出useServerPrepStmts=true参数（MySQL Server 4.1之前的版本是不支持预编译的，而Connector/J在5.0.5以后的版本，默认是没有开启预编译功能的）。\n例如：jdbc:mysql://localhost:3306/test?useServerPrepStmts=true\n预编译语句可以重复使用这些计划，减少 SQL 编译所需要的时间，还可以解决动态 SQL 所带来的 SQL 注入的问题。\n只传参数，比传递 SQL 语句更高效。\n相同语句可以一次解析，多次使用，提高处理效率。\n避免数据类型的隐式转换 # 隐式转换会导致索引失效如: 这里id应该不是字符型(但是这个好像是例外，如果字段是数字，而查询的是字符，索引还是有效的)\nselect name,phone from customer where id = \u0026#39;111\u0026#39;; 详细解读可以看：MySQL 中的隐式转换造成的索引失效 这篇文章\n避免使用子查询，可以把子查询优化为join操作 # 通常子查询在 in 子句中，且子查询中为简单 SQL(不包含 union、group by、order by、limit 从句) 时,才可以把子查询转化为关联查询进行优化。\n子查询性能差的原因： 子查询的结果集无法使用索引，通常子查询的结果集会被存储到临时表中，不论是内存临时表还是磁盘临时表都不会存在索引，所以查询性能会受到一定的影响。特别是对于返回结果集比较大的子查询，其对查询性能的影响也就越大。由于子查询会产生大量的临时表也没有索引，所以会消耗过多的 CPU 和 IO 资源，产生大量的慢查询。\n避免JOIN关联太多的表 # 对于 MySQL 来说，是存在关联缓存的，缓存的大小可以由 join_buffer_size 参数进行设置。\n在 MySQL 中，对于同一个 SQL 多关联（join）一个表，就会多分配一个关联缓存，如果在一个 SQL 中关联的表越多，所占用的内存也就越大。\n如果程序中大量的使用了多表关联的操作，同时 join_buffer_size 设置的也不合理的情况下，就容易造成服务器内存溢出的情况，就会影响到服务器数据库性能的稳定性。\n同时对于关联操作来说，会产生临时表操作，影响查询效率，MySQL 最多允许关联 61 个表，建议不超过 5 个。\n减少同数据库的交互次数 # 数据库更适合处理批量操作，合并多个相同的操作到一起，可以提高处理效率。\n对应同一列进行or判断时，使用in代替or # in 的值不要超过 500 个，in 操作可以更有效的利用索引，or 大多数情况下很少能利用到索引。\n禁止使用order by rand() 进行随机排序 # order by rand() 会把表中所有符合条件的数据装载到内存中，然后在内存中对所有数据根据随机生成的值进行排序，并且可能会对每一行都生成一个随机值，如果满足条件的数据集非常大，就会消耗大量的 CPU 和 IO 及内存资源。\n推荐在程序中获取一个随机值，然后从数据库中获取数据的方式。\nWHERE从句中禁止对列进行函数转换和计算 # 对列进行函数转换或计算时会导致无法使用索引\n不推荐：\nwhere date(create_time)=\u0026#39;20190101\u0026#39; 推荐：\nwhere create_time \u0026gt;= \u0026#39;20190101\u0026#39; and create_time \u0026lt; \u0026#39;20190102\u0026#39; 在明显不会有重复值时使用UNION ALL 而不是 UNION # UNION 会把两个结果集的所有数据放到临时表中后再进行去重操作 UNION ALL 不会再对结果集进行去重操作 拆分复杂的大SQL为多个小SQL # 大 SQL 逻辑上比较复杂，需要占用大量 CPU 进行计算的 SQL MySQL 中，一个 SQL 只能使用一个 CPU 进行计算 SQL 拆分后可以通过并行执行来提高处理效率 程序连接不同的数据库使用不同的账号，禁止跨库查询 # 为数据库迁移和分库分表留出余地 降低业务耦合度 避免权限过大而产生的安全风险 数据库操作行为规范 # 超 100 万行的批量写 (UPDATE,DELETE,INSERT) 操作,要分批多次进行操作 # 大批量操作可能会造成严重的主从延迟\n主从环境中,大批量操作可能会造成严重的主从延迟，大批量的写操作一般都需要执行一定长的时间，而只有当主库上执行完成后，才会在其他从库上执行，所以会造成主库与从库长时间的延迟情况\nbinlog 日志为 row 格式时会产生大量的日志\n大批量写操作会产生大量日志，特别是对于 row 格式二进制数据而言，由于在 row 格式中会记录每一行数据的修改，我们一次修改的数据越多，产生的日志量也就会越多，日志的传输和恢复所需要的时间也就越长，这也是造成主从延迟的一个原因\n避免产生大事务操作\n大批量修改数据，一定是在一个事务中进行的，这就会造成表中大批量数据进行锁定，从而导致大量的阻塞，阻塞会对 MySQL 的性能产生非常大的影响。\n特别是长时间的阻塞会占满所有数据库的可用连接，这会使生产环境中的其他应用无法连接到数据库，因此一定要注意大批量写操作要进行分批\n对于大表使用 pt-online-schema-change 修改表结构 # 避免大表修改产生的主从延迟 避免在对表字段进行修改时进行锁表 对大表数据结构的修改一定要谨慎，会造成严重的锁表操作，尤其是生产环境，是不能容忍的。\npt-online-schema-change 它会首先建立一个与原表结构相同的新表，并且在新表上进行表结构的修改，然后再把原表中的数据复制到新表中，并在原表中增加一些触发器。把原表中新增的数据也复制到新表中，在行所有数据复制完成之后，把新表命名成原表，并把原来的表删除掉。\n把原来一个 DDL 操作，分解成多个小的批次进行。\n禁止为程序使用的账号赋予 super 权限 # 当达到最大连接数限制时，还运行 1 个有 super 权限的用户连接 super 权限只能留给 DBA 处理问题的账号使用 对于程序连接数据库账号,遵循权限最小原则 # 程序使用数据库账号只能在一个 DB 下使用，不准跨库 程序使用的账号原则上不准有 drop 权限 "},{"id":113,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0610lymysql-questions-01/","title":"MySQL常见面试题总结","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!====\nMySQL基础 # 关系型数据库介绍 # 关系型数据库，建立在关系模型的基础上的数据库。表明数据库中所存储的数据之间的联系（一对一、一对多、多对多） 关系型数据库中，我们的数据都被存放在各种表中（比如用户表），表中的每一行存放着一条数据（比如一个用户的信息） 大部分关系型数据库都使用SQL来操作数据库中的数据，并且大部分关系型数据库都支持事务的四大特性（ACID） 常见的关系型数据库\nMySQL、PostgreSQL、Oracle、SQL Server、SQLite（微信本地的聊天记录的存储就是用的 SQLite） \u0026hellip;\u0026hellip;\nMySQL介绍 # MySQL是一种关系型数据库，主要用于持久化存储我们系统中的一些数据比如用户信息\n由于 MySQL 是开源免费并且比较成熟的数据库，因此，MySQL 被大量使用在各种系统中。任何人都可以在 GPL(General Public License 通用性公开许可证) 的许可下下载并根据个性化的需要对其进行修改。MySQL 的默认端口号是3306。\nMySQL基础架构 # MySQL的一个简要机构图，客户端的一条SQL语句在MySQL内部如何执行 MySQL主要由几部分构成 连接器：身份认证和权限相关（登录MySQL的时候） 查询缓存：执行查询语句的时候，会先查询缓存（MySQL8.0版本后移除，因为这个功能不太实用） 分析器：没有命中缓存的话，SQL语句就会经过分析器，分析器说白了就是要先看你的SQL语句要干嘛，再检查你的SQL语句语法是否正确 优化器：按照MySQL认为最优的方案去执行 执行器：执行语句，然后从存储引擎返回数据。执行语句之前会先判断是否有权限，如果没有权限，就会报错 插件式存储引擎：主要负责数据的存储和读取，采用的是插件式架构，支持InnoDB、MyISAM、Memory等多种存储引擎 MySQL存储引擎 # MySQL核心在于存储引擎\nMySQL支持哪些存储引擎？默认使用哪个？ # MySQL支持多种存储引擎，可以通过show engines命令来查看MySQL支持的所有存储引擎 默认存储引擎为InnoDB，并且，所有存储引擎中只有InnoDB是事务性存储引擎，也就是说只有InnoDB支持事务\n这里使用MySQL 8.x MySQL 5.5.5之前，MyISAM是MySQL的默认存储引擎；5.5.5之后，InnoDB是MySQL的默认存储引擎，可以通过select version()命令查看你的MySQL版本\nmysql\u0026gt; select version(); +-----------+ | version() | +-----------+ | 8.0.27 | +-----------+ 1 row in set (0.00 sec) 使用show variables like %storage_engine%命令直接查看MySQL当前默认的存储引擎 如果只想查看数据库中某个表使用的存储引擎的话，可以使用show table status from db_name where name = 'table_name'命令\n如果你想要深入了解每个存储引擎以及它们之间的区别，推荐你去阅读以下 MySQL 官方文档对应的介绍(面试不会问这么细，了解即可)：\nInnoDB 存储引擎详细介绍：https://dev.mysql.com/doc/refman/8.0/en/innodb-storage-engine.html 。 其他存储引擎详细介绍：https://dev.mysql.com/doc/refman/8.0/en/storage-engines.html 。 MySQL存储引擎架构了解吗？ # MySQL 存储引擎采用的是插件式架构，支持多种存储引擎，我们甚至可以为不同的数据库表设置不同的存储引擎以适应不同场景的需要。存储引擎是基于表的，而不是数据库 可以根据 MySQL 定义的存储引擎实现标准接口来编写一个属于自己的存储引擎。这些非官方提供的存储引擎可以称为第三方存储引擎，区别于官方存储引擎 像目前最常用的 InnoDB 其实刚开始就是一个第三方存储引擎，后面由于过于优秀，其被 Oracle 直接收购了。\nMySQL 官方文档也有介绍到如何编写一个自定义存储引擎，地址：https://dev.mysql.com/doc/internals/en/custom-engine.html\nMyISAM和InnoDB的区别是什么？ # ISAM全称：Indexed Sequential Access Method(索引 顺序 访问 方法) 虽然，MyISAM 的性能还行，各种特性也还不错（比如全文索引、压缩、空间函数等）。但是，MyISAM 不支持事务和行级锁，而且最大的缺陷就是崩溃后无法安全恢复 是否支持行级锁 MyISAM 只有表级锁(table-level locking)，而 InnoDB 支持行级锁(row-level locking)和表级锁,默认为行级锁。\nMyISAM 一锁就是锁住了整张表，这在并发写的情况下是多么滴憨憨啊！这也是为什么 InnoDB 在并发写的时候，性能更牛皮了！\n是否支持事务\nMyISAM不支持事务，InnoDB提供事务支持\nInnoDB实现了SQL标准，定义了四个隔离级别，具有提交（commit）和回滚（rollback）事务的能力 InnoDB默认使用的REPEATABLE-READ(可重复读)隔离级别是可以解决幻读问题发生的（部分幻读），基于MVCC和Next-Key Lock（间隙锁） 详细可以查看MySQL 事务隔离级别详解\n是否支持外键\nMyISAM不支持，而InnoDB支持\n外键对于维护数据一致性非常有帮助，但是对性能有一定的损耗。因此，通常情况下，我们是不建议在实际生产项目中使用外键的，在业务代码中进行约束即可！\n阿里的《Java 开发手册》也是明确规定禁止使用外键的。\n不过，在代码中进行约束的话，对程序员的能力要求更高，具体是否要采用外键还是要根据你的项目实际情况而定\n一般我们也是不建议在数据库层面使用外键的，应用层面可以解决。不过，这样会对数据的一致性造成威胁。具体要不要使用外键还是要根据你的项目来决定 是否支持数据库异常崩溃后的安全恢复 MyISAM 不支持，而 InnoDB 支持。\n使用 InnoDB 的数据库在异常崩溃后，数据库重新启动的时候会保证数据库恢复到崩溃前的状态。这个恢复的过程依赖于 redo log 是否支持MVCC MyISAM 不支持，而 InnoDB 支持。\nMyISAM 连行级锁都不支持。MVCC 可以看作是行级锁的一个升级，可以有效减少加锁操作，提高性能。 索引实现不一样\n虽然 MyISAM 引擎和 InnoDB 引擎都是使用 B+Tree 作为索引结构，但是两者的实现方式不太一样。 InnoDB 引擎中，其数据文件本身就是索引文件。而 MyISAM中，索引文件和数据文件是分离的 InnoDB引擎中，表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。 详细区别，推荐 ： MySQL 索引详解\nMyISAM和InnoDB 如何选择 # 大多数时候我们使用的都是 InnoDB 存储引擎，在某些读密集的情况下，使用 MyISAM 也是合适的。不过，前提是你的项目不介意 MyISAM 不支持事务、崩溃恢复等缺点（可是~我们一般都会介意啊！）\n《MySQL 高性能》上面有一句话这样写到:\n不要轻易相信“MyISAM 比 InnoDB 快”之类的经验之谈，这个结论往往不是绝对的。在很多我们已知场景中，InnoDB 的速度都可以让 MyISAM 望尘莫及，尤其是用到了聚簇索引，或者需要访问的数据都可以放入内存的应用。\n一般情况下我们选择 InnoDB 都是没有问题的，但是某些情况下你并不在乎可扩展能力和并发能力，也不需要事务支持，也不在乎崩溃后的安全恢复问题的话，选择 MyISAM 也是一个不错的选择。但是一般情况下，我们都是需要考虑到这些问题的。\n对于咱们日常开发的业务系统来说，你几乎找不到什么理由再使用 MyISAM 作为自己的 MySQL 数据库的存储引擎\nMySQL 索引 # MySQL 索引相关的问题比较多，对于面试和工作都比较重要，于是，我单独抽了一篇文章专门来总结 MySQL 索引相关的知识点和问题： MySQL 索引详解]\nMySQL查询缓存 # 执行查询语句的时候，会先查询缓存。不过**，MySQL 8.0 版本后移除**，因为这个功能不太实用\nmy.cnf 加入以下配置，重启 MySQL 开启查询缓存\nquery_cache_type=1 query_cache_size=600000 执行以下命令也可以开启查询缓存\nset global query_cache_type=1; set global query_cache_size=600000; 开启查询缓存后在同样的查询条件以及数据情况下，会直接在缓存中返回结果：\n查询条件包括查询本身、当前要查询的数据库、客户端协议版本号等一些可能影响结果的信息\n查询缓存不命中的情况：\n任何两个查询在任何字符上的不同都会导致缓存不命中 如果查询中包含任何用户自定义函数、存储函数、用户变量、临时表、MySQL 库中的系统表，其查询结果也不会被缓存 缓存建立之后，MySQL 的查询缓存系统会跟踪查询中涉及的每张表，如果这些表（数据或结构）发生变化，那么和这张表相关的所有缓存数据都将失效 缓存虽然能够提升数据库的查询性能，但是缓存同时也带来了额外的开销，每次查询后都要做一次缓存操作，失效后还要销毁。 因此，开启查询缓存要谨慎，尤其对于写密集的应用来说更是如此。如果开启，要注意合理控制缓存空间大小，一般来说其大小设置为几十 MB 比较合适。此外，**还可以通过 sql_cache 和 sql_no_cache 来控制某个查询语句是否需要缓存\nselect sql_no_cache count(*) from usr; MySQL事务 # 何谓事务 # 我们设想一个场景，这个场景中我们需要插入多条相关联的数据到数据库，不幸的是，这个过程可能会遇到下面这些问题：\n数据库中途突然因为某些原因挂掉了。 客户端突然因为网络原因连接不上数据库了。 并发访问数据库时，多个线程同时写入数据库，覆盖了彼此的更改。 \u0026hellip;\u0026hellip; 上面的任何一个问题都可能会导致数据的不一致性。为了保证数据的一致性，系统必须能够处理这些问题。事务就是我们抽象出来简化这些问题的首选机制。事务的概念起源于数据库，目前，已经成为一个比较广泛的概念\n事务是逻辑上的一组操作，要么都执行，要么都不执行\n最经典的就是转账，假如小明要给小红转账1000元，这个转账涉及到两个关键操作，这两个操作必须都成功或者都失败\n将小明的余额减少1000元 将小红的余额增加1000元 事务会把两个操作看成逻辑上的一个整体，这个整体包含的操作要么都成功，要么都失败。这样就不会出现小明余额减少而小红余额却没有增加的情况\n何谓数据库事务 # 多数情况下，我们谈论事务的时候，如果没有特指分布式事务，往往指的是数据库事务\n数据库事务在日常开发中接触最多，如果项目属于单体架构，接触的往往就是数据库事务\n数据库事务的作用\n可以保证多个对数据库的操作（也就是SQL语句）构成一个逻辑上的整体，构成这个逻辑上整体的这些数据库操作遵循：要么全部执行成功，要么全部不执行\n# 开启一个事务 START TRANSACTION; # 多条 SQL 语句 SQL1,SQL2... ## 提交事务 COMMIT; 关系型数据库（比如MySQL、SQLServer、Oracle等）事务都有ACID特性\n原子性（Atomicity） ： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性（Consistency）： 执行事务前后，数据保持一致，例如转账业务中，无论事务是否成功，转账者和收款人的总额应该是不变的；（其实一致性是结果） 隔离性（Isolation）： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性（Durability）： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 只有保证了事务的持久性、原子性、隔离性之后，一致性才能得到保障。也就是说 A、I、D 是手段，C 是目的！ 想必大家也和我一样，被 ACID 这个概念被误导了很久! 我也是看周志明老师的公开课《周志明的软件架构课》open in new window才搞清楚的（多看好书！！）\n另外，DDIA 也就是 《Designing Data-Intensive Application（数据密集型应用系统设计）》open in new window 的作者在他的这本书中如是说：\nAtomicity, isolation, and durability are properties of the database, whereas consis‐ tency (in the ACID sense) is a property of the application. The application may rely on the database’s atomicity and isolation properties in order to achieve consistency, but it’s not up to the database alone.\n翻译过来的意思是：原子性，隔离性和持久性是数据库的属性，而一致性（在 ACID 意义上）是应用程序的属性。应用可能依赖数据库的原子性和隔离属性来实现一致性，但这并不仅取决于数据库。因此，字母 C 不属于 ACID 《Designing Data-Intensive Application（数据密集型应用系统设计）》这本书强推一波，值得读很多遍！豆瓣有接近 90% 的人看了这本书之后给了五星好评。另外，中文翻译版本已经在 Github 开源，地址：https://github.com/Vonng/ddiaopen in new window\n并发事务带来了哪些问题 # 典型应用程序中，多个事务并发运行，经常会操作相同数据来完成各自任务(多个用户对统一数据进行操作)。并发虽然是必须的，但是会导致一下的问题\n脏读（Dirty read） **\n一个事务读取数据并且对数据进行了修改，这个修改对其他事务来说是可见的(其实就是读未提交），即使当前事务没有提交。这时另外一个事务读取了这个还未提交的数据，但第一个事务突然回滚，导致数据并没有被提交到数据库，那第二个事务读取到的就是脏数据**，这也就是脏读的由来。\n例如：事务 1 读取某表中的数据 A=20，事务 1 修改 A=A-1，事务 2 读取到 A = 19,事务 1 回滚导致对 A 的修改并为提交到数据库， A 的值还是 20\n丢失修改（Lost to modify） 在一个事务读取一个数据时，另外一个事务也访问了该数据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事务内的修改结果就被丢失，因此称为丢失修改。\n例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 先修改 A=A-1，事务 2 后来也修改 A=A-1，最终结果 A=19，事务 1 的修改被丢失。 (这里例子举得不好，用事务2进行了A = A - 2 操作会比较明显) 不可重复读（Unrepeatable read)\n指在一个事务内多次读同一数据。在这个事务还没有结束时，另一个事务也访问该数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改导致第一个事务两次读取的数据可能不太一样。这就发生了在一个事务内两次读到的数据是不一样的情况，因此称为不可重复读。\n例如：事务 1 读取某表中的数据 A=20，事务 2 也读取 A=20，事务 1 修改 A=A-1，事务 2 再次读取 A =19，此时读取的结果和第一次读取的结果不同。\n幻读\n幻读与不可重复读类似。它发生在一个事务读取了几行数据，接着另一个并发事务插入了一些数据时。在随后的查询中，第一个事务就会发现多了一些原本不存在的记录，就好像发生了幻觉一样，所以称为幻读。\n例如：事务 2 读取某个范围的数据，事务 1 在这个范围插入了新的数据，事务 1 再次读取这个范围的数据发现相比于第一次读取的结果多了新的数据。\n不可重复读和幻读有什么区别 # 不可重复读的重点是内容修改或者记录减少。比如多次读取一条记录发现其中某些记录的值被修改；\n幻读的重点在于记录新增比如多次执行同一条查询语句（DQL）时，发现查到的记录增加了。\n幻读其实可以看作是不可重复读的一种特殊情况，单独把区分幻读的原因主要是解决幻读和不可重复读的方案不一样。\n举个例子：执行 delete 和 update 操作的时候，可以直接对记录加锁，保证事务安全。而执行 insert 操作的时候，由于记录锁（Record Lock）只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁（Gap Lock）。也就是说执行 insert 操作的时候需要依赖 Next-Key Lock（Record Lock+Gap Lock） 进行加锁来保证不出现幻读。 (这里说的是完全解决幻读，其实也可以依靠MVCC部分解决幻读) 使用MVCC机制（只在事务第一次select的时候生成ReadView解决不可重复读的问题） SQL标准定义了哪些事务隔离级别 # SQL标准定义了**四个隔离级别 **\nREAD-UNCOMMITTED(读取未提交) ： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交) ： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读) ： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化) ： 最高的隔离级别，完全服从 ACID 的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL的隔离级别是基于锁实现的吗 # MySQL 的隔离级别基于锁和 MVCC 机制共同实现的。\nSERIALIZABLE 隔离级别，是通过锁来实现的。除了 SERIALIZABLE 隔离级别，其他的隔离级别都是基于 MVCC 实现。 不过， SERIALIZABLE 之外的其他隔离级别可能也需要用到锁机制，就比如 REPEATABLE-READ 在当前读情况下需要使用加锁读来保证不会出现幻读（这就是MVCC不能解决幻读的例外之一）。 上述总结 # MySQL的默认隔离级别是什么 # MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）。我们可以通过**SELECT @@tx_isolation;**命令来查看，MySQL 8.0 该命令改为SELECT @@transaction_isolation;\nmysql\u0026gt; SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ ------ 关于 MySQL 事务隔离级别的详细介绍，可以看看我写的这篇文章： MySQL 事务隔离级别详解\nMySQL锁 # 表级锁和行级锁了解吗？有什么区别 # MyISAM 仅仅支持表级锁(table-level locking)，一锁就锁整张表，这在并发写的情况下性非常差。 InnoDB 不光支持表级锁(table-level locking)，还支持行级锁(row-level locking)，默认为行级锁。行级锁的粒度更小，仅对相关的记录上锁即可（对一行或者多行记录加锁），所以对于并发写入操作来说， InnoDB 的性能更高。 表级锁和行级锁对比 ：\n表级锁： MySQL 中锁定粒度最大的一种锁（全局锁除外），是针对非索引字段加的锁，对当前操作的整张表加锁，实现简单，资源消耗也比较少，加锁快，不会出现死锁。其锁定粒度最大，触发锁冲突的概率最高，并发度最低，MyISAM 和 InnoDB 引擎都支持表级锁。\n行级锁： MySQL 中锁定粒度最小的一种锁，是针对索引字段加的锁，只针对当前操作的行记录进行加锁。 行级锁能大大减少数据库操作的冲突。其加锁粒度最小，并发度高，但加锁的开销也最大，加锁慢，会出现死锁。\n行级锁的使用有什么注意事项 # InnoDB 的行锁是针对索引字段加的锁，表级锁是针对非索引字段加的锁。\n当我们执行 UPDATE、DELETE 语句时，如果 WHERE条件中字段没有命中唯一索引或者索引失效的话，就会导致扫描全表对表中的所有行记录进行加锁。这个在我们日常工作开发中经常会遇到，一定要多多注意！！！\n不过，很多时候即使用了索引也有可能会走全表扫描，这是因为 MySQL 优化器的原因。\n共享锁和排他锁 # 不论是表级锁还是行级锁，都存在**共享锁（Share Lock，S 锁）和排他锁（Exclusive Lock，X 锁）**这两类\n共享锁（S 锁） ：又称读锁，事务在读取记录的时候获取共享锁，允许多个事务同时获取（锁兼容）。 排他锁（X 锁） ：又称写锁/独占锁，事务在修改记录的时候获取排他锁，不允许多个事务同时获取。如果一个记录已经被加了排他锁，那其他事务不能再对这条事务加任何类型的锁**（锁不兼容）**。 排他锁与任何的锁都不兼容，共享锁仅和共享锁兼容。\nS 锁 X 锁 S 锁 不冲突 冲突 X 锁 冲突 冲突 由于 MVCC 的存在，对于一般的 SELECT 语句，InnoDB 不会加任何锁。不过， 你可以通过以下语句显式加共享锁或排他锁：\n# 共享锁 SELECT ... LOCK IN SHARE MODE; # 排他锁 SELECT ... FOR UPDATE; 意向锁有什么作用 # ★★ 重点 ：如果需要用到表锁的话，如何判断表中的记录没有行锁呢？一行一行遍历肯定是不行，性能太差。我们需要用到一个叫做意向锁的东东来快速判断是否可以对某个表使用表锁。\n意向锁是表级锁（这句话很重要，意向锁是描述某个表的某个属性（这个表是否有记录加了共享锁/或者排他锁）），共有两种：\n意向共享锁（Intention Shared Lock，IS 锁）：事务有意向对表中的某些记录加共享锁（S 锁），加共享锁前必须先取得该表的 IS 锁。\n意向排他锁（Intention Exclusive Lock，IX 锁）：事务有意向对表中的某些记录加排他锁（X 锁），加排他锁之前必须先取得该表的 IX 锁。\n意向锁是有数据引擎自己维护的，用户无法手动操作意向锁，在为数据行加共享 / 排他锁之前，InooDB 会先获取（如果获取到了，其实就是“加了锁”）该数据行所在在数据表的对应意向锁。\n意向锁之间是互相兼容的 ：\n理由很简单，表里某一条记录加了排他锁（即这个表加了意向排他锁），不代表不能操作其他记录\nIS 锁 IX 锁 IS 锁 兼容 兼容 IX 锁 兼容 兼容 意向锁和共享锁和排它锁互斥（这里指的是表级别的共享锁和排他锁，意向锁不会与行级的共享锁和排他锁互斥，★★括号里这句话极其重要，要不然就看不懂下面的表了）。\nIS 锁 IX 锁 S 锁 兼容 互斥 X 锁 互斥 互斥 《MySQL 技术内幕 InnoDB 存储引擎》这本书对应的描述应该是笔误了。\nInnoDB 有哪几类行锁 # MySQL InnoDB 支持三种行锁定方式：\n记录锁（Record Lock） ：也被称为记录锁，属于单个行记录上的锁。 间隙锁（Gap Lock） ：锁定一个范围，不包括记录本身。 临键锁（Next-key Lock） ：Record Lock+Gap Lock，锁定一个范围，包含记录本身。记录锁只能锁住已经存在的记录，为了避免插入新记录，需要依赖间隙锁。 InnoDB 的默认隔离级别 RR（可重读）是可以解决幻读问题发生的，主要有下面两种情况：\n快照读（一致性非锁定读） ：由 MVCC 机制来保证不出现幻读。 当前读 （一致性锁定读）： 使用 Next-Key Lock 进行加锁来保证不出现幻读。 当前读和快照读有什么区别 # 快照读（一致性非锁定读）就是单纯的 SELECT 语句，但不包括下面这两类 SELECT 语句：\nSELECT ... FOR UPDATE SELECT ... LOCK IN SHARE MODE 快照即记录的历史版本，每行记录可能存在多个历史版本（多版本技术）。\n快照读的情况下，如果读取的记录正在执行 UPDATE/DELETE 操作，读取操作不会因此去等待记录上 X 锁的释放，而是会去读取行的一个快照。\n只有在事务隔离级别 RC(读取已提交，ReadCommit) 和 **RR（可重读，RepeatableCommit）**下，InnoDB 才会使用一致性非锁定读：\n在 RC 级别下，对于快照数据，一致性非锁定读总是读取被锁定行的最新一份（可见）快照数据。 在 RR 级别下，对于快照数据，一致性非锁定读总是读取本事务开始时的行数据版本。 快照读比较适合对于数据一致性要求不是特别高且追求极致性能的业务场景。\n当前读 （一致性锁定读）就是给行记录加 X 锁或 S 锁。（使用当前读的话在RR级别下就无法解决幻读）\n当前读的一些常见 SQL 语句类型如下：\n# 对读的记录加一个X锁 SELECT...FOR UPDATE # 对读的记录加一个S锁 SELECT...LOCK IN SHARE MODE # 对修改的记录加一个X锁 INSERT... UPDATE... DELETE... MySQL 性能优化 # 关于 MySQL 性能优化的建议总结，请看这篇文章： MySQL 高性能优化规范建议总结\n能用MySQL直接存储文件（比如图片）吗 # 可以是可以，直接存储文件对应的二进制数据即可。不过，还是建议不要在数据库中存储文件，会严重影响数据库性能，消耗过多存储空间。\n数据库只存储文件地址信息，文件由文件存储服务负责存储。\n可以选择使用云服务厂商提供的开箱即用的文件存储服务，成熟稳定，价格也比较低。 也可以选择自建文件存储服务，实现起来也不难，基于 FastDFS、MinIO（推荐） 等开源项目就可以实现分布式文件服务。\n相关阅读：Spring Boot 整合 MinIO 实现分布式文件服务\nMySQL如何存储IP 地址 # 可以将 IP 地址转换成整形数据存储，性能更好，占用空间也更小。\nMySQL 提供了两个方法来处理 ip 地址\nINET_ATON() ： 把 ip 转为无符号整型 (4-8 位) INET_NTOA() :把整型的 ip 转为地址 插入数据前，先用 INET_ATON() 把 ip 地址转为整型，显示数据时，使用 INET_NTOA() 把整型的 ip 地址转为地址显示即可\n有哪些常见的SQL优化手段吗 # 《Java 面试指北》open in new window 的「技术面试题篇」有一篇文章详细介绍了常见的 SQL 优化手段，非常全面，清晰易懂！\n"},{"id":114,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0609lyindex-invalidation-caused-by-implicit-conversion/","title":"MySQL中的隐式转换造成的索引失效","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n本篇文章基于MySQL 5.7.26，原文：https://www.guitu18.com/post/2019/11/24/61.html\n前言 # 关于数据库优化，最常见的莫过于索引失效，数据量多的时候比较明显，处理不及时会造成雪球效应，最终导致数据库卡死甚至瘫痪。 这里说的是隐式转换造成的索引失效 数据准备 # -- 创建测试数据表 DROP TABLE IF EXISTS test1; CREATE TABLE `test1` ( `id` int(11) NOT NULL, `num1` int(11) NOT NULL DEFAULT \u0026#39;0\u0026#39;, `num2` varchar(11) NOT NULL DEFAULT \u0026#39;\u0026#39;, `type1` int(4) NOT NULL DEFAULT \u0026#39;0\u0026#39;, `type2` int(4) NOT NULL DEFAULT \u0026#39;0\u0026#39;, `str1` varchar(100) NOT NULL DEFAULT \u0026#39;\u0026#39;, `str2` varchar(100) DEFAULT NULL, PRIMARY KEY (`id`), KEY `num1` (`num1`), KEY `num2` (`num2`), KEY `type1` (`type1`), KEY `str1` (`str1`), KEY `str2` (`str2`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- 创建存储过程 DROP PROCEDURE IF EXISTS pre_test1; DELIMITER // CREATE PROCEDURE `pre_test1`() BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; WHILE i \u0026lt; 10000000 DO SET i = i + 1; SET @str1 = SUBSTRING(MD5(RAND()),1,20); -- 每100条数据str2产生一个null值 IF i % 100 = 0 THEN SET @str2 = NULL; ELSE SET @str2 = @str1; END IF; INSERT INTO test1 (`id`, `num1`, `num2`, `type1`, `type2`, `str1`, `str2`) VALUES (CONCAT(\u0026#39;\u0026#39;, i), CONCAT(\u0026#39;\u0026#39;, i), CONCAT(\u0026#39;\u0026#39;, i), i%5, i%5, @str1, @str2); -- 事务优化，每一万条数据提交一次事务 IF i % 10000 = 0 THEN COMMIT; END IF; END WHILE; END; // DELIMITER ; -- 执行存储过程 CALL pre_test1(); 其中，七个字段，首先使用存储过程生成 1000 万条测试数据， 测试表一共建立了 7 个字段（包括主键），num1和num2保存的是和ID一样的顺序数字，其中num2是字符串类型。 type1和type2保存的都是主键对 5 的取模，目的是模拟实际应用中常用类似 type 类型的数据，但是**type2是没有建立索引的。 str1和str2都是保存了一个 20 位长度的随机字符串，str1不能为NULL，str2允许为NULL，相应的生成测试数据的时候我也会在str2字段生产少量NULL值**（每 100 条数据产生一个NULL值）。\n数据量比较大，还涉及使用MD5生成随机字符串，所以速度有点慢，稍安勿躁，耐心等待即可。\n1000 万条数据，我用了 33 分钟才跑完（实际时间跟你电脑硬件配置有关）。这里贴几条生成的数据，大致长这样。 数据如下所示：\nSQL测试 # 注：num1是int类型，num2是varchar类型。\n1: SELECT * FROM `test1` WHERE num1 = 10000; 2: SELECT * FROM `test1` WHERE num1 = \u0026#39;10000\u0026#39;; 3: SELECT * FROM `test1` WHERE num2 = 10000; 4: SELECT * FROM `test1` WHERE num2 = \u0026#39;10000\u0026#39;; 这四条 SQL 都是有针对性写的，12 查询的字段是 int 类型，34 查询的字段是varchar类型。12 或 34 查询的字段虽然都相同，但是一个条件是数字，一个条件是用引号引起来的字符串。这样做有什么区别呢？先不看下边的测试结果你能猜出这四条 SQL 的效率顺序吗？\n经测试这四条 SQL 最后的执行结果却相差很大，其中 124 三条 SQL 基本都是瞬间出结果，大概在 0.0010.005 秒，在千万级的数据量下这样的结果可以判定这三条 SQL 性能基本没差别了。但是第三条 SQL，多次测试耗时基本在 4.54.8 秒之间\n也就是说 左int 右字符不影响效率；而左字符右int则影响效率，后面会解释\n下面看1234的执行计划\n可以看到，124 三条 SQL 都能使用到索引，连接类型都为ref，扫描行数都为 1，所以效率非常高。再看看第三条 SQL，没有用上索引，所以为全表扫描，rows直接到达 1000 万了，所以性能差别才那么大\n34 两条 SQL 查询的字段num2是varchar类型的，查询条件等号右边加引号的第 4 条 SQL 是用到索引的，那么是查询的数据类型和字段数据类型不一致造成的吗？如果是这样那 12 两条 SQL 查询的字段num1是int类型，但是第 2 条 SQL 查询条件右边加了引号为什么还能用上索引呢。 官方文档： 12.2 Type Conversion in Expression Evaluationopen in new window\n当操作符与不同类型的操作数一起使用时，会发生类型转换以使操作数兼容。某些转换是隐式发生的。例如，MySQL 会根据需要自动将字符串转换为数字，反之亦然。以下规则描述了比较操作的转换方式：\n两个参数至少有一个是NULL时，比较的结果也是NULL，特殊的情况是使用\u0026lt;=\u0026gt;对两个NULL做比较时会返回1，这两种情况都不需要做类型转换 两个参数都是字符串，会按照字符串来比较，不做类型转换 两个参数都是整数，按照整数来比较，不做类型转换 十六进制的值和非数字做比较时，会被当做二进制串 有一个参数是TIMESTAMP或DATETIME，并且另外一个参数是常量，常量会被转换为timestamp 有一个参数是decimal类型，如果另外一个参数是decimal或者整数，会将整数转换为decimal后进行比较，如果另外一个参数是浮点数，则会把decimal转换为浮点数进行比较 所有其他情况下，两个参数都会被转换为浮点数再进行比较 根据官方文档的描述，我们的第 23 两条 SQL 都发生了隐式转换，第 2 条 SQL 的查询条件num1 = '10000'，左边是int类型右边是字符串，第 3 条 SQL 相反，那么根据官方转换规则第 7 条，左右两边都会转换为浮点数再进行比较\n★★\n先看第 2 条 SQL：SELECT * FROMtest1WHERE num1 = '10000';左边为 int 类型10000，转换为浮点数还是10000，右边字符串类型'10000'，转换为浮点数也是10000。两边的转换结果都是唯一确定的，所以不影响使用索引\n也就是说，这个sql是要找到索引num1的值为浮点数10000的行，所以能用上索引\n第 3 条 SQL：SELECT * FROMtest1WHERE num2 = 10000;左边是字符串类型'10000'，转浮点数为 10000 是唯一的，右边int类型10000转换结果也是唯一的。但是，因为左边是检索条件，'10000'转到10000虽然是唯一，但是其他字符串也可以转换为10000，比如'10000a'，'010000'，'10000'等等都能转为浮点数10000，这样的情况下，是不能用到索引的。\n也就是说，如果我把10000当作索引去查，是不行的。因为正确结果应该是把 \u0026lsquo;10000a\u0026rsquo;，\u0026lsquo;10000-\u0026lsquo;这种都查出来。而如果使用索引，也只能查出'10000\u0026rsquo;，结果不对。所以肯定会用上全表扫描\n也就是说，这个sql是要找到索引num2的值(字符串)转换后是'10000\u0026rsquo;的行，因为10000a,10000b转换后也都是10000，所以用不上索引\n对第二点的后半部分再做解释\n关于这个隐式转换我们可以通过查询测试验证一下，先插入几条数据，其中num2='10000a'、'010000'和'10000'：\nINSERT INTO `test1` (`id`, `num1`, `num2`, `type1`, `type2`, `str1`, `str2`) VALUES (\u0026#39;10000001\u0026#39;, \u0026#39;10000\u0026#39;, \u0026#39;10000a\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;2df3d9465ty2e4hd523\u0026#39;, \u0026#39;2df3d9465ty2e4hd523\u0026#39;); INSERT INTO `test1` (`id`, `num1`, `num2`, `type1`, `type2`, `str1`, `str2`) VALUES (\u0026#39;10000002\u0026#39;, \u0026#39;10000\u0026#39;, \u0026#39;010000\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;2df3d9465ty2e4hd523\u0026#39;, \u0026#39;2df3d9465ty2e4hd523\u0026#39;); INSERT INTO `test1` (`id`, `num1`, `num2`, `type1`, `type2`, `str1`, `str2`) VALUES (\u0026#39;10000003\u0026#39;, \u0026#39;10000\u0026#39;, \u0026#39; 10000\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;0\u0026#39;, \u0026#39;2df3d9465ty2e4hd523\u0026#39;, \u0026#39;2df3d9465ty2e4hd523\u0026#39;); 然后使用第三条 SQL 语句SELECT * FROMtest1WHERE num2 = 10000;进行查询：\n从结果可以看到，后面插入的三条数据也都匹配上了。那么这个字符串隐式转换的规则是什么呢？为什么num2='10000a'、'010000'和'10000'这三种情形都能匹配上呢？查阅相关资料发现规则如下：\n不以数字开头的字符串都将转换为0。如'abc'、'a123bc'、'abc123'都会转化为0； 以数字开头的字符串转换时会进行截取，从第一个字符截取到第一个非数字内容为止。比如'123abc'会转换为123，'012abc'会转换为012也就是12，'5.3a66b78c'会转换为5.3，其他同理。 现对以上规则做如下测试验证：\n如此也就印证了之前的查询结果了\n再次写一条 SQL 查询 str1 字段：SELECT * FROMtest1WHERE str1 = 1234;\n分析和总结 # 通过上面的测试我们发现 MySQL 使用操作符的一些特性：\n当操作符左右两边的数据类型不一致时，会发生隐式转换。 当 where 查询操作符左边为数值类型时发生了隐式转换，那么对效率影响不大，但还是不推荐这么做。 当 where 查询操作符左边为字符类型时发生了隐式转换，那么会导致索引失效，造成全表扫描效率极低。 字符串转换为数值类型时，非数字开头的字符串会转化为0，以数字开头的字符串会截取从第一个字符到第一个非数字内容为止的值为转化结果。 所以，我们在写 SQL 时一定要养成良好的习惯，查询的字段是什么类型，等号右边的条件就写成对应的类型。特别当查询的字段是字符串时，等号右边的条件一定要用引号引起来标明这是一个字符串，否则会造成索引失效触发全表扫描\n"},{"id":115,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0608lysome-thoughts-on-database-storage-time/","title":"MySQL数据库时间类型数据存储建议","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n不要用字符串存储日期 # 优点：简单直白 缺点 字符串占有的空间更大 字符串存储的日期效率比较低（逐个字符进行比较），无法用日期相关的API进行计算和比较 Datetime和Timestamp之间抉择 # Datetime 和 Timestamp 是 MySQL 提供的两种比较相似的保存时间的数据类型。他们两者究竟该如何选择呢？\n通常我们都会首选 Timestamp\nDatetime类型没有时区信息 # DateTime 类型是没有时区信息的（时区无关） ，DateTime 类型保存的时间都是当前会话所设置的时区对应的时间。这样就会有什么问题呢？当你的时区更换之后，比如你的服务器更换地址或者更换客户端连接时区设置的话，就会导致你从数据库中读出的时间错误。不要小看这个问题，很多系统就是因为这个问题闹出了很多笑话。 Timestamp 和时区有关。Timestamp 类型字段的值会随着服务器时区的变化而变化，自动换算成相应的时间，说简单点就是在不同时区，查询到同一个条记录此字段的值会不一样 案例\n-- 建表 CREATE TABLE `time_zone_test` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `date_time` datetime DEFAULT NULL, `time_stamp` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8; -- 插入数据 INSERT INTO time_zone_test(date_time,time_stamp) VALUES(NOW(),NOW()); -- 查看数据 select date_time,time_stamp from time_zone_test; -- 结果 /* +---------------------+---------------------+ | date_time | time_stamp | +---------------------+---------------------+ | 2020-01-11 09:53:32 | 2020-01-11 09:53:32 | +---------------------+---------------------+ ------ */ 修改时区并查看数据\nset time_zone=\u0026#39;+8:00\u0026#39;; /* +---------------------+---------------------+ | date_time | time_stamp | +---------------------+---------------------+ | 2020-01-11 09:53:32 | 2020-01-11 17:53:32 | +---------------------+---------------------+ ------ */ 关于MySQL时区设置的一个常用sql命令\n# 查看当前会话时区 SELECT @@session.time_zone; # 设置当前会话时区 SET time_zone = \u0026#39;Europe/Helsinki\u0026#39;; SET time_zone = \u0026#34;+00:00\u0026#34;; # 数据库全局时区设置 SELECT @@global.time_zone; # 设置全局时区 SET GLOBAL time_zone = \u0026#39;+8:00\u0026#39;; SET GLOBAL time_zone = \u0026#39;Europe/Helsinki\u0026#39;; DateTime类型耗费空间更大 # Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间。但是，这样同样造成了一个问题，Timestamp 表示的时间范围更小。\nDateTime ：1000-01-01 00:00:00 ~ 9999-12-31 23:59:59 Timestamp： 1970-01-01 00:00:01 ~ 2037-12-31 23:59:59 Timestamp 在不同版本的 MySQL 中有细微差别。\n再看MySQL日期类型存储空间 # MySQL 5.6 版本中日期类型所占的存储空间 可以看出 5.6.4 之后的 MySQL 多出了一个需要 0 ～ 3 字节的小数位。DateTime 和 Timestamp 会有几种不同的存储空间占用。 为了方便，本文我们还是默认 Timestamp 只需要使用 4 个字节的存储空间，但是 DateTime 需要耗费 8 个字节的存储空间 数值型时间戳是更好的选择吗 # 使用int或者bigint类型数值，即时间戳来表示时间\n优点：使用它进行日期排序以及对比等操作效率更高，跨系统也方便 缺点：可读性差 时间戳的定义\n时间戳的定义是从一个基准时间开始算起，这个基准时间是「1970-1-1 00:00:00 +0:00」，从这个时间开始，用整数表示，以秒计时，随着时间的流逝这个时间整数不断增加。这样一来，我只需要一个数值，就可以完美地表示时间了，而且这个数值是一个绝对数值，即无论的身处地球的任何角落，这个表示时间的时间戳，都是一样的，生成的数值都是一样的，并且没有时区的概念，所以在系统的中时间的传输中，都不需要进行额外的转换了，只有在显示给用户的时候，才转换为字符串格式的本地时间\n实际操作\nmysql\u0026gt; select UNIX_TIMESTAMP(\u0026#39;2020-01-11 09:53:32\u0026#39;); +---------------------------------------+ | UNIX_TIMESTAMP(\u0026#39;2020-01-11 09:53:32\u0026#39;) | +---------------------------------------+ | 1578707612 | +---------------------------------------+ 1 row in set (0.00 sec) mysql\u0026gt; select FROM_UNIXTIME(1578707612); +---------------------------+ | FROM_UNIXTIME(1578707612) | +---------------------------+ | 2020-01-11 09:53:32 | +---------------------------+ 1 row in set (0.01 sec) 总结 # 推荐使用《高性能MySQL》\n对比\n"},{"id":116,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0605lyhow-sql-executed-in-mysql/","title":"SQL语句在MySQL中的执行过程","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n原文 https://github.com/kinglaw1204 感谢作者\n本篇文章会分析一个SQL语句在MySQL的执行流程，包括SQL的查询在MySQL内部会怎么流转，SQL语句的更新是怎么完成的 分析之前先看看MySQL的基础架构，知道了MySQL由哪些组件组成以及这些组件的作用是什么，可以帮助我们理解和解决这些问题 MySQL基础架构分析 # MySQL基本架构概览 # 下图是MySQL的简要架构图，从下图可以看到用户的SQL语句在MySQL内部是如何执行的 先简单介绍一个下图涉及的一些组件的基本作用 连接器： 身份认证和权限相关（登录MySQL的时候） 查询缓存：执行查询语句的时候，会先查询缓存（MySQL8.0版本后移除，因为这个功能不太实用） 分析器：没有命中缓存的话，SQL语句就会经过分析器，分析器说白了就是要先看你的SQL语句干嘛，再检查你的SQL语句语法是否正确 优化器：按照MySQL认为最优的方案去执行 执行器：执行语句，然后从存储引擎返回数据 简单来说 MySQL 主要分为 Server 层和存储引擎层： Server 层：主要包括连接器、查询缓存、分析器、优化器、执行器等，所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图，函数等，还有一个通用的日志模块 binlog 日志模块。 存储引擎： 主要负责数据的存储和读取，采用可以替换的插件式架构，支持 InnoDB、MyISAM、Memory 等多个存储引擎，其中 InnoDB 引擎有自有的日志模块 redolog 模块。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5 版本开始就被当做默认存储引擎了 Server层基本组件介绍 # 连接器 连接器主要和身份认证和权限相关的功能相关，就好比一个级别很高的门卫一样\n主要负责用户登录数据库，进行用户的身份认证，包括校验账户密码，权限等操作，如果用户账户密码已通过，连接器会到权限表中查询该用户的所有权限，之后在这个连接里的权限逻辑判断都是会依赖此时读取到的权限数据，也就是说，后续只要这个连接不断开，即使管理员修改了该用户的权限，该用户也是不受影响的。\n查询缓存（MySQL8.0 版本后移除）\n查询缓存主要用来缓存我们所执行的 SELECT 语句以及该语句的结果集。\n连接建立后，执行查询语句的时候，会先查询缓存，MySQL 会先校验这个 SQL 是否执行过，以 Key-Value 的形式缓存在内存中，Key 是查询预计，Value 是结果集。如果缓存 key 被命中，就会直接返回给客户端，如果没有命中，就会执行后续的操作，完成后也会把结果缓存起来，方便下一次调用。当然在真正执行缓存查询的时候还是会校验用户的权限，是否有该表的查询条件。\nMySQL 查询不建议使用缓存，因为查询缓存失效在实际业务场景中可能会非常频繁，假如你对一个表更新的话，这个表上的所有的查询缓存都会被清空。对于不经常更新的数据来说，使用缓存还是可以的。所以，一般在大多数情况下我们都是不推荐去使用查询缓存的。\nMySQL 8.0 版本后删除了缓存的功能，官方也是认为该功能在实际的应用场景比较少，所以干脆直接删掉了\n分析器MySQL 没有命中缓存，那么就会进入分析器，分析器主要是用来分析 SQL 语句是来干嘛的，分析器也会分为几步：\n第一步，词法分析，一条 SQL 语句有多个字符串组成，首先要提取关键字，比如 select，提出查询的表，提出字段名，提出查询条件等等。做完这些操作后，就会进入第二步。\n第二步，语法分析，主要就是判断你输入的 SQL 是否正确，是否符合 MySQL 的语法。\n完成这 2 步之后，MySQL 就准备开始执行了，但是如何执行，怎么执行是最好的结果呢？这个时候就需要优化器上场了。\n优化器\n优化器的作用就是它认为的最优的执行方案去执行（有时候可能也不是最优，这篇文章涉及对这部分知识的深入讲解），比如多个索引的时候该如何选择索引，多表查询的时候如何选择关联顺序等。\n可以说，经过了优化器之后可以说这个语句具体该如何执行就已经定下来\n执行器\n当选择了执行方案后，MySQL 就准备开始执行了，首先执行前会校验该用户有没有权限，如果没有权限，就会返回错误信息，如果有权限，就会去调用引擎的接口，返回接口执行的结果\n语句分析 # SQL分为两种，一种是查询，一种是更新（增加、修改、删除）\n查询语句 # select * from tb_student A where A.age='18' and A.name=' 张三 ';\n结合上面说明，分析下面这个语句的执行流程：\n先检查该语句是否有权限，如果没有权限，直接返回错误信息，如果有权限，在 MySQL8.0 版本以前，会先查询缓存，以这条 SQL 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。\n通过分析器进行词法分析，提取 SQL 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student，需要查询所有的列，查询条件是这个表的 id=\u0026lsquo;1\u0026rsquo;。然后判断这个 SQL 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。\n接下来就是优化器进行确定执行方案，上面的 SQL 语句，可以有两种执行方案：\na.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。 b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。 那么优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。那么确认了执行计划后就准备开始执行了\n进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果\n更新语句 # 以上就是一条查询 SQL 的执行流程，那么接下来我们看看一条更新语句如何执行的呢？SQL 语句如下：\nupdate tb_student A set A.age='19' where A.name=' 张三 ';\n我们来给张三修改下年龄，在实际数据库肯定不会设置年龄这个字段的，不然要被技术负责人打的 (因为可能有生日，年龄是不可人为手动修改) 其实这条语句也基本上会沿着上一个查询的流程走，只不过执行更新的时候肯定要记录日志啦，这就会引入日志模块了，MySQL 自带的日志模块是 binlog（归档日志） ，所有的存储引擎都可以使用，我们常用的 InnoDB 引擎还自带了一个日志模块 redo log（重做日志），我们就以 InnoDB 模式下来探讨这个语句的执行流程 先查询到张三这一条数据，如果有缓存，也是会用到缓存。 然后拿到查询的语句，把 age 改为 19，然后调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。 更新完成 这里肯定有同学会问，为什么要用两个日志模块，用一个日志模块不行吗?\n这是因为最开始 MySQL 并没有 InnoDB 引擎（InnoDB 引擎是其他公司以插件形式插入 MySQL 的），MySQL 自带的引擎是 MyISAM，但是我们知道 redo log 是 InnoDB 引擎特有的，其他存储引擎都没有，这就导致会没有 crash-safe 的能力(crash-safe 的能力即使数据库发生异常重启，之前提交的记录都不会丢失)，binlog 日志只能用来归档。\n并不是说只用一个日志模块不可以，只是 InnoDB 引擎就是通过 redo log 来支持事务的。那么，又会有同学问，我用两个日志模块，但是不要这么复杂行不行，为什么 redo log 要引入 prepare 预提交状态？这里我们用反证法来说明下为什么要这么做？\n先写 redo log 直接提交，然后写 binlog，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 binlog 并没有记录该数据，**后续进行机器备份(从机)**的时候，就会丢失这一条数据，同时主从同步也会丢失这一条数据。 先写 binlog，然后写 redo log，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。 如果采用 redo log 两阶段提交的方式就不一样了，写完 binlog 后，然后再提交 redo log 就会防止出现上述的问题，从而保证了数据的一致性。那么问题来了，有没有一个极端的情况呢？假设 redo log 处于预提交状态，binlog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下：\n判断 redo log 是否完整，如果判断是完整的，就立即提交。 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log, 不完整就回滚事务。 这样就解决了数据一致性的问题\n总结 # MySQL 主要分为 Server 层和引擎层，Server 层主要包括连接器、查询缓存、分析器、优化器、执行器，同时还有一个日志模块（binlog），这个日志模块所有执行引擎都可以共用，redolog 只有 InnoDB 有。\n引擎层是插件式的，目前主要包括，MyISAM,InnoDB,Memory 等。\n查询语句的执行流程如下：权限校验（如果命中缓存）\u0026mdash;\u0026gt;查询缓存\u0026mdash;\u0026gt;分析器\u0026mdash;\u0026gt;优化器\u0026mdash;\u0026gt;权限校验\u0026mdash;\u0026gt;执行器\u0026mdash;\u0026gt;引擎\n更新语句执行流程如下：分析器\u0026mdash;-\u0026gt;权限校验\u0026mdash;-\u0026gt;执行器\u0026mdash;\u0026gt;引擎\u0026mdash;redo log(prepare 状态)\u0026mdash;\u0026gt;binlog\u0026mdash;\u0026gt;redo log(commit状态\n"},{"id":117,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0604lyinnodb-implementation-of-mvcc/","title":"innodb引擎对MVCC的实现","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n一致性非锁定读和锁定读 # 一致性非锁定读 # ★★非锁定★★\n对于一致性非锁定读（Consistent Nonlocking Reads）的实现，通常做法是加一个版本号或者时间戳字段，在更新数据的同时版本号+1或者更新时间戳。查询时，将当前可见的版本号与对应记录的版本号进行比对，如果记录的版本小于可见版本，则表示该记录可见 InnoDB存储引擎中，多版本控制（multi versioning）即是非锁定读的实现。如果读取的行正在执行DELETE或UPDATE操作，这时读取操作不会去等待行上 锁的释放.相反地，Inn哦DB存储引擎会去读取行的一个快照数据，对于这种读取历史数据的方式，我们叫它快照读（snapshot read）。 在 Repeatable Read 和 Read Committed 两个隔离级别下，如果是执行普通的 select 语句（不包括 select ... lock in share mode ,select ... for update）则会使用 一致性非锁定读（MVCC）。并且在 Repeatable Read 下 MVCC 实现了可重复读和防止部分幻读 锁定读 # 如果执行的是下列语句，就是锁定读（Locking Reads）\nselect ... lock in share\nselect ... for update\ninsert 、upate、delete\n锁定读下，读取的是数据的最新版本，这种读也被称为当前读current read。锁定读会对读取到的记录加锁\nselect ... lock in share mode ：对(读取到的)记录加S锁，其他事务也可以加S锁，如果加X锁则会被阻塞\nselect ... for update、insert、update、delete：对记录加X锁，且其他事务不能加任何锁\n在一致性非锁定读下，即使读取的记录已被其他事务加上X锁，这时记录也是可以被读取的，即读取的快照数据。\n在RepeatableRead下MVCC防止了部分幻读，这边的“部分”是指在一致性非锁定读情况下，只能读取到第一次查询之前所插入的数据（根据ReadView判断数据可见性，ReadView在第一次查询时生成），但如果是当前读，每次读取的都是最新数据，这时如果两次查询中间有其他事务插入数据，就会产生幻读 所以，InnoDB在实现RepeatableRead时，如果执行的是当前读，则会对读取的记录使用Next-key Lock，来防止其他事务在间隙间插入数据。 RR产生幻读的另一个场景\n假设有这样一张表\n事务 A 执行查询 id = 5 的记录，此时表中是没有该记录的，所以查询不出来。\n# 事务 A mysql\u0026gt; begin; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; select * from t_stu where id = 5; Empty set (0.01 sec) 然后事务 B 插入一条 id = 5 的记录，并且提交了事务。\n# 事务 B mysql\u0026gt; begin; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; insert into t_stu values(5, \u0026#39;小美\u0026#39;, 18); Query OK, 1 row affected (0.00 sec) mysql\u0026gt; commit; Query OK, 0 rows affected (0.00 sec) 此时，事务 A 更新 id = 5 这条记录，对没错，事务 A 看不到 id = 5 这条记录，但是他去更新了这条记录，这场景确实很违和，然后再次查询 id = 5 的记录，事务 A 就能看到事务 B 插入的纪录了，幻读就是发生在这种违和的场景。\n# 事务 A mysql\u0026gt; update t_stu set name = \u0026#39;小林coding\u0026#39; where id = 5; Query OK, 1 row affected (0.01 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u0026gt; select * from t_stu where id = 5; +----+--------------+------+ | id | name | age | +----+--------------+------+ | 5 | 小林coding | 18 | +----+--------------+------+ 1 row in set (0.00 sec) 时序图如下\n在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。\n因为这种特殊现象的存在，所以我们认为 MySQL Innodb 中的 MVCC 并不能完全避免幻读现象。\nInnoDB对MVCC的实现 # MVCC的实现依赖于：隐藏字段（每条记录的）、ReadView（当前事务生成的）、undo log（当前事务执行时，为每个操作（记录）生成的） 内部实现中，InnoDB通过数据行的DB_TRX_ID和Read View来判断数据的可见性，如不可见，则通过数据行的DB_ROLL_PTR找到undo log中的历史版本。因此，每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建ReadView之前**（其实这个说法不太准确，m_up_limit_id不一定大于当前事务id）已经提交的修改和该事务本身做的修改** 隐藏字段 # 内部，InnoDB存储引擎为每行数据添加了三个隐藏字段： DB_TRX_ID(6字节)：表示最后一次插入或更新该行的事务id。此外，delete操作在内部被视为更新，只不过会在记录头Record header中的deleted_flag字段将其标记为已删除 DB_ROLL_PTR(7字节)：回滚指针，指向该行的undo log。如果该行未被更新，则为空 DB_ROW_ID(6字节)：如果没有设置主键且该表没有唯一非空索引时，InnoDB会使用该id来生成聚簇索引 ReadView # class ReadView { /* ... */ private: trx_id_t m_low_limit_id; /* 大于等于这个 ID 的事务均不可见 */ trx_id_t m_up_limit_id; /* 小于这个 ID 的事务均可见 */ trx_id_t m_creator_trx_id; /* 创建该 Read View 的事务ID */ trx_id_t m_low_limit_no; /* 事务 Number, 小于该 Number 的 Undo Logs 均可以被 Purge */ ids_t m_ids; /* 创建 Read View 时的活跃事务列表 */ m_closed; /* 标记 Read View 是否 close */ } Read View 主要是用来做可见性判断，里面保存了 “当前对本事务不可见的其他活跃事务”\nReadView主要有以下字段\nm_low_limit_id：目前出现过的最大的事务 ID+1，即下一个将被分配的事务 ID。大于等于这个 ID 的数据版本均不可见 m_up_limit_id：活跃事务列表 m_ids 中最小的事务 ID，如果 m_ids 为空，则 m_up_limit_id 为 m_low_limit_id。小于这个 ID 的数据版本均可见 m_ids：Read View 创建时其他未提交的活跃事务 ID 列表。创建 Read View时，将当前未提交事务 ID 记录下来，后续即使它们修改了记录行的值，对于当前事务也是不可见的。m_ids 不包括当前事务自己和已提交的事务（正在内存中） m_creator_trx_id：创建该 Read View 的事务 ID 事务可见性示意图（这个图容易理解）：\n为什么不是分大于m_low_limit_id和在小于m_low_limit_id里过滤存在于活跃事务列表，应该和算法有关吧\nundo-log # undo log主要有两个作用\n当事务回滚时用于将数据恢复到修改前的样子 用于MVCC，读取记录时，若该记录被其他事务占用或当前版本对该事务不可见，则可以通过undo log 读取之前的版本数据，以此实现非锁定读 InnoDB存储引擎中undo log分为两种：insert undo log和update undo log\ninsert undo log：指在insert操作中产生的undo log，因为insert操作的记录只对事务本身可见，对其他事务不可见，故该undo log可以在事务提交后直接删除。不需要进行purge操作（purge：清洗）\ninsert时的数据初始状态：(DB_ROLL_PTR为空)\nupdate undo log：undate或delete操作产生的undo log。该undo log 可能需要提供给MVCC机制，因此不能在事务提交时就进行删除。提交时放入undo log链表，等待purge线程进行最后的删除\n数据第一次修改时\n数据第二次被修改时\n不同事务或者相同事务的对同一记录行的修改，会使该记录行的 undo log 成为一条链表，链首就是最新的记录，链尾就是最早的旧记录。\n数据可见性算法 # 在 InnoDB 存储引擎中，创建一个新事务后，执行每个 select 语句前(RC下是)，都会创建一个快照（Read View），快照中保存了当前数据库系统中正处于活跃（没有 commit）的事务的 ID 号。其实简单的说保存的是系统中当前不应该被本事务看到的其他事务 ID 列表（即 m_ids）。当用户在这个事务中要读取某个记录行的时候，InnoDB 会将该记录行的 DB_TRX_ID 与 Read View 中的一些变量及当前事务 ID 进行比较，判断是否满足可见性条件\n具体的比较算法\n如果记录 DB_TRX_ID \u0026lt; m_up_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之前就提交了，所以该记录行的值对当前事务是可见的 如果 DB_TRX_ID \u0026gt;= m_low_limit_id，那么表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照之后才修改该行，所以该记录行的值对当前事务不可见。跳到步骤 5 m_ids 为空[那就不用排除啦，只要小于m_low_limit_id都可见]（且DB_TRX_ID \u0026lt; m_low_limit_id），则表明在当前事务创建快照之前，修改该行的事务就已经提交了，所以该记录行的值对当前事务是可见的 如果 m_up_limit_id \u0026lt;= DB_TRX_ID \u0026lt; m_low_limit_id，表明最新修改该行的事务（DB_TRX_ID）在当前事务创建快照的时候可能处于“活动状态”或者“已提交状态”；所以就要对活跃事务列表 m_ids 进行查找（源码中是用的二分查找，因为是有序的） 如果在活跃事务列表 m_ids 中能找到 DB_TRX_ID，表明：① 在当前事务创建快照前，该记录行的值被事务 ID 为 DB_TRX_ID 的事务修改了，但没有提交；或者 ② 在当前事务创建快照后，该记录行的值被事务 ID 为 DB_TRX_ID 的事务修改了且提交了(可重复读)。这些情况下，这个记录行的值对当前事务都是不可见的。跳到步骤 5 在活跃事务列表中找不到，则表明“id 为 trx_id 的事务”在修改“该记录行的值”后，在**“当前事务”创建快照前就已经提交**了，所以记录行对当前事务可见 在该记录行的 DB_ROLL_PTR 指针所指向的 undo log 取出快照记录，用快照记录的 DB_TRX_ID 跳到步骤 1 重新开始判断，直到找到满足的快照版本或返回空 RC 和 RR 隔离级别下 MVCC 的差异 # 在事务隔离级别 RC 和 RR （InnoDB 存储引擎的默认事务隔离级别）下，InnoDB 存储引擎使用 MVCC（非锁定一致性读），但它们生成 Read View 的时机却不同 【RC：Read Commit 读已提交，RR：Repeatable Read 可重复读】\n在 RC 隔离级别下的 每次select 查询前都生成一个Read View (m_ids 列表) 在 RR 隔离级别下只在事务开始后 第一次select 数据前生成一个Read View（m_ids 列表） MVCC解决不可重复读问题 # 虽然 RC 和 RR 都通过 MVCC 来读取快照数据，但由于 生成 Read View 时机不同，从而在 RR 级别下实现可重复读\n举例： （Tn 表示时间线）\n在RC下ReadView生成情况 # 1. 假设时间线来到 T4 ，那么此时数据行 id = 1 的版本链为：\n由于 RC 级别下每次查询都会生成Read View ，并且事务 101、102 并未提交，此时 103 事务生成的 Read View 中活跃的事务 m_ids 为：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103\n此时最新记录的 DB_TRX_ID 为 101，m_up_limit_id \u0026lt;= 101 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是 101，不可见 继续找上一条 DB_TRX_ID为 1，满足 1 \u0026lt; m_up_limit_id，可见，所以事务 103 查询到数据为 name = 菜花 2. 时间线来到 T6 ，数据的版本链为：\n因为在 RC 级别下，重新生成 Read View，这时事务 101 已经提交，102 并未提交，所以此时 Read View 中活跃的事务 m_ids：[102] ，m_low_limit_id为：104，m_up_limit_id为：102，m_creator_trx_id为：103\n此时最新记录的 DB_TRX_ID 为 102，m_up_limit_id \u0026lt;= 102 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 为 101，满足 101 \u0026lt; m_up_limit_id，记录可见，所以在 T6 时间点查询到数据为 name = 李四，与时间 T4 查询到的结果不一致，不可重复读！ 3. 时间线来到 T9 ，数据的版本链为：\n重新生成 Read View， 这时事务 101 和 102 都已经提交，所以 m_ids 为空，则 m_up_limit_id = m_low_limit_id = 104，最新版本事务 ID 为 102，满足 102 \u0026lt; m_low_limit_id，可见，查询结果为 name = 赵六\n总结： 在 RC 隔离级别下，事务在每次查询开始时都会生成并设置新的 Read View，所以导致不可重复读\n在RR下ReadView生成情况 # 在可重复读级别下，只会在事务开始后第一次读取数据时生成一个 Read View（m_ids 列表）\n1. 在 T4 情况下的版本链为：\n在当前执行 select 语句时生成一个 Read View，此时 m_ids：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103\n此时和 RC 级别下一样：\n最新记录的 DB_TRX_ID 为 101，m_up_limit_id \u0026lt;= 101 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是 101，不可见 继续找上一条 DB_TRX_ID为 1，满足 1 \u0026lt; m_up_limit_id，可见，所以事务 103 查询到数据为 name = 菜花 2. 时间点 T6 情况下：\n在 RR 级别下只会生成一次Read View，所以此时依然沿用 m_ids ：[101,102] ，m_low_limit_id为：104，m_up_limit_id为：101，m_creator_trx_id 为：103\n最新记录的 DB_TRX_ID 为 102，m_up_limit_id \u0026lt;= 102 \u0026lt; m_low_limit_id，所以要在 m_ids 列表中查找，发现 DB_TRX_ID 存在列表中，那么这个记录不可见 根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 为 101，不可见 【从这步开始就跟T4一样了】 继续根据 DB_ROLL_PTR 找到 undo log 中的上一版本记录，上一条记录的 DB_TRX_ID 还是 101，不可见 继续找上一条 DB_TRX_ID为 1，满足 1 \u0026lt; m_up_limit_id，可见，所以事务 103 查询到数据为 name = 菜花 3. 时间点 T9 情况下：\n此时情况跟 T6 完全一样，由于已经生成了 Read View，此时依然沿用 m_ids ：[101,102] ，所以查询结果依然是 name = 菜花\nMVCC+Next-key -Lock防止幻读 # InnoDB存储引擎在 RR 级别下通过 MVCC和 Next-key Lock 来解决幻读问题：\n1、执行普通 select，此时会以 MVCC 快照读的方式读取数据\n在快照读的情况下，RR 隔离级别只会在事务开启后的第一次查询生成 Read View ，并使用至事务提交。所以在生成 Read View 之后其它事务所做的更新、插入记录版本对当前事务并不可见，实现了可重复读和防止快照读下的 “幻读”\n2、执行 select\u0026hellip;for update/lock in share mode、insert、update、delete 等当前读\n在当前读下，读取的都是最新的数据，如果其它事务有插入新的记录，并且刚好在当前事务查询范围内，就会产生幻读！\nInnoDB 使用 Next-key Lockopen in new window 来防止这种情况。当执行当前读时，会锁定读取到的记录的同时，锁定它们的间隙，防止其它事务在查询范围内插入数据。只要我不让你插入，就不会发生幻读\nNext-Key* Lock(临键锁) 是Record Lock(记录锁) 和Gap Lock(间隙锁) 的结合 间隙锁是(左，右] ，即左开右闭。 "},{"id":118,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0603lytransaction-isolation-level/","title":"MySQL事务隔离级别详解","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n事务隔离级别总结 # SQL标准定义了四个隔离级别\nREAD-UNCOMMITTED(读取未提交)：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 READ-COMMITED(读取已提交)：允许读取并发事务 已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 REPEATABLE-READ(可重复读)：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生 SERIALIZABLE(可串行化)：最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 隔离级别 脏读 不可重复读 幻读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × MySQL InnoDB 存储引擎的默认支持的隔离级别是 REPEATABLE-READ（可重读）\n使用命令查看，通过SELECT @@tx_isolation;。\nMySQL 8.0 该命令改为SELECT @@transaction_isolation;\nMySQL\u0026gt; SELECT @@tx_isolation; +-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 从上面对SQL标准定义了四个隔离级别的介绍可以看出，标准的SQL隔离级别里，REPEATABLE-READ(可重复读)是不可以防止幻读的。但是，InnoDB实现的REPEATABLE-READ 隔离级别其实是可以解决幻读问题发生的，分两种情况\n快照读：由MVCC机制来保证不出现幻读 当前读：使用Next-Key Lock进行加锁来保证不出现幻读，Next-Key Lock是行锁（Record Lock ）和间隙锁（Gap Lock）的结合，行锁只能锁住已经存在的行，为了避免插入新行，需要依赖间隙锁 (只用间隙锁不行，因为间隙锁是 \u0026gt; 或 \u0026lt; ，不包括等于，所以再可重复读下原记录可能会被删掉) 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是 READ-COMMITTED ，但是你要知道的是 InnoDB 存储引擎默认使用 REPEATABLE-READ 并不会有任何性能损失。\nInnoDB 存储引擎在分布式事务的情况下一般会用到 SERIALIZABLE 隔离级别\nInnoDB 存储引擎提供了对 XA 事务的支持，并通过 XA 事务来支持分布式事务的实现。 分布式事务指的是允许多个独立的事务资源（transactional resources）参与到一个全局的事务中。事务资源通常是关系型数据库系统，但也可以是其他类型的资源。全局事务要求在其中的所有参与的事务要么都提交，要么都回滚，这对于事务原有的 ACID 要求又有了提高。 在使用分布式事务时，InnoDB 存储引擎的事务隔离级别必须设置为 SERIALIZABLE。 实际情况演示 # 下面会使用2个命令行MySQL，模拟多线程（多事务）对同一份数据的(脏读等)问题\nMySQL 命令行的默认配置中事务都是自动提交的，即执行 SQL 语句后就会马上执行 COMMIT 操作。如果要显式地开启一个事务需要使用命令：START TRANSACTION\n通过下面的命令来设置隔离级别 session ：更改只有本次会话有效；global：更改在所有会话都有效，且不会影响已开启的session\nSET [SESSION|GLOBAL] TRANSACTION ISOLATION LEVEL [READ UNCOMMITTED|READ COMMITTED|REPEATABLE READ|SERIALIZABLE] 实际操作中使用到的一些并发控制的语句\nSTART TRANSACTION | BEGIN：显示地开启一个事务 （begin也能开启一个事务） COMMIT：提交事务，使得对数据库做的所有修改成为永久性 ROLLBACK：回滚，会结束用户的事务，并撤销正在进行的所有未提交的修改 脏读（读未提交） # 事务1 设置为读未提交级别 SET SESSION TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;\n事务1开启事务并查看数据\nSTART TRANSACTION; SELECT salary FROM employ WHERE id = 1; # 结果 +--------+ | salary | +--------+ | 5000 | +--------+ 开启新连接，事务2 开启事务并更新数据\nSTART TRANSACTION; UPDATE employ SET salary = 4500 ; 事务1查看 SELECT salary FROM employ WHERE id = 1;\n+--------+ | salary | +--------+ | 4500 | +--------+ 此时事务2 进行回滚 ROLLBACK; 使用事务1再次查看 SELECT salary FROM employ WHERE id = 1;\n+--------+ | salary | +--------+ | 5000 | +--------+ 事务二进行了回滚，但是之前事务1却读取到了4500（是个脏数据）\n避免脏读（读已提交） # 不要在上面的连接里继续\n事务1 设置为读已提交SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;\n事务1 开启事务并查询数据\nSTART TRANSACTION; SELECT salary FROM employ WHERE id = 1; # 结果 +--------+ | salary | +--------+ | 5000 | +--------+ 事务2 开启并修改数据(未提交)\nSTART TRANSACTION; UPDATE employ SET salary = 4500 ; 事务1查看数据 SELECT salary FROM employ WHERE id = 1; 因为事务隔离级别为读已提交，所以不会发生脏读\n# 结果 +--------+ | salary | +--------+ | 5000 | +--------+ 事务2提交 COMMIT;后，事务1再次读取数据\nSELECT salary FROM employ WHERE id = 1; +--------+ | salary | +--------+ | 4500 | +--------+ 不可重复读 # 还是刚才读已提交的那些步骤，重复操作可以知道 虽然避免了读未提交，但是出现了，一个事务还没结束，就发生了不可重复读问题\n同一个数据，在同一事务内读取多次但值不一样\n可重复读 # 断开连接后重新连接MySQL，默认就是REPEATABLE-READ 可重复读\n事务1查看当前事务隔离级别 select @@tx_isolation;\n+-----------------+ | @@tx_isolation | +-----------------+ | REPEATABLE-READ | +-----------------+ 事务1 开启事务并查询数据\nSTART TRANSACTION; SELECT salary FROM employ WHERE id = 1; # 结果 +--------+ | salary | +--------+ | 5000 | +--------+ 事务2 开启事务并更新数据\nSTART TRANSACTION; UPDATE employ SET salary = 4500 WHERE id = 1; 事务1 读取数据（结果仍不变，避免了读未提交的问题）\nSELECT salary FROM employ WHERE id = 1; +--------+ | salary | +--------+ | 5000 | +--------+ 事务2提交事务 COMMIT ;\n提交后事务1再次读取\nSELECT salary FROM employ WHERE id = 1; +--------+ | salary | +--------+ | 5000 | +--------+ 与MySQL建立新连接并查询数据（发现数据确实是已经更新了的）\nSELECT salary FROM employ WHERE id = 1; +--------+ | salary | +--------+ | 4500 | +--------+ 幻读 # 接下来测试一下该隔离策略下是否幻读 这里是在可重复读下\n先查看一下当前数据库表的数据\nSELECT * FROM test; +----+--------+ | id | salary | +----+--------+ | 1 | 8000 | | 6 | 500 | +----+--------+ use lydb; \u0026mdash;\u0026gt; 事务1和事务2都开启事务 START TRANSACTION;\n事务2插入一条薪资为500的数据并提交\nINSERT INTO test(salary) values (500); COMMIT; #此时数据库已经有两条500的数据了(事务2) select * from test; +----+--------+ | id | salary | +----+--------+ | 1 | 8000 | | 6 | 500 | | 10 | 500 | +----+--------+ 事务1查询500的数据(★★如果在事务2提交之前查询 SELECT * FROM test WHERE salary = 500; 或者 SELECT * FROM test; 那么这里[快照读]就只会查出一条，但是不管怎么样 [当前读]都会查出两条)\n#---------------- # 快照读------------------ SELECT * FROM test WHERE salary = 500; +----+--------+ | id | salary | +----+--------+ | 6 | 500 | +----+--------+ #----------------# 当前读------------------ SELECT * FROM test WHERE salary = 500 FOR UPDATE; +----+--------+ | id | salary | +----+--------+ | 6 | 500 | | 11 | 500 | +----+--------+ SQL 事务1 在第一次查询工资为 500 的记录时只有一条，SQL 事务2 插入了一条工资为 500 的记录，提交之后；SQL 事务1 在同一个事务中再次使用当前读查询发现出现了两条工资为 500 的记录这种就是幻读。\n这里说明一下当前读和快照读：\nMySQL 里除了普通查询是快照读，其他都是当前读，比如 update、insert、delete，这些语句执行前都会查询最新版本的数据，然后再做进一步的操作 【为什么上面要先进行查询的原因】可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行 begin 语句后），在执行第一个查询语句后，会创建一个 Read View，后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的，即使中途有其他事务插入了新纪录，是查询不出来这条数据的，所以就很好了避免幻读问题。 解决幻读的方法 # 解决幻读的方式有很多，但是它们的核心思想就是一个事务在操作某张表数据的时候，另外一个事务不允许新增或者删除这张表中的数据了。解决幻读的方式主要有以下几种：（由重到轻）\n将事务隔离级别调整为 SERIALIZABLE 。 在可重复读的事务级别下，给事务操作的这张表添加表锁。 在可重复读的事务级别下，给事务操作的这张表添加 Next-key Lock（Record Lock+Gap Lock）。 "},{"id":119,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0602lymysql-logs/","title":"日志","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n前言 # 首先要了解一个东西 ：WAL，全称 Write-Ahead Logging，它的关键点就是先写日志，再写磁盘\n在概念上，innodb通过***force log at commit***机制实现事务的持久性，即在事务提交的时候，必须先将该事务的所有事务日志写入到磁盘上的redo log file和undo log file中进行持久化\nWAL 机制的原理也很简单：修改并不直接写入到数据库文件中，而是写入到另外一个称为 WAL 的文件中；如果事务失败，WAL 中的记录会被忽略，撤销修改；如果事务成功，它将在随后的某个时间被写回到数据库文件中，提交修改\n使用 WAL 的数据库系统不会再每新增一条 WAL 日志就将其刷入数据库文件中，一般积累一定的量然后批量写入，通常使用页为单位，这是磁盘的写入单位。 同步 WAL 文件和数据库文件的行为被称为 checkpoint（检查点），一般在 WAL 文件积累到一定页数修改的时候；当然，有些系统也可以手动执行 checkpoint。执行 checkpoint 之后，WAL 文件可以被清空，这样可以保证 WAL 文件不会因为太大而性能下降。\n有些数据库系统读取请求也可以使用 WAL，通过读取 WAL 最新日志就可以获取到数据的最新状态\n关于checkpoint：https://www.cnblogs.com/chenpingzhao/p/5107480.html思考一下这个场景：如果重做日志可以无限地增大，同时缓冲池也足够大 ，那么是不需要将缓冲池中页的新版本刷新回磁盘。因为当发生宕机时，完全可以通过重做日志来恢复整个数据库系统中的数据到宕机发生的时刻。但是这需要两个前提条件：1、缓冲池可以缓存数据库中所有的数据；2、重做日志可以无限增大\n因此Checkpoint（检查点）技术就诞生了，目的是解决以下几个问题：1、缩短数据库的恢复时间；2、缓冲池不够用时，将脏页刷新到磁盘；3、重做日志不可用时，刷新脏页。\n当数据库发生宕机时，数据库不需要重做所有的日志，因为Checkpoint之前的页都已经刷新回磁盘。数据库只需对Checkpoint后的重做日志进行恢复，这样就大大缩短了恢复的时间。 当缓冲池不够用时，根据LRU算法会溢出最近最少使用的页，若此页为脏页，那么需要强制执行Checkpoint，将脏页也就是页的新版本刷回磁盘。 当重做日志出现不可用时，因为当前事务数据库系统对重做日志的设计都是循环使用的，并不是让其无限增大的，重做日志可以被重用的部分是指这些重做日志已经不再需要，当数据库发生宕机时，数据库恢复操作不需要这部分的重做日志，因此这部分就可以被覆盖重用。如果重做日志还需要使用，那么必须强制Checkpoint，将缓冲池中的页至少刷新到当前重做日志的位置。 mysql 的 WAL，大家可能都比较熟悉。mysql 通过 redo、undo 日志实现 WAL。redo log 称为重做日志，每当有操作时，在数据变更之前将操作写入 redo log，这样当发生掉电之类的情况时系统可以在重启后继续操作。undo log 称为撤销日志，当一些变更执行到一半无法完成时，可以根据撤销日志恢复到变更之间的状态。mysql 中用 redo log 来在系统 Crash 重启之类的情况时修复数据（事务的持久性），而 undo log 来保证事务的原子性。\nMySQL 日志 主要包括错误日志、查询日志、慢查询日志、事务日志、二进制日志几大类\nmysql执行\n总结\n比较重要的\n二进制日志： binlog（归档日志）【server层】 事务日志：redo log（重做日志）和undo log（回滚日志） 【引擎层】 redo log是记录物理上的改变；\nundo log是从逻辑上恢复，产生时机：事务开始之前 MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。 redo log # redo log（重做日志）是InnoDB存储引擎独有的，它让MySQL拥有了崩溃恢复的能力\n比如 MySQL 实例挂了或宕机了，重启时，InnoDB存储引擎会使用redo log恢复数据，保证数据的持久性与完整性。\n再具体点：防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性\nMySQL中数据是以页（这个很重要，重点是针对页）为单位，你查询一条记录，会从硬盘把一页的数据加载出来，加载出来的数据叫数据页，会放到Buffer Pool中 (这个时候 如果更新，buffer pool 中的数据页就与磁盘上的数据页内容不一致，我们称 buffer pool 的数据页为 dirty page 脏数据)\n以页为单位：\n页是InnoDB 管理存储空间的基本单位，一个页的大小一般是16KB 。可以理解为创建一个表时，会创建一个大小为16KB大小的空间，也就是数据页。新增数据时会往该页中User Records中添加数据，如果页的大小不够使用了继续创建新的页。也就是说一般情况下一次最少从磁盘读取16kb的内容到内存，一次最少把16kb的内容刷新到磁盘中，其作用有点缓存行的意思 原文链接：https://blog.csdn.net/qq_31142237/article/details/125447413\n后续的查询都是先从 Buffer Pool 中找，没有命中再去硬盘加载，减少硬盘 IO 开销，提升性能。\n更新表数据的时候，也是如此，发现 Buffer Pool 里存在要更新的数据，就直接在 Buffer Pool 里更新\n把“在某个数据页上做了什么修改”记录到重做日志缓存（redo log buffer）里，接着刷盘到 redo log 文件里\n即 从 硬盘上db数据文件 \u0026ndash;\u0026gt; BufferPool \u0026ndash;\u0026gt; redo log buffer \u0026ndash;\u0026gt; redo log 理想情况，事务一提交就会进行刷盘操作，但实际上，刷盘的时机是根据策略\n每条redo记录由**”表空间号+数据页号+偏移量+修改数据长度+具体修改的数据“**组成\n刷盘时机 # InnoDB 存储引擎为 redo log 的刷盘策略提供了 innodb_flush_log_at_trx_commit 参数，它支持三种策略\n0：设置为0时，表示每次事务提交时不进行刷盘操作\n1：设置为1时，表示每次事务提交时都将进行刷盘操作（默认值）\n2：设置为2时，表示每次事务提交时都只把redo log buffer内容写入page cache(系统缓存)\ninnodb_flush_log_at_trx_commit 参数默认为 1 ，也就是说当事务提交时会调用 fsync 对 redo log 进行刷盘\nInnoDB 存储引擎有一个后台线程，每隔1 秒，就会把 redo log buffer 中的内容写到文件系统缓存（page cache），然后调用 fsync 刷盘。(★★重要★★即使没有提交事务的redo log记录，也有可能会刷盘，因为在事务执行过程 redo log 记录是会写入redo log buffer 中，这些 redo log 记录会被后台线程刷盘。)\n除了后台线程每秒1次的轮询操作，还有一种情况，当 redo log buffer 占用的空间即将达到 innodb_log_buffer_size 一半的时候，后台线程会主动刷盘\n不同刷盘策略的流程图\ninnodb_flush_log_at_trx_commit=0（不对是否刷盘做出处理） # 为0时，如果MySQL挂了或宕机可能会有1秒数据的丢失。\n（由于事务提交成功也不会主动写入page cache，所以即使只有MySQL 挂了，没有宕机，也会丢失。）\ninnodb_flush_log_at_trx_commit=1 # 为1时， 只要事务提交成功，redo log记录就一定在硬盘里，不会有任何数据丢失。如果事务执行期间MySQL挂了或宕机，这部分日志丢了，但是事务并没有提交，所以日志丢了也不会有损失。\ninnodb_flush_log_at_trx_commit=2 # 为2时， 只要事务提交成功，redo log buffer中的内容只写入文件系统缓存（page cache）。\n如果仅仅只是MySQL挂了不会有任何数据丢失，但是宕机可能会有1秒数据的丢失。\n日志文件组 # 硬盘上存储的 redo log 日志文件不只一个，而是以一个日志文件组的形式出现的，每个的redo日志文件大小都是一样的\n比如可以配置为一组**4个文件**，每个文件的大小是 1GB，整个 redo log 日志文件组可以记录**4G**的内容\n它采用的是环形数组形式，从头开始写，写到末尾又回到头循环写，如下图所示\n在一个日志文件组中还有两个重要的属性，分别是 write pos、checkpoint\nwrite pos 是当前记录的位置，一边写一边后移 checkpoint 是当前要擦除的位置，也是往后推移 write pos 和 checkpoint 之间的还空着的部分可以用来写入新的 redo log 记录。 ly: 我的理解是有个缓冲带\n如果 write pos 追上 checkpoint (ly: 没有可以擦除的地方了），表示日志文件组满了，这时候不能再写入新的 redo log 记录，MySQL 得停下来，清空一些记录，把 checkpoint 推进一下。\nredo log 小结 # ★★这里有个很重要的问题，就是为什么允许擦除★★\n因为redo log记录的是数据页上的修改，如果Buffer Pool中数据页已经刷磁盘（这里说的磁盘是数据库数据吧）后，那这些记录就失效了，新日志会将这些失效的记录进行覆盖擦除。 redo log日志满了，在擦除之前，需要确保这些要被擦除记录对应在内存中的数据页都已经刷到磁盘中了。擦除旧记录腾出新空间这段期间，是不能再接收新的更新请求的，此刻MySQL的性能会下降。所以在并发量大的情况下，合理调整redo log的文件大小非常重要。 那为什么要绕这么一圈呢，只要每次把修改后的数据页直接刷盘不就好了，还有 redo log 什么事？\n1 Byte = 8bit 1 KB = 1024 Byte 1 MB = 1024 KB 1 GB = 1024 MB 1 TB = 1024 GB 实际上，数据页是16KB，刷盘比较耗时，有时候可能就修改了数据页里的几Byte数据，有必要把完整的数据页刷盘吗\n数据页刷盘是随机写，因为一个数据页对应的位置可能在硬盘文件的随机位置，所以性能是很差\n一个数据页对应的位置可能在硬盘文件的随机位置，即1页是16KB，这16KB，可能是在某个硬盘文件的某个偏移量到某个偏移量之间\n如果是写 redo log，一行记录可能就占几十 Byte，只包含表空间号、数据页号、磁盘文件偏移 量、更新值，再加上是顺序写，所以刷盘速度很快。\n其实内存的数据页在一定时机也会刷盘，我们把这称为页合并，讲 Buffer Pool的时候会对这块细说\nbinlog # redo log是物理日志，记录内容是**“在某个数据页上做了什么修改”，属于InnoDB 存储引擎**；而bin log是逻辑日志，记录内容是语句的原始逻辑，类似于 “给ID = 2 这一行的 c 字段加1”，属于MYSQL Server层\n无论用什么存储引擎，只要发生了表数据更新，都会产生于binlog 日志\nMySQL的数据库的数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性。 binlog会记录所有涉及更新数据的逻辑操作，而且是顺序写\n记录格式 # binlog 日志有三种格式，可以通过**binlog_format**参数指定。 statement row mixed 指定**statement，记录的内容是SQL语句原文**，比如执行一条update T set update_time=now() where id=1，记录的内容如下 同步数据时会执行记录的SQL语句，但有个问题，update_time = now() 会获取当前系统时间，直接执行会导致与原库的数据不一致\n为了解决上面问题，需要指定row，记录的不是简单的SQL语句，还包括操作的具体数据，记录内容如下\nrow格式的记录内容看不到详细信息，需要用mysqlbinlog工具解析出来 update_time=now()变成了具体的时间update_time=1627112756247，条件后面的@1、@2、@3 都是该行数据第 1 个~3 个字段的原始值（假设这张表只有 3 个字段） 这样就能保证同步数据的一致性，通常情况下都是指定row，可以为数据库的恢复与同步带来更好的可靠性\n但是由于row需要更大的容量来记录，比较占用空间，恢复与同步更消耗IO资源，影响执行速度。 折中方案，指定为mixed，记录内容为两者混合：MySQL会判断这条SQL语句是否引起数据不一致，如果是就用row格式，否则就使用statement格式\n写入机制 # binlog的写入时机：事务执行过程中，先把日志写到binlog cache，事务提交的时候（这个很重要，他不像redo log，binlog只有提交的时候才会刷盘），再把binlog cache写到binlog文件中\n因为一个事务的**binlog不能被拆开**，无论这个事务多大，也要确保一次性写入，所以系统会给每个线程分配一个块内存作为binlog cache\n我们可以通过binlog_cache_size参数控制单个线程 binlog cache 大小，如果存储内容超过了这个参数，就要暂存到磁盘（Swap）：\nbinlog日志刷盘流程如下\n上图的 write，是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快 上图的 fsync，才是将数据持久化到磁盘的操作 write和fsync的时机，由sync_binlog控制，默认为0\n为0时，表示每次提交的事务都只write，由系统自行判断什么时候执行fsync 虽然性能会提升，但是如果机器宕机，page cache里面的binlog会丢失\n设置为1，表示每次提交事务都会fsync ，就如同redo log日志刷盘流程 一样\n折中，可以设置为N\n在出现IO瓶颈的场景里，将sync_binlog设置成一个较大的值，可以提升性能\n同理，如果机器宕机，会丢失最近N个事务的binlog日志\n两阶段提交 # redo log（重做日志）让InnoDB存储引擎拥有了崩溃恢复的能力 binlog（归档日志）保证了MySQL集群架构的数据一致性 两者都属于持久性的保证，但侧重点不同\n更新语句过程，会记录redo log和binlog两块日志，以基本的事务为单位\nredo log在事务执行过程中可以不断地写入，而binlog只有在提交事务时才写入，所以redo log和binlog写入时机不一样\nredo log与binlog 两份日志之间的逻辑不一样，会出现什么问题？\n以update语句为例，假设id=2的记录，字段c值是0，把字段c值更新成1，SQL语句为update T set c=1 where id= 2\n假设执行过程中写完redo log日志后，binlog日志写期间发生了异常，会出现什么情况 由于binlog没写完就异常，这时候**binlog里面没有对应的修改记录**。因此，之后用**binlog日志恢复(备库)数据时，就会少这一次更新，恢复出来的这一行c值是0，而原库因为redo log日志恢复，这一行c值是1，最终数据不一致**。\n为了解决两份日志之间的逻辑一致问题，InnoDB存储引擎使用两阶段提交方案 即将redo log的写入拆成了两个步骤prepare和commit，这就是两阶段提交（其实就是等binlog正式写入后redo log才正式提交） 使用两阶段提交后，写入binlog时发生异常也不会有影响，因为**MySQL根据redo log日志恢复数据时，发现redo log还处于prepare阶段（也就是下图的非commit阶段），并且没有对应binlog日志**，就会回滚该事务。\n其实下图中，是否存在对应的binlog，就是想知道binlog是否是完整的，如果完整的话 redolog就可以提交 （箭头前面是否commit阶段，是的话就表示binlog写入期间没有出错，即binlog完整） 还有个问题，redo log设置commit阶段发生异常，那会不会回滚事务呢？ 并不会回滚事务，它会执行上图框住的逻辑，虽然redo log是处于prepare阶段，但是能通过事务id找到对应的binlog日志，所以**MySQL认为(binlog)是完整的**，就会提交事务恢复数据。\nundo log # 如果想要保证事务的原子性，就需要在异常发生时，对已经执行的操作进行回滚，在 MySQL 中，恢复机制是通过 回滚日志（undo log） 实现的，所有事务进行的修改都会先记录到这个回滚日志中，然后再执行相关的操作 如果执行过程中遇到异常的话，我们直接利用 回滚日志 中的信息将数据回滚到修改之前的样子即可！ 回滚日志会先于数据（数据库数据）持久化到磁盘上。这样就保证了即使遇到数据库突然宕机等情况，当用户再次启动数据库的时候，数据库还能够通过查询回滚日志来回滚将之前未完成的事务。 关于undo log：\n参考https://blog.csdn.net/Weixiaohuai/article/details/117867353\nundo log是逻辑日志，而且记录的是相反的语句\nundo log日志里面不仅存放着数据更新前的记录，还记录着RowID、事务ID、回滚指针。其中事务ID每次递增，回滚指针第一次如果是insert语句的话，回滚指针为NULL**，第二次update之后的undo log的回滚指针就会指向刚刚那一条undo log日志**，依次类推，就会形成一条undo log的回滚链，方便找到该条记录的历史版本\n更新数据之前，MySQL会提前生成undo log日志，当事务提交的时候，并不会立即删除undo log，因为后面可能需要进行回滚操作，要执行回滚（rollback）操作时，从缓存中读取数据。undo log日志的删除是通过通过后台purge线程进行回收处理的。\n举例\n假设有A、B两个数据，值分别为1,2。\nA. 事务开始\nB. 记录A=1到undo log中\nC. 修改A=3\nD. 记录B=2到undo log中\nE. 修改B=4\nF. 将undo log写到磁盘 \u0026mdash;\u0026mdash;-undo log持久化\nG. 将数据写到磁盘 \u0026mdash;\u0026mdash;-数据持久化\nH. 事务提交 \u0026mdash;\u0026mdash;-提交事务\n由于以下特点，所以能保证原子性和持久化\n更新数据前记录undo log。 为了保证持久性，必须将数据在事务提交前写到磁盘，只要事务成功提交，数据必然已经持久化到磁盘。 undo log必须先于数据持久化到磁盘。如果在G,H之间发生系统崩溃，undo log是完整的，可以用来回滚。 如果在A - F之间发生系统崩溃，因为数据没有持久化到磁盘，所以磁盘上的数据还是保持在事务开始前的状态。 参考https://developer.aliyun.com/article/1009683\nhttps://www.cnblogs.com/defectfixer/p/15835714.html\nMySQL 的 InnoDB 存储引擎使用“Write-Ahead Log”日志方案实现本地事务的原子性、持久性。\n“提前写入”（Write-Ahead），就是在事务提交之前，允许将变动数据写入磁盘。与“提前写入”相反的就是，在事务提交之前，不允许将变动数据写入磁盘，而是等到事务提交之后再写入。\n“提前写入”的好处是：有利于利用空闲 I/O 资源。但“提前写入”同时也引入了新的问题：在事务提交之前就有部分变动数据被写入磁盘，那么如果事务要回滚，或者发生了崩溃，这些提前写入的变动数据就都成了错误。“Write-Ahead Log”日志方案给出的解决办法是：增加了一种被称为 Undo Log 的日志，用于进行事务回滚。\n变动数据写入磁盘前，必须先记录 Undo Log，Undo Log 中存储了回滚需要的数据。在事务回滚或者崩溃恢复时，根据 Undo Log 中的信息对提前写入的数据变动进行擦除。\n更新一条语句的执行过程(ly:根据多方资料验证，这个是对的，事务提交前并不会持久化到db磁盘数据库文件中)\n回答题主的问题，对MySQL数据库来说，事务提交之前，操作的数据存储在数据库在内存区域中的缓冲池中，即写的是内存缓冲池中的页(page cache)，同时会在缓冲池中写undolog(用于回滚)和redolog、binlog(用于故障恢复，保证数据持久化的一致性)，事务提交后，有数据变更的页，即脏页，会被持久化到物理磁盘。\n作者：王同学 链接：https://www.zhihu.com/question/278643174/answer/1998207141 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。\n执行后的几个步骤\n事务开始 申请加锁：表锁、MDL 锁、行锁、索引区间锁（看情况加哪几种锁） 执行器找存储引擎取数据。 如果记录所在的数据页本来就在内存（innodb_buffer_cache）中，存储引擎就直接返回给执行器； 否则，存储引擎需要先将该数据页从磁盘读取到内存，然后再返回给执行器。 执行器拿到存储引擎给的行数据，进行更新操作后，再调用存储引擎接口写入这行新数据(6 - 9)。 存储引擎将回滚需要的数据记录到 Undo Log，并将这个更新操作记录到 Redo Log，此时 Redo Log 处于 prepare 状态。并将这行新数据更新到内存（innodb_buffer_cache）中。同时，然后告知执行器执行完成了，随时可以提交事务。 手动事务 commit：执行器生成这个操作的 Binary Log，并把 Binary Log 写入磁盘。 执行器调用存储引擎的提交事务接口，存储引擎把刚刚写入的 Redo Log 改成 commit 状态。 事务结束 MVCC # MVCC 的实现依赖于：隐藏字段、Read View、undo log。\n内部实现中，InnoDB 通过数据行的 DB_TRX_ID 和 Read View 来判断数据的可见性，如不可见，则通过数据行的 DB_ROLL_PTR 找到 undo log 中的历史版本。\n每个事务读到的数据版本可能是不一样的，在同一个事务中，用户只能看到该事务创建 Read View 之前已经提交的修改和该事务本身做的修改\n总结 # MySQL InnoDB 引擎使用 redo log(重做日志) 保证事务的持久性，使用 undo log(回滚日志) 来保证事务的原子性。 MySQL数据库的数据备份、主备、主主、主从都离不开binlog，需要依靠binlog来同步数据，保证数据一致性。 三大日志大概的流程 "},{"id":120,"href":"/zh/docs/technology/Review/java_guide/database/MySQL/ly0601lymysql-index/","title":"索引","section":"MySQL","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n补充索引基础知识(引自b站sgg视频) # 存储引擎，数据的基本单位是页，如果数据很少，只有一页，那就简单，是直接二分查找(不涉及磁盘IO)；如果数据很多，有好几个页，那么需要对页建立一种数据结构，能够最快定位到哪一页，然后减少磁盘IO 索引介绍 # 索引是一种用于快速查询和检索数据的数据结构，其本质可以看成是一种排序好的数据结构\n索引的作用就相当于书的目录。打个比方: 我们在查字典的时候，如果没有目录，那我们就只能一页一页的去找我们需要查的那个字，速度很慢。如果有目录了，我们只需要先去目录里查找字的位置，然后直接翻到那一页就行了\n索引底层数据结构存在很多种类型，常见的索引结构有：B树，B+树和Hash、红黑树。在MySQL中，无论是Innodb还是MyIsam，都使用了B+树作为索引结构\n索引的优缺点 # 优点：\n使用索引可以大大加快 数据的检索速度（大大减少检索的数据量）, 这也是创建索引的最主要的原因。 通过创建唯一性索引，可以保证数据库表中每一行数据的唯一性。 缺点：\n创建索引和维护索引需要耗费许多时间。当对表中的数据进行增删改的时候，如果数据有索引，那么索引也需要动态的修改，会降低 SQL 执行效率。 索引需要使用物理文件存储，也会耗费一定空间 索引一定会提高查询性能吗\n多数情况下，索引查询都是比全表扫描要快的。但是如果数据库的数据量不大，那么使用索引也不一定能够带来很大提升 索引的底层数据结构 # Hash表 # 哈希表是键值对的集合，通过键（key）即可快速取出对应的值（value），因此哈希表可以快速检索数据（接近O(1))\n为何能够通过key快速取出value呢？原因在于哈希算法（也叫散列算法）。通过哈希算法，我们可以快速找到key对应的index，找到了index也就找到了对应的value\nhash = hashfunc(key) index = hash % array_size 注意，图中keys[天蓝色]是字符串，不是什么莫名其妙的人 哈希算法有个 Hash 冲突 问题，也就是说多个不同的 key 最后得到的 index 相同。通常情况下，我们常用的解决办法是 链地址法。链地址法就是将哈希冲突数据存放在链表中。就比如 JDK1.8 之前 HashMap 就是通过链地址法来解决哈希冲突的。不过，JDK1.8 以后HashMap为了减少链表过长的时候搜索时间过长引入了红黑树。\n为了减少 Hash 冲突的发生，一个好的哈希函数应该**“均匀地”将数据分布**在整个可能的哈希值集合中\n由于Hash索引不支持顺序和范围查询，假如要对表中的数据进行排序或者进行范围查询，那Hash索引就不行了，并且，每次IO只能取一个\n例如： SELECT * FROM tb1 WHERE id \u0026lt; 500 ; 这种范围查询中，B+树 优势非常大 直接遍历比500小的叶子节点即可 如果使用Hash索引，由于Hash索引是根据hash算法来定位的，难不成把1 ~499 （小于500）的数据都进行一次hash计算来定位吗？这就是Hash最大的缺点 这里其实说的是已经找到了索引，但是索引没有数据的情形。要么通过hash一个个取数据，要么利用B+树的特性（叶子节点有完整数据）\nB树\u0026amp; B+ 树 # B树也称B-树，全称为多路平衡查找树，B+树是B树的一种变体\nB树和B+树中的B是Balanced（平衡）的意思\n目前大部分数据库以及文件系统都采用B-Tree或者其变种B+Tree作为索引结构\nB树\u0026amp;B+树两者有何异同呢\nB树的所有结点既存放键（key）也存放数据（data），而B+树只有叶子结点存放key和data，其他内节点只存放key B树的叶子节点都是独立的；B+树的叶子节点有一条引用链指向与它相邻的叶子节点 B树的检索的过程相当于对范围内的每个结点的关键字做二分查找，可能还没有到达叶子节点，检索就结束了。而B+树的检索效率比较稳定，任何查找都是从根节点到叶子节点的过程，叶子结点的顺序检索很明显 B树中某个子节点，他都包括了父节点的某个节点 如图 在MySQL中，MyISAM引擎和InnoDB引擎都是使用B+Tree作为索引结构，但是，两者的实现方式有点不太一样\nMyISAM 引擎中，B+Tree 叶节点的 data 域存放的是数据记录的地址。在索引检索的时候，首先按照 B+Tree 搜索算法搜索索引，如果指定的 Key 存在，则取出其 data 域的值，然后以 data 域的值为地址读取相应的数据记录。这被称为“非聚簇索引（非聚集索引）”。【反例，B+ 树非叶子节点没有存储数据记录的地址/数据记录本身】 InnoDB 引擎中，其数据文件本身就是索引文件。 MyISAM 的 索引文件和数据文件是分离的，而InnoDB引擎中其表数据文件本身就是按 B+Tree 组织的一个索引结构，树的叶节点 data 域保存了完整的数据记录。这个索引的 key 是数据表的主键（而非地址），因此 InnoDB 表数据文件本身就是主索引。这被称为“聚簇索引（聚集索引）”，而其余的索引都作为 辅助索引 ，辅助索引的 data 域存储相应记录主键的值而不是地址，这也是和 MyISAM 不同的地方。在根据主索引搜索时，直接找到 key 所在的节点即可取出数据；在根据辅助索引查找时，则需要先取出主键的值，再走一遍主索引。 在设计表的时候，不建议使用过长的字段作为主键，也不建议使用非单调的字段作为主键，这样会造成主索引频繁分裂。\n原因： InnoDB的辅助索引data域存储相应记录主键的值而不是地址。所以不建议使用过长的字段作为主键，因为所有辅助索引都引用主索引，过长的主索引会令辅助索引变得过大 (不建议使用过长的字段作为主键) InnoDB数据文件本身是一颗B+Tree，非单调的主键会造成在插入新记录时数据文件为了维持B+Tree的特性而频繁的分裂调整，十分低效。(不建议使用非单调的字段作为主键) MySQL底层数据结构总结 # 索引类型总结 # 按照数据结构维度划分：\nBTree 索引：MySQL 里默认和最常用的索引类型。只有叶子节点存储 value，非叶子节点只有指针和 key。存储引擎 MyISAM 和 InnoDB 实现 BTree 索引都是使用 B+Tree，但二者实现方式不一样（前面已经介绍了）。 哈希索引：类似键值对的形式，一次即可定位。 RTree 索引：一般不会使用，仅支持 geometry 数据类型，优势在于范围查找，效率较低，通常使用搜索引擎如 ElasticSearch 代替。 全文索引：对文本的内容进行分词，进行搜索。目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替。 按照底层存储方式角度划分：\n聚簇索引（聚集索引）：索引结构和数据一起存放的索引，InnoDB 中的主键索引就属于聚簇索引。 非聚簇索引（非聚集索引）：索引结构和数据分开存放的索引，**二级索引(辅助索引)**就属于非聚簇索引。MySQL 的 MyISAM 引擎，不管主键还是非主键，使用的都是非聚簇索引。 按照应用维度划分：\n主键索引：加速查询 + 列值唯一（不可以有 NULL）+ 表中只有一个。 普通索引：仅加速查询。 唯一索引：加速查询 + 列值唯一（可以有 NULL）。 覆盖索引：一个索引包含（或者说覆盖）所有需要查询的字段的值。 联合索引：多列值组成一个索引，专门用于组合搜索，其效率大于索引合并。 全文索引：对文本的内容进行分词，进行搜索。目前只有 CHAR、VARCHAR ，TEXT 列上可以创建全文索引。一般不会使用，效率较低，通常使用搜索引擎如 ElasticSearch 代替。 MySQL 8.x 中实现的索引新特性：\n隐藏索引：也称为不可见索引，不会被优化器使用，但是仍然需要维护，通常会软删除和灰度发布的场景中使用。主键不能设置为隐藏（包括显式设置或隐式设置）。 降序索引：之前的版本就支持通过 desc 来指定索引为降序，但实际上创建的仍然是常规的升序索引。直到 MySQL 8.x 版本才开始真正支持降序索引。另外，在 MySQL 8.x 版本中，不再对 GROUP BY 语句进行隐式排序。 函数索引：从 MySQL 8.0.13 版本开始支持在索引中使用函数或者表达式的值，也就是在索引中可以包含函数或者表达式。 索引类型 # 主键索引（Primary Key） # 数据表的主键列，使用的就是主键索引 一张数据表只能有一个主键，并且主键不能为null，不能重复 在 MySQL 的 InnoDB 的表中，当没有显示的指定表的主键时，InnoDB 会自动先检查表中是否有唯一索引且不允许存在 null 值的字段，如果有，则选择该字段为默认的主键，否则 InnoDB 将会自动创建一个 6Byte 的自增主键 如图 二级索引（辅助索引） # 二级索引又称为辅助索引，是因为二级索引的叶子结点存储的数据是主键。也就是说，通过二级索引，可以定位主键的位置（还有值）\n唯一索引、普通索引、前缀索引等索引都属于二级索引\n唯一索引 Unique Key：是一种约束，该索引的属性列不能出现重复的数据，但是允许数据为NULL，一张表允许创建多个唯一索引。建立唯一索引的目的多是为了该属性列的数据的唯一性，而不是为了查询效率\n普通索引 Index：普通索引的唯一作用就是为了快速查询数据，一张表允许创建多个普通索引，并允许数据重复和NULL\n前缀索引 Prefix：前缀索引只适用于字符串类型的数据。前缀索引是对文本的前几个字符创建索引，相比普通索引建立的数据更小，因为只取前几个字符\n全文索引Full Text：全文索引主要是为了检索大文本数据中的关键字的信息，是目前搜索引擎数据库使用的一种技术。\nMysql5.6 之前只有 MYISAM 引擎支持全文索引，5.6 之后 InnoDB 也支持了全文索引。\n二级索引： 聚簇索引与非聚簇索引 # 聚簇索引（聚集索引） # 聚簇索引介绍 聚簇索引即索引结构和数据一起存放的索引，并不是一种单独的索引类型。InnoDB中的主键索引就属于聚簇索引 MySQL中InnoDB引擎的表的**.ibd 文件就包含了该表的索引和数据**，对于InnoDB引擎表来说，该表的索引（B+树）的每个非叶子节点存储索引(和页地址)，叶子结点存储索引和索引对应的数据 聚簇索引的优缺点 优点 查询速度非常快：聚簇索引的查询速度非常的快，因为整个B+树本身就是一颗多差平衡树，叶子节点也都是有序的，定位到索引的节点，就相当于定位到了数据。相比于非聚簇索引，聚簇索引少了一次读取数据的IO操作 对排序查找和范围查找优化：聚簇索引对于逐渐的排序查找和范围查找速度非常快 缺点 依赖于有序的数据：因为B+树是多路平衡树，如果索引的数据不是有序的，那么就需要在插入时排序。如果数据是整型还好，否则类似于字符串或UUID这种又长有难比较的数据，插入或查找的速度较慢 更新代价大：如果对索引列的数据被修改时，那么对应的索引也将会被修改，而且聚簇索引的叶子节点还存放着数据，修改代价肯定是较大的，所以对于主键索引来说，主键一般都是不可被修改的 非聚簇索引（非聚集索引） # 优点：\n更新代价比聚簇索引要小。因为非聚簇索引的叶子节点是不存放数据的\n缺点：\n依赖于有序数据：跟聚簇索引一样，非聚簇索引也依赖于有序数据 可能会二次查询（回表）：这应该是非聚簇索引最大的缺点了。 当查到索引对应的指针或主键后，可能还需要根据指针或主键再到数据文件或表中查询 MySQL的表的文件截图： 聚簇索引和非聚簇索引：\n聚簇索引一定回表查询吗(覆盖索引)\n非聚簇索引不一定回表查询\n试想一种情况，用户准备使用 SQL 查询用户名，而用户名字段正好建立了索引。\nSELECT name FROM table WHERE name=\u0026#39;guang19\u0026#39;; 那么这个索引的 key 本身就是 name，查到对应的 name 直接返回就行了，无需回表查询。\n即使是 MYISAM 也是这样，虽然 MYISAM 的主键索引确实需要回表，因为它的主键索引的叶子节点存放的是指针。但是！如果 SQL 查的就是主键(本身)呢?\nSELECT id FROM table WHERE id=1; 主键索引本身的 key 就是主键，查到返回就行了。这种情况就称之为覆盖索引了\n覆盖索引和联合索引 # 覆盖索引 # 如果一个索引包含（或者说覆盖）所有需要查询的字段的值，我们就称之为“覆盖索引”。（也就是不用回表）\n我们知道在 InnoDB 存储引擎中，如果不是主键索引（叶子节点存储的是主键+列值），最终还是要“回表”，也就是要通过主键再查找一次，这样就会比较慢。覆盖索引就是把要查询出的列和索引是对应的，不做回表操作！\n覆盖索引即需要查询的字段正好事索引的字段，那么直接根据该索引，就可以查到数据了，而无需回表查询\n如主键索引，如果一条 SQL 需要查询主键，那么正好根据主键索引就可以查到主键。\n再如普通索引，如果一条 SQL 需要查询 name，name 字段正好有索引， 那么直接根据这个索引就可以查到数据，也无需回表。\n我觉得覆盖索引要在联合索引上体现的话功能会比较突出\n联合索引 # 使用表中的多个字段创建索引，也就是联合索引，也叫组合索引，或复合索引\n最左前缀匹配原则 # 最左前缀匹配原则指的是，在使用联合索引时，MySQL 会根据联合索引中的字段顺序，从左到右依次到查询条件中去匹配，如果查询条件中存在与联合索引中最左侧字段相匹配的字段，则就会使用该字段过滤一批数据，直至联合索引中全部字段匹配完成，或者在执行过程中遇到范围查询，如 \u0026gt;、\u0026lt;、between 和 以%开头的like查询 等条件，才会停止匹配。 所以，我们在使用联合索引时，可以将区分度高的字段放在最左边，这也可以过滤更多数据 索引下推 # 索引下推（Index Condition Pushdown） 是 MySQL 5.6 版本中提供的一项索引优化功能，可以在非聚簇索引遍历过程中，对（即能用索引先用索引）索引中包含的字段先做判断，过滤掉不符合条件的记录，减少回表次数。\n例子：\n对于SELECT * from user where name like '陈%' and age=20这条语句 其中主要几个字段有：id、name、age、address。建立联合索引（name，age）\n最关键的一点： 组合索引满足最左匹配，但是遇到非等值判断时匹配停止。 name like \u0026lsquo;陈%\u0026rsquo; 不是等值匹配，所以 age = 20 这里就用不上 (name,age) 组合索引了。如果没有索引下推，组合索引只能用到 name，age 的判定就需要回表才能做了。5.6之后有了索引下推，age = 20 可以直接在组合索引里判定。\n5.6之前的版本是没有索引下推这个优化的，会忽略age这个字段，直接通过name进行查询，在(name,age)这课树上查找到了两个结果，id分别为2,1，然后拿着取到的id值一次次的回表查询，因此这个过程需要回表两次 5.6版本添加了索引下推这个优化 InnoDB并没有忽略age这个字段，而是在索引内部就判断了age是否等于20，对于不等于20的记录直接跳过，因此在(name,age)这棵索引树中只匹配到了一个记录，此时拿着这个id去主键索引树中回表查询全部数据，这个过程只需要回表一次 争取使用索引的一些建议 # 选择合适的字段创建索引 # 不为 NULL 的字段 ：索引字段的数据应该尽量不为 NULL，因为对于数据为 NULL 的字段，数据库较难优化。如果字段频繁被查询，但又避免不了为 NULL，建议使用 0,1,true,false 这样语义较为清晰的短值或短字符作为替代。 被频繁查询的字段 ：我们创建索引的字段应该是查询操作非常频繁的字段。 被作为条件查询的字段 ：被作为 WHERE 条件查询的字段，应该被考虑建立索引。 频繁需要排序的字段 ：索引已经排序，这样查询可以利用索引的排序，加快排序查询时间。 被经常频繁用于连接的字段 ：经常用于连接的字段可能是一些外键列，对于外键列并不一定要建立外键，只是说该列涉及到表与表的关系。对于频繁被连接查询的字段，可以考虑建立索引，提高多表连接查询的效率 被频繁更新的字段应该慎重建索引 # 虽然索引能带来查询上的效率，但是维护索引的成本也是不小的。 如果一个字段不被经常查询，反而被经常修改，那么就更不应该在这种字段上建立索引了。\n尽可能地考虑建立联合索引而不是单列索引 # 因为索引是需要占用磁盘空间的，可以简单理解为每个索引都对应着一颗 B+树。如果一个表的字段过多，索引过多，那么当这个表的数据达到一个体量后，索引占用的空间也是很多的，且修改索引时，耗费的时间也是较多的。如果是联合索引，多个字段在一个索引上，那么将会节约很大磁盘空间，且修改数据的操作效率也会提升。\n注意避免冗余索引 # 冗余索引指的是索引的功能相同，能够命中索引(a, b)就肯定能命中索引(a) ，那么索引(a)就是冗余索引\n（name,city ）和（name ）这两个索引就是冗余索引，能够命中前者的查询肯定是能够命中后者的 在大多数情况下，都应该尽量扩展已有的索引而不是创建新索引\n考虑在字符串类型的字段上使用前缀索引代替普通索引 # 前缀索引仅限于字符串类型，较普通索引会占用更小的空间，所以可以考虑使用前缀索引带替普通索引。\n避免索引失效 # 使用 SELECT * 进行查询;\n创建了组合索引，但查询条件未准守最左匹配原则;\n在索引列上进行计算、函数、类型转换等操作;\n以 % 开头的 LIKE 查询比如 like '%abc';\n%在左边，即使有索引，也会失效 只有当%在右边时，才会生效 查询条件中使用 or，且 or 的前后条件中有一个列没有索引，涉及的索引都不会被使用到(也就是说，反正都是要全表扫描，所以就不用索引了)\n删除长期未使用的索引 # 删除长期未使用的索引，不用的索引的存在会造成不必要的性能损耗\nMySQL 5.7 可以通过查询 sys 库的 schema_unused_indexes 视图来查询哪些索引从未被使用\n"},{"id":121,"href":"/zh/docs/technology/Review/java_guide/database/ly0502lycharactor-set/","title":"字符集详解","section":"数据库","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n图示总结\nMySQL字符编码集有两套UTF-8编码实现：utf-8 和 utf8mb4\n而其中，utf-8 不支持存储emoji符号和一些比较复杂的汉字、繁体字，会出错 何为字符集 # 字符是各种文字和符号的统称，包括各个国家文字、标点符号、表情、数字等等\n字符集就是一系列字符的集合，字符集的种类较多，每个字符集可以表示的字符范围通常不同，就比如说有些字符集无法表示汉字 计算机只能存储二进制的数据，那英文、汉字、表情等字符应该如何存储呢\n我们要将这些字符和二进制的数据一一对应起来，比如说字符“a”对应“01100001”，反之，“01100001”对应 “a”。我们将字符对应二进制数据的过程称为\u0026quot;字符编码\u0026quot;，反之，二进制数据解析成字符的过程称为“字符解码”。\n有哪些常见的字符集 # 常见的字符集有ASCLL、GB2312、GBK、UTF-8 不同的字符集的主要区别在于 可以表示的字符范围 编码方式 ASCLL # ASCII (American Standard Code for Information Interchange，美国信息交换标准代码) 是一套主要用于现代美国英语的字符集（这也是 ASCII 字符集的局限性所在）\n为什么 ASCII 字符集没有考虑到中文等其他字符呢？ 因为计算机是美国人发明的，当时，计算机的发展还处于比较雏形的时代，还未在其他国家大规模使用。因此，美国发布 ASCII 字符集的时候没有考虑兼容其他国家的语言\nASCII 字符集至今为止共定义了 128 个字符，其中有 33 个控制字符（比如回车、删除）无法显示\n一个 ASCII 码长度是一个字节也就是 8 个 bit，比如“a”对应的 ASCII 码是“01100001”。不过，最高位是 0 仅仅作为校验位，其余 7 位使用 0 和 1 进行组合，所以，ASCII 字符集可以定义 128（2^7）个字符\n由于，ASCII 码可以表示的字符实在是太少了。后来，人们对其进行了扩展得到了 ASCII 扩展字符集 。ASCII 扩展字符集使用 8 位（bits）表示一个字符，所以，ASCII 扩展字符集可以定义 256（2^8）个字符\n总共128个，下面少了33个无法显示的控制字符 GB2312 # 我们上面说了，ASCII 字符集是一种现代美国英语适用的字符集。因此，很多国家都捣鼓了一个适合自己国家语言的字符集。\nGB2312 字符集是一种对汉字比较友好的字符集，共收录 6700 多个汉字，基本涵盖了绝大部分常用汉字。不过，GB2312 字符集不支持绝大部分的生僻字和繁体字 （对于中英文字符，使用的字节数不一样 ( 1和2 ) ）对于英语字符，GB2312 编码和 ASCII 码是相同的，1 字节编码即可。对于非英字符，需要 2 字节编码。 GBK # GBK 字符集可以看作是 GB2312 字符集的扩展，兼容 GB2312 字符集，共收录了 20000 多个汉字。\nGBK 中 K 是汉语拼音 Kuo Zhan（扩展）中的“Kuo”的首字母\nGB18030 # GB18030 完全兼容 GB2312 和 GBK 字符集，纳入中国国内少数民族的文字，且收录了日韩汉字，是目前为止最全面的汉字字符集，共收录汉字 70000 多个\nBIG5 # BIG5 主要针对的是繁体中文，收录了 13000 多个汉字。\nUnicode \u0026amp; UTF-8编码 # 了更加适合本国语言，诞生了很多种字符集。\n我们上面也说了不同的字符集可以表示的字符范围以及编码规则存在差异。这就导致了一个非常严重的问题：使用错误的编码方式查看一个包含字符的文件就会产生乱码现象。 就比如说你使用 UTF-8 编码方式打开 GB2312 编码格式的文件就会出现乱码。示例：“牛”这个汉字 GB2312 编码后的十六进制数值为 “C5A3”，而 “C5A3” 用 UTF-8 解码之后得到的却是 “ţ”。\n你可以通过这个网站在线进行编码和解码：https://www.haomeili.net/HanZi/ZiFuBianMaZhuanHuan 乱码的本质：编码和解码时用了不同或者不兼容的字符集\n如果我们能够有一种字符集将世界上所有的字符都纳入其中就好了，于是Unicode带着这个使命诞生了。\nUnicode 字符集中包含了世界上几乎所有已知的字符。不过，Unicode 字符集并没有规定如何存储这些字符（也就是如何使用二进制数据表示这些字符） 于是有了 UTF-8（8-bit Unicode Transformation Format）。类似的还有 UTF-16、 UTF-32\n其中，UTF-8 使用1-4个字节为每个字符编码，UTF-16使用2或4个字节为每个字符编码，UTF-32固定使用4个字节为每个字符编码\nUTF-8 可以根据不同的符号自动选择编码的长短，像英文字符只需要 1 个字节就够了，这一点 ASCII 字符集一样 。因此，对于英语字符，UTF-8 编码和 ASCII 码是相同的\nUTF-32 的规则最简单，不过缺陷也比较明显，对于英文字母这类字符消耗的空间是 UTF-8 的 4 倍之多。\nUTF-8 是目前使用最广的一种字符编码 MySQL字符集 # MySQL支持很多字符编码的方式，比如UTF-8，GB2312，GBK，BIG5\n使用SHOW CHARSET命令查看 通常情况下，我们建议使用UTF-8作为默认的字符编码方式\n然而，MySQL字符编码中有两套UTF-8编码实现\nutf-8：utf8编码只支持1-3个字节 。 在 utf8 编码中，中文是占 3 个字节，其他数字、英文、符号占一个字节。但 emoji 符号占 4 个字节，一些较复杂的文字、繁体字也是 4 个字节 utf8mb4 ： UTF-8 的完整实现，正版！最多支持使用 4 个字节表示字符，因此，可以用来存储 emoji 符号 为何会有两套UTF-8编码实现，原因如下 因此，如果你需要存储emoji类型的数据或者一些比较复杂的文字、繁体字到 MySQL 数据库的话，数据库的编码一定要指定为utf8mb4 而不是utf8 ，要不然存储的时候就会报错了。 测试：\n环境，MySQL 5.7 + 建表语句： ，这里指定数据库CHARSET为utf8\nCREATE TABLE `user` ( `id` varchar(66) NOT NULL, `name` varchar(33) NOT NULL, `phone` varchar(33) DEFAULT NULL, `password` varchar(100) DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8; CREATE TABLE `user` ( `id` varchar(66) CHARACTER SET utf8mb4 NOT NULL, `name` varchar(33) CHARACTER SET utf8mb4 NOT NULL, `phone` varchar(33) CHARACTER SET utf8mb4 DEFAULT NULL, `password` varchar(100) CHARACTER SET utf8mb4 DEFAULT NULL ) ENGINE=InnoDB DEFAULT CHARSET=utf8; ------ 这边应该是写错了，如果是这个sql，是可以插入成功的 著作权归所有 原文链接：https://javaguide.cn/database/character-set.html 插入\nINSERT INTO `user` (`id`, `name`, `phone`, `password`) VALUES (\u0026#39;A00003\u0026#39;, \u0026#39;guide哥😘😘😘\u0026#39;, \u0026#39;181631312312\u0026#39;, \u0026#39;123456\u0026#39;); -- 报错 Incorrect string value: \u0026#39;\\xF0\\x9F\\x98\\x98\\xF0\\x9F...\u0026#39; for column \u0026#39;name\u0026#39; at row 1 "},{"id":122,"href":"/zh/docs/technology/Review/java_guide/cs_basics/data-structure/tree/","title":"树","section":"数据结构","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n树是一种类似现实生活中的树的数据结构（倒置的树）\n任何一颗非空树只有一个根节点\n一棵树具有以下特点：\n一棵树中的任何两个节点有且仅有唯一的一条路相通 （因为每个结点只会有一个父节点） 一棵树如果有n个节点，那么它一定恰好有n-1条边 一棵树不包括回路 下面是一颗二叉树 深度和高度是对应的；根节点所在层为1层\n常用概念\n节点：树中每个元素都可以统称为节点\n根节点：顶层节点，或者说没有父节点的节点。上图中A节点就是根节点\n父节点：若一个节点含有子节点，则这个节点称为其子节点的父节点。上图中的 B 节点是 D 节点、E 节点的父节点\n兄弟节点：具有相同父节点的节点互称为兄弟节点。上图中 D 节点、E 节点的共同父节点是 B 节点，故 D 和 E 为兄弟节点。\n叶子节点：没有子节点的节点。上图中的 D、F、H、I 都是叶子节点\n节点的高度**（跟叶子节点有关，同一层不一定一样）：该节点到叶子节点的最长路径所包含的边数。\n节点的深度**（跟根节点有关，同一层是一样的）：根节点到该节点的路径所包含的边数**\n节点的层数：节点的深度+1\n树的高度：根节点的高度\n二叉树的分类 # **二叉树（Binary tree）**是每个节点最多只有两个分支（即不存在分支度大于2的节点）的树结构 二叉树的分支，通常被称为左子树或右子树，并且，二叉树的分支具有左右次序，不能随意颠倒 二叉树的第i层至多拥有2^(i-1) 个节点\n深度为k的二叉树至多总共有 2^(k+1) -1 个节点 （深度为k，最多k + 1 层，最多为满二叉树的情况）\n至少有2^(k) 个节点，即 深度为k-1的二叉树的最多的节点再加1 （关于节点的深度的定义国内争议比较多，我个人比较认可维基百科对节点深度的定义open in new window）。 满二叉树 # 一个二叉树，如果每一个层的结点数都达到最大值，则这个二叉树就是 满二叉树。也就是说，如果一个二叉树的层数为 K，且结点总数是(2^k) -1 ，则它就是 满二叉树。 完全二叉树 # 定义：除最后一层外，若其余层都是满的，并且最后一层或者是满的，或者是在右边缺少连续若干节点，则这个二叉树就是 完全二叉树 。\n大家可以想象为一棵树从根结点开始扩展，扩展完左子节点才能开始扩展右子节点，每扩展完一层，才能继续扩展下一层。如下图所示：\n从左到右，从上到下：\n完全二叉树的性质：父结点和子节点的序号有着对应关系\n细心的小伙伴可能发现了，当根节点的值为 1 的情况下，若父结点的序号是 i，那么左子节点的序号就是 2i，右子节点的序号是 2i+1。这个性质使得完全二叉树利用数组存储时可以极大地节省空间，以及利用序号找到某个节点的父结点和子节点，后续二叉树的存储会详细介绍。\n平衡二叉树 # 平衡二叉树是一颗二叉排序树，且具有以下性质\n可以是一棵空树 如果不是空树，那么左右两个子树的高度差的绝对值不超过1，并且左右两个子树都是一棵平衡二叉树 平衡二叉树的常用实现方法有 红黑树、AVL 树、替罪羊树、加权平衡树、伸展树 等。\n下面看一颗不太正常的树 这玩意儿还真叫树，只不过这棵树已经退化为一个链表了，我们管它叫 斜树。\n二叉树相比于链表，由于父子节点以及兄弟节点之间往往具有某种特殊的关系，这种关系使得我们在树中对数据进行搜索和修改时，相对于链表更加快捷便利。 如果二叉树退化为一个链表了，那么那么树所具有的优秀性质就难以表现出来，效率也会大打折，为了避免这样的情况，我们希望每个做 “家长”（父结点） 的，都 一碗水端平，分给左儿子和分给右儿子的尽可能一样多，相差最多不超过一层，如下图所示： 二叉树的存储 # 二叉树的存储主要分为链式存储和顺序存储 链式存储 # 和链表类似，二叉树的链式存储依靠指针将各个结点串联起来，不需要连续的存储空间 每个节点包括三个属性 数据data data不一定是单一的数据，根据情况不同，可以是多个具有不同类型的数据 左节点指针 left 右节点指针 right Java没有指针，而是直接引用对象 顺序存储 # 就是利用数组进行存储，数组中每一个位置仅存储结点的data，不存储左右子节点的指针，子节点的索引通过数组下标完成（类似堆） 根节点的序号为1，对于每个节点 Node，假设它存储在数组中下标为 i 的位置，那么它的左子节点就存储在 2i 的位置，它的右子节点存储在下标为 2i+1 的位置。 如图 存储如下数组，会发现问题：如果要存储的二叉树不是完全二叉树，在数组中就会出现空隙，导致内存利用率降低 二叉树的遍历 # 先序遍历 # 定义：先输出根节点，再遍历左子树，最后遍历右子树。\u0026lt;遍历左子树和右子树的时候，同样遵循先序遍历的规则\u0026gt;。也就是说，可以使用递归实现先序遍历\npublic void preOrder(TreeNode root){ if(root == null){ return; } system.out.println(root.data); preOrder(root.left); preOrder(root.right); } 中序遍历 # 定义：先递归中序遍历左子树，再输出根结点的值，再递归中序遍历右子树，大家可以想象成一巴掌把树压扁，父结点被拍到了左子节点和右子节点的中间（倒影、映射）\npublic void inOrder(TreeNode root){ if(root == null){ return; } inOrder(root.left); system.out.println(root.data); inOrder(root.right); } 如图所示 后续遍历 # 定义：先递归后序遍历左子树，再递归后序遍历右子树，最后输出根结点的值\n代码\npublic void postOrder(TreeNode root){ if(root == null){ return; } postOrder(root.left); postOrder(root.right); system.out.println(root.data); } 如图\n"},{"id":123,"href":"/zh/docs/technology/Review/java_guide/cs_basics/data-structure/heap/","title":"堆","section":"数据结构","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n什么是堆 # 堆是满足以下条件的树 堆中每一个节点值都大于等于（或小于等于）子树中所有节点。或者说，任意一个节点的值**都大于等于（或小于等于）**所有子节点的值\n大家可以把堆(最大堆)理解为一个公司,这个公司很公平,谁能力强谁就当老大,不存在弱的人当老大,老大手底下的人一定不会比他强。这样有助于理解后续堆的操作。\n堆不一定是完全二叉树，为了方便存储和索引，我们通常用完全二叉树的形式来表示堆\n广为人知的斐波那契堆和二项堆就不是完全二叉树，它们甚至都不是二叉树 (二叉)堆是一个数组，它可以被看成是一个近似的完全二叉树 下面给出的图是否是堆（通过定义）\n1，2是。 3不是。 堆的用途 # 当我们只关心所有数据中的最大值或者最小值，存在多次获取最大值或者最小值，多次插入或删除数据时，就可以使用堆。\n有小伙伴可能会想到用有序数组，初始化一个有序数组时间复杂度是 O(nlog(n))[也就是将一堆数字乱序排序，最快是O(nlog(n))]，查找最大值或者最小值时间复杂度都是 O(1)，但是，涉及到更新（插入或删除）数据时，时间复杂度为 O(n)，即使是使用复杂度为 O(log(n)) 的二分法找到要插入或者删除的数据，在移动数据时也需要 O(n) 的时间复杂度。\n相对于有序数组而言，堆的主要优势在于更新数据效率较高\n堆的初始化时间复杂度为O(nlog(n))，堆可以做到O(1)的时间复杂度取出最大值或者最小值，O(log(n))的时间复杂度插入或者删除数据 堆的分类 # 堆分为最大堆和最小堆，二者的区别在于节点的排序方式 最大堆：堆中的每一个节点的值都大于子树中所有节点的值 最小堆：堆中的每一个节点的值都小于子树中所有节点的值 如图，图1是最大堆，图2是最小堆 堆的存储 # 由于完全二叉树的优秀性质，利用数组存储二叉树即节省空间，又方便索引（若根结点的序号为1，那么对于树中任意节点i，其左子节点序号为 2*i，右子节点序号为 2*i+1）。 为了方便存储和索引，（二叉）堆可以用完全二叉树的形式进行存储。存储的方式如下图所示 堆的操作 # 堆的更新操作主要包括两种：插入元素和删除堆顶元素\n堆是一个公平的公司，有能力的人自然会走到与他能力所匹配的位置\n插入元素 # 将要插入的元素放到最后 从底向上，如果父节点比该元素小，则该节点和父节点交换（其实就是一棵树有3个（最多）节点，与树上最大的节点比较） 直到无法交换（已经与根节点比较过） 删除堆顶元素 # 根据堆的性质可知，最大堆的堆盯元素为所有元素中最大的，最小堆的堆顶元素是所有元素中最小的\n当我们需要多次查找最大元素或者最小元素的时候，可以利用堆来实现\n删除堆顶元素后，为了保持堆的性质，需要对堆的结构进行调整，我们可以将这个过程称之为堆化\n自底向上的堆化，上述的插入元素所使用的，就是自顶向上的堆化，元素从最底部向上移动 自顶向下的堆化，元素由顶部向下移动。在讲解删除堆顶元素的方法时，我将阐述这两种操作的过程 自底向上堆化\n在堆这个公司中，会出现老大离职的现象，老大离职之后，它的位置就空出来了\n首先删除堆顶元素，使得数组中下标为1的位置空出 那么他的位置由谁来接替呢，当然是他的直接下属了，谁能力强就让谁上\n比较根节点（当前节点）的左子节点和右子节点，也就是下标为 2 ，3 的数组元素，将较大的元素填充到**根节点（下标为1）（当前遍历节点）**的位置 此时又空出一个位置了，老规矩，谁有能力谁上\n一直循环比较空出位置的左右子节点，并将较大者移至空位，直到堆的最底部 此时已经完成自顶向上的堆化，没有元素可以填补空缺。但会发现数组中出现了”气泡”，导致存户空间的浪费。\n解决办法：自顶向下堆化\n自顶向下堆化 自顶向下的堆化用一个词形容就是“石沉大海”\n第一件事情，就是把石头抬起来，从海面扔下去。这个石头就是堆的最后一个元素，我们将最后一个元素移动到堆顶。 将这个石头沉入海底，不停的与左右子节点的值进行比较，和较大的子节点交换位置，直到无法交换位置 结果 堆的操作总结 # 插入元素：先将元素放置数组末尾，再自底向上堆化，将末尾元素上浮\n删除堆顶元素：删除堆顶元素，将末尾元素放置堆顶，再自顶向下堆化，将堆顶元素下沉。\n也可以自底向上堆化，但是会产生气泡，浪费存储空间。不建议\n堆排序 # 堆排序的过程分两步\n建堆，将一个无序的数组，建立成堆 排序，[ 将堆顶元素取出，然后对剩下的元素堆化 ]。 反复迭代，直到所有元素被取出 建堆 # 也就是对所有非叶子结点进行自顶向下\n如图，红色区域分别是堆的情况下。对于T，如果只自顶向下到P、L这层，被换到了这层的那个元素是不一定就比其他树大的，所以还是要依次自顶向下\n这个构建堆操作的时间复杂度为O(n) 首先要了解哪些是非叶节点，最后一个结点的父节点及它（这个父节点）之前的元素，都是非叶节点。也就是说，如果节点个数为n，那么我们需要对n/2到1的节点进行自顶向下（沉底）堆化\n如图 首先将初始的无序数组抽象为一棵树，图中的节点个数为6，所以4，5，6是叶子节点，1，2，3节点为非叶节点 对1，2，3节点进行**自顶向下（沉底）**堆化，注意，顺序是从后往前堆化，从3号开始，一直到1号节点。 3号节点堆化结果\n2号节点堆化结果 1号节点堆化结果 排序 # 方法：由于堆顶元素是所有元素中最大的，所以我们重复取出堆顶元素，将这个最大的堆顶元素放至数组末尾，并对剩下的元素进行堆化即可 现在思考两个问题： 删除堆顶元素后需要执行**自顶向下（沉底）堆化还是自底向上（上浮）**堆化？ 取出的堆顶元素存在哪，新建一个数组存？ 答案 需要使用自顶向下（沉底）堆化，这个堆化一开始要将末尾元素移动至堆顶。由于这个时候末尾的位置已经空出来了由于堆中元素已经减小，这个位置不会再被使用，所以我们可以将取出的元素放在末尾。 其实是做了一次交换操作，将堆顶和末尾元素调换位置，从而将取出堆顶元素和**堆化的第一步(将末尾元素放至根结点位置)**进行合并 步骤 取出第一个元素并堆化 取出第2个元素并堆化 取出第3个元素并堆化 取出第4个元素并堆化 取出第5个元素并堆化 取出第6个元素并堆化 排序完成 "},{"id":124,"href":"/zh/docs/technology/Review/java_guide/cs_basics/data-structure/graph/","title":"图","section":"数据结构","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n图是一种较为复杂的非线性结构 线性数据结构的元素满足唯一的线性关系，每个元素（除第一个和最后一个外）只有一个直接前驱和一个直接后继 树形数据结构的元素之间有着明显的层级关系 图形结构的元素之间的关系是任意的 图就是由顶点的有穷非空集合和顶点之间的边组成的集合，通常表示为：G（V，E），其中，G表示一个图，V表示顶点的集合，E表示边的集合 下面显示的即图这种数据结构，而且还是一张有向图 图的基本概念 # 顶点 # 图中的数据元素，我们称之为顶点，图至少有一个顶点（有穷非空集合） 对应到好友关系图，每一个用户就代表一个顶点 边 # 顶点之间的关系用边表示 对应到好友关系图，两个用户是好友的话，那两者之间就存在一条边 度 # 度表示一个顶点包含多少条边 有向图中，分为出度和入度，出度表示从该顶点出去的边的条数，入度表示从进入该顶点的边的条数 对应到好友关系图，度就代表了某个人的好友数量 无向图和有向图 # 边表示的是顶点之间的关系，有的关系是双向的，比如同学关系，A是B的同学，那么B也肯定是A的同学，那么在表示A和B的关系时，就不用关注方向，用不带箭头的边表示，这样的图就是无向图。\n有的关系是有方向的，比如父子关系，师生关系，微博的关注关系，A是B的爸爸，但B肯定不是A的爸爸，A关注B，B不一定关注A。在这种情况下，我们就用带箭头的边表示二者的关系，这样的图就是有向图。\n无权图和带权图 # 对于一个关系，如果我们只关心关系的有无，而不关心关系有多强，那么就可以用无权图表示二者的关系。\n对于一个关系，如果我们既关心关系的有无，也关心关系的强度，比如描述地图上两个城市的关系，需要用到距离，那么就用带权图来表示，带权图中的每一条边一个数值表示权值，代表关系的强度。\n下图就是一个带权有向图。\n图的存储 # 邻接矩阵存储 # 邻接矩阵将图用二维矩阵存储，是一种比较直观的表示方式 如果第i个顶点和第j个顶点有关系，且关系权值为n，则A[i] [j] = n 在无向图中，我们只关心关系的有无，所以当顶点i和顶点j有关系时，A[i] [j]=1 ; 当顶点i和顶点j没有关系时，A[i] [j] = 0 ，如下图所示\n无向图的邻接矩阵是一个对称矩阵，因为在无向图中，顶点i和顶点j有关系，则顶点j和顶点i必有关系 有向图的邻接矩阵存储 邻接矩阵存储的方式优点是简单直接（直接使用一个二维数组即可），并且在获取两个顶点之间的关系的时候也非常高效*直接获取指定位置的数组元素。但是这种存储方式的确定啊也比较明显即 比较浪费空间 邻接表存储 # 针对上面邻接矩阵比较浪费内存空间的问题，诞生了图的另一种存储方法\u0026ndash;邻接表\n邻接链表使用一个链表来存储某个顶点的所有后继相邻顶点。对于图中每个顶点Vi ，把所有邻接于Vi 的顶点Vj 链接成一个单链表\n无向图的邻接表存储 有向图的邻接表存储 邻接表中存储的元素的个数（顶点数）以及图中边的条数\n无向图中，邻接表的元素个数等于边的条数的两倍，如下图 7条边，邻接表存储的元素个数为14 （即每条边存储了两次）\n有向图中，邻接表元素个数等于边的条数，如图所示的有向图中，边的条数为8，邻接表 图的搜索 # 广度优先搜索 # 广度优先搜索：像水面上的波纹一样，一层一层向外扩展，如图 具体实现方式，用到了队列，过程如下\n初始状态：将要搜索的源顶点放入队列 取出队首节点，输出0，将0的后继顶点（全部）（未访问过的）放入队列 取出队首节点，输出1，将1的后继顶点（所有）（未访问过的）放入队列 截止到第3步就很清楚了，就是输出最近的一个结点的全部关系节点\n取出队首节点，输出4，将4的后继顶点（未访问过的）放入队列 取出队首节点，输出2，将2的后继顶点（未访问过的）放入队列 取出队首节点，输出3，将3的后继顶点（未访问过的）放入队列，队列为空，结束 总结 先初始化首结点，之后不断从队列取出并将这个结点的有关系的结点 依次放入队列\n深度优先搜索 # 深度优先，即一条路走到黑。从源顶点开始，一直走到后继节点，才回溯到上一顶点，然后继续一条路走到黑 和广度优先搜索类似，深度优先搜索的具体实现，用到了另一种线性数据结构\u0026mdash;栈 初始状态，将要搜索的源顶点放入栈中 取出栈顶元素，输出0，将0的后继顶点（未访问过的）放入栈中 取出栈顶元素，输出4（因为后进先出），将4的后继顶点（未访问过的）放入栈中 取出栈顶元素，输出3，将3的后继顶点（未访问过的）放入栈中 其实到这部就非常明显了，即 前面元素的关系元素，大多都是被一直压在栈底的，会一直走走到 源顶点的直系关系顶点没有了，再往回走\n取出栈顶元素，输出2，将2的后继顶点（为访问过的）放入栈中 取出栈顶元素，输出1，将1的后继顶点（未访问过的）放入栈中，栈为空，结束 "},{"id":125,"href":"/zh/docs/technology/Review/java_guide/cs_basics/data-structure/linear-data-structure/","title":"线性数据结构","section":"数据结构","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n数组 # 数组（Array）是一种常见数据结构，由相同类型的元素（element）组成，并且是使用一块连续的内存来存储 直接可以利用元素的**索引（index）**可以计算出该元素对应的存储地址 数组的特点是：提供随机访问并且容量有限 假设数组长度为n：\n访问：O(1) //访问特定位置的元素\n插入：O(n) //最坏的情况插入在数组的首部并需要移动所有元素时\n删除：O(n) //最坏的情况发生在删除数组的开头并需要移动第一元素后面所有的元素时\n链表 # 链表简介 # 链表（LinkedList）虽然是一种线性表，但是并不会按线性的顺序存储数据，使用的不是连续的内存空间来存储数据\n链表的插入和删除操作的复杂度为O(1)，只需要直到目标位置元素的上一个元素即可。但是，在查找一个节点或者访问特定位置的节点的时候复杂度为O(n)\n使用链表结构可以克服数组需要预先知道数据大小的缺点，链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理\n但链表不会节省空间，相比于数组会占用更多空间，因为链表中每个节点存放的还有指向其他节点的指针。除此之外，链表不具有数组随机读取的优点\n链表分类 # 单链表、双向链表、循环链表、双向循环链表\n假设链表中有n个元素\n访问：O(n) //访问特地给位置的元素\n插入删除：O(1) //必须要知道插入元素的位置\n单链表 # 单链表只有一个方向，结点只有一个后继指针next指向后面的节点。因此，链表这种数据结构通常在物理内存上是不连续的 我们习惯性地把第一个结点叫做头结点，链表通常有一个不保存任何值的head节点（头结点），通过头结点我们可以遍历整个链表，尾结点通常指向null 如下图 循环链表 # 循环链表是一种特殊的单链表，和单链表不同的是循环链表的尾结点不是指向null，而是指向链表的头结点 如图 双向链表 # 双向链表包含两个指针，一个prev指向前一个节点，另一个next指向 如图 双向循环链表 # 双向循环链表的最后一个节点的next指向head，而head的prev指向最后一个节点，构成一个环\n应用场景 # 如果需要支持随机访问的话，链表无法做到 如果需要存储的数据元素个数不确定，并且需要经常添加和删除数据的话，使用链表比较合适 如果需要存储的数据元素的个数确定，并且不需要经常添加和删除数据的话，使用数组比较合适 数组 vs 链表 # 数组支持随机访问，链表不支持 数组使用的是连续内存空间 对CPU缓存机制友好，链表则相反 数组的大小固定，而链表则天然支持动态扩容。如果生命的数组过小，需要另外申请一个更大的内存空间存放数组元素，然后将原数组拷贝进去，这个操作比较耗时 栈 # 栈简介 # 栈（stack）只允许在有序的线性数据集合的一端（称为栈顶top）进行加入数据（push）和移除数据（pop）。因而按照**后进先出（LIFO，Last In First Out）**的原理运作。 栈中，push和pop的操作都发生在栈顶 栈常用一维数组或链表来实现，用数组实现的叫顺序栈，用链表实现的叫做链式栈 假设堆栈中有n个元素。 访问：O（n）//最坏情况 插入删除：O（1）//顶端插入和删除元素\n如图：\n栈的常见应用场景 # 当我们要处理的数据，只涉及在一端插入和删除数据，并且满足后进先出（LIFO，LastInFirstOut）的特性时，我们就可以使用栈这个数据结构。\n实现浏览器的回退和前进功能 # 我们只需要使用两个栈(Stack1 和 Stack2)和就能实现这个功能。比如你按顺序查看了 1,2,3,4 这四个页面，我们依次把 1,2,3,4 这四个页面压入 Stack1 中。当你想回头看 2 这个页面的时候，你点击回退按钮，我们依次把 4,3 这两个页面从 Stack1 弹出，然后压入 Stack2 中。假如你又想回到页面 3，你点击前进按钮，我们将 3 页面从 Stack2 弹出，然后压入到 Stack1 中。示例图如下\n检查符号是否承兑出现 # 给定一个只包括 '('，')'，'{'，'}'，'['，']' 的字符串，判断该字符串是否有效。\n有效字符串需满足：\n左括号必须用相同类型的右括号闭合。 左括号必须以正确的顺序闭合。 比如 \u0026ldquo;()\u0026quot;、\u0026rdquo;()[]{}\u0026quot;、\u0026quot;{[]}\u0026quot; 都是有效字符串，而 \u0026ldquo;(]\u0026rdquo; 、\u0026quot;([)]\u0026quot; 则不是。\n这个问题实际是 Leetcode 的一道题目，我们可以利用栈 Stack 来解决这个问题。\n首先我们将括号间的对应规则存放在 Map 中，这一点应该毋容置疑； 创建一个栈。遍历字符串，如果字符是左括号就直接加入stack中，否则将stack 的栈顶元素与这个括号做比较，如果不相等就直接返回 false。遍历结束，如果stack为空，返回 true。 public boolean isValid(String s){ // 括号之间的对应规则 HashMap\u0026lt;Character, Character\u0026gt; mappings = new HashMap\u0026lt;Character, Character\u0026gt;(); mappings.put(\u0026#39;)\u0026#39;, \u0026#39;(\u0026#39;); mappings.put(\u0026#39;}\u0026#39;, \u0026#39;{\u0026#39;); mappings.put(\u0026#39;]\u0026#39;, \u0026#39;[\u0026#39;); Stack\u0026lt;Character\u0026gt; stack = new Stack\u0026lt;Character\u0026gt;(); char[] chars = s.toCharArray(); for (int i = 0; i \u0026lt; chars.length; i++) { if (mappings.containsKey(chars[i])) { char topElement = stack.empty() ? \u0026#39;#\u0026#39; : stack.pop(); if (topElement != mappings.get(chars[i])) { return false; } } else { stack.push(chars[i]); } } return stack.isEmpty(); } 反转字符串 # 将字符串中的每个字符先入栈再出栈就可以了。\n维护函数调用 # 最后一个被调用的函数必须先完成执行，符合栈的 后进先出（LIFO, Last In First Out） 特性。\n栈的实现 # 栈既可以通过数组实现，也可以通过链表实现。两种情况下，入栈、出栈的时间复杂度均为O(1)\n下面使用数组下实现栈，具有push()、pop() （返回栈顶元素并出栈）、peek() （返回栈顶元素不出栈）、isEmpty() 、size() 这些基本的方法\n每次入栈前先判断栈容量是否够用，如果不够用就用Arrays.copyOf() 进行扩容\npublic class MyStack { private int[] storage;//存放栈中元素的数组 private int capacity;//栈的容量 private int count;//栈中元素数量 private static final int GROW_FACTOR = 2; //不带初始容量的构造方法。默认容量为8 public MyStack() { this.capacity = 8; this.storage=new int[8]; this.count = 0; } //带初始容量的构造方法 public MyStack(int initialCapacity) { if (initialCapacity \u0026lt; 1) throw new IllegalArgumentException(\u0026#34;Capacity too small.\u0026#34;); this.capacity = initialCapacity; this.storage = new int[initialCapacity]; this.count = 0; } //入栈 public void push(int value) { if (count == capacity) { ensureCapacity(); } storage[count++] = value; } //确保容量大小 private void ensureCapacity() { int newCapacity = capacity * GROW_FACTOR; storage = Arrays.copyOf(storage, newCapacity); capacity = newCapacity; } //返回栈顶元素并出栈 private int pop() { if (count == 0) throw new IllegalArgumentException(\u0026#34;Stack is empty.\u0026#34;); count--; return storage[count]; } //返回栈顶元素不出栈 private int peek() { if (count == 0){ throw new IllegalArgumentException(\u0026#34;Stack is empty.\u0026#34;); }else { return storage[count-1]; } } //判断栈是否为空 private boolean isEmpty() { return count == 0; } //返回栈中元素的个数 private int size() { return count; } } /*---- MyStack myStack = new MyStack(3); myStack.push(1); myStack.push(2); myStack.push(3); myStack.push(4); myStack.push(5); myStack.push(6); myStack.push(7); myStack.push(8); System.out.println(myStack.peek());//8 System.out.println(myStack.size());//8 for (int i = 0; i \u0026lt; 8; i++) { System.out.println(myStack.pop()); } System.out.println(myStack.isEmpty());//true myStack.pop();//报错：java.lang.IllegalArgumentException: Stack is empty. */ 队列 # 队列简介 # 队列是**先进先出（FIFO，First In，First Out）**的线性表\n通常用链表或数组来实现，用数组实现的队列叫做顺序队列，用链表实现的队列叫做链式队列。\n队列只允许在后端（rear）进行插入操作也就是入队enqueue，在前端（front）进行删除操作也就是出队 dequeue\n队列的操作方式和堆栈类似，唯一的区别在于队列只允许新数据在后端进行添加（不允许在后端删除）\n假设队列中有n个元素。 访问：O（n）//最坏情况 插入删除：O（1）//后端插入前端删除元素\n队列分类 # 单队列 # 这是常见的队列，每次添加元素时，都是添加到队尾。单队列又分为顺序队列（数组实现）和链式队列（链表实现）\n顺序队列存在假溢出：即明明有位置却不能添加\n假设下图是一个顺序队列，我们将前两个元素 1,2 出队，并入队两个元素 7,8。当进行入队、出队操作的时候，front 和 rear 都会持续往后移动，当 rear 移动到最后的时候,我们无法再往队列中添加数据，即使数组中还有空余空间，这种现象就是 ”假溢出“ 。除了假溢出问题之外，如下图所示，当添加元素 8 的时候，rear 指针移动到数组之外（越界）\n为了避免当只有一个元素的时候，队头和队尾重合使处理变得麻烦，所以引入两个指针，front 指针指向对头元素（不是头结点），rear 指针指向队列最后一个元素的下一个位置，这样当 front 等于 rear 时，此队列不是还剩一个元素，而是空队列。——From 《大话数据结构》\n（当只有一个元素时，front 指向0，rear指向1）\n循环队列 # 循环队列可以解决顺序队列的假溢出和越界问题。解决办法就是：从头开始，这样也就会形成头尾相接的循环，这也就是循环队列名字的由来。 （超出的时候，将rear指向0下标）。之后再添加时，向后移动即可\n顺序队列中，我们说 front==rear 的时候队列为空，循环队列中则不一样，也可能为满，如上图所示。解决办法有两种： 可以设置一个标志变量 flag,当 front==rear 并且 flag=0 的时候队列为空，当front==rear 并且 flag=1 的时候队列为满。 队列为空的时候就是 front==rear ，队列满的时候，我们保证数组还有一个空闲的位置，rear 就指向这个空闲位置，如下图所示，那么现在判断队列是否为满的条件就是： (rear+1) % QueueSize= front 。 其实也就是换一个定义罢了 常见应用场景 # 当我们需要按照一定顺序来处理数据的时候可以考虑使用队列这个数据结构\n阻塞队列： 阻塞队列可以看成在队列基础上加了阻塞操作的队列。当队列为空的时候，出队操作阻塞，当队列满的时候，入队操作阻塞。使用阻塞队列我们可以很容易**实现“生产者 - 消费者“**模型 线程池中的请求/任务队列： 线程池中没有空闲线程时，新的任务请求线程资源时，线程池该如何处理呢？答案是将这些请求放在队列中，当有空闲线程的时候，会循环中反复从队列中获取任务来执行。队列分为无界队列(基于链表)和有界队列(基于数组)。无界队列的特点就是可以一直入列，除非系统资源耗尽，比如 ：FixedThreadPool 使用无界队列 LinkedBlockingQueue。但是有界队列就不一样了，当队列满的话后面再有任务/请求就会拒绝，在 Java 中的体现就是会抛出**java.util.concurrent.RejectedExecutionException** 异常。 Linux 内核进程队列（按优先级排队） 现实生活中的派对，播放器上的播放列表; 消息队列 等等\u0026hellip;\u0026hellip; "},{"id":126,"href":"/zh/docs/technology/Review/java_guide/database/ly0501lybasis/","title":"数据库基础","section":"数据库","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n这部分内容由于涉及太多概念性内容，所以参考了维基百科和百度百科相应的介绍。\n什么是数据库，数据库管理系统，数据库系统，数据库管理员 # 数据库：数据库（DataBase 简称DB）就是信息的集合或者说数据库管理系统管理的数据的集合。 数据库管理系统：数据库管理系统（Database Management System 简称DBMS）是一种操纵和管理数据库的大型软件，通常用于建立、使用和维护 数据库。 数据库系统（范围最大）：数据库系统（Data Base System，简称DBS）通常由**软件、数据和数据管理员（DBA）**组成。 数据库管理员：数据库管理员（Database Adminitrator，简称DBA）负责全面管理和控制数据库系统 (是一个人) 数据库系统基本构成如下图所示\n什么是元组，码，候选码，主码，外码，主属性，非主属性 # 元组：元组（tuple）是关系数据库中的基本概念，关系是一张表，表中的每行（即数据库中的每条记录）就是一个元组，每列就是一个属性。在二维表里，元组也成为行 码：码就是能唯一标识实体的属性，对应表中的列 候选码：若关系中的某一属性或属性组的值能唯一的标识一个元组，而其任何、子集都不能再标识，则称该属性组为候选码。例如：在学生实体中，“学号”是能唯一的区分学生实体的，同时又假设“姓名”、“班级”的属性组合足以区分学生实体，那么**{学号}和{姓名，班级}都是候选码**。 主码：主码也叫主键，主码是从候选码中选出来的。一个实体集中只能有一个主码，但可以有多个候选码 外码：外码也叫外键。如果一个关系中的一个属性是另外一个关系中的主码则这个属性为外码。 主属性 ： 候选码中出现过的属性称为主属性(这里强调单个）。比如关系 工人（工号，身份证号，姓名，性别，部门）. 显然工号和身份证号都能够唯一标示这个关系，所以都是候选码。工号、身份证号这两个属性就是主属性。如果主码是一个属性组，那么属性组中的属性都是主属性。 非主属性： 不包含在任何一个候选码中的属性称为非主属性。比如在关系——学生（学号，姓名，年龄，性别，班级）中，主码是“学号”，那么其他的“姓名”、“年龄”、“性别”、“班级”就都可以称为非主属性。 主键和外键有什么区别 # 主键(主码) ：主键用于唯一标识一个元组，不能有重复，不允许为空。一个表只能有一个主键。 外键(外码) ：外键用来和其他表建立联系用，外键是另一表的主键，外键是可以有重复的，可以是空值。一个表可以有多个外键 为什么不推荐使用外键与级联 # 对于外键和级联，阿里巴巴开发手册这样说道\n【强制】不得使用外键与级联，一切外键概念必须在应用层解决。\n说明: 以学生和成绩的关系为例，学生表中的 student_id 是主键，那么成绩表中的 student_id 则为外键。如果更新学生表中的 student_id，同时触发成绩表中的 student_id 更新，即为级联更新。\n缺点： 外键与级联更新适用于单机低并发，不适合分布式、高并发集群; 级联更新是强阻塞，存在数据库更新风暴的风 险; 外键影响数据库的插入速度\n为什么不要使用外键\n增加了复杂性\na. 每次做DELETE 或者UPDATE都必须考虑外键约束，会导致开发的时候很痛苦, 测试数据极为不方便; b. 外键的主从关系是定的，假如那天需求有变化，数据库中的这个字段根本不需要和其他表有关联的话就会增加很多麻烦。\n增加了额外操作\n数据库需要增加维护外键的工作，比如当我们做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，保证数据的的一致性和正确性，这样会不得不消耗资源；（个人觉得这个不是不用外键的原因，因为即使你不使用外键，你在应用层面也还是要保证的。所以，我觉得这个影响可以忽略不计。）\n对分库分表很不友好：因为分库分表下外键无法生效\n\u0026hellip;\n外键的一些好处\n保证了数据库数据的一致性和完整性； 级联操作方便，减轻了程序代码量； \u0026hellip;\u0026hellip; 如果系统不涉及分库分表，并发量不是很高的情况还是可以考虑使用外键的\n什么是ER图 # 做一个项目的时候一定要试着画 ER 图来捋清数据库设计，这个也是面试官问你项目的时候经常会被问道的。\nE-R图，也称 实体-联系图（Entity Relationship Diagram），提供表示实体类型、属性和关系，用来描述现实世界的概念模型。它是描述现实世界关系概念模型的有效方法，是表示概念关系模型的一种方式 下图是一个学生选课的 ER 图，每个学生可以选若干门课程，同一门课程也可以被若干人选择，所以它们之间的关系是多对多（M: N）。另外，还有其他两种关系是：1 对 1（1:1）、1 对多（1: N） 将ER图转换成数据库实际的关系模型（实际设计中，我们通常会将任课教师也作为一个实体来处理）\n数据库范式了解吗 # 1NF(第一范式) 属性（对应于表中的字段）不能再被分割，也就是这个字段只能是一个值，不能再分为多个其他的字段了。1NF 是所有关系型数据库的最基本要求 ，也就是说关系型数据库中创建的表一定满足第一范式。\n2NF(第二范式) 2NF 在 1NF 的基础之上，消除了非主属性对于码的部分函数依赖。如下图所示，展示了第一范式到第二范式的过渡。第二范式在第一范式的基础上增加了一个列，这个列称为主键，非主属性都依赖于主键。\n第二范式要求，在满足第二范式的基础上，还要满足数据表里得每一条数据记录，都是可唯一标识的。而且所有非主键字段，都必须完全依赖主键，不能只依赖主键的一部分，如下，主键为商品名称、供应商名称，是主码是属性组。而供应商电话只依赖于供应商id，商品价格只依赖于价格。所以不满足第二范式\n3NF(第三范式)3NF 在 2NF 的基础之上，消除了非主属性对于码的传递函数依赖 。符合 3NF 要求的数据库设计，基本上解决了数据冗余过大，插入异常，修改异常，删除异常的问题。比如在关系 R(学号 , 姓名, 系名，系主任)中，学号 → 系名，系名 → 系主任，所以存在非主属性系主任对于学号的传递函数依赖，所以该表的设计，不符合 3NF 的要求。\n确保数据表中的每一个非主键字段都和主键字段相关，也就是说，要求数据表中的所有非主键字段不能依赖于其他非主键字段。（即，不能存在非主属性A依赖于非主属性B，非主属性B依赖于主键C的情况，即存在A \u0026ndash;\u0026gt; B \u0026ndash;\u0026gt; C 的决定关系），规则的意思是所有非主键属性之间不能有依赖关系，必须互相独立\n简单举例：\n部门信息表：每个部门有部门编号(dept_id)、部门名称、部门简介等消息\n员工信息表：每个员工有员工编号、姓名、部门编号。（注意，列出部门编号就不能再将部门名称、部门简介等部门相关的信息再加入员工信息表中，否则将不满足第3范式（但其实是满足第二范式的））\n总结\n1NF：属性不可再分。 2NF：1NF 的基础之上，消除了非主属性对于码的部分函数依赖。 3NF：3NF 在 2NF 的基础之上，消除了非主属性对于码的传递函数依赖 。 一些概念：\n函数依赖（functional dependency） ：若在一张表中，在属性（或属性组）X 的值确定的情况下，必定能确定属性 Y 的值，那么就可以说 Y 函数依赖于 X，写作 X → Y。 部分函数依赖（partial functional dependency） ：如果 X→Y，并且存在 X 的一个真子集 X0，使得 X0→Y，则称 Y 对 X 部分函数依赖。比如学生基本信息表 R 中（学号，身份证号，姓名）当然学号属性取值是唯一的，在 R 关系中，（学号，身份证号）-\u0026gt;（姓名），（学号）-\u0026gt;（姓名），（身份证号）-\u0026gt;（姓名）；所以姓名部分函数依赖与（学号，身份证号）；（感觉这个例子虽然是对的，但是不利于理解第二范式） 完全函数依赖(Full functional dependency) ：在一个关系中，若某个非主属性数据项依赖于全部关键字称之为完全函数依赖。比如学生基本信息表 R（学号，班级，姓名）假设不同的班级学号有相同的，班级内学号不能相同，在 R 关系中，（学号，班级）-\u0026gt;（姓名），但是（学号）-\u0026gt;(姓名)不成立，（班级）-\u0026gt;(姓名)不成立，所以姓名完全函数依赖与（学号，班级）； 传递函数依赖 ： 在关系模式 R(U)中，设 X，Y，Z 是 U 的不同的属性子集，如果 X 确定 Y、Y 确定 Z，且有 X 不包含 Y，Y 不确定 X，（X∪Y）∩Z=空集合，则称 Z 传递函数依赖(transitive functional dependency) 于 X。传递函数依赖会导致数据冗余和异常。传递函数依赖的 Y 和 Z 子集往往同属于某一个事物，因此可将其合并放到一个表中。比如在关系 R(学号 , 姓名, 系名，系主任)中，学号 → 系名，系名 → 系主任，所以存在非主属性系主任对于学号的传递函数依赖。。 什么是存储过程 # 作用：我们可以把存储过程看成是一些 SQL 语句的集合，中间加了点逻辑控制语句。存储过程在业务比较复杂的时候是非常实用的，比如很多时候我们完成一个操作可能需要写一大串 SQL 语句，这时候我们就可以写有一个存储过程，这样也方便了我们下一次的调用。存储过程一旦调试完成通过后就能稳定运行，另外，使用存储过程比单纯 SQL 语句执行要快，因为存储过程是预编译过的。\n存储过程在互联网公司应用不多，因为存储过程难以调试和扩展，而且没有移植性，还会消耗数据库资源\n阿里巴巴Java开发手册要求禁止使用存储过程 drop、delete与truncate区别 # 用法不同 # drop(丢弃数据): drop table 表名 ，直接将表都删除掉，在删除表的时候使用。 truncate (清空数据) : truncate table 表名 ，只删除表中的数据，再插入数据的时候自增长 id 又从 1 开始，在清空表中数据的时候使用。 delete（删除数据） : delete from 表名 where 列名=值，删除某一行的数据，如果不加 where 子句和truncate table 表名作用类似。 truncate 和不带 where 子句的 delete、以及 drop 都会删除表内的数据，但是 truncate 和 delete 只删除数据不删除表的结构(定义)，执行 drop 语句，此表的结构也会删除，也就是执行 drop 之后对应的表不复存在。\n属于不同的数据库语言 # truncate 和 drop 属于 DDL(数据定义语言)语句，操作立即生效，原数据不放到 rollback segment 中，不能回滚，操作不触发 trigger。而 **delete 语句是 DML (数据库操作语言)**语句，这个操作会放到 rollback segement 中，事务提交之后才生效。 DML语句和DDL语句区别 DML 是**数据库操作语言（Data Manipulation Language）**的缩写，是指对数据库中表记录的操作，主要包括表记录的插入（insert）、更新（update）、删除（delete）和查询（select），是开发人员日常使用最频繁的操作。 **DDL （Data Definition Language）**是数据定义语言的缩写，简单来说，就是对数据库内部的对象进行创建、删除、修改的操作语言。它和 DML 语言的最大区别是 DML 只是对表内部数据的操作，而不涉及到表的定义、结构的修改，更不会涉及到其他对象。DDL 语句更多的被数据库管理员（DBA）所使用，一般的开发人员很少使用。 由于select不会对表进行破坏，所以有的地方也会把select单独区分开叫做数据库查询语言DQL（Data Query Language） 执行速度不同 # 一般来说：drop \u0026gt; truncate \u0026gt; delete（这个我没有设计测试过）\ndelete命令执行的时候会产生数据库的binlog日志，而日志记录是需要消耗时间的，但是也有个好处方便数据回滚恢复。\ntruncate命令执行的时候不会产生数据库日志，因此比delete要快。除此之外，还会把表的自增值重置和索引恢复到初始大小等。\ndrop命令会把表占用的空间全部释放掉。\nTips：你应该更多地关注在使用场景上，而不是执行效率。\n数据库设计通常分为哪几步 # 需求分析 : 分析用户的需求，包括数据、功能和性能需求。 概念结构设计 : 主要采用 E-R 模型进行设计，包括画 E-R 图。 逻辑结构设计 : 通过将 E-R 图转换成表，实现从 E-R 模型到关系模型的转换。 物理结构设计 : 主要是为所设计的数据库选择合适的存储结构和存取路径。 数据库实施 : 包括编程、测试和试运行 数据库的运行和维护 : 系统的运行与数据库的日常维护。 "},{"id":127,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0408lyjdk-monitoring-and-troubleshooting-tools/","title":"jvm监控和故障处理工具 总结","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\nJDK 命令行工具 # 这些命令在 JDK 安装目录下的 bin 目录下：\njps (JVM Process Status）: 类似 UNIX 的 ps 命令。用于查看所有 Java 进程的启动类、传入参数和 Java 虚拟机参数等信息； jstat（JVM Statistics Monitoring Tool）: 用于收集 HotSpot 虚拟机各方面的运行数据; jinfo (Configuration Info for Java) : Configuration Info for Java,显示虚拟机配置信息; jmap (Memory Map for Java) : 生成堆转储快照; jhat (JVM Heap Dump Browser) : 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果; jstack (Stack Trace for Java) : 生成虚拟机当前时刻的线程快照，线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合。 jps: 查看所有 Java 进程 # jps(JVM Process Status) 命令类似 UNIX 的 ps 命令。\njps：显示虚拟机执行主类名称以及这些进程的本地虚拟机唯一 ID（Local Virtual Machine Identifier,LVMID）。jps -q ：只输出进程的本地虚拟机唯一 ID。\nC:\\Users\\SnailClimb\u0026gt;jps 7360 NettyClient2 17396 7972 Launcher 16504 Jps 17340 NettyServer jps -l:输出主类的全名，如果进程执行的是 Jar 包，输出 Jar 路径。\nC:\\Users\\SnailClimb\u0026gt;jps -l 7360 firstNettyDemo.NettyClient2 17396 7972 org.jetbrains.jps.cmdline.Launcher 16492 sun.tools.jps.Jps 17340 firstNettyDemo.NettyServer jps -v：输出虚拟机进程启动时 JVM 参数。\njps -m：输出传递给 Java 进程 main() 函数的参数。\njstat：监视虚拟机各种运行状态信息 # jstat ( JVM Statistics Monitoring Tool ) 使用于监视虚拟机各种运行状态信息的命令行工具。\n可以显示本地或者远程（需要远程主机提供RMI支持）虚拟机进程中的类信息、内存、垃圾收集、JIT编译等运行数据，在没有GUI，只提供了纯文本控制台环境的服务器上，它将是运行期间定位虚拟机性能问题的首选工具\njstat 命令使用格式\njstat -\u0026lt;option\u0026gt; [-t] [-h\u0026lt;lines\u0026gt;] \u0026lt;vmid\u0026gt; [\u0026lt;interval\u0026gt; [\u0026lt;count\u0026gt;]] 比如 jstat -gc -h3 31736 1000 10表示分析进程 id 为 31736 的 gc 情况，每隔 1000ms 打印一次记录，打印 10 次停止，每 3 行后打印指标头部。\nλ jstat -gc -h3 12224 1000 10 常见的option如下 , 下面的vmid，即vm的id （id值）\njstat -class vmid ：显示 ClassLoader 的相关信息； jstat -compiler vmid ：显示 JIT 编译的相关信息； jstat -gc vmid ：显示与 GC 相关的堆信息； jstat -gccapacity vmid ：显示各个代的容量及使用情况； jstat -gcnew vmid ：显示新生代信息； jstat -gcnewcapcacity vmid ：显示新生代大小与使用情况； jstat -gcold vmid ：显示老年代和永久代的行为统计，从jdk1.8开始,该选项仅表示老年代，因为永久代被移除了； jstat -gcoldcapacity vmid ：显示老年代的大小； jstat -gcpermcapacity vmid ：显示永久代大小，从jdk1.8开始,该选项不存在了，因为永久代被移除了； jstat -gcutil vmid ：显示垃圾收集信息 使用jstat -gcutil -h3 12224 1000 10\n另外，加上 -t参数可以在输出信息上加一个 Timestamp 列，显示程序的运行时间。 各个参数的含义\njinfo：实时地查看和调整虚拟机各项参数 # jinfo vmid :输出当前 jvm 进程的全部参数和系统属性 (第一部分是系统的属性，第二部分是 JVM 的参数)。 如下图： jinfo -flag name vmid :输出对应名称的参数的具体值。比如输出 MaxHeapSize、查看当前 jvm 进程是否开启打印 GC 日志 ( -XX:PrintGCDetails :详细 GC 日志模式，这两个都是默认关闭的)。\nC:\\Users\\SnailClimb\u0026gt;jinfo -flag MaxHeapSize 17340 -XX:MaxHeapSize=2124414976 C:\\Users\\SnailClimb\u0026gt;jinfo -flag PrintGC 17340 -XX:-PrintGC 使用 jinfo 可以在不重启虚拟机的情况下，可以动态的修改 jvm 的参数。尤其在线上的环境特别有用,请看下面的例子： 使用```jinfo -flag [+|-]name vmid 开启或者关闭对应名称的参数：\nC:\\Users\\SnailClimb\u0026gt;jinfo -flag PrintGC 17340 -XX:-PrintGC C:\\Users\\SnailClimb\u0026gt;jinfo -flag +PrintGC 17340 C:\\Users\\SnailClimb\u0026gt;jinfo -flag PrintGC 17340 -XX:+PrintGC jmap：生成堆转储快照 # jmap(Memory Map for Java )命令用于生成堆转储快照。如果不使用jmap命令，要想获取java堆转储，可以使用-XX:+HeapDumpOutOfMemoryError参数，可以让虚拟机在OOM异常出现之后，自动生成dump文件，Linux命令下通过kill -3发送进程推出信号也能拿到dump文件\njmap 的作用并不仅仅是为了获取 dump 文件，它还可以查询 finalizer 执行队列、Java 堆和永久代的详细信息，如空间使用率、当前使用的是哪种收集器等。和jinfo一样，jmap有不少功能在 Windows 平台下也是受限制的。\n将指定应用程序的堆 快照输出到桌面，后面可以通过jhat、Visual VM等工具分析该堆文件\nC:\\Users\\SnailClimb\u0026gt;jmap -dump:format=b,file=C:\\Users\\SnailClimb\\Desktop\\heap.hprof 17340 Dumping heap to C:\\Users\\SnailClimb\\Desktop\\heap.hprof ... Heap dump file created jhat：分析heapdump文件 # jhat 用于分析 heapdump 文件，它会建立一个 HTTP/HTML 服务器，让用户可以在浏览器上查看分析结果。\nC:\\Users\\SnailClimb\u0026gt;jhat C:\\Users\\SnailClimb\\Desktop\\heap.hprof Reading from C:\\Users\\SnailClimb\\Desktop\\heap.hprof... Dump file created Sat May 04 12:30:31 CST 2019 Snapshot read, resolving... Resolving 131419 objects... Chasing references, expect 26 dots.......................... Eliminating duplicate references.......................... Snapshot resolved. Started HTTP server on port 7000 Server is ready. 之后访问 http://localhost:7000/ 即可，如下： 进入/histo 会发现，有这个东西 这个对象创建了9次，因为我是在第9次循环后dump堆快照的\n//测试代码如下 public class MyMain { private byte[] x = new byte[10 * 1024 * 1024];//10M public static void main(String[] args) throws InterruptedException { System.out.println(\u0026#34;开始循环--\u0026#34;); int i=0; while (++i\u0026gt;0) { String a=new Date().toString(); MyMain myMain = new MyMain(); System.out.println(i+\u0026#34;循环中---\u0026#34; + new Date()); TimeUnit.SECONDS.sleep(10); } } } jstack： 生成虚拟机当前时刻的线程快照 # jstack (Stack Trace for Java ) 命令用于生成虚拟机当前时刻的线程快照。线程快照就是当前虚拟机内每一条线程正在执行的方法堆栈的集合\n生成线程快照的目的主要是定位线程长时间出现停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等都是导致线程长时间停顿的原因。线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做些什么事情，或者在等待些什么资源。\n线程死锁的代码，通过jstack 命令进行死锁检查，输出死锁信息，找到发生死锁的线程\npackage com.jvm; public class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 1\u0026#34;).start(); new Thread(() -\u0026gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource1\u0026#34;); synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); } } /*------ Thread[线程 1,5,main]get resource1 Thread[线程 2,5,main]get resource2 Thread[线程 2,5,main]waiting get resource1 Thread[线程 1,5,main]waiting get resource2 */ 分析 线程 A 通过 synchronized (resource1) 获得 resource1 的监视器锁，然后通过 Thread.sleep(1000);让线程 A 休眠 1s 为的是让线程 B 得到执行然后获取到 resource2 的监视器锁。线程 A 和线程 B 休眠结束了都开始企图请求获取对方的资源，然后这两个线程就会陷入互相等待的状态，这也就产生了死锁。\n通过jstack 命令分析\n# 先使用jps 找到思索地那个类 C:\\Users\\SnailClimb\u0026gt;jps 13792 KotlinCompileDaemon 7360 NettyClient2 17396 7972 Launcher 8932 Launcher 9256 DeadLockDemo 10764 Jps 17340 NettyServer ## 然后使用jstack命令分析 C:\\Users\\SnailClimb\u0026gt;jstack 9256 输出的部分如下\nFound one Java-level deadlock: ============================= \u0026#34;线程 2\u0026#34;: waiting to lock monitor 0x000000000333e668 (object 0x00000000d5efe1c0, a java.lang.Object), which is held by \u0026#34;线程 1\u0026#34; \u0026#34;线程 1\u0026#34;: waiting to lock monitor 0x000000000333be88 (object 0x00000000d5efe1d0, a java.lang.Object), which is held by \u0026#34;线程 2\u0026#34; Java stack information for the threads listed above: =================================================== \u0026#34;线程 2\u0026#34;: at DeadLockDemo.lambda$main$1(DeadLockDemo.java:31) - waiting to lock \u0026lt;0x00000000d5efe1c0\u0026gt; (a java.lang.Object) - locked \u0026lt;0x00000000d5efe1d0\u0026gt; (a java.lang.Object) at DeadLockDemo$$Lambda$2/1078694789.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) \u0026#34;线程 1\u0026#34;: at DeadLockDemo.lambda$main$0(DeadLockDemo.java:16) - waiting to lock \u0026lt;0x00000000d5efe1d0\u0026gt; (a java.lang.Object) - locked \u0026lt;0x00000000d5efe1c0\u0026gt; (a java.lang.Object) at DeadLockDemo$$Lambda$1/1324119927.run(Unknown Source) at java.lang.Thread.run(Thread.java:748) Found 1 deadlock. 找到了发生死锁的线程的具体信息\nJDK可视化分析工具 # JConsole：Java监视与管理控制台 # JConsole 是基于 JMX 的可视化监视、管理工具。可以很方便的监视本地及远程服务器的 java 进程的内存使用情况。你可以在控制台输出**console命令启动或者在 JDK 目录下的 bin 目录找到jconsole.exe然后双击启动**. 对于远程连接\n在启动方\n-Djava.rmi.server.hostname=外网访问 ip 地址 -Dcom.sun.management.jmxremote.port=60001 //监控的端口号 -Dcom.sun.management.jmxremote.authenticate=false //关闭认证 -Dcom.sun.management.jmxremote.ssl=false 实例：\njava -Djava.rmi.server.hostname=192.168.200.200 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=60001 -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.authenticate=false com.jvm.DeadLockDemo # 其中 192.168.200.200 为启动该类的机器的ip，而不是谁要连接 在使用 JConsole 连接时，远程进程地址如下：\n外网访问 ip 地址:60001 注意，虚拟机中（这里ip xxx.200是虚拟机ip），需要开放的端口不只是60001，还要通过 netstat -nltp开放另外两个端口 centos中使用\nfirewall-cmd --zone=public --add-port=45443/tcp --permanent firewall-cmd --zone=public --add-port=36521/tcp --permanent firewall-cmd --zone=public --add-port=60001/tcp --permanent firewall-cmd --reload #重启firewall 之后才能连接上\n内存监控 # JConsole 可以显示当前内存的详细信息。不仅包括堆内存/非堆内存的整体信息，还可以细化到 eden 区、survivor 区等的使用情况，如下图所示。\n点击右边的“执行 GC(G)”按钮可以强制应用程序执行一个 Full GC。\n新生代 GC（Minor GC）:指发生新生代的的垃圾收集动作，Minor GC 非常频繁，回收速度一般也比较快。\n老年代 GC（Major GC/Full GC）:指发生在老年代的 GC，出现了 Major GC 经常会伴随至少一次的 Minor GC（并非绝对），Major GC 的速度一般会比 Minor GC 的慢 10 倍以上。\n线程监控 # 类似我们前面讲的 jstack 命令，不过这个是可视化的。\n最下面有一个\u0026quot;检测死锁 (D)\u0026ldquo;按钮，点击这个按钮可以自动为你找到发生死锁的线程以及它们的详细信息 。 VisualVM： 多合一故障处理工具 # VisualVM 提供在 Java 虚拟机 (Java Virutal Machine, JVM) 上运行的 Java 应用程序的详细信息。在 VisualVM 的图形用户界面中，您可以方便、快捷地查看多个 Java 应用程序的相关信息。Visual VM 官网：https://visualvm.github.io/open in new window 。Visual VM 中文文档:https://visualvm.github.io/documentation.htmlopen in new window。\n下面这段话摘自《深入理解 Java 虚拟机》。\nVisualVM（All-in-One Java Troubleshooting Tool）是到目前为止随 JDK 发布的功能最强大的运行监视和故障处理程序，官方在 VisualVM 的软件说明中写上了“All-in-One”的描述字样，预示着他除了运行监视、故障处理外，还提供了很多其他方面的功能，如性能分析（Profiling）。VisualVM 的性能分析功能甚至比起 JProfiler、YourKit 等专业且收费的 Profiling 工具都不会逊色多少，而且 VisualVM 还有一个很大的优点：不需要被监视的程序基于特殊 Agent 运行，因此他对应用程序的实际性能的影响很小，使得他可以直接应用在生产环境中。这个优点是 JProfiler、YourKit 等工具无法与之媲美的。\nVisualVM 基于 NetBeans 平台开发，因此他一开始就具备了插件扩展功能的特性，通过插件扩展支持，VisualVM 可以做到：\n显示虚拟机进程以及进程的配置、环境信息（jps、jinfo）。 监视应用程序的 CPU、GC、堆、方法区以及线程的信息（jstat、jstack）。 dump 以及分析堆转储快照（jmap、jhat）。 方法级的程序运行性能分析，找到被调用最多、运行时间最长的方法。 离线程序快照：收集程序的运行时配置、线程 dump、内存 dump 等信息建立一个快照，可以将快照发送开发者处进行 Bug 反馈。 其他 plugins 的无限的可能性\u0026hellip;\u0026hellip; 这里就不具体介绍 VisualVM 的使用，如果想了解的话可以看:\nhttps://visualvm.github.io/documentation.htmlopen in new window https://www.ibm.com/developerworks/cn/java/j-lo-visualvm/index.html "},{"id":128,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0406lyjvm-params/","title":"jvm参数","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n本文由 JavaGuide 翻译自 https://www.baeldung.com/jvm-parametersopen in new window，并对文章进行了大量的完善补充。翻译不易，如需转载请注明出处，作者：baeldungopen in new window 。\n概述 # 本篇文章中，将掌握最常用的JVM参数配置。下面提到了一些概念，堆、方法区、垃圾回收等。\n堆内存相关 # Java 虚拟机所管理的内存中最大的一块，Java 堆是所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎 所有的对象实例以及数组都在这里分配内存。\n显式指定堆内存-Xms和-Xmx # 与性能相关的最常见实践之一是根据应用程序要求初始化堆内存。\n如果我们需要指定最小和最大堆大小（推荐显示指定大小）：\n-Xms\u0026lt;heap size\u0026gt;[unit] -Xmx\u0026lt;heap size\u0026gt;[unit] heap size 表示要初始化内存的具体大小。 unit 表示要初始化内存的单位。单位为***“ g”*** (GB) 、“ m”（MB）、“ k”（KB）。 举例，为JVM分配最小2GB和最大5GB的堆内存大小\n-Xms2G -Xmx5G 显示新生代内存（Young Generation） # 在堆总可用内存配置完成之后，第二大影响因素是为 Young Generation 在堆内存所占的比例。默认情况下，YG 的最小大小为 1310 MB，最大大小为无限制。\n两种指定 新生代内存(Young Generation) 大小的方法\n通过 -XX:NewSize 和 -XX:MaxNewSize -XX:NewSize=\u0026lt;young size\u0026gt;[unit] -XX:MaxNewSize=\u0026lt;young size\u0026gt;[unit] 如，为新生代分配最小256m的内存，最大1024m的内存我们的参数为：\n-XX:NewSize=256m -XX:MaxNewSize=1024m 通过-Xmn\u0026lt;young size\u0026gt;[unit] 指定 举例，为新生代分配256m的内存（NewSize与MaxNewSize设为一致） -Xmn256m 将新对象预留在新生代，由于 Full GC 的成本远高于 Minor GC，因此尽可能将对象分配在新生代是明智的做法，实际项目中根据 GC 日志分析新生代空间大小分配是否合理，适当通过“-Xmn”命令调节新生代大小，最大限度降低新对象直接进入老年代的情况。\n另外，你还可以通过 -XX:NewRatio=\u0026lt;int\u0026gt; 来设置老年代与新生代内存的比值。\n下面的参数，设置老年代与新生代内存的比例为1，即 老年代：新生代 = 1：1，新生代占整个堆栈的1/2 -XX:NewRadio=1\n显示指定永久代/元空间的大小 # 从Java 8开始，如果我们没有指定 Metaspace(元空间) 的大小，随着更多类的创建，虚拟机会耗尽所有可用的系统内存（永久代并不会出现这种情况）\nJDK 1.8 之前永久代还没被彻底移除的时候通常通过下面这些参数来调节方法区大小\n-XX:PermSize=N //方法区 (永久代) 初始大小 -XX:MaxPermSize=N //方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但**并非数据进入方法区后就“永久存在”**了。\nJDK 1.8 的时候，方法区（HotSpot 的永久代）被彻底移除了（JDK1.7 就已经开始了），取而代之是元空间，元空间使用的是本地内存\n-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小，如果不指定大小的话，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。 垃圾收集相关 # 垃圾回收器 # 为了提高应用程序的稳定性，选择正确的垃圾收集算法至关重要\nJVM具有四种类型的GC实现：\n串行垃圾收集器 并行垃圾收集器 CMS垃圾收集器（并发） G1垃圾收集器（并发） 使用下列参数实现：\n-XX:+UseSerialGC -XX:+UseParallelGC -XX:+UseParNewGC -XX:+UseG1GC GC记录 # 为了严格监控应用程序的运行状况，应该始终检查JVM的垃圾回收性能。最简单的方法是以人类可读的格式记录GC活动\n通过以下参数\n-XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=\u0026lt; number of log files \u0026gt; -XX:GCLogFileSize=\u0026lt; file size \u0026gt;[ unit ] -Xloggc:/path/to/gc.log "},{"id":129,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0403lyclass-structure/","title":"类文件结构","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n概述 # Java中，JVM可以理解的代码就叫做字节码（即扩展名为.class的文件），它不面向任何特定的处理器，只面向虚拟机 Java语言通过字节码的方式，在一定程度上解决了传统解释型语言执行效率低的问题，同时又保留了解释型语言可移植的特点。所以Java程序运行时效率极高，且由于字节码并不针对一种特定的机器。因此，Java程序无需重新编译便可在多种不通操作系统的计算机运行 Clojure（Lisp 语言的一种方言）、Groovy、Scala 等语言都是运行在 Java 虚拟机之上。下图展示了不同的语言被不同的编译器编译成.class文件最终运行在 Java 虚拟机之上。.class文件的二进制格式可以使用 WinHexopen in new window 查看。 .class文件是不同语言在Java虚拟机之间的重要桥梁，同时也是支持Java跨平台很重要的一个原因\nClass文件结构总结 # 根据Java虚拟机规范，Class文件通过ClassFile定义，有点类似C语言的结构体\nClassFile的结构如下：\nClassFile { u4 magic; //Class 文件的标志 u2 minor_version;//Class 的小版本号 u2 major_version;//Class 的大版本号 u2 constant_pool_count;//常量池的数量 cp_info constant_pool[constant_pool_count-1];//常量池 u2 access_flags;//Class 的访问标记 u2 this_class;//当前类 u2 super_class;//父类 u2 interfaces_count;//接口 u2 interfaces[interfaces_count];//一个类可以实现多个接口 u2 fields_count;//Class 文件的字段属性 field_info fields[fields_count];//一个类可以有多个字段 u2 methods_count;//Class 文件的方法数量 method_info methods[methods_count];//一个类可以有个多个方法 u2 attributes_count;//此类的属性表中的属性数 attribute_info attributes[attributes_count];//属性表集合 } 通过IDEA插件jclasslib查看，可以直观看到Class 文件结构\n使用jclasslib不光能直观地查看某个类对应的字节码文件，还可以查看类的基本信息、常量池、接口、属性、函数等信息\n下面介绍一下Class文件结构涉及到的一些组件\n魔数（Magic Number） # u4 magic; //Class 文件的标志 每个Class文件的头4个字节称为魔数（Magic Number），它的唯一作用是确定这个文件是否为一个能被虚拟机接收的Class文件\n程序设计者很多时候都喜欢用一些特殊的数字表示固定的文件类型或者其它特殊的含义。\n这里前两个字节是cafe 英[ˈkæfeɪ]，后两个字节 babe 英[beɪb]\nJAVA为 CA FE BA BE，十六进制(一个英文字母[这里说的是字母，不是英文中文之分]代表4位，即2个英文字母为1字节）\nClass文件版本号（Minor\u0026amp;Major Version） # u2 minor_version;//Class 的小版本号 u2 major_version;//Class 的大版本号 前4个字节存储Class 文件的版本号：第5位和第6位是次版本号，第7位和第8位是主版本号。 比如Java1.8 为00 00 00 34 JDK1.8 = 52 JDK1.7 = 51 JDK1.6 = 50 JDK1.5 = 49 JDK1.4 = 48 如图，下图是在java8中编译的，使用javap -v 查看 每当Java发布大版本（比如Java8 ，Java9 ）的时候，主版本号都会+1\n注：高版本的 Java 虚拟机可以执行低版本编译器生成的 Class 文件，但是低版本的 Java 虚拟机不能执行高版本编译器生成的 Class 文件。所以，我们在实际开发的时候要确保开发的的 JDK 版本和生产环境的 JDK 版本保持一致\n常量池（Constant Pool） # u2 constant_pool_count;//常量池的数量 cp_info constant_pool[constant_pool_count-1];//常量池 主次版本号之后的是常量池，常量池实际数量为constant_pool_count -1 （常量池计数器是从 1 开始计数的，将第 0 项常量空出来是有特殊考虑的，索引值为 0 代表“不引用任何一个常量池项”）\n常量池主要包括两大常量：字面量和符号引用。\n字面量比较接近于Java语言层面的常量概念，如文本字符串、声明为final的常量值等\n注意，非常量是不会在这里的， 没有找到3\n符号引用则属于编译原理方面的概念，包括下面三类常量\n类和接口的全限定名 字段的名称和描述符 方法的名称和描述符 常量池中的每一项常量都是一个表，这14种表有一个共同特点：开始第一位是一个u1类型的标志位 -tag 来标识常量的类型，代表当前这个常量属于哪种常量类型\n.class 文件可以通过javap -v class类名 指令来看一下其常量池中的信息(javap -v class类名-\u0026gt; temp.txt ：将结果输出到 temp.txt 文件)。\n访问标志（Access Flag） # 常量池结束后，紧接着两个字节代表访问标志，这个标志用于识别一些类或者接口 层次的访问信息，包括\n这个Class是类还是接口，是否为public或者abstract类型，如果是类的话是否声明为final等等\n类访问和属性修饰符\n【这里好像漏了一个0x0002 ，private 】\n上图转自： https://www.cnblogs.com/qdhxhz/p/10676337.html\n其实是所有值相加，所以对于 public interface A ，是0x601 ，即 0x200 + 0x400 + 0x001\n对于 public final class MyEntity extends MyInterface即0x31：0x0001 + 0x0010 + 0x0020\n再举个例子：\npackage top.snailclimb.bean; public class Employee { ... } 通过 javap -v class类名指令来看一下类的访问标志\n当前类（This Class）、父类（Super Class）、接口（Interfaces）索引集合 # u2 this_class;//当前类 u2 super_class;//父类 u2 interfaces_count;//接口 u2 interfaces[interfaces_count];//一个类可以实现多个接口 类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名，由于 Java 语言的单继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。 接口索引集合用来描述这个类实现了那些接口，这些被实现的接口将按 implements (如果这个类本身是接口的话则是extends) 后的接口顺序从左到右排列在接口索引集合中。 字段表集合 （Fields） # u2 fields_count;//Class 文件的字段的个数 field_info fields[fields_count];//一个类可以有多个字段 字段表（filed info）用于描述接口或类中声明的变量\n字段包括类级变量以及实例变量，但不包括在方法内部声明的局部变量 filed info(字段表)的结构：\naccess_flag：字段的作用域（public、private、protected修饰符）、是实例变量还是类变量（static修饰符）、可否被序列化（transient修饰符）、可变性（final）、可见性（volatile修饰符，是否强制从主内存读写） name_index：对常量池的引用，表示的字段的名称 descriptor_index：对常量池的引用，表示字段和方法的描述符 attributes_count：一个字段还会拥有额外的属性，attributes_count 存放属性的个数 attributes[attriutes_count]: 存放具体属性具体内容 上述这些信息中，各个修饰符都是布尔值，要么有某个修饰符，要么没有，很适合使用标志位来表示。而字段叫什么名字、字段被定义为什么数据类型这些都是无法固定的，只能引用常量池中常量来描述。\n方法表集合（Methods） # u2 methods_count;//Class 文件的方法的数量 method_info methods[methods_count];//一个类可以有个多个方法 methods_count 表示方法的数量，而 method_info 表示方法表。-\nClass 文件存储格式中对方法的描述与对字段的描述几乎采用了完全一致的方式。方法表的结构如同字段表一样，依次包括了访问标志、名称索引、描述符索引、属性表集合几项。\nmethod_info（方法表的）结构\n方法表的 access_flag 取值： 注意：因为volatile修饰符和transient修饰符不可以修饰方法，所以方法表的访问标志中没有这两个对应的标志，但是增加了synchronized、native、abstract等关键字修饰方法，所以也就多了这些关键字对应的标志。\n属性表集合（Attributes） # 如上，字段和方法都拥有属性 属性大概就是这种 u2 attributes_count;//此类的属性表中的属性数 attribute_info attributes[attributes_count];//属性表集合 在 Class 文件，字段表，方法表中都可以携带自己的属性表集合，以用于描述某些场景专有的信息 与 Class 文件中其它的数据项目要求的顺序、长度和内容不同，属性表集合的限制稍微宽松一些，不再要求各个属性表具有严格的顺序，并且只要不与已有的属性名重复，任何人实现的编译器都可以向属性表中写 入自己定义的属性信息，Java 虚拟机运行时会忽略掉它不认识的属性 "},{"id":130,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0405lyclassloader-detail/","title":"类加载器详解","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n回顾一下类加载过程 # 开始介绍类加载器和双亲委派模型之前，简单回顾一下类加载过程。\n类加载过程：加载-\u0026gt;连接-\u0026gt;初始化。 连接过程又可分为三步：验证-\u0026gt;准备-\u0026gt;解析。 [\n加载是类加载过程的第一步，主要完成下面 3 件事情：\n通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构转换为方法区的运行时数据结构 在内存中生成一个代表该类的 Class 对象，作为方法区这些数据的访问入口 类加载器 # 类加载器介绍 # 类加载器从 JDK 1.0 就出现了，最初只是为了满足 Java Applet（已经被淘汰） 的需要。后来，慢慢成为 Java 程序中的一个重要组成部分，赋予了 Java 类可以被动态加载到 JVM 中并执行的能力。\n根据官方 API 文档的介绍：\nA class loader is an object that is responsible for loading classes. The class ClassLoader is an abstract class. Given the binary name of a class, a class loader should attempt to locate or generate data that constitutes a definition for the class. A typical strategy is to transform the name into a file name and then read a \u0026ldquo;class file\u0026rdquo; of that name from a file system.\nEvery Class object contains a reference to the ClassLoader that defined it.\nClass objects for array classes are not created by class loaders, but are created automatically as required by the Java runtime. The class loader for an array class, as returned by Class.getClassLoader() is the same as the class loader for its element type; if the element type is a primitive type, then the array class has no class loader.\n翻译过来大概的意思是：\n类加载器是一个负责加载类的对象。ClassLoader 是一个抽象类。给定类的二进制名称，类加载器应尝试定位或生成构成类定义的数据。典型的策略是将名称转换为文件名，然后从文件系统中读取该名称的“类文件”。\n每个 Java 类都有一个引用指向加载它的 ClassLoader。不过，数组类不是通过 ClassLoader 创建的，而是 JVM 在需要的时候自动创建的，数组类通过getClassLoader()方法获取 ClassLoader 的时候和该数组的元素类型的 ClassLoader 是一致的。\n从上面的介绍可以看出:\n类加载器是一个负责加载类的对象，用于实现类加载过程中的加载这一步。 每个 Java 类都有一个引用指向加载它的 ClassLoader。 数组类不是通过 ClassLoader 创建的（数组类没有对应的二进制字节流），是由 JVM 直接生成的。 class Class\u0026lt;T\u0026gt; { ... private final ClassLoader classLoader; @CallerSensitive public ClassLoader getClassLoader() { //... } ... } 简单来说，类加载器的主要作用就是加载 Java 类的字节码（ .class 文件）到 JVM 中（在内存中生成一个代表该类的 Class 对象）。 字节码可以是 Java 源程序（.java文件）经过 javac 编译得来，也可以是通过工具动态生成或者通过网络下载得来。\n其实除了加载类之外，类加载器还可以加载 Java 应用所需的资源如文本、图像、配置文件、视频等等文件资源。本文只讨论其核心功能：加载类。\n类加载器加载规则 # JVM 启动的时候，并不会一次性加载所有的类，而是根据需要去动态加载。也就是说，大部分类在具体用到的时候才会去加载，这样对内存更加友好。\n对于已经加载的类会被放在 ClassLoader 中。在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载。也就是说，对于一个类加载器来说，相同二进制名称的类只会被加载一次。\npublic abstract class ClassLoader { ... private final ClassLoader parent; // 由这个类加载器加载的类。 private final Vector\u0026lt;Class\u0026lt;?\u0026gt;\u0026gt; classes = new Vector\u0026lt;\u0026gt;(); // 由VM调用，用此类加载器记录每个已加载类。 void addClass(Class\u0026lt;?\u0026gt; c) { classes.addElement(c); } ... } 类加载器总结 # JVM 中内置了三个重要的 ClassLoader：\nBootstrapClassLoader(启动类加载器) ：最顶层的加载类，由 C++实现，通常表示为 null（意思是如果用代码来get，会得到null），并且没有父级，主要用来加载 JDK 内部的核心类库（ **%JAVA_HOME%/lib**目录下的 rt.jar 、resources.jar 、charsets.jar等 jar 包和类）以及被 -Xbootclasspath参数指定的路径下的所有类。 ExtensionClassLoader(扩展类加载器) ：主要负责加载 %JRE_HOME%/lib/ext 目录下的 jar 包和类以及被 java.ext.dirs 系统变量所指定的路径下的所有类。 AppClassLoader(应用程序类加载器) ：面向我们用户的加载器，负责加载当前应用 classpath 下的所有 jar 包和类。 🌈 拓展一下：\nrt.jar ： rt 代表“RunTime”，rt.jar是Java基础类库，包含Java doc里面看到的所有的类的类文件。也就是说，我们常用内置库 java.xxx.* 都在里面，比如java.util.*、java.io.*、java.nio.*、java.lang.*、java.sql.*、java.math.*。 Java 9 引入了模块系统，并且略微更改了上述的类加载器。扩展类加载器被改名为平台类加载器（platform class loader）。Java SE 中除了少数几个关键模块，比如说 java.base 是由启动类加载器加载之外，其他的模块均由平台类加载器所加载。 除了这三种类加载器之外，用户还可以加入自定义的类加载器来进行拓展，以满足自己的特殊需求。就比如说，我们可以对 Java 类的字节码（ .class 文件）进行加密，加载时再利用自定义的类加载器对其解密。\n除了 BootstrapClassLoader 是 JVM 自身的一部分之外，其他所有的类加载器都是在 JVM 外部实现的，并且全都继承自 ClassLoader抽象类。这样做的好处是用户可以自定义类加载器，以便让应用程序自己决定如何去获取所需的类。\n每个 ClassLoader 可以通过**getParent()获取其父 ClassLoader，如果获取到 ClassLoader 为null**的话，那么该类是通过 BootstrapClassLoader 加载的。\npublic abstract class ClassLoader { ... // 父加载器 private final ClassLoader parent; @CallerSensitive public final ClassLoader getParent() { //... } ... } 为什么 获取到 ClassLoader 为null就是 BootstrapClassLoader 加载的呢？ 这是因为BootstrapClassLoader 由 C++ 实现，由于这个 C++ 实现的类加载器在 Java 中是没有与之对应的类的，所以拿到的结果是 null。\n下面我们来看一个获取 ClassLoader 的小案例：\npublic class PrintClassLoaderTree { public static void main(String[] args) { ClassLoader classLoader = PrintClassLoaderTree.class.getClassLoader(); StringBuilder split = new StringBuilder(\u0026#34;|--\u0026#34;); boolean needContinue = true; while (needContinue){ System.out.println(split.toString() + classLoader); if(classLoader == null){ needContinue = false; }else{ classLoader = classLoader.getParent(); split.insert(0, \u0026#34;\\t\u0026#34;); } } } } 输出结果(JDK 8 )：\n|--sun.misc.Launcher$AppClassLoader@18b4aac2 |--sun.misc.Launcher$ExtClassLoader@53bd815b |--null 从输出结果可以看出：\n我们编写的 Java 类 PrintClassLoaderTree 的 ClassLoader 是AppClassLoader； AppClassLoader的父 ClassLoader 是ExtClassLoader； ExtClassLoader的父ClassLoader是Bootstrap ClassLoader，因此输出结果为 null。 自定义类加载器 # 我们前面也说说了，除了 BootstrapClassLoader 其他类加载器均由 Java 实现且全部继承自java.lang.ClassLoader。如果我们要自定义自己的类加载器，很明显需要继承 ClassLoader抽象类。\nClassLoader 类有两个关键的方法：\nprotected Class loadClass(String name, boolean resolve)：加载指定二进制名称的类**，实现了双亲委派机制** 。name 为类的二进制名称，resove 如果为 true，在加载时调用 resolveClass(Class\u0026lt;?\u0026gt; c) 方法解析该类。 protected Class findClass(String name)：根据类的二进制名称来查找类，默认实现是空方法。 官方 API 文档中写到：\nSubclasses of ClassLoader are encouraged to override findClass(String name), rather than this method.\n建议 ClassLoader的子类重写 findClass(String name)方法而不是loadClass(String name, boolean resolve) 方法。\n如果我们不想打破双亲委派模型，就重写 ClassLoader 类中的 findClass() 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 loadClass() 方法。\n双亲委派模型 # 双亲委派模型介绍 # 类加载器有很多种，当我们想要加载一个类的时候，具体是哪个类加载器加载呢？这就需要提到双亲委派模型了。\n根据官网介绍：\nThe ClassLoader class uses a delegation model to search for classes and resources. Each instance of ClassLoader has an associated parent class loader. When requested to find a class or resource, a ClassLoader instance will delegate the search for the class or resource to its parent class loader before attempting to find the class or resource itself. The virtual machine\u0026rsquo;s built-in class loader, called the \u0026ldquo;bootstrap class loader\u0026rdquo;, does not itself have a parent but may serve as the parent of a ClassLoader instance.\n翻译过来大概的意思是：\nClassLoader 类使用委托模型来搜索类和资源。每个 ClassLoader 实例都有一个相关的父类加载器。需要查找类或资源时，ClassLoader 实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器。 虚拟机中被称为 \u0026ldquo;bootstrap class loader\u0026quot;的内置类加载器本身没有父类加载器，但是可以作为 ClassLoader 实例的父类加载器。\n从上面的介绍可以看出：\nClassLoader 类使用委托模型来搜索类和资源。 双亲委派模型要求除了顶层的启动类加载器外，其余的类加载器都应有自己的父类加载器。 ClassLoader 实例会在试图亲自查找类或资源之前，将搜索类或资源的任务委托给其父类加载器。 下图展示的各种类加载器之间的层次关系被称为类加载器的“双亲委派模型(Parents Delegation Model)”。\n注意⚠️：双亲委派模型并不是一种强制性的约束，只是 JDK 官方推荐的一种方式。如果我们因为某些特殊需求想要打破双亲委派模型，也是可以的，后文会介绍具体的方法。\n其实这个双亲翻译的容易让别人误解，我们一般理解的双亲都是父母，这里的双亲更多地表达的是“父母这一辈”的人而已，并不是说真的有一个 MotherClassLoader 和一个FatherClassLoader 。个人觉得翻译成单亲委派模型更好一些，不过，国内既然翻译成了双亲委派模型并流传了，按照这个来也没问题，不要被误解了就好。\n另外，类加载器之间的父子关系一般不是以继承的关系来实现的，而是通常使用组合关系来复用父加载器的代码。\npublic abstract class ClassLoader { ... // 组合 private final ClassLoader parent; protected ClassLoader(ClassLoader parent) { this(checkCreateClassLoader(), parent); } ... } 在面向对象编程中，有一条非常经典的设计原则： 组合优于继承，多用组合少用继承。\n双亲委派模型的执行流程 # 双亲委派模型的实现代码非常简单，逻辑非常清晰，都集中在 java.lang.ClassLoader 的 loadClass() 中，相关代码如下所示。\nprivate final ClassLoader parent; protected Class\u0026lt;?\u0026gt; loadClass(String name, boolean resolve) throws ClassNotFoundException { synchronized (getClassLoadingLock(name)) { // 首先，检查请求的类是否已经被加载过 Class\u0026lt;?\u0026gt; c = findLoadedClass(name); if (c == null) { //如果 c 为 null，则说明该类没有被加载过 long t0 = System.nanoTime(); try { if (parent != null) {//父加载器不为空，调用父加载器loadClass()方法处理 //注意，这里是一层层抛上去，有点类似把方法放进栈，然后如果BootstrapClassLoader加载不了，就会抛异常，由自己加载（如果自己加载不了，还是会抛异常，然后再次加载权回到子类） c = parent.loadClass(name, false); } else {//父加载器为空，使用启动类加载器 BootstrapClassLoader 加载 c = findBootstrapClassOrNull(name); } } catch (ClassNotFoundException e) { //抛出异常说明父类加载器无法完成加载请求 } if (c == null) { //当父类加载器无法加载时，则调用findClass方法来加载该类 //用户可通过覆写该方法，来自定义类加载器 long t1 = System.nanoTime(); //自己尝试加载 c = findClass(name); //用于统计类加载器相关的信息 // this is the defining class loader; record the stats sun.misc.PerfCounter.getParentDelegationTime().addTime(t1 - t0); sun.misc.PerfCounter.getFindClassTime().addElapsedTimeFrom(t1); sun.misc.PerfCounter.getFindClasses().increment(); } } if (resolve) { //对类进行link操作 resolveClass(c); } return c; } } 每当一个类加载器接收到加载请求时，它会先将请求转发给父类加载器。在父类加载器没有找到所请求的类的情况下，该类加载器才会尝试去加载。\n结合上面的源码，简单总结一下双亲委派模型的执行流程：\n在类加载的时候，系统会首先判断当前类是否被加载过。已经被加载的类会直接返回，否则才会尝试加载（每个父类加载器都会走一遍这个流程）。 类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成（调用父加载器 loadClass()方法来加载类）。这样的话，所有的请求最终都会传送到顶层的启动类加载器 BootstrapClassLoader 中。 只有当父加载器反馈自己无法完成这个加载请求（它的搜索范围中没有找到所需的类）时，子加载器才会尝试自己去加载（调用自己的 findClass() 方法来加载类）。 🌈 拓展一下：\nJVM 判定两个 Java 类是否相同的具体规则 ：JVM 不仅要看类的全名是否相同，还要看加载此类的类加载器是否一样。只有两者都相同的情况，才认为两个类是相同的。即使两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那这两个类就必定不相同。\n双亲委派模型的好处 # 双亲委派模型保证了 Java 程序的稳定运行，可以避免类的重复加载（JVM 区分不同类的方式不仅仅根据类名，相同的类文件被不同的类加载器加载产生的是两个不同的类），也保证了 Java 的核心 API 不被篡改。\n如果没有使用双亲委派模型，而是每个类加载器加载自己的话就会出现一些问题，比如我们编写一个称为 java.lang.Object 类的话，那么程序运行的时候，系统就会出现两个不同的 Object 类。双亲委派模型可以保证加载的是 JRE 里的那个 Object 类，而不是你写的 Object 类。这是因为 AppClassLoader 在加载你的 Object 类时，会委托给 ExtClassLoader 去加载，而 ExtClassLoader 又会委托给 BootstrapClassLoader，BootstrapClassLoader 发现自己已经加载过了 Object 类，会直接返回，不会去加载你写的 Object 类。\n打破双亲委派模型方法 # 为了避免双亲委托机制，我们可以自己定义一个类加载器，然后重写 loadClass() 即可。\n🐛 修正（参见：issue871 ） ：自定义加载器的话，需要继承 ClassLoader 。如果我们不想打破双亲委派模型，就重写 ClassLoader 类中的 findClass() 方法即可，无法被父类加载器加载的类最终会通过这个方法被加载。但是，如果想打破双亲委派模型则需要重写 loadClass() 方法。\n为什么是重写 loadClass() 方法打破双亲委派模型呢？双亲委派模型的执行流程已经解释了：\n类加载器在进行类加载的时候，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成（调用父加载器 loadClass()方法来加载类）。\n我们比较熟悉的 Tomcat 服务器为了能够优先加载 Web 应用目录下的类，然后再加载其他目录下的类，就自定义了类加载器 WebAppClassLoader 来打破双亲委托机制。这也是 Tomcat 下 Web 应用之间的类实现隔离的具体原理。\nTomcat 的类加载器的层次结构如下：\n感兴趣的小伙伴可以自行研究一下 Tomcat 类加载器的层次结构，这有助于我们搞懂 Tomcat 隔离 Web 应用的原理，推荐资料是《深入拆解 Tomcat \u0026amp; Jetty》。\n推荐阅读 # 《深入拆解 Java 虚拟机》 深入分析 Java ClassLoader 原理：https://blog.csdn.net/xyang81/article/details/7292380 Java 类加载器(ClassLoader)：http://gityuan.com/2016/01/24/java-classloader/ Class Loaders in Java：https://www.baeldung.com/java-classloaders Class ClassLoader - Oracle 官方文档：https://docs.oracle.com/javase/8/docs/api/java/lang/ClassLoader.html 老大难的 Java ClassLoader 再不理解就老了：https://zhuanlan.zhihu.com/p/51374915 "},{"id":131,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0404lyclassloader-process/","title":"类加载过程","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n类的声明周期 # 类加载过程 # Class文件，需要加载到虚拟机中之后才能运行和使用，那么虚拟机是如何加载这些Class文件呢 系统加载Class类文件需要三步：加载-\u0026gt;连接-\u0026gt;初始化。连接过程又分为三步：验证-\u0026gt;准备-\u0026gt;解析\n加载 # 类加载的第一步，主要完成3件事情\n构造与类相关联的方法表\n通过全类名获取定义此类的二进制字节流 将字节流所代表的静态存储结构，转换为方法区的运行时数据结构 在内存中生成一个该类的Class对象，作为方法区这些数据的访问入口 虚拟机规范对上面3点不具体，比较灵活\n对于1 没有具体指明从哪里获取、怎样获取。可以从ZIP包读取 （JAR/EAR/WAR格式的基础）、其他文件生成（JSP）等 非数组类的加载阶段（加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，这一步我们可以去完成还可以自定义类加载器去控制字节流的获取方式（重写一个类加载器的**loadClass()**方法 数组类型不通过类加载器创建，它由Java虚拟机直接创建 加载阶段和连接阶段的部分内容是交叉执行的，即加载阶段尚未结束，连接阶段就可能已经开始了\n验证 # 验证是连接阶段的第一步，这一阶段的目的是确保 Class 文件的字节流中包含的信息符合《Java 虚拟机规范》的全部约束要求，保证这些信息被当作代码运行后不会危害虚拟机自身的安全。\n验证阶段主要由四个检验阶段组成：\n文件格式验证（Class 文件格式检查） 元数据验证（字节码语义检查） 字节码验证（程序语义检查） 符号引用验证（类的正确性检查） 准备 # 准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配，注意：\n这时候进行内存分配的仅包括类变量（ClassVariables，即静态变量：被static关键字修饰的变量，只与类相关，因此被称为类变量），而不包括实例变量。\n实例变量会在对象实例化时，随着对象一块分配到Java堆中\n从概念上讲，类变量所使用的内存都应当在 方法区 中进行分配。不过有一点需要注意的是：JDK 7 之前，HotSpot 使用永久代来实现方法区的时候，实现是完全符合这种逻辑概念的。 而在 JDK 7 及之后，HotSpot 已经把原本放在永久代的字符串常量池、静态变量等移动到堆中，这个时候类变量则会随着 **Class 对象（上面有提到，内存区生成Class对象）**一起存放在 Java 堆中\n这里所设置的初始值**\u0026ldquo;通常情况\u0026rdquo;下是数据类型默认的零值（如 0、0L、null、false 等**），比如我们定义了**public static int value=111** ，那么 value 变量在准备阶段的初始值就是 0 而不是 111（初始化阶段才会赋值）。特殊情况：比如给 value 变量加上了 final 关键字public static final int value=111 ，那么准备阶段 value 的值就被赋值为 111\n基本数据类型的零值 解析 # 解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程\n解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用限定符7类符号引用进行\n符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄\n程序实际运行时，只有符号引用是不够的。 在程序执行方法时，系统需要明确知道这个方法所在的位置 Java虚拟机为每个类都准备了一张方法表来存放类中所有的方法。当需要调用一个类的方法的时候，只要知道这个方法在方法表中的偏移量就可以直接调用该方法了（针对其他类X或者当前类的方法）\n通过解析操作符号引用就可以直接转变为目标方法在类中方法表的位置，从而使得方法可以被调用。（将当前类中代码转为 上面说的类的偏移量）\n对下面的内容简化一下就是，编译后的class文件中，以 [类数组] 的方式，保存了类中的方法表的位置（偏移量）（通过得到每个数组元素可以得到方法的信息）。而这里我们只能知道偏移量，但是当正式加载到方法区之后，我们就能根据偏移量，计算出具体的 [内存地址] 了。\n具体详情https://blog.csdn.net/luanlouis/article/details/41113695 ，这里涉及到几个概念，一个是方法表。通过 javap -v xxx查看反编译的信息（class文件的信息）\nclass文件是这样的结构，里面有个方法表的概念\n如下，可能会有好几个方法，所以方法表，其实是一个类数组结构，而每个方法信息（method_info）呢，\n进一步，对于每个method_info结构体的定义\n方法表的结构体由：访问标志(*access_flags*)、名称索引(*name_index*)、描述索引(*descriptor_index*)、属性表(*attribute_info*)集合组成。\n而对于属性表，（其中：属性表集合\u0026ndash;用来记录方法的机器指令和抛出异常等信息）\nJava之所以能够运行，就是从Code属性中，取出的机器码\n解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，也就是得到类或者字段、方法在内存中的指针或者偏移量。（因为此时那些class文件已经早就加载到方法区之中了，所以可以改成指向方法区的某个内存地址\n如下，我的理解是，把下面的 com/test/Student.a ()V 修改成了直接的内存地址 类似的意思\n初始化 # 初始化阶段，是执行初始化方法clinit()方法的过程，是类加载的最后一步，这一步JVM才开始真正执行类中定义的Java程序代码（字节码）\nclinit()方法是编译之后自动生成的\n对于clinit () 方法的调用，虚拟机会自己确保其在多线程环境中的安全性。因为 clinit () 方法是带锁线程安全，所以在多线程环境下进行类初始化的话可能会引起多个线程阻塞，并且这种阻塞很难被发现。\n对于初始化阶段，虚拟机严格规范了有且只有 5 种情况下，必须对类进行初始化(只有主动去使用类才会初始化类)：\n当遇到 new 、 getstatic、putstatic 或 invokestatic 这 4 条直接码指令时，比如 new 一个类，读取一个静态字段(未被 final 修饰)、或调用一个类的静态方法时。\n当 jvm 执行 new 指令时会初始化类。即当程序创建一个类的实例对象。 当 jvm 执行 getstatic 指令时会初始化类。即程序访问类的静态变量(不是静态常量，常量会被加载到运行时常量池)。 当 jvm 执行 putstatic 指令时会初始化类。即程序给类的静态变量赋值。 当 jvm 执行 invokestatic 指令时会初始化类。即程序调用类的静态方法。 使用 java.lang.reflect 包的方法对类进行反射调用时如 Class.forname(\u0026quot;...\u0026quot;), newInstance() 等等。如果类没初始化，需要触发其初始化。\n初始化一个类，如果其父类还未初始化，则先触发该父类的初始化。\n当虚拟机启动时，用户需要定义一个要执行的主类 (包含 main 方法的那个类)，虚拟机会先初始化这个类。\nMethodHandle 和 VarHandle 可以看作是轻量级的反射调用机制，而要想使用这 2 个调用， 就必须先使用 findStaticVarHandle 来初始化要调用的类。\n「补充，来自issue745open in new window」 当一个接口中定义了 JDK8 新加入的默认方法（被 default 关键字修饰的接口方法）时，如果有这个接口的实现类发生了初始化，那该接口要在其之前被初始化。\n卸载 # 卸载类即该类的 Class 对象被 GC。\n卸载类需要满足 3 个要求:\n该类的所有的实例对象都已被 GC，也就是说堆不存在该类的实例对象。 该类没有在其他任何地方被引用 该类的类加载器的实例已被 GC JVM的生命周期内，由jvm自带的类加载器的类是不会被卸载的，而由我们自定义的类加载器加载的类是可能被卸载的\n只要想通一点就好了，jdk 自带的 BootstrapClassLoader, ExtClassLoader, AppClassLoader 负责加载 jdk 提供的类，所以它们(类加载器的实例)肯定不会被回收。而我们自定义的类加载器的实例是可以被回收的，所以使用我们自定义加载器加载的类是可以被卸载掉的。\n"},{"id":132,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0402lygarbage-collection/","title":"java垃圾回收","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n前言 # 当需要排查各种内存溢出问题、当垃圾收集成为系统达到更高并发的瓶颈时，我们就需要对这些**“自动化”的技术实施必要的监控和调节**\n堆空间的基本结构 # Java的自动内存管理主要是针对对象内存的回收和对象内存的分配。且Java自动内存管理最核心的功能是堆内存中的对象分配和回收\nJava堆是垃圾收集器管理的主要区域，因此也被称作GC堆（Garbage Collected Heap）\n从垃圾回收的角度来说，由于现在收集器基本都采用分代垃圾收集算法，所以Java堆被划分为了几个不同的区域，这样我们就可以根据各个区域的特点选择合适的垃圾收集算法\nJDK7版本及JDK7版本之前，堆内存被通常分为下面三部分：\n新生代内存（Young Generation） 老生代（Old Generation） 永久代（Permanent Generation） JDK8版本之后PermGen（永久）已被Metaspace（元空间）取代，且已经不在堆里面了，元空间使用的是直接内存。\n内存分配和回收原则 # 对象优先在Eden区分配 # 多数情况下，对象在新生代中Eden区分配。当Eden区没有足够空间进行分配时，会触发一次MinorGC 首先，先添加一下参数打印GC详情：-XX:+PrintGCDetails\npublic class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2; allocation1 = new byte[30900*1024];//会用掉3万多K } } 运行后的结果（这里应该是配过xms和xmx了，即堆内存大小） 如上，Eden区内存几乎被分配完全（即使程序什么都不做，新生代也会使用2000多K）\n注： PSYoungGen 为 38400K ，= 33280K + 5120K （Survivor区总会有一个是空的，所以只加了一个5120K ）\n假如我们再为allocation2分配内存会怎么样(不处理的话，年轻代会溢出)\nallocation2 = new byte[900 * 1024]; 在给allocation2分配内存之前，Eden区内存几乎已经被分配完。所以当Eden区没有足够空间进行分配时，虚拟机将发起一次MinorGC。GC期间虚拟机又发现allocation1无法存入空间，所以只好通过分配担保机制，把新生代的对象，提前转移到老年代去，老年代的空间足够存放allocation1，所以不会出现Full GC（这里可能是之前的说法，可能只是要表达老年代的GC，而不是Full GC(整堆GC) ）　执行MinorGC后，后面分配的对象如果能够存在Eden区的话，还是会在Eden区分配内存\n执行如下代码验证：\npublic class GCTest { public static void main(String[] args) { byte[] allocation1, allocation2,allocation3,allocation4,allocation5; allocation1 = new byte[32000*1024]; allocation2 = new byte[1000*1024]; allocation3 = new byte[1000*1024]; allocation4 = new byte[1000*1024]; allocation5 = new byte[1000*1024]; } } 大对象直接进入老年代 # 大对象就是需要连续空间的对象（字符串、数组等） 大对象直接进入老年代，主要是为了避免为大对象分配内存时，由于分配担保机制(这好像跟分配担保机制没有太大关系)带来的复制而降低效率。 假设大对象最后会晋升老年代，而新生代是基于复制算法来回收垃圾的，由两个Survivor区域配合完成复制算法，如果新生代中出现大对象且能屡次躲过GC，那这个对象就会在两个Survivor区域中来回复制，直至最后升入老年代，而大对象在内存里来回复制移动，就会消耗更多的时间。\n假设大对象最后不会晋升老年代，新生代空间是有限的，在新生代里的对象大部分都是朝生夕死的，如果让一个大对象占据了新生代空间，那么相比起正常的对象被分配在新生代，大对象无疑会让新生代GC提早发生，因为内存空间会更快不够用，如果这个大对象因为业务原因，并不会马上被GC回收，那么这个对象就会进入到Survivor区域，默认情况下，Survivor区域本来就不会被分配的很大，那此时被大对象占据了大部分空间，很可能会导致之后的新生代GC后，存活下来的对象，Survivor区域空间不够放不下，导致大部分对象进入老年代，这就加快了老年代GC发生的时间，而老年代GC对系统性能的负面影响则远远大于新生代GC了。\n长期存活的对象进入老年代 # 内存回收时必须能够识别，哪些对象放在新生代，哪些对象放在老年代\u0026mdash;\u0026gt; 因此，虚拟机给每个对象一个**对象年龄（Age）**计数器\n\u0026lt;流程\u0026gt; : 大部分情况下，对象都会首先在Eden区域分配。如果对象在Eden出生并经过第一次MinorGC后仍然能够存活，并且能被Survivor容纳的话，将被移动到Survivor空间（S0或S1）中，并将对象年龄设为1(Eden区 \u0026ndash;\u0026gt; Survivor区后对象初始年龄变为1 )\n后续，对象在Survivor区中每熬过一次MinorGC，年龄就增加1岁，当年龄增加到一定程序（默认为15岁），就会被晋升到老年代中。对象晋升到老年代的年龄阈值，可以通过参数**-XX:MaxTenuringThreshold**来设置 ★★修正： “Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的 50% 时（默认值是 50%，可以通过 -XX:TargetSurvivorRatio=percent 来设置，参见 issue1199open in new window ），取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。 动态年龄计算的代码：\nuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double)survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age \u0026lt; table_size) { //sizes数组是每个年龄段对象大小 total += sizes[age]; if (total \u0026gt; desired_survivor_size) { break; } age++; //注意这里，age是递增的，最终是去某个值，而不是区间的值计算 } uint result = age \u0026lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... } 例子： 如**对象年龄5的占30%，年龄6的占36%，年龄7的占34%，加入某个年龄段（如例子中的年龄6）**后，总占用超过Survivor空间*TargetSurvivorRatio的时候，从该年龄段开始及大于的年龄对象就要进入老年代（即例子中的年龄6对象，就是年龄6和年龄7晋升到老年代），这时候无需等到MaxTenuringThreshold中要求的15\n关于默认的晋升年龄是 15，这个说法的来源大部分都是《深入理解 Java 虚拟机》这本书。 如果你去 Oracle 的官网阅读相关的虚拟机参数open in new window，你会发现-XX:MaxTenuringThreshold=threshold这里有个说明\nSets the maximum tenuring threshold for use in adaptive GC sizing. The largest value is 15. The default value is 15 for the parallel (throughput) collector, and 6 for the CMS collector.默认晋升年龄并不都是 15，这个是要区分垃圾收集器的，CMS 就是 6.\n主要进行gc的区域 # 如图：（太长跳过了，直接看下面的总结）\n总结：\n针对HotSpotVM的实现，它里面的GC准确分类只有两大种：\n部分收集（Partial GC） 新生代收集（Minor GC/ Young GC ）：只对新生代进行垃圾收集 老年代（Major GC / Old GC )：只对老年代进行垃圾收集。★★：注意，MajorGC在有的语境中也用于指代整堆收集 混合收集（Mixed GC）：对整个新生代和部分老年代进行垃圾收集 整堆收集（Full GC）：收集整个Java堆和方法区 空间分配担保 # 为了确保在MinorGC之前老年代本身还有容纳新生代所有对象的剩余空间\n《深入理解Java虚拟机》第三章对于空间分配担保的描述如下：\nJDK 6 Update 24 之前，在发生 Minor GC 之前，虚拟机必须先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果这个条件成立，那这一次 Minor GC 可以确保是安全的。如果不成立，则虚拟机会先查看 -XX:HandlePromotionFailure 参数的设置值是否允许担保失败(Handle Promotion Failure);如果允许，那会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，将尝试进行一次 Minor GC，尽管这次 Minor GC 是有风险的;如果小于，或者 -XX: HandlePromotionFailure 设置不允许冒险，那这时就要改为进行一次 Full GC。\nJDK6 Update24之后，规则变为只要老年代的连续空间大于新生代对象总大小，或者历次晋升的平均大小，就会进行MinorGC，否则将进行Full GC\n死亡对象判断方法 # 堆中几乎放着所有的对象实例，对堆垃圾回收前的第一步就是要判断哪些对象已经死亡（即不能再被任何途径使用的对象）\n引用计数法 # 给对象中添加一个引用计数器\n每当有一个地方引用它，计数器就加1 当引用失效，计数器就减1 任何时候计数器为0的对象就是不可能再被使用的 这个方法实现简单，效率高，但是目前主流的虚拟机中并没有选择这个算法来管理内存，其最主要的原因是它很难解决对象之间相互循环引用的问题。\n除了对象 objA 和 objB 相互引用着对方之外，这两个对象之间再无任何引用。但是他们因为互相引用对方，导致它们的引用计数器都不为 0，于是引用计数算法无法通知 GC 回收器回收他们\n★其实我觉得只跟相互有关，跟是不是循环关系不会太大\nly 改：相互在语言逻辑上也可以理解成**“循环”**\npublic class ReferenceCountingGc { Object instance = null; public static void main(String[] args) { ReferenceCountingGc objA = new ReferenceCountingGc(); ReferenceCountingGc objB = new ReferenceCountingGc(); objA.instance = objB; objB.instance = objA; objA = null; objB = null; } } 可达性分析算法 # 该算法的基本思想就是通过一系列称为**“GC Roots\u0026quot;的对象作为起点，从这些节点开始向下搜索**，节点所走过的路径 称为引用链，当一个对象到GC Roots没有任何引用链相连的话，证明该对象不可用，需要被回收 下图中由于Object 6 ~ Object 10之间有引用关系，但它们到GC不可达，所以需要被回收 哪些对象可以作为GC Roots呢\n虚拟机栈（栈帧中的本地变量表）中引用的对象 本地方法栈（Native方法）中引用的对象 方法区中类静态属性引用的对象 （Class 的static变量） 方法区中常量引用的变量（Class 的final static变量） 所有被同步锁持有的对象 （synchronized(obj)) 对象可以被回收，就代码一定会被回收吗 即使在可达性分析法中不可达的对象，也并非是“非死不可”的，这时候它们暂时处于“缓刑阶段”，要真正宣告一个对象死亡，至少要经历两次标记过程：\n可达性分析中不可达的对象被第一次标记并且进行一次筛选：筛选的条件是此对象是否有必要执行finalize方法（有必要则放入）\n当对象没有覆盖finalize方法，或finalize方法已经被虚拟机调用过，则虚拟机将两种情况视为没有必要执行，该对象会被直接回收\n如果这个对象被判定为有必要执行finalize()方法，那么这个对象将会被放置在一个叫做F-Queue的队列中，然后由Finalizer线程去执行。GC将会对F-Queue中的对象进行第二次标记，如果对象在finalize()方法中重新与引用链上的任何一个对象建立关联，那么在第二次标记时将会被移除“即将回收”的集合，否则该对象将会被回收。\n（比如：把自己（this关键字）赋值给某个类变量(static修饰)或者对象的成员变量(在finalize方法中) ）\nObject 类中的 finalize 方法一直被认为是一个糟糕的设计，成为了 Java 语言的负担，影响了 Java 语言的安全和 GC 的性能。JDK9 版本及后续版本中各个类中的 finalize 方法会被逐渐弃用移除。忘掉它的存在吧！\n引用类型总结 # 不论是通过引用计数法判断对象引用数量，还是通过可达性分析法判断对象的引用链是否可达，判定对象的存活都与**”引用“**有关 JDK1.2 之前，Java中引用的定义很传统：如果reference类型的数据存储的数值代表的是另一块内存的起始地址，就称这块内存代表一个引用 JDK1.2 之后，Java对引用的概念进行了扩充，将引用(具体)分为强引用、软引用、弱引用、虚引用四种（引用强度逐渐减弱） 强引用（Strong Reference）\n大部分引用实际上是强引用。如果对象具有强引用，那么类似于生活中必不可少，垃圾回收器绝不会回收它 内存空间不足时，宁愿抛出OutOfMemoryErro错误，使程序异常终止，也不会回收强引用对象解决对象内存不足 软引用（SoftReference）\n如果对象只具有软引用，那就类似可有可无的生活用品。 内存够则不会回收；内存不足则回收这些对象。只要垃圾回收器没有回收，那么对象就可以被程序使用。 软引用可用来实现内存敏感的高速缓存 软引用可以和一个引用队列（ReferenceQueue）联合使用，如果软引用所引用的对象被垃圾回收，Java虚拟机就会把这个软引用加入到与之关联的引用队列中 弱引用（WeakReference）\n如果对象只具有弱引用，则类似于可有可无的生活用品 弱引用和软引用的区别：只具有弱引用的对象拥有更短暂的生命周期 垃圾回收器线程扫描它所管辖的内存区域的过程中，一旦发现只具有弱引用的对象，不管当前内存足够与否，都会回收它的内存。不过垃圾回收器是一个优先级很低的线程，因此不一定会很快发现那些只具有弱引用的对象 弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java虚拟机就会把这个弱引用加入到与之关联的引用队列中 虚引用（PhantomReference） [ˈfæntəm] 英\n与其他引用不同，虚引用并不会决定对象声明周期。如果一个仅持有虚拟引用，那么它就跟没有任何引用一样，在任何时候都可能被垃圾回收\n虚引用主要用来跟踪对象被垃圾回收的活动\n虚引用、软引用和弱引用的区别：虚引用必须和引用队列（ReferenceQueue）联合使用。\n当垃圾回收器准备回收一个对象时，如果发现它还有虚引用，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列。 程序可以通过判断引用队列是否加入虚引用，来了解被引用的对象是否将被垃圾回收 如果发现某个虚引用已经被加入到引用队列，那么就可以在所引用的对象被回收之前采取必要的行动 在程序设计中一般很少使用弱引用与虚引用，使用软引用的情况较多，这是因为软引用可以加速 JVM 对垃圾内存的回收速度，可以维护系统的运行安全，防止内存溢出（OutOfMemory）等问题的产生\nThreadLocal中的key用到了弱引用\n如何判断一个常量是废弃常量 # 运行时常量池主要回收的是废弃的常量\nJDK1.7 之前，运行时常量池逻辑，包括字符串常量池，存放在方法区，此时hotspot虚拟机对方法区的实现为永久代 JDK1.7字符串常量池（以及静态变量）被从方法区拿到了堆中，这里没有提到运行时常量池，也就是说字符串常量池被单独拿到堆，运行时常量池剩下的东西，还在方法区。即hotspot中的永久代 JDK1.8 hotspot移除了永久代，用元空间Metaspace取代之，这时候字符串常量池还在堆，运行时常量池还在方法区，只不过方法区的实现从永久代变成了元空间Metaspace ★★ 假如字符串常量池存在字符串“abc”，如果当前没有任何String对象引用该字符串常量的话，就说明常量“abc”是废弃常量。如果这时发生内存回收并且有必要的话，“abc”就会被系统清理出常量池\n如何判断一个类是无用类 # 方法区主要回收的是无用的类，判断一个类是否是无用的类相对苛刻，需要同时满足下面条件\n该类所有实例都已经被回收，即Java堆中不存在该类的任何实例 加载该类的ClassLoader已经被回收 该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类方法 Java虚拟机可以对满足上述3个条件的无用类进行回收，是**“可以”，而不是必然**\n垃圾收集算法 # 标记-清除算法 # 该算法分为**“标记”和“清除”阶段：\n标记出所有不需要回收的对象**，在标记完成后统一回收掉所有没有被标记的对象\n这是最基础的收集算法，后续的算法都是对其不足进行改进得到，有两个明显问题：\n效率问题 空间问题（标记清除后会产生大量不连续碎片） 标记-复制算法 # 将内存分为大小相同的两块，每次使用其中一块 当这块内存使用完后，就将还存活的对象复制到另一块去，然后再把使用的空间一次清理掉 这样每次内存回收都是对内存区间的一半进行回收 标记-整理算法 # 根据老年代特点提出的一种标记算法，标记过程仍然与**“标记-清除”算法一样，但后续不是直接对可回收对象回收，而是让所有存活对象向一端移动**，然后直接清理掉端边界以外的内存\n分代收集算法 # 当前虚拟机的垃圾收集都采用分代收集算法，没有新的思想，只是根据对象存活周期的不同将内存分为几块。\n对象存活周期，也就是有些对象活的时间短，有些对象活的时间长。\n一般将Java堆分为新生代和老年代，这样就可以根据各个年代的特点，选择合适的垃圾收集算法\n新生代中，每次收集都会有大量对象死去，所以可以选择**“标记-复制”算法，只需要付出少量对象的复制成本就可以完成每次垃圾收集** 老年代对象存活几率是比较高的，而且没有额外的空间对它进行分配担保，所以必须选择标记-清除或者**“标记-整理”**算法进行垃圾收集 垃圾收集器 # 收集算法是内存回收的方法论，而垃圾收集器则是内存回收的具体实现 没有最好的垃圾收集器，也没有万能的，应该根据具体应用场景，选择适合自己的垃圾收集器 汇总 # 新生代的垃圾回收器：Serial（串行\u0026ndash;标记复制），ParNew（并行\u0026ndash;标记复制），ParallelScavenge（并行\u0026ndash;标记复制） 老年代的垃圾回收器：SerialOld（串行\u0026ndash;标记整理），ParallelOld（并行\u0026ndash;标记整理），CMS（并发\u0026ndash;标记清除） 只有CMS和G1是并发，且CMS只作用于老年代，而G1都有 JDK8为止，默认垃圾回收器是Parallel Scavenge和Parallel Old【并行\u0026ndash;复制和并行\u0026ndash;标记整理】 JDK9开始，G1收集器成为默认的垃圾收集器，目前来看，G1回收期停顿时间最短且没有明显缺点，偏适合Web应用 jdk8中测试Web应用，堆内存6G中新生代4.5G的情况下\nParallelScavenge回收新生代停顿长达1.5秒。 G1回收器回收同样大小的新生代只停顿0.2秒 Serial 收集器 # Serial 串行 收集器是最基本、历史最悠久的垃圾收集器\n这是一个单线程收集器，它的单线程意义不仅意味着它只会使用一条垃圾收集线程去完成垃圾收集工作，更重要的是它在进行垃圾收集工作时必须暂停其他所有的工作线程**（”Stop The World“），直到它收集结束**。\n新生代采用标记-复制算法，老年代采用标记-整理算法 StopTheWorld会带来不良用户体验，所以在后续垃圾收集器设计中停顿时间不断缩短。（仍然有停顿，垃圾收集器的过程仍然在继续） 优点：简单而高效（与其他收集器的单线程相比） 且由于其没有线程交互的开销，自然可以获得很高的单线程收集效率 Serial收集器对于运行在Client模式下的虚拟机来说是个不错的选择 -XX:+UseSerialGC #虚拟机运行在Client模式下的默认值，Serial+Serial Old。 ParNew 收集器 # ParNew收集器其实就是Serial收集器的多线程版本，除了使用多线程进行垃圾收集外，其余行为（控制参数、收集算法、回收策略等等）和Serial收集器完全一样\n新生代采用标记-复制算法，老年代采用标记-整理算法\n★★★ 这是许多运行在Server模式下的虚拟机的首要选择，除了Serial收集器外，只有它能与CMS收集器（真正意义上的并发收集器）配合工作（ParNew是并行）\n并行和并发概念补充\n并行（Parallel）：指多条垃圾收集线程并行工作，但此时用户线程仍然处于等待状态 并发（Concurrent）：指用户线程与垃圾收集线程 同时执行（不一定并行，可能会交替执行），用户程序在继续执行，而收集收集器运行在另一个CPU上 -XX:+UseParNewGC #ParNew+Serial Old，在JDK1.8被废弃，在JDK1.7还可以使用。 ParallelScavenge 收集器 # 它也是标记-复制算法的多线程收集器，看上去几乎和ParNew一样，区别\n部分参数 (有点争议，先以下面为准)\n-XX:+UseParallelGC # 虚拟机运行在Server模式下的默认值(1.8) 新生代使用ParallelGC，老年代使用回收器 ; ★★ JDK1.7之后，能达到UseParallelOldGC 的效果 ## 参考自 https://zhuanlan.zhihu.com/p/353458348 -XX:+UseParallelOldGC # 新生代使用ParallelGC，老年代使用ParallelOldGC Parallel Scavenge收集器关注点是吞吐量（高效率利用CPU），CMS等垃圾收集器关注点是用户的停顿时间（提高用户体验）\n所谓吞吐量就是CPU中用于运行用户代码的时间与CPU总消耗时间的比值 （也就是希望消耗少量CPU就能运行更多代码）\nParallel Scavenge 收集器提供了很多参数供用户找到最合适的停顿时间或最大吞吐量，如果对于收集器运作不太了解，手工优化存在困难的时候，使用 Parallel Scavenge 收集器配合自适应调节策略，把内存管理优化交给虚拟机去完成也是一个不错的选择。\n新生代采用标记-复制，老年代采用标记-整理算法 这是JDK1.8 的默认收集器 使用 java -XX:+PrintCommandLineFlags -version 命令查看 如下，两种情况：\n#默认 λ java -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=531924800 -XX:MaxHeapSize=8510796800 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC java version \u0026#34;1.8.0_202\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_202-b08) Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode) 第二种情况：(注意：-XX:-UseParallelOldGC)\nλ java -XX:-UseParallelOldGC -XX:+PrintCommandLineFlags -version -XX:InitialHeapSize=531924800 -XX:MaxHeapSize=8510796800 -XX:+PrintCommandLineFlags -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:-UseLargePagesIndividualAllocation -XX:+UseParallelGC -XX:-UseParallelOldGC java version \u0026#34;1.8.0_202\u0026#34; Java(TM) SE Runtime Environment (build 1.8.0_202-b08) Java HotSpot(TM) 64-Bit Server VM (build 25.202-b08, mixed mode) SerialOld 收集器 # Serial收集器的老年代版本，是一个单线程收集器 在JDK1.5以及以前的版本中，与Parallel Scavenge收集器搭配时候 作为CMS收集器的后备方案 ParallelOld 收集器 # Parallel Scavenge收集器的老年代版本，使用多线程和标记-整理算法 在注重吞吐量以及CPU资源的场合，都可以考虑ParallelScavenge和ParallelOld收集器 CMS 收集器 # CMS，Concurrent Mark Sweep，是一种以获取最短回收停顿时间为目标的收集器，非常符合注重用户体验的引用上使用\nCMS收集器是HotSpot虚拟机上第一款真正意义上的并发收集器，第一次实现了让垃圾收集线程与用户线程（基本上）同时工作\nMark-Sweep，是一种“标记-清除”算法，运作过程相比前面几种垃圾收集器来说更加复杂，步骤：\n初始标记：暂停所有其他线程，记录直接与root相连的对象，速度很快\n并发标记：同时 开启GC和用户线程 ，用一个闭包结构记录可达对象。但这个阶段结束，这个闭包结构并不能保证包含当前所有的可达对象。\n因为用户线程会不断更新引用域，所以GC线程无法保证可达性分析的实时性\n所以这个算法里会跟踪记录这些发生引用更新的地方\n重新标记：目的是修正并发标记期间因为用户程序继续运行而导致标记产生变动的那一部分对象的标记记录。\n这个阶段停顿时间一般会被初始标记阶段时间稍长，远远比并发标记阶段时间短\n并发清除：开启用户线程，同时GC线程开始对未扫描的区域做清扫\n从名字可以看出这是一款优秀的收集器：并发收集、低停顿。但有三个明显缺点\n对CPU资源敏感\n无法处理浮动垃圾\n浮动垃圾的解释：就是之前被gc 标记为 可达对象，也就是 存活对象，在两次gc线程之间被业务线程删除了引用，那么颜色不会更改，还是之前的颜色（黑色or灰色），但是其实是白色，所以这一次gc 无法对其回收，需要等下一次gc初始标记启动才会被刷成白色 作者：Yellowtail 链接：https://www.jianshu.com/p/6590aaad82f7 来源：简书\n它使用的收集算法**“标记-清除”算法会导致收集结束时会有大量空间碎片产生**\nG1 收集器 # G1(Garbage-First)，是一款面向服务器的垃圾收集器，主要针对配备多颗处理器以及大容量内存的极其，以极高概率满足GC停顿时间要求的同时，还具备高吞吐量性能特征\nJDK1.7中HotSpot虚拟机的一个重要进化特征，具备特点：\n并行与并发：\nG1 能充分利用 CPU、多核环境下的硬件优势，使用多个 CPU（CPU 或者 CPU 核心）来缩短 Stop-The-World 停顿时间。部分其他收集器原本需要停顿 Java 线程执行的 GC 动作，G1 收集器仍然可以通过并发的方式让 java 程序继续执行\n分代收集：\n虽然 G1 可以不需要其他收集器配合就能独立管理整个 GC 堆，但是还是保留了分代的概念。\n空间整合：\n与 CMS 的“标记-清理”算法不同，G1 从整体来看是基于**“标记-整理”算法实现的收集器；从局部上来看是基于“标记-复制”**算法实现的。\n可预测的停顿：\n这是 G1 相对于 CMS 的另一个大优势，降低停顿时间是 G1 和 CMS 共同的关注点，但 G1 除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为 M 毫秒的时间片段内。\nG1 收集器的运作大致分为以下几个步骤\n初始标记 并发标记 最终标记 筛选回收 G1 收集器在后台维护了一个优先列表，每次根据允许的收集时间，优先选择回收价值最大的 Region(这也就是它的名字 Garbage-First 的由来) 。这种使用 Region 划分内存空间以及有优先级的区域回收方式，，保证了 G1 收集器在有限时间内可以尽可能高的收集效率（把内存化整为零）\nZGC 收集器 # The Z Garbage Collector\n与 CMS 中的 ParNew 和 G1 类似，ZGC 也采用标记-复制算法，不过 ZGC 对该算法做了重大改进。\n在 ZGC 中出现 Stop The World 的情况会更少！\nJDK11，相关文章 https://tech.meituan.com/2020/08/06/new-zgc-practice-in-meituan.html\n"},{"id":133,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0407lyjvm-intro/","title":"jvm-intro","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide（添加小部分笔记）感谢作者!\n原文地址： https://juejin.im/post/5e1505d0f265da5d5d744050#heading-28 感谢原作者分享！！\nJVM的基本介绍 # JVM，JavaVirtualMachine的缩写，虚拟出来的计算机，通过在实际的计算机上仿真模拟各类计算机功能实现 JVM类似一台小电脑，运行在windows或者linux这些真实操作系统环境下，直接和操作系统交互，与硬件不直接交互，操作系统帮我们完成和硬件交互的工作 Java文件是如何运行的 # 场景假设：我们写了一个HelloWorld.java，这是一个文本文件。JVM不认识文本文件，所以需要一个编译，让其(xxx.java)成为一个JVM会读的二进制文件\u0026mdash;\u0026gt; HelloWorld.class\n类加载器 如果JVM想要执行这个.class文件，需要将其**(这里应该指的二进制文件)装进类加载器**中，它就像一个搬运工一样，会把所有的.class文件全部搬进JVM里面 方法区\n类加载器将.class文件搬过来，就是先丢到这一块上\n方法区是用于存放类似于元数据信息方面的数据的，比如类信息、常量、静态变量、编译后代码\u0026hellip;等\n堆 堆主要放一些存储的数据，比如对象实例、数组\u0026hellip;等，它和方法区都同属于线程共享区域，即它们都是线程不安全的\n栈\n线程独享\n栈是我们代码运行空间，我们编写的每一个方法都会放到栈里面运行。\n名词：本地方法栈或本地方法接口，不过我们基本不会涉及这两块内容，这两底层使用C进行工作，和Java没有太大关系\n程序计数器 主要就是完成一个加载工作，类似于一个指针一样的，指向下一行我们需要执行的代码。和栈一样，都是线程独享的，就是每一个线程都会自己对应的一块区域而不会存在并发和多线程问题。\n小总结 Java文件经过编译后编程.class字节码文件 字节码文件通过类加载器被搬运到 JVM虚拟机中 虚拟机主要的5大块：方法区、堆 都为线程共享区域，有线程安全问题；栈和本地方法栈和计数器都是独享区域，不存在线程安全问题，而JVM的调优主要就是围绕堆、栈两大块进行 简单的代码例子 # 一个简单的学生类及main方法:\npublic class Student { public String name; public Student(String name) { this.name = name; } public void sayName() { System.out.println(\u0026#34;student\u0026#39;s name is : \u0026#34; + name); } } main方法：\npublic class App { public static void main(String[] args) { Student student = new Student(\u0026#34;tellUrDream\u0026#34;); student.sayName(); } } ★★ 执行main方法的步骤如下\n编译好App.java后得到App.class后，执行APP.class，系统会启动一个JVM进程，从classpath类路径中找到一个名为APP.class的二进制文件，将APP的类信息加载到运行时数据区的方法区内，这个过程叫做APP类的加载 JVM找到APP的主程序入口，执行main方法 这个main的第一条语句**(指令)**为 Student student = new Student(\u0026quot;tellUrDream\u0026quot;)，就是让JVM创建一个Student对象，但是这个时候方法区是没有Student类的信息的，所以JVM马上加载Student类，把Student类的信息放到方法区中 加载完Student类后，JVM在堆中为一个新的Student实例分配内存，然后调用构造函数初始化Student实例，这个Student实例**(对象)持有指向方法区中的Student类的类型信息**的引用 执行student.sayName;时，JVM根据student的引用找到student对象，然后根据student对象持有的引用定位到方法区中student类的类型信息的方法表，获得sayName()的字节码地址。 执行sayName() 其实也不用管太多，只需要知道对象实例初始化时，会去方法区中找到类信息（没有的话先加载），完成后再到栈那里去运行方法\n类加载器的介绍 # 类加载器负责加载.class文件，.class文件的开头会有特定的文件标识，将class文件字节码内容加载到内存中，并将这些内容转换成方法区中的运行时数据结构，并且ClassLoader只负责class文件的加载，而能否运行则由Execution Engine来决定\n类加载器的流程 # 从类被加载到虚拟机内存中开始，到释放内存总共有7个步骤：\n加载，验证，准备，解析，初始化，使用，卸载。\n其中验证，准备，解析三个部分统称为链接\n加载 # 将class文件加载到内存 将静态数据结构转化成方法区中运行的数据结构 在堆中生成一个代表这个类的java.lang.Class对象作为数据访问的入口 链接 # 验证：确保加载的类符合JVM规范和安全，保证被校验类的方法在运行时不会做出危害虚拟机的事件，其实就是一个安全检查 准备：为static变量在方法区分配内存空间，设置变量的初始值，例如static int = 3 （注意：准备阶段只设置类中的静态变量（方法区中），不包括实例变量（堆内存中），实例变量是对象初始化时赋值的） 解析：虚拟机将常量池内的符号引用，替换为直接引用的过程（符号引用比如我现在 import java.util.ArrayList 这就算符号引用，直接引用就是指针或者对象地址，注意引用对象一定是在内存进行） 初始化 # 初始化就是执行类构造器方法的clinit()的过程，而且要保证执行前父类的clinit()方法已经执行完毕。 这个方法由编译器收集(也就是编译时产生)，顺序执行所有类变量(static 修饰的成员变量) 显示初始化和静态代码块中语句 此时准备阶段时的那个static int a 由默认初始化的0变成了显示初始化的3。由于执行顺序缘故，初始化阶段类变量如果在静态代码中又进行更改，则会覆盖类变量的显式初始化，最终值会为静态代码块中的赋值 字节码文件中初始化方法有两种，非静态资源初始化的init和静态资源初始化的clinit 类构造器方法clinit() 不同于类的构造器，这些方法都是字节码文件中只能给JVM识别的特殊方法 卸载 # GC将无用对象从内存中卸载\n类加载器的加载顺序 # 加载一个Class类的顺序也是有优先级的**(加载，也可以称\u0026quot;查找\u0026quot;)** ，类加载器 从最底层开始往上的顺序：\nBootStrap ClassLoader： rt.jar (lib/rt.jar) Extension ClassLoader: 加载扩展的jar包 (lib/ext/xxx.jar) APP ClassLoader： 指定的classpath下面的jar包 Custom ClassLoader： 自定义的类加载器 双亲委派机制 # 当一个类收到了加载请求时，它是不会先自己去尝试加载的，而是委派给父类去完成，比如我现在要 new 一个 Person，这个 Person 是我们自定义的类，如果我们要加载它，就会先委派 App ClassLoader ，只有当父类加载器都反馈自己无法完成这个请求（也就是父类加载器都没有找到加载所需的 Class）时，子类加载器才会自行尝试加载。\n好处：加载位于 rt.jar 包中的类时不管是哪个加载器加载，最终都会委托到 BootStrap ClassLoader 进行加载，这样保证了使用不同的类加载器得到的都是同一个结果。\n其实这起了一个隔离的作用，避免自己写的代码影响JDK的代码\npackage java.lang; public class String { public static void main(String[] args) { System.out.println(); } } 尝试运行当前类的 main 函数的时候，我们的代码肯定会报错。这是因为在加载的时候其实是找到了 rt.jar 中的java.lang.String，然而发现这个里面并没有 main 方法。\n运行时数据区 # 本地方法栈和程序计数器 # 比如说我们现在点开Thread类的源码，会看到它的start0方法带有一个native关键字修饰，而且不存在方法体，这种用native修饰的方法就是本地方法，这是使用C来实现的，然后一般这些方法都会放到一个叫做本地方法栈的区域。 程序计数器其实就是一个指针，它指向了我们程序中下一句需要执行的指令，它也是内存区域中唯一一个不会出现OutOfMemoryError的区域，而且占用内存空间小到基本可以忽略不计。这个内存仅代表当前线程所执行的字节码的行号指示器，字节码解析器通过改变这个计数器的值选取下一条需要执行的字节码指令。 如果执行的是native方法，那这个指针就不工作了 方法区 # 主要存放类的元数据信息、常量和静态变量\u0026hellip;等。 存储过大时，会在无法满足内存分配时报错 虚拟机栈和虚拟机堆 # 栈管运行，堆管存储 虚拟机栈负责运行代码，虚拟机堆负责存储数据 虚拟机栈的概念 # 虚拟机栈是Java方法执行的内存模型 对局部变量、动态链表、方法出口、栈的操作(入栈和出栈)进行存储，且线程独享。 如果我们听到局部变量表，就是在说虚拟机栈 public class Person{ int a = 1; public void doSomething(){ int b = 2; } } 虚拟机栈存在的异常 # 如果线程请求的栈的深度，大于虚拟机栈的最大深度，就会报StackOverflowError(比如递归) Java虚拟机也可以动态扩展，但随着扩展会不断地申请内存，当无法申请足够内存时就会报错 OutOfMemoryError 虚拟机栈的生命周期 # 栈不存在垃圾回收，只要程序运行结束，栈的空间自然释放 栈的生命周期和所处的线程一致 8种基本类型的变量+对象的引用变量+实例方法，都是在栈里面分配内存 虚拟机栈的执行 # 栈帧数据，在JVM中叫栈帧，Java中叫方法，它也是放在栈中 栈中的数据以栈帧的格式存在，它是一个关于方法和运行期数据的数据集 比如我们执行一个方法a，就会对应产生一个栈帧A1，然后A1会被压入栈中。同理方法b会有一个B1，方法c会有一个C1，等到这个线程执行完毕后，栈会先弹出C1，后B1,A1。它是一个先进后出，后进先出原则。\n局部变量的复用 # 用于存放方法参数和方法内部所定义的局部变量\n容量以Slot为最小单位，一个slot可以存放32以内的数据类型。\n在局部变量表里，32位以内的类型只占用一个slot（包括returnAddress类型），64位的类型（long和double）占两个slot。\n虚拟机通过索引方式使用局部变量表，范围为 [ 0 , 局部变量表的slot的数量 ]。方法中的参数就会按一定顺序排列在这个局部变量表中\n为了节省栈帧空间，这些slot是可以复用的。当方法执行位置超过了某个变量（这里意思应该是用过了这个变量），那么这个变量的slot可以被其它变量复用。当然如果需要复用，那我们的垃圾回收自然就不会去动这些内存\n虚拟机堆的概念 # JVM内存会划分为堆内存和非堆内存，堆内存也会划分为年轻代和老年代，而非堆内存则为永久代。\n年轻代又分为Eden和Survivor区，Survivor还分为FromPlace和ToPlace，toPlace的survivor区域是空的\nEden：FromPlace：ToPlace的默认占比是8：1：1，当然这个东西也可以通过一个-XX:+UsePSAdaptiveSurvivorSizePolicy参数来根据生成对象的速率动态调整\n（因为存活的对象相对较少）\n堆内存中存放的是对象，垃圾收集就是收集这些对象然后交给GC算法进行回收。非堆内存其实我们已经说过了，就是方法区。在1.8中已经移除永久代，替代品是一个元空间(MetaSpace)，最大区别是metaSpace是不存在于JVM中的，它使用的是本地内存。并有两个参数：\nMetaspaceSize：初始化元空间大小，控制发生GC MaxMetaspaceSize：限制元空间大小上限，防止占用过多物理内存。 移除的原因\n融合HotSpot JVM和JRockit VM而做出的改变，因为JRockit是没有永久代的，不过这也间接性地解决了永久代的OOM问题。\nEden年轻代的介绍 # 当new一个对象后，会放到Eden划分出来的一块作为存储空间的内存，由于堆内存共享，所以可能出现两个对象共用一个内存的情况。\nJVM的处理：为每个内存都预先申请好一块连续的内存空间并规定对象存放的位置，如果空间不足会再申请多块内存空间。这个操作称为TLAB\nEden空间满了之后，会触发MinorGC（发生在年轻代的GC）操作，存活下来的对象移动到Survivor0区。Survivor0满后会触发MInorGC，将存活对象（这里应该包括Eden的存活对象?）移动到Survivor1区，此时还会把from和to两个指针交换，这样保证一段时间内总有一个survivor区为空且所指向的survivor区为空。\n经过多次的MinorGC后仍然存活的对象(这里存活判断是15次，对应的虚拟机参数为-XX:MaxTenuringThreshold 。HotSpot会在对象中的标记字段里记录年龄，分配到的空间仅有4位，所以最多记录到15)会移动到老年代。\n老年代是存储长期存活对象的，占满时就会触发我们常说的FullGC，期间会停止所有线程等待GC的完成。所以对于响应要求高的应用，应该尽量去减少发生FullGC从而避免响应超时的问题\n当老年区执行full gc周仍然无法进行对象保存操作，就会产生OOM。这时候就是虚拟机中堆内存不足，原因可能会是堆内存设置大小过小，可以通过参数**-Xms、-Xmx来调整。也可能是代码中创建对象大且多**，而且它们一直在被引用从而长时间垃圾收集无法收集它们\n关于-XX:TargetSurvivorRatio参数的问题。其实也不一定是要满足-XX:MaxTenuringThreshold才移动到老年代。可以举个例子：如**对象年龄5的占30%，年龄6的占36%，年龄7的占34%，加入某个年龄段（如例子中的年龄6）**后，总占用超过Survivor空间*TargetSurvivorRatio的时候，从该年龄段开始及大于的年龄对象就要进入老年代（即例子中的年龄6对象，就是年龄6和年龄7晋升到老年代），这时候无需等到MaxTenuringThreshold中要求的15\n如何判断一个对象需要被干掉 # 首先看一下对象的虚拟机的一些流程\n图例有点问题，橙色是线程共享，青绿色是线程独享 图中程序计数器、虚拟机栈、本地方法栈，3个区域随着线程生存而生存。内存分配和回收都是确定的，随着线程的结束内存自然就被回收了，因此不需要考虑垃圾回收问题。\nJava堆和方法区则不一样，各线程共享，内存的分配和回收都是动态的，垃圾收集器所关注的就是堆和方法区这部分内存\n垃圾回收前，判断哪些对象还存活，哪些已经死去。下面介绍连个基础计算方法：\n引用计数器计算：给对象添加一个引用计数器，每次引用这个对象时计数器加一，引用失效时减一，计数器等于0就是不会再次使用的。不过有一种情况，就是 出现对象的循环引用时GC没法回收（我觉得不是非得循环，如果一个对象a中有属性引用另一个对象b，而a指向null，那么按这种方式，b就没有办法被回收）。\n可达性分析计算：一种类似二叉树的实现，将一系列的GC ROOTS作为起始的存活对象集，从这个结点往下搜索，搜索所走过的路径成为引用链，把能被该集合引用到的对象加入该集合中。\n当一个对象到GC Roots没有使用任何引用链时，则说明该对象是不可用的。Java，C#都是用这个方法判断对象是否存活\nJava语言汇总作为GCRoots的对象分为以下几种：\n虚拟机栈（栈帧中的本地方法表）中引用的对象（局部变量）\n方法区中静态变量所引用的对象（静态变量）\n方法区中常量引用的变量\n本地方法栈（即native修饰的方法）中JNI引用的对象\n（JNI是Java虚拟机调用对应的C函数的方式，通过JNI函数也可以创建新的Java对象。且JNI对于对象的局部引用或者全局引用都会把它们指向的对象都标记为不可回收）\n已启动的且未终止的Java线程【这个描述好像是有问题的(不全)，应该是用作同步监视器的对象】\n这种方法的优点是，能够解决循环引用的问题，可它的实现耗费大量资源和时间，也需要GC(分析过程引用关系不能发生变化，所以需要停止所有进程)\n如何宣告一个对象的真正死亡 # 首先，需要提到finalize()方法，是Object类的一个方法，一个对象的finalize()方法只会被系统自动调用一次，经过finalize()方法逃脱死亡的对象(比如在方法中，其他变量又一次引用了该对象)，第二次不会再被调用\n并不提倡在程序中调用finalize()来进行自救。建议忘掉Java程序中该方法的存在。因为它执行的时间不确定，甚至是否被执行也不确定（Java程序的不正常退出），而且运行代价高昂，无法保证各个对象的调用顺序（甚至有不同线程中调用）。在Java9中已经被标记为 deprecated ，且 java.lang.ref.Cleaner（也就是强、软、弱、幻象引用的那一套）中已经逐步替换掉它，会比 finalize 来\ndeprecated英[ˈdeprəkeɪtɪd]美[ˈdeprəkeɪtɪd]\n判断一个对象的死亡至少需要两次标记\n如果对象可达性分析之后没发现与GC Roots相连的引用链，那它将会被第一次标记并且进行一次筛选，判断条件是是决定**这个对象是否有必要执行finalize()**方法。如果对象有必要执行finalize()，则被放入F-Queue队列 GC堆F-Queue队列中的对象进行二次标记。如果对象在finalize()方法中重新与引用链上的任何一个对象建立了关联，那么二次标记时则会将它移出“即将回收”集合。如果此时对象还没成功逃脱，那么只能被回收了。 垃圾回收算法 # 确定对象已经死亡，此刻需要回收这些垃圾。常用的有标记清除、复制、标记整理、和分代收集算法。\n标记清除算法 # 标记清除算法就是分为**”标记“和”清除“**两个阶段。标记出所有需要回收的对象，标记结束后统一回收。后续算法都根据这个基础来加以改进 即：把已死亡的对象标记为空闲内存，然后记录在空闲列表中，当我们需要new一个对象时，内存管理模块会从空闲列表中寻找空闲的内存来分给新的对象 不足方面：标记和清除效率比较低，且这种做法让内存中碎片非常多 。导致如果我们需要使用较大内存卡时，无法分配到足够的连续内存 如图，可使用的内存都是零零散散的，导致大内存对象问题 复制算法 # 为了解决效率问题，出现了复制算法。将内存按容量划分成两等份，每次只使用其中的一块，和survivor一样用from和to两个指针。fromPlace存满了，就把存活对象copy到另一块toPlace上，然后交换指针内容，就解决了碎片问题\n代价：内存缩水，即堆内存的使用效率变低了 默认情况Eden和Survivor 为 8: 2 （Eden : S0 : S1 = 8：1：1）\n标记整理 # 复制算法在对象存活率高的时候，仍然有效率问题（要复制的多）。 标记整理\u0026ndash;\u0026gt; 标记过程与标记-清除一样，但后续不是直接对可回收对象进行清理，而是让所有存活对象都向一端移动，然后直接清理掉边界以外内存 分代收集算法 # 这种算法并没有什么新的思想，只是根据对象存活周期的不同将内存划分为几块 一般是将Java堆分为新生代和老年代，即可根据各个年代特点采用最适当的收集算法 新生代中，每次垃圾收集时会有大批对象死去，只有少量存活，就采用复制算法，只需要付出少量存活对象的复制成本即可完成收集 老年代中，因为存活对象存活率高，也没有额外空间对它进行分配担保（新生代如果不够可以放老年代，而老年代清理失败就会OutOfMemory，不像新生代可以移动到老年代），所以必须使用**“标记-清理”或者“标记-整理”**来进行回收 即：具体问题具体分析 （了解）各种各样的垃圾回收器 # 新生代的垃圾回收器：Serial（串行\u0026ndash;复制），ParNew（并行\u0026ndash;复制），ParallelScavenge（并行\u0026ndash;复制）\n老年代的垃圾回收器：SerialOld（串行\u0026ndash;标记整理），ParallelOld（并行\u0026ndash;标记整理），CMS（并发\u0026ndash;标记清除）\n只有CMS和G1是并发，且CMS只作用于老年代，而G1都有\nJDK8为止，默认垃圾回收器是Parallel Scavenge和Parallel Old【并行\u0026ndash;复制和并行\u0026ndash;标记整理】\nJDK9开始，G1收集器成为默认的垃圾收集器，目前来看，G1回收期停顿时间最短且没有明显缺点，偏适合Web应用\njdk8中测试Web应用，堆内存6G中新生代4.5G的情况下\nParallelScavenge回收新生代停顿长达1.5秒。 G1回收器回收同样大小的新生代只停顿0.2秒 （了解） JVM的常用参数 # JVM的参数非常之多，这里只列举比较重要的几个，通过各种各样的搜索引擎也可以得知这些信息。\n参数名称 含义 默认值 说明 -Xms 初始堆大小 物理内存的1/64(\u0026lt;1GB) 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制. -Xmx 最大堆大小 物理内存的1/4(\u0026lt;1GB) 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 -Xmn 年轻代大小(1.4or later) 注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。整个堆大小=年轻代大小 + 老年代大小 + 持久代（永久代）大小.增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8 -XX:NewSize 设置年轻代大小(for 1.3/1.4) -XX:MaxNewSize 年轻代最大值(for 1.3/1.4) -XX:PermSize 设置持久代(perm gen)初始值 物理内存的1/64 -XX:MaxPermSize 设置持久代最大值 物理内存的1/4 -Xss 每个线程的堆栈大小 JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.根据应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右一般小的应用， 如果栈不是很深， 应该是128k够用的 大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:-Xss is translated in a VM flag named ThreadStackSize”一般设置这个值就可以了 -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代) -XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。 -XX:SurvivorRatio Eden区与Survivor区的大小比值 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10 -XX:+DisableExplicitGC 关闭System.gc() 这个参数需要严格的测试 -XX:PretenureSizeThreshold 对象超过多大是直接在旧生代分配 0 单位字节 新生代采用Parallel ScavengeGC时无效另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象. -XX:ParallelGCThreads 并行收集器的线程数 此值最好配置与处理器数目相等 同样适用于CMS -XX:MaxGCPauseMillis 每次年轻代垃圾回收的最长时间(最大暂停时间) 如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值. 其实还有一些打印及CMS方面的参数，这里就不以一一列举了\n关于JVM调优的一些方面 # 默认\n年轻代：老年代 = 1: 2 年轻代中 Eden : S0 : S 1 = 8 : 1 ：1 根据刚刚涉及的jvm知识点，可以尝试对JVM进行调优，主要是堆内存那块\n所有线程共享数据区大小=新生代大小+老年代大小+持久代大小 （即 堆 + 方法区）\n持久代一般固定大小为64m，\njava堆中增大年轻代后，会减少老年代大小（因为老年代的清理使用fullgc，所以老年代过小的话反而会增多fullgc）。 年轻代 -Xmn的值推荐配置为java堆的3/8\n调整最大堆内存和最小堆内存 # -Xmx -Xms：指定java堆最大值（默认 物理内存的1/4 (\u0026lt;1 GB ) ) 和 初始java堆最小值（默认值是物理内存的1/64 (\u0026lt;1GB) ）\n默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制.，默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制。\n简单点来说，你不停地往堆内存里面丢数据，等它剩余大小小于40%了，JVM就会动态申请内存空间不过会小于-Xmx，如果剩余大小大于70%，又会动态缩小不过不会小于–Xms。就这么简单\n开发过程中，通常会将 -Xms 与 Xmx 两个参数设置成相同的值\n为的是能够在java垃圾回收机制清理完堆区后，不需要重新分隔计算堆区的大小而浪费资源（向系统请求/释放内存资源）\n代码\npublic class App { public static void main(String[] args) { System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的最大空间-Xmx--运行几次都不变 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的空闲空间--每次运行都变 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //当前可用的总空间 与Xms有关--运行几次都不变 } } /* ----- Xmx=7389184.0KB free mem=493486.0546875KB total mem=498688.0KB */ maxMemory()这个方法返回的是java虚拟机(这个进程)能构从操纵系统那里挖到的最大的内存 freeMemory：挖过来而又没有用上的内存，实际上就是 freeMemory()，所以freeMemory()的值一般情况下都是很小的(totalMemory一般比需要用得多一点，剩下的一点就是freeMemory) totalMemory：程序运行的过程中，内存总是慢慢的从操纵系统那里挖的，基本上是用多少挖多少，直 挖到maxMemory()为止，所以totalMemory()是慢慢增大的 原文链接：https://blog.csdn.net/weixin_35671171/article/details/114189796 编辑VM options参数后再看效果：\n-Xmx20m -Xms5m -XX:+PrintGCDetails，堆最大以及堆初始值 20m和5m\n/* 效果 [GC (Allocation Failure) [PSYoungGen: 1024K-\u0026gt;488K(1536K)] 1024K-\u0026gt;608K(5632K), 0.0007606 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Xmx=18432.0KB free mem=4249.90625KB total mem=5632.0KB Heap PSYoungGen total 1536K, used 1326K [0x00000000ff980000, 0x00000000ffb80000, 0x0000000100000000) eden space 1024K, 81% used [0x00000000ff980000,0x00000000ffa51ad0,0x00000000ffa80000) from space 512K, 95% used [0x00000000ffa80000,0x00000000ffafa020,0x00000000ffb00000) to space 512K, 0% used [0x00000000ffb00000,0x00000000ffb00000,0x00000000ffb80000) ParOldGen total 4096K, used 120K [0x00000000fec00000, 0x00000000ff000000, 0x00000000ff980000) object space 4096K, 2% used [0x00000000fec00000,0x00000000fec1e010,0x00000000ff000000) Metaspace used 3164K, capacity 4496K, committed 4864K, reserved 1056768K class space used 344K, capacity 388K, committed 512K, reserved 1048576K */ 如上， Allocation Failure 因为分配失败导致YoungGen total mem (此时申请到的总内存)：\nPSYoungGen + ParOldGen = 1536 + 4096 = 5632 KB freeMemory (申请后没有使用的内存)\n1324 + 120 = 1444 KB 5632 - 4249 = 1383 KB 差不多 使用1M后\npublic class App { public static void main(String[] args) { System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的最大空间-Xmx--运行几次都不变 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的空闲空间--每次运行都变 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //当前可用的总空间 与Xms有关--运行几次都不变 byte[] b = new byte[1 * 1024 * 1024]; System.out.println(\u0026#34;分配了1M空间给数组\u0026#34;); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); } } /** [GC (Allocation Failure) [PSYoungGen: 1024K-\u0026gt;488K(1536K)] 1024K-\u0026gt;608K(5632K), 0.0007069 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Xmx=18432.0KB free mem=4270.15625KB total mem=5632.0KB 分配了1M空间给数组 Xmx=18.0M free mem=3.1700592041015625M //少了1M total mem=5.5M Heap PSYoungGen total 1536K, used 1270K [0x00000000ff980000, 0x00000000ffb80000, 0x0000000100000000) eden space 1024K, 76% used [0x00000000ff980000,0x00000000ffa43aa0,0x00000000ffa80000) from space 512K, 95% used [0x00000000ffa80000,0x00000000ffafa020,0x00000000ffb00000) to space 512K, 0% used [0x00000000ffb00000,0x00000000ffb00000,0x00000000ffb80000) ParOldGen total 4096K, used 1144K [0x00000000fec00000, 0x00000000ff000000, 0x00000000ff980000) object space 4096K, 27% used [0x00000000fec00000,0x00000000fed1e020,0x00000000ff000000) Metaspace used 3155K, capacity 4496K, committed 4864K, reserved 1056768K class space used 344K, capacity 388K, committed 512K, reserved 1048576K */ 此时free memory就又缩水了，不过total memory是没有变化的。Java会尽可能将total mem的值维持在最小堆内存大小\n这时候我们创建了一个10M的字节数据，这时候最小堆内存是顶不住的。我们会发现现在的total memory已经变成了15M，这就是已经申请了一次内存的结果。\npublic class App { public static void main(String[] args) { System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的最大空间-Xmx--运行几次都不变 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的空闲空间--每次运行都变 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //当前可用的总空间 与Xms有关--运行几次都不变 byte[] b = new byte[1 * 1024 * 1024]; System.out.println(\u0026#34;分配了1M空间给数组\u0026#34;); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); byte[] c = new byte[10 * 1024 * 1024]; System.out.println(\u0026#34;分配了10M空间给数组\u0026#34;); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //当前可用的总空间 } } /** ---- [GC (Allocation Failure) [PSYoungGen: 1024K-\u0026gt;488K(1536K)] 1024K-\u0026gt;600K(5632K), 0.0006681 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Xmx=18432.0KB free mem=4257.953125KB total mem=5632.0KB 分配了1M空间给数组 Xmx=18.0M free mem=3.1153564453125M total mem=5.5M 分配了10M空间给数组 Xmx=18.0M free mem=2.579681396484375M total mem=15.0M Heap PSYoungGen total 1536K, used 1363K [0x00000000ff980000, 0x00000000ffb80000, 0x0000000100000000) eden space 1024K, 85% used [0x00000000ff980000,0x00000000ffa5acc0,0x00000000ffa80000) from space 512K, 95% used [0x00000000ffa80000,0x00000000ffafa020,0x00000000ffb00000) to space 512K, 0% used [0x00000000ffb00000,0x00000000ffb00000,0x00000000ffb80000) ParOldGen total 13824K, used 11376K [0x00000000fec00000, 0x00000000ff980000, 0x00000000ff980000) object space 13824K, 82% used [0x00000000fec00000,0x00000000ff71c020,0x00000000ff980000) Metaspace used 3242K, capacity 4500K, committed 4864K, reserved 1056768K class space used 351K, capacity 388K, committed 512K, reserved 1048576K */ 此时我们再跑一下这个代码\n此时要调整垃圾收集器(-XX:+UseG1GC)且b、c要指向null，才能让系统回收这部分内存，即-Xmx20m -Xms5m -XX:+PrintGCDetails -XX:+UseG1GC 注：使用-XX: +UseSerialGC或者-XX:+UseParallelGC都是不能达到效果的\n此时我们手动执行了一次fullgc，此时total memory的内存空间又变回6.0M了，此时又是把申请的内存释放掉的结果。\npublic class App { public static void main(String[] args) throws InterruptedException { System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的最大空间-Xmx--运行几次都不变 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //系统的空闲空间--每次运行都变 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 + \u0026#34;KB\u0026#34;); //当前可用的总空间 与Xms有关--运行几次都不变 byte[] b = new byte[1 * 1024 * 1024]; System.out.println(\u0026#34;分配了1M空间给数组\u0026#34;); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); byte[] c = new byte[10 * 1024 * 1024]; System.out.println(\u0026#34;分配了10M空间给数组\u0026#34;); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //当前可用的总空间 b=null; c=null; System.gc(); System.out.println(\u0026#34;进行了gc\u0026#34;); System.out.println(\u0026#34;Xmx=\u0026#34; + Runtime.getRuntime().maxMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的最大空间 System.out.println(\u0026#34;free mem=\u0026#34; + Runtime.getRuntime().freeMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //系统的空闲空间 System.out.println(\u0026#34;total mem=\u0026#34; + Runtime.getRuntime().totalMemory() / 1024.0 / 1024 + \u0026#34;M\u0026#34;); //当前可用的总空间 } } /*-------- Xmx=20480.0KB free mem=4290.3671875KB total mem=6144.0KB 分配了1M空间给数组 Xmx=20.0M free mem=3.1897964477539062M total mem=6.0M [GC pause (G1 Humongous Allocation) (young) (initial-mark), 0.0014754 secs] [Parallel Time: 1.1 ms, GC Workers: 8] [GC Worker Start (ms): Min: 105.0, Avg: 105.1, Max: 105.3, Diff: 0.3] [Ext Root Scanning (ms): Min: 0.5, Avg: 0.5, Max: 0.8, Diff: 0.4, Sum: 4.4] [Update RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0] [Processed Buffers: Min: 0, Avg: 0.0, Max: 0, Diff: 0, Sum: 0] [Scan RS (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0] [Code Root Scanning (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.0] [Object Copy (ms): Min: 0.1, Avg: 0.3, Max: 0.4, Diff: 0.2, Sum: 2.5] [Termination (ms): Min: 0.0, Avg: 0.0, Max: 0.0, Diff: 0.0, Sum: 0.3] [Termination Attempts: Min: 1, Avg: 6.0, Max: 9, Diff: 8, Sum: 48] [GC Worker Other (ms): Min: 0.0, Avg: 0.0, Max: 0.1, Diff: 0.1, Sum: 0.2] [GC Worker Total (ms): Min: 0.8, Avg: 0.9, Max: 1.0, Diff: 0.3, Sum: 7.4] [GC Worker End (ms): Min: 106.0, Avg: 106.1, Max: 106.1, Diff: 0.0] [Code Root Fixup: 0.0 ms] [Code Root Purge: 0.0 ms] [Clear CT: 0.1 ms] [Other: 0.3 ms] [Choose CSet: 0.0 ms] [Ref Proc: 0.1 ms] [Ref Enq: 0.0 ms] [Redirty Cards: 0.1 ms] [Humongous Register: 0.0 ms] [Humongous Reclaim: 0.0 ms] [Free CSet: 0.0 ms] [Eden: 2048.0K(3072.0K)-\u0026gt;0.0B(1024.0K) Survivors: 0.0B-\u0026gt;1024.0K Heap: 2877.6K(6144.0K)-\u0026gt;1955.9K(6144.0K)] [Times: user=0.00 sys=0.00, real=0.00 secs] [GC concurrent-root-region-scan-start] [GC concurrent-root-region-scan-end, 0.0005373 secs] [GC concurrent-mark-start] [GC concurrent-mark-end, 0.0000714 secs] [GC remark [Finalize Marking, 0.0001034 secs] [GC ref-proc, 0.0000654 secs] [Unloading, 0.0005193 secs], 0.0007843 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] [GC cleanup 11M-\u0026gt;11M(17M), 0.0003613 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 分配了10M空间给数组 Xmx=20.0M free mem=5.059120178222656M total mem=17.0M [Full GC (System.gc()) 11M-\u0026gt;654K(6144K), 0.0031959 secs] [Eden: 1024.0K(1024.0K)-\u0026gt;0.0B(2048.0K) Survivors: 1024.0K-\u0026gt;0.0B Heap: 11.9M(17.0M)-\u0026gt;654.4K(6144.0K)], [Metaspace: 3152K-\u0026gt;3152K(1056768K)] [Times: user=0.00 sys=0.00, real=0.00 secs] 进行了gc Xmx=20.0M free mem=5.2661590576171875M total mem=6.0M Heap garbage-first heap total 6144K, used 654K [0x00000000fec00000, 0x00000000fed00030, 0x0000000100000000) region size 1024K, 1 young (1024K), 0 survivors (0K) Metaspace used 3243K, capacity 4500K, committed 4864K, reserved 1056768K class space used 351K, capacity 388K, committed 512K, reserved 1048576K */ 调整新生代和老年代的比值 # -XX:NewRatio \u0026mdash; 新生代（eden+2*Survivor）和老年代（不包含永久区）的比值\n例如：-XX:NewRatio=4，表示新生代:老年代=1:4，即新生代占整个堆的1/5。在Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。 注：Xmn为直接设置大小，如-Xmn2G\n调整Survivor区和Eden区的比值 # -XX:SurvivorRatio（幸存代）\u0026mdash; 设置两个Survivor区和eden的比值\n例如：8，表示两个Survivor:eden=2:8，即一个Survivor占年轻代的1/10\n设置年轻代和老年代的大小 # -XX:NewSize \u0026mdash; 设置年轻代大小\n-XX:MaxNewSize \u0026mdash; 设置年轻代最大值\n可以通过设置不同参数来测试不同的情况，反正最优解当然就是官方的Eden和Survivor的占比为8:1:1，然后在刚刚介绍这些参数的时候都已经附带了一些说明，感兴趣的也可以看看。反正最大堆内存和最小堆内存如果数值不同会导致多次的gc，需要注意。\n我的理解是会经常调整totalMemory而导致多次gc，避免临界条件下的垃圾回收和内存申请和分配\n注： 最大堆内存和最小堆内存设置成一样，为的是能够在java垃圾回收机制清理完堆区后，不需要重新分隔计算堆区的大小而浪费资源（向系统请求/释放内存资源）\n小总结 # 根据实际事情调整新生代和幸存代的大小，官方推荐新生代占java堆的3/8，幸存代占新生代的1/10\nJava堆：新生代 (3/8)，老年代\n新生代：SO (1/10) ，S1 ，Eden\n在OOM时，记得Dump出堆，确保可以排查现场问题，通过下面命令可以输出一个.dump 文件，该文件用VisualVM或Java自带的JavaVisualVM 工具\n-Xmx20m -Xms5m -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=你要输出的日志路径 一般我们也可以通过编写脚本的方式来让OOM出现时给我们报个信，可以通过发送邮件或者重启程序等来解决\n永久区的设置 # -XX:PermSize -XX:MaxPermSize，应该说的是永久代\n初始空间（默认为物理内存的1/64）和最大空间（默认为物理内存的1/4）。也就是说，jvm启动时，永久区一开始就占用了PermSize大小的空间，如果空间还不够，可以继续扩展，但是不能超过MaxPermSize，否则会OOM。\n如果堆空间没有用完也抛出了OOM，有可能是永久区导致的。堆空间实际占用非常少，但是永久区溢出 一样抛出OOM\nJVM的栈参数调优 # 调整每个线程栈空间的大小 # 可以通过**-Xss**：调整每个线程栈空间的大小\nJDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。在相同物理内存下,减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右\n设置线程栈的大小 # -XXThreadStackSize： #设置线程栈的大小(0 means use default stack size) 补充：\n-Xss是OpenJDK和Oracle JDK的-XX:ThreadStackSize的别名。\n尽管他们对参数的解析不同： -Xss可以接受带K，M或G后缀的数字； -XX:ThreadStackSize=需要一个整数(无后缀)-堆栈大小(以千字节为单位)\n(可以直接跳过了)JVM其他参数介绍 # 形形色色的参数很多，就不会说把所有都扯个遍了，因为大家其实也不会说一定要去深究到底。\n设置内存页的大小 # -XXThreadStackSize： 设置内存页的大小，不可设置过大，会影响Perm的大小 设置原始类型的快速优化 # -XX:+UseFastAccessorMethods： 设置原始类型的快速优化 设置关闭手动GC # -XX:+DisableExplicitGC： 设置关闭System.gc()(这个参数需要严格的测试) 设置垃圾最大年龄 # -XX:MaxTenuringThreshold 设置垃圾最大年龄。如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率。如果将此值设置为一个较大值, 则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活时间, 增加在年轻代即被回收的概率。该参数只有在串行GC时才有效. 加快编译速度 # -XX:+AggressiveOpts 加快编译速度\n改善锁机制性能 # -XX:+UseBiasedLocking 禁用垃圾回收 # -Xnoclassgc 设置堆空间存活时间 # -XX:SoftRefLRUPolicyMSPerMB 设置每兆堆空闲空间中SoftReference的存活时间，默认值是1s。 设置对象直接分配在老年代 # -XX:PretenureSizeThreshold 设置对象超过多大时直接在老年代分配，默认值是0。 设置TLAB占eden区的比例 # -XX:TLABWasteTargetPercent 设置TLAB占eden区的百分比，默认值是1% 。 设置是否优先YGC # -XX:+CollectGen0First 设置FullGC时是否先YGC，默认值是false。 finally # 附录：\n真的扯了很久这东西，参考了多方的资料，有极客时间的《深入拆解虚拟机》和《Java核心技术面试精讲》，也有百度，也有自己在学习的一些线上课程的总结。希望对你有所帮助，谢谢。\n"},{"id":134,"href":"/zh/docs/technology/Review/java_guide/java/JVM/ly0401lymemory-area/","title":"memory-area","section":"Java基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n如果没有特殊说明，针对的都是HotSpot虚拟机\n前言 # 对于Java程序员，虚拟机自动管理机制，不需要像C/C++程序员为每一个new 操作去写对应的delete/free 操作，不容易出现内存泄漏 和 内存溢出问题 但由于内存控制权交给Java虚拟机，一旦出现内存泄漏和溢出方面问题，如果不了解虚拟机是怎么样使用内存，那么很难排查任务 运行时数据区域 # Java虚拟机在执行Java程序的过程中，会把它管理的内存，划分成若干个不同的数据区域\nJDK1.8之前：\n线程共享 堆，方法区【永久代】(包括运行时常量池) 线程私有 虚拟机栈、本地方法栈、程序计数器 本地内存(包括直接内存) JDK1.8之后：\n1.8之后整个永久代改名叫\u0026quot;元空间\u0026quot;，且移到了本地内存中\n规范（概括）：\n线程私有：程序计数器，虚拟机栈，本地方法栈\n线程共享：堆，方法区，直接内存（非运行时数据区的一部分）\nJava虚拟机规范对于运行时数据区域的规定是相当宽松的，以堆为例：\n堆可以是连续，也可以不连续 大小可以固定，也可以运行时按需扩展 虚拟机实现者可以使用任何垃圾回收算法管理堆，设置不进行垃圾收集 程序计数器 # 是一块较小内存空间，看作是当前线程所执行的字节码的行号指示器\njava程序流程\n字节码解释器，工作时通过改变这个计数器的值来选取下一条需要执行的字节码指令\n分支、循环、跳转、异常处理、线程恢复等功能都需要依赖这个计数器\n而且，为了线程切换后恢复到正确执行位置，每条线程需要一个独立程序计数器，各线程计数器互不影响，独立存储，我们称这类内存区域为**\u0026ldquo;线程私有\u0026rdquo;**的内存\n总结，程序计数器的作用\n字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制 多线程情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切回来的时候能够知道该线程上次运行到哪 程序计数器是唯一一个不会出现OutOfMemoryError的内存区域，它的生命周期随线程创建而创建，线程结束而死亡\nJava虚拟机栈 # Java虚拟机栈，简称\u0026quot;栈\u0026quot;，也是线程私有的，生命周期和线程相同，随线程创建而创建，线程死亡而死亡 除了Native方法调用的是通过本地方法栈实现的，其他所有的Java方法调用都是通过栈来实现的（需要和其他运行时数据区域比如程序计数器配合） 方法调用的数据需要通过栈进行传递，每一次方法调用都会有一个对应的栈帧被压入栈，每一个方法调用结束后，都会有一个栈帧被弹出。 栈由一个个栈帧组成，每个栈帧包括局部变量表、操作数栈、动态链接、方法返回地址。 栈为先进后出，且只支持出栈和入栈 局部变量表：存放编译器可知的各种数据类型(boolean、byte、char、short、int、float、long、double)、对象引用(reference 类型，不同于对象本身，可能是一个指向对象起始地址的引用指针，也可能是一个指向一个代表对象的句柄或其他与此对象相关的位置) 操作数栈 作为方法调用的中转站使用，用于存放方法执行过程中产生的中间计算结果。计算过程中产生的临时变量也放在操作数栈中\n动态链接 主要服务一个方法需要调用其他方法的场景。\n在 Java 源文件被编译成字节码文件时，所有的变量和方法引用都作为符号引用（Symbilic Reference）保存在 Class 文件的常量池里。当一个方法要调用其他方法，需要将常量池中指向方法的符号引用转化为其在内存地址中的直接引用。动态链接的作用就是为了将符号引用转换为调用方法的直接引用。\n如果函数调用陷入无限循环，会导致栈中被压入太多栈帧而占用太多空间，导致栈空间过深。当线程请求栈的深度超过当前Java虚拟机栈的最大深度时，就会抛出StackOverFlowError错误\nJava 方法有两种返回方式，一种是 return 语句正常返回，一种是抛出异常。不管哪种返回方式，都会导致栈帧被弹出。也就是说， 栈帧随着方法调用而创建，随着方法结束而销毁。无论方法正常完成还是异常完成都算作方法结束。\n除了 StackOverFlowError 错误之外，栈还可能会出现OutOfMemoryError错误，这是因为如果栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出**OutOfMemoryError**异常。\n总结，程序运行中栈可能出现的两种错误\nStackOverFlowError：若栈的内存大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度的时候，就抛出 StackOverFlowError 错误。 OutOfMemoryError： 如果栈的内存大小可以动态扩展， 如果虚拟机在动态扩展栈时无法申请到足够的内存空间，则抛出OutOfMemoryError异常。 本地方法栈 # 和虚拟机栈作用相似，区别：虚拟机栈为虚拟机执行Java方法（字节码）服务，本地方法栈则为虚拟机使用到的Native方法服务。HotSpot虚拟机中和Java虚拟机栈合二为一\n同上，本地方法被执行时，本地方法栈会创建一个栈帧，用于存放本地方法的局部变量表、操作数栈、动态链接、出口信息\n方法执行完毕后相应的栈帧也会出栈并释放内存空间，也会出现StackOverFlowError和OutOfMemoryError两种错误\n堆 # Java虚拟机所管理的内存中最大的一块，Java堆是所有线程共享的一块区域，在虚拟机启动时创建\n此内存区域唯一目的是存放对象实例，几乎所有的对象实例及数组，都在这里分配内存\n“几乎”，因为随着JIT编译器的发展与逃逸分析技术逐渐成熟，栈上分配、标量替换导致微妙变化。从JDK1.7开始已经默认逃逸分析，如果某些方法的对象引用没有被返回或者未被外面使用（未逃逸出去），那么对象可以直接在栈上分配内存。\nJava堆是垃圾收集器管理的主要区域，因此也称GC堆（Garbage Collected Heap）\n现在收集器基本都采用分代垃圾收集算法，从垃圾回收的角度，Java堆还细分为：新生代和老年代。再细致：Eden，Survivor，Old等空间。\u0026gt; 目的是更好的回收内存，或更快地分配内存\nJDK7及JDK7之前，堆内存被分为三部分\n新生代内存(Young Generation)，包括Eden区、两个Survivor区S0和S1【8:1:1】 老生代(Old Generation) 【新生代 : 老年代= 1: 2】 永久代(Permanent Generation) JDK8之后PermGen（永久）已被Metaspace（元空间）取代，且元空间使用直接内存\n大部分情况，对象都会首先在 Eden 区域分配，在一次新生代垃圾回收后，如果对象还存活，则会进入 S0 或者 S1，并且对象的年龄还会加 1(Eden 区-\u0026gt;Survivor 区后对象的初始年龄变为 1)，当它的年龄增加到一定程度（默认为 15 岁），就会被晋升到老年代中。\n对象晋升到老年代的年龄阈值，可以通过参数 -XX:MaxTenuringThreshold 来设置。\n修正（参见：issue552open in new window） ：“Hotspot 遍历所有对象时，按照年龄从小到大对其所占用的大小进行累积，当累积的某个年龄大小超过了 survivor 区的一半时，取这个年龄和 MaxTenuringThreshold 中更小的一个值，作为新的晋升年龄阈值”。图解：\n代码如下：\nuint ageTable::compute_tenuring_threshold(size_t survivor_capacity) { //survivor_capacity是survivor空间的大小 size_t desired_survivor_size = (size_t)((((double) survivor_capacity)*TargetSurvivorRatio)/100); size_t total = 0; uint age = 1; while (age \u0026lt; table_size) { total += sizes[age];//sizes数组是每个年龄段对象大小 if (total \u0026gt; desired_survivor_size) break; age++; } uint result = age \u0026lt; MaxTenuringThreshold ? age : MaxTenuringThreshold; ... } 堆里最容易出现OutOfMemoryError错误，出现这个错误之后的表现形式：\njava.lang.OutOfMemoryError: GC Overhead Limit Exceeded ： 当 JVM 花太多时间执行垃圾回收并且只能回收很少的堆空间时，就会发生此错误。\njava.lang.OutOfMemoryError: Java heap space :假如在创建新的对象时, 堆内存中的空间不足以存放新创建的对象, 就会引发此错误。\n(和配置的最大堆内存有关，且受制于物理内存大小。最大堆内存可通过-Xmx参数配置，若没有特别配置，将会使用默认值，详见：Default Java 8 max heap sizeopen in new window)\n\u0026hellip;\n方法区 # 方法区属于JVM运行时数据区域的一块逻辑区域，是各线程共享的内存区域\n“逻辑”，《Java虚拟机规范》规定了有方法区这么个概念和它的作用，方法区如何实现是虚拟机的事。即，不同虚拟机实现上，方法区的实现是不同的\n当虚拟机要使用一个类时，它需要读取并解析Class文件获取相关信息，再将信息存入方法区。方法区会存储已被虚拟机加载的类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据\n方法区和永久代以及元空间有什么关系呢？\n方法区和永久代以及元空间的关系很像Java中接口和类的关系，类实现了接口，这里的类就可以看作是永久代和元空间，接口则看作是方法区 永久代及元空间，是HotSpot虚拟机对虚拟机规范中方法区的两种实现方式 永久代是JDK1.8之前的方法区实现，元空间是JDK1.8及之后方法区的实现 为什么将永久代（PermGen）替换成元空间（MetaSpace）呢\n下图来自《深入理解Java虚拟机》第3版\n整个永久代有一个JVM本身设置的固定大小上限（也就是参数指定），无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制。虽然元空间仍旧可能溢出，但比原来出现的机率会更小\n元空间溢出将得到错误： java.lang.OutOfMemoryError: MetaSpace\n-XX: MaxMetaspaceSize设置最大元空间大小，默认为unlimited，即只受系统内存限制 -XX: MetaspaceSize调整标志定义元空间的初始大小，如果未指定此标志，则Metaspace将根据运行时应用程序需求，动态地重新调整大小。 元空间里存放的是类的元数据，这样加载多少类的元数据就不由MaxPermSize控制了，而由系统的实际可用空间控制，这样加载的类就更多了\n在 JDK8，合并 HotSpot 和 JRockit 的代码时, JRockit 从来没有一个叫永久代的东西, 合并之后就没有必要额外的设置这么一个永久代的地方了\n方法区常用参数有哪些 JDK1.8之前永久代还没有被彻底移除时通过下面参数调节方法区大小\n-XX:PermSize=N//方法区 (永久代) 初始大小 -XX:MaxPermSize=N//方法区 (永久代) 最大大小,超过这个值将会抛出 OutOfMemoryError 异常:java.lang.OutOfMemoryError: PermGen 相对而言，垃圾收集行为在这个区域是比较少出现的，但**并非数据进入方法区后就“永久存在”**了。\nJDK1.7方法区(HotSpot的永久代)被移除一部分，JDK1.8时方法区被彻底移除，取而代之的是元空间，元空间使用直接内存，下面是常用参数\n-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小） -XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小 与永久代不同，如果不指定大小，随着更多类的创建，虚拟机会耗尽所有可用的系统内存。\n运行时常量池 # Class文件中除了有类的版本、字段、方法、接口等描述信息外，还有用于存放编译器期生成的各种字面量（Literal）和符号引用（Symbolic Reference）的常量池表（注意，这里的常量池表，说的是刚刚编译后的那个class文件字节码代表的含义）\n字面量是源代码中的固定值表示法，即通过字面我们就知道其值的含义。字面量包括整数、浮点数和字符串字面量；符号引用包括类符号引用、字段符号引用、方法符号引用和接口方法符号引用。\n常量池表会在类加载后存放到方法区的运行时常量池中\n运行时常量池的功能类似于传统编程语言的符号表(但是包含了比典型符号表更广泛的数据)\n运行时常量池是方法区的一部分，所以受到方法区内存的限制，当常量池无法再申请到内存时会抛出OutOfMemoryError的错误\n字符串常量池 # 字符串常量池是JVM为了提升性能和减少内存消耗针对字符串(String类)专门开辟的一块区域，主要目的是为了避免字符串得重复创建\n// 在堆中创建字符串对象”ab“ // 将字符串对象”ab“的引用保存在字符串常量池中 String aa = \u0026#34;ab\u0026#34;; // 直接返回字符串常量池中字符串对象”ab“的引用 String bb = \u0026#34;ab\u0026#34;; System.out.println(aa==bb);// true HotSpot 虚拟机中字符串常量池的实现是 src/hotspot/share/classfile/stringTable.cpp ,StringTable 本质上就是一个HashSet\u0026lt;String\u0026gt; ,容量为 StringTableSize（可以通过 -XX:StringTableSize 参数来设置）。\nStringTable 中保存的是字符串对象的引用，字符串对象的引用指向堆中的字符串对象。\nJDK1.7之前， (字符串常量池、静态变量)存放在永久代。JDK1.7字符串常量池和静态变量从永久代移动到了Java堆中 JDK1.7为什么要将字符串常量池移动到堆中\n因为永久代（方法区实现）的GC回收效率太低，只有在整堆收集（Full GC）的时候才会被执行GC。Java程序中通常有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够高效及时地回收字符串内存。\nJVM常量池中存储的是对象还是引用\n如果您说的确实是runtime constant pool（而不是interned string pool / StringTable之类的其他东西）的话，其中的引用类型常量（例如CONSTANT_String、CONSTANT_Class、CONSTANT_MethodHandle、CONSTANT_MethodType之类）都存的是引用，实际的对象还是存在Java heap上的。【运行时常量池】\n运行时常量池、方法区、字符串常量池这些都是不随虚拟机实现而改变的逻辑概念，是公共且抽象的，Metaspace、Heap 是与具体某种虚拟机实现相关的物理概念，是私有且具体的。\n总结 # 直接内存 # 直接内存并不是虚拟机运行时数据区的一部分，也不是虚拟机规范中定义的内存区域，但是这部分内存也被频繁使用，也可能导致OutOfMemoryError错误出现 JDK1.4中新加入的NIO(New Input/Output)类，引入一种基于通道（Channel）与缓冲区（Buffer）的I/O方式，它可以直接使用Native函数库直接分配堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作。这样就能在一些场景中显著提高性能，因为避免了在Java堆和Native堆之间来回复制数据 本机直接内存的分配不会受到Java堆的限制，但是，既然是内存就会受到本机总内存大小以及处理器寻址空间的限制 HotSpot虚拟机对象探秘 # 了解一下HotSport虚拟机在Java堆中对象分配、布局和访问的全过程\n对象的创建 # 默认：\n类加载检查 虚拟机遇到一条new指令时，首先将去检查这个指令的参数（也就是后面说的符号引用），看是否能在常量池中定位到这个类的符号引用，并且检查这个符号引用代表的类是否已被加载过、解析和初始化过。如果没有，那必须先执行相应的类加载过程\n分配内存 类加载检查通过后，接下来虚拟机将为新生对象分配内存。对象所需内存大小在类加载后便可确定，为对象分配空间的任务，等同于把一块确定大小的内存从Java堆中划分出来**。 分配方式有**”指针碰撞“和”空闲列表“两种，选择哪种分配方式由Java堆是否规整决定**，而Java堆是否规整又由所采用的垃圾收集器是否带有压缩整理功能决定 内存分配的两种方式：\n指针碰撞\n适用场合：**堆内存规整（即没有内存碎片）**的情况下 原理：用过的内存全部整合到一边，没有用过的内存放在另一边，中间有一个分界指针只需要向着没用过的内存方向，将该指针移动对象内存的大小的位置即可 使用该分配方式的GC收集器：Serial，PartNew 空闲列表\n适合场合：堆内存不规整的情况下 原理：虚拟机会维护一个列表，该列表中会记录哪些内存块是可用的，在分配的时候，找一块足够大的内存块来划分给对象实例，最后更新列表记录 使用该分配方式的GC收集器：CMS 选择以上两种方式中的哪一种，取决于 Java 堆内存是否规整。而 Java 堆内存是否规整，取决于 GC 收集器的算法是**\u0026ldquo;标记-清除\u0026rdquo;，还是\u0026ldquo;标记-整理\u0026rdquo;（也称作\u0026ldquo;标记-压缩\u0026rdquo;**），值得注意的是，复制算法内存也是规整的\n内存分配并发问题：\n创建对象时的重要问题\u0026mdash;线程安全，因为在实际开发过程中，创建对象是很频繁的事，作为虚拟机来说，必须要保证线程安全的，虚拟机采用两种方式保证线程安全：\nCAS+失败重试:\nCAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止\n虚拟机采用CAS+失败重试的方式保证更新操作的原子性\nTLAB：为每一个线程 预先 在Eden区分配一块内存，JVM在给线程中的对象分配内存时，首先在TLAB（该内存区域）分配，当对象大于TLAB中的剩余内存或TLAB的内存已用尽时，再采用上述的CAS进行内存分配\n在预留这个操作发生的时候，需要进行加锁或者采用CAS等操作进行保护，避免多个线程预留同一个区域\n初始化零值 内存分配完成后，虚拟机需要将分配到的内存空间都初始化为零值（不包括对象头），这一步操作保证了对象的实例字段在 Java 代码中可以不赋初始值就直接使用，程序能访问到这些字段的数据类型所对应的零值。\n设置对象头\n初始化零值完成之后，虚拟机要对对象进行必要的设置，例如这个对象是哪个类的实例、如何才能找到类的元数据信息、对象的哈希码、对象的 GC 分代年龄等信息。 这些信息存放在对象头中。 另外，根据虚拟机当前运行状态的不同，如是否启用偏向锁等，对象头会有不同的设置方式。\n执行init方法\n在上面工作都完成之后，从虚拟机的视角来看，一个新的对象已经产生了，但从 Java 程序的视角来看，对象创建才刚开始，\u0026lt;init\u0026gt; 方法还没有执行，所有的字段都还为零。所以一般来说，执行 new 指令之后会接着执行 \u0026lt;init\u0026gt; 方法，把对象按照程序员的意愿进行初始化，这样一个真正可用的对象才算完全产生出来。\n附： Java 在编译之后会在字节码文件中生成 init 方法，称之为实例构造器，该实例构造器会将语句块，变量初始化，调用父类的构造器等操作收敛到 init 方法中，收敛顺序为： 变量---\u0026gt; 语句块 ---\u0026gt; 构造函数\n父类变量初始化 父类语句块 父类构造函数 子类变量初始化 子类语句块 子类构造函数 收敛到 init 方法的意思是：将这些操作放入到 init 中去执行。 转自： https://juejin.cn/post/6844903957836333063\n对象的内存布局 # Hotspot虚拟机中，对象在内存中的布局分为3块区域：对象头、实例数据和对齐填充 对象头又包括两部分信息，第一部分用于存储对象自身的运行时数据（哈希码、GC分代年龄、锁状态标志等等），即markword；第二部分是类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例数据部分是对象真正存储的有效信息，也是在程序中所定义的各种类型的字段内容 对齐填充部分不是必然存在的，没特别含义，只起占位作用。因为Hotspot虚拟机的自动内存管理系统要求对象起始地址必须是8字节的整数倍，即对象的大小必须是8字节的整数倍。而对象头部分正好是8字节的倍数（1倍或2倍），因此当对象实例数据部分没有对齐时，就需要通过对齐填充来补全 对象的访问定位 # 建立对象就是为了使用对象，我们的 Java 程序通过栈上的 reference 数据来操作堆上的具体对象。对象的访问方式由虚拟机实现而定，目前主流的访问方式有：使用句柄、直接指针。\n句柄 如果使用句柄的话，那么 Java 堆中将会划分出一块内存来作为句柄池，reference 中存储的就是对象的句柄地址，而句柄中包含了对象实例数据与对象类型数据各自的具体地址信息。\n【也就是多了一层】\n直接指针 如果使用直接指针访问，reference 中存储的直接就是对象的地址。 这两种对象访问方式各有优势。使用句柄来访问的最大好处是 reference 中存储的是稳定的句柄地址，在对象被移动时只会改变句柄中的实例数据指针，而 reference 本身不需要修改。使用直接指针访问方式最大的好处就是速度快，它节省了一次指针定位的时间开销。\nHotSpot 虚拟机主要使用的就是这种方式**(直接指针**)来进行对象访问。\n"},{"id":135,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0311lycompletablefuture-intro/","title":"completablefuture-intro","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nJava8被引入的一个非常有用的用于异步编程的类【没看】\n简单介绍 # CompletableFuture同时实现了Future和CompletionStage接口\npublic class CompletableFuture\u0026lt;T\u0026gt; implements Future\u0026lt;T\u0026gt;, CompletionStage\u0026lt;T\u0026gt; { } CompletableFuture 除了提供了更为好用和强大的 Future 特性之外，还提供了函数式编程的能力。\nFuture接口有5个方法：\nboolean cancel(boolean mayInterruptIfRunning) ：尝试取消执行任务。 boolean isCancelled() ：判断任务是否被取消。 boolean isDone() ： 判断任务是否已经被执行完成。 get() ：等待任务执行完成并获取运算结果。 get(long timeout, TimeUnit unit) ：多了一个超时时间。 CompletionStage\u0026lt;T\u0026gt; 接口中的方法比较多，CompoletableFuture的函数式能力就是这个接口赋予的，大量使用Java8引入的函数式编程\n常见操作 # 创建CompletableFuture # 两种方法：new关键字或 CompletableFuture自带的静态工厂方法 runAysnc()或supplyAsync()\n通过new关键字 这个方式，可以看作是将CompletableFuture当作Future来使用，如下：\n我们通过创建了一个结果值类型为 RpcResponse\u0026lt;Object\u0026gt; 的 CompletableFuture，你可以把 resultFuture 看作是异步运算结果的载体\nCompletableFuture\u0026lt;RpcResponse\u0026lt;Object\u0026gt;\u0026gt; resultFuture = new CompletableFuture\u0026lt;\u0026gt;(); 如果后面某个时刻，得到了最终结果，可以调用complete()方法传入结果，表示resultFuture已经被完成：\n// complete() 方法只能调用一次，后续调用将被忽略。 resultFuture.complete(rpcResponse); 通过isDone()检查是否完成：\npublic boolean isDone() { return result != null; } 获取异步结果，使用get() ，调用get()方法的线程会阻塞 直到CompletableFuture完成运算： rpcResponse = completableFuture.get();\npublic class CompletableFutureTest { public static void main(String[] args) throws ExecutionException, InterruptedException { /*CompletableFuture\u0026lt;Object\u0026gt; resultFuture=new CompletableFuture\u0026lt;\u0026gt;(); resultFuture.complete(\u0026#34;hello world\u0026#34;); System.out.println(resultFuture.get());*/ CompletableFuture\u0026lt;String\u0026gt; stringCompletableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;hello,world!\u0026#34;; }); System.out.println(\u0026#34;被阻塞啦----\u0026#34;); String s = stringCompletableFuture.get(); System.out.println(\u0026#34;结果---\u0026#34;+s); } } 如果已经知道结果：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;); assertEquals(\u0026#34;hello!\u0026#34;, future.get()); //completedFuture() 方法底层调用的是带参数的 new 方法，只不过，这个方法不对外暴露。 public static CompletableFuture completedFuture(U value) { return new CompletableFuture((value == null) ? NIL : value); } 基于CompletableFuture自带的静态工厂方法：runAsync()、supplyAsync() Supplier 供应商; 供货商; 供应者; 供货方; 这两个方法可以帮助我们封装计算逻辑\nstatic CompletableFuture supplyAsync(Supplier supplier); // 使用自定义线程池(推荐) static CompletableFuture supplyAsync(Supplier supplier, Executor executor); static CompletableFuture\u0026lt;Void\u0026gt; runAsync(Runnable runnable); // 使用自定义线程池(推荐) static CompletableFuture\u0026lt;Void\u0026gt; runAsync(Runnable runnable, Executor executor); //简单使用 public class CompletableFutureTest { public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { //3s后返回结果 try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;abc\u0026#34;; }); //这里会被阻塞 String s = completableFuture.get(); System.out.println(s); } } //例子2 import java.util.concurrent.CompletableFuture; import java.util.concurrent.CountDownLatch; import java.util.concurrent.ExecutionException; import java.util.concurrent.TimeUnit; public class CompletableFutureTest { public static void main(String[] args) throws ExecutionException, InterruptedException { CountDownLatch countDownLatch=new CountDownLatch(2); //相当于使用了一个线程池，开启线程，提交了任务 CompletableFuture\u0026lt;Void\u0026gt; a = CompletableFuture.runAsync(() -\u0026gt; { System.out.println(\u0026#34;a\u0026#34;); //执行了3s的任务 try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } countDownLatch.countDown(); }); CompletableFuture\u0026lt;Void\u0026gt; b = CompletableFuture.runAsync(() -\u0026gt; { System.out.println(\u0026#34;b\u0026#34;); //执行了3s的任务 try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } countDownLatch.countDown(); }); countDownLatch.await(); System.out.println(\u0026#34;执行完毕\u0026#34;);//3s后会执行 } } 备注，自定义线程池使用：\nThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, //5 MAX_POOL_SIZE, //10 KEEP_ALIVE_TIME, //1L TimeUnit.SECONDS, //单位 new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY),//100 new ThreadPoolExecutor.CallerRunsPolicy()); //主线程中运行 runAsync() 方法接受的参数是 Runnable ，这是一个函数式接口，不允许返回值。当你需要异步操作且不关心返回结果的时候可以使用 runAsync() 方法。\n@FunctionalInterface public interface Runnable { public abstract void run(); } supplyAsync() 方法接受的参数是 Supplier ，这也是一个函数式接口，U 是返回结果值的类型。\n@FunctionalInterface public interface Supplier\u0026lt;T\u0026gt; { /** * Gets a result. * * @return a result */ T get(); } 当需要异步操作且关心返回的结果时，可以使用supplyAsync()方法\n```java CompletableFuture\u0026lt;Void\u0026gt; future = CompletableFuture.runAsync(() -\u0026gt; System.out.println(\u0026quot;hello!\u0026quot;)); future.get();// 输出 \u0026quot;hello!\u0026quot; **注意，不是get()返回的** CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026quot;hello!\u0026quot;); assertEquals(\u0026quot;hello!\u0026quot;, future2.get()); ``` 处理异步结算的结果 # 可以对异步计算的结果，进行进一步的处理，常用的方法有：\nthenApply() 接收结果 产生结果 ``thenAccept()` 接受结果不产生结果\nthenRun 不接受结果不产生结果 whenComplete() 结束时处理结果\n例子：\npublic class CompletableFutureTest { public static void main(String[] args) throws ExecutionException, InterruptedException { CompletableFuture\u0026lt;String\u0026gt; stringCompletableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;hello,world!\u0026#34;; }); System.out.println(\u0026#34;被阻塞啦----\u0026#34;); stringCompletableFuture .whenComplete((s,e)-\u0026gt;{ System.out.println(\u0026#34;complete1----\u0026#34;+s); }) .whenComplete((s,e)-\u0026gt;{ System.out.println(\u0026#34;complete2----\u0026#34;+s); }) .thenAccept(s-\u0026gt;{ System.out.println(\u0026#34;打印结果\u0026#34;+s); }) .thenRun(()-\u0026gt;{ System.out.println(\u0026#34;阻塞结束啦\u0026#34;); }); while (true){ } } } /*------------- 2022-12-07 10:16:44 上午 [Thread: main] INFO:被阻塞啦---- 2022-12-07 10:16:47 上午 [Thread: ForkJoinPool.commonPool-worker-1] INFO:complete1----hello,world! 2022-12-07 10:16:47 上午 [Thread: ForkJoinPool.commonPool-worker-1] INFO:complete2----hello,world! 2022-12-07 10:16:47 上午 [Thread: ForkJoinPool.commonPool-worker-1] INFO:打印结果hello,world! 2022-12-07 10:16:47 上午 [Thread: ForkJoinPool.commonPool-worker-1] INFO:阻塞结束啦 */ thenApply()方法接受Function实例，用它来处理结果 // 沿用上一个任务的线程池 public CompletableFuture thenApply( Function\u0026lt;? super T,? extends U\u0026gt; fn) { return uniApplyStage(null, fn); } //使用默认的 ForkJoinPool 线程池（不推荐） public CompletableFuture thenApplyAsync( Function\u0026lt;? super T,? extends U\u0026gt; fn) { return uniApplyStage(defaultExecutor(), fn); } // 使用自定义线程池(推荐) public CompletableFuture thenApplyAsync( Function\u0026lt;? super T,? extends U\u0026gt; fn, Executor executor) { return uniApplyStage(screenExecutor(executor), fn); } 使用示例：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;); assertEquals(\u0026#34;hello!world!\u0026#34;, future.get()); // 这次调用将被忽略。 //**我猜是因为只能get()一次** future.thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;); assertEquals(\u0026#34;hello!world!\u0026#34;, future.get()); 流式调用：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;).thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;); assertEquals(\u0026#34;hello!world!nice!\u0026#34;, future.get()); 如果不需要从回调函数中返回结果，可以使用thenAccept()或者thenRun() ，两个方法区别在于thenRun()不能访问异步计算的结果(因为thenAccept方法的参数为 Consumer\u0026lt;? super T\u0026gt; ) public CompletableFuture\u0026lt;Void\u0026gt; thenAccept(Consumer\u0026lt;? super T\u0026gt; action) { return uniAcceptStage(null, action); } public CompletableFuture\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action) { return uniAcceptStage(defaultExecutor(), action); } public CompletableFuture\u0026lt;Void\u0026gt; thenAcceptAsync(Consumer\u0026lt;? super T\u0026gt; action, Executor executor) { return uniAcceptStage(screenExecutor(executor), action); } 顾名思义，Consumer 属于消费型接口，它可以接收 1 个输入对象然后进行“消费”。\n@FunctionalInterface public interface Consumer\u0026lt;T\u0026gt; { void accept(T t); default Consumer\u0026lt;T\u0026gt; andThen(Consumer\u0026lt;? super T\u0026gt; after) { Objects.requireNonNull(after); return (T t) -\u0026gt; { accept(t); after.accept(t); }; } } thenRun() 的方法是的参数是 Runnable\npublic CompletableFuture\u0026lt;Void\u0026gt; thenRun(Runnable action) { return uniRunStage(null, action); } public CompletableFuture\u0026lt;Void\u0026gt; thenRunAsync(Runnable action) { return uniRunStage(defaultExecutor(), action); } public CompletableFuture\u0026lt;Void\u0026gt; thenRunAsync(Runnable action, Executor executor) { return uniRunStage(screenExecutor(executor), action); } 使用如下：\nCompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;).thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;).thenAccept(System.out::println);//hello!world!nice! //可以接收参数 CompletableFuture.completedFuture(\u0026#34;hello!\u0026#34;) .thenApply(s -\u0026gt; s + \u0026#34;world!\u0026#34;).thenApply(s -\u0026gt; s + \u0026#34;nice!\u0026#34;).thenRun(() -\u0026gt; System.out.println(\u0026#34;hello!\u0026#34;));//hello! whenComplete()的方法参数是BiConsumer\u0026lt;? super T , ? super Throwable \u0026gt;\npublic CompletableFuture\u0026lt;T\u0026gt; whenComplete( BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action) { return uniWhenCompleteStage(null, action); } public CompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync( BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action) { return uniWhenCompleteStage(defaultExecutor(), action); } // 使用自定义线程池(推荐) public CompletableFuture\u0026lt;T\u0026gt; whenCompleteAsync( BiConsumer\u0026lt;? super T, ? super Throwable\u0026gt; action, Executor executor) { return uniWhenCompleteStage(screenExecutor(executor), action); } 相比Consumer，BiConsumer可以接收2个输入对象然后进行\u0026quot;消费\u0026quot;\n@FunctionalInterface public interface BiConsumer\u0026lt;T, U\u0026gt; { void accept(T t, U u); default BiConsumer\u0026lt;T, U\u0026gt; andThen(BiConsumer\u0026lt;? super T, ? super U\u0026gt; after) { Objects.requireNonNull(after); return (l, r) -\u0026gt; { accept(l, r); after.accept(l, r); }; } } 使用：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;hello!\u0026#34;) .whenComplete((res, ex) -\u0026gt; { // res 代表返回的结果 // ex 的类型为 Throwable ，代表抛出的异常 System.out.println(res); // 这里没有抛出异常所有为 null assertNull(ex); }); assertEquals(\u0026#34;hello!\u0026#34;, future.get()); 其他区别暂时不知道\n异常处理 # 使用handle（） 方法来处理任务执行过程中可能出现的抛出异常的情况\npublic CompletableFuture handle( BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn) { return uniHandleStage(null, fn); } public CompletableFuture handleAsync( BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn) { return uniHandleStage(defaultExecutor(), fn); } public CompletableFuture handleAsync( BiFunction\u0026lt;? super T, Throwable, ? extends U\u0026gt; fn, Executor executor) { return uniHandleStage(screenExecutor(executor), fn); } 代码：\npublic static void test() throws ExecutionException, InterruptedException { CompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { if (true) { throw new RuntimeException(\u0026#34;Computation error!\u0026#34;); } return \u0026#34;hello!\u0026#34;; }).handle((res, ex) -\u0026gt; { // res 代表返回的结果 // ex 的类型为 Throwable ，代表抛出的异常 return res != null ? res : ex.toString()+\u0026#34;world!\u0026#34;; }); String s = future.get(); log.info(s); } /** 2022-12-07 11:14:44 上午 [Thread: main] INFO:java.util.concurrent.CompletionException: java.lang.RuntimeException: Computation error!world! */ 通过exceptionally处理异常\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; { if (true) { throw new RuntimeException(\u0026#34;Computation error!\u0026#34;); } return \u0026#34;hello!\u0026#34;; }).exceptionally(ex -\u0026gt; { System.out.println(ex.toString());// CompletionException return \u0026#34;world!\u0026#34;; }); assertEquals(\u0026#34;world!\u0026#34;, future.get()); 让异步的结果直接就抛异常\nCompletableFuture\u0026lt;String\u0026gt; completableFuture = new CompletableFuture\u0026lt;\u0026gt;(); // ... completableFuture.completeExceptionally( new RuntimeException(\u0026#34;Calculation failed!\u0026#34;)); // ... completableFuture.get(); // ExecutionException 组合CompletableFuture # 使用thenCompose() 按顺序连接两个CompletableFuture对象\npublic CompletableFuture thenCompose( Function\u0026lt;? super T, ? extends CompletionStage\u0026gt; fn) { return uniComposeStage(null, fn); } public CompletableFuture thenComposeAsync( Function\u0026lt;? super T, ? extends CompletionStage\u0026gt; fn) { return uniComposeStage(defaultExecutor(), fn); } public CompletableFuture thenComposeAsync( Function\u0026lt;? super T, ? extends CompletionStage\u0026gt; fn, Executor executor) { return uniComposeStage(screenExecutor(executor), fn); } 使用示例：\nCompletableFuture\u0026lt;String\u0026gt; future = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;hello!\u0026#34;) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; s + \u0026#34;world!\u0026#34;)); assertEquals(\u0026#34;hello!world!\u0026#34;, future.get()); 在实际开发中，这个方法还是非常有用的。比如说，我们先要获取用户信息然后再用用户信息去做其他事情。\n和thenCompose()方法类似的还有thenCombine()方法，thenCombine()同样可以组合两个CompletableFuture对象\nCompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;hello!\u0026#34;) .thenCombine(CompletableFuture.supplyAsync( () -\u0026gt; \u0026#34;world!\u0026#34;), (s1, s2) -\u0026gt; s1 + s2) .thenCompose(s -\u0026gt; CompletableFuture.supplyAsync(() -\u0026gt; s + \u0026#34;nice!\u0026#34;)); assertEquals(\u0026#34;hello!world!nice!\u0026#34;, completableFuture.get()); ★★ thenCompose() 和 thenCombine()有什么区别呢\nthenCompose() 可以两个 CompletableFuture 对象，并将前一个任务的返回结果作为下一个任务的参数，它们之间存在着先后顺序。 thenCombine() 会在两个任务都执行完成后，把两个任务的结果合并。两个任务是并行执行的，它们之间并没有先后依赖顺序。 /* 结果是有顺序的，但是执行的过程是无序的 */ CompletableFuture\u0026lt;String\u0026gt; completableFuture = CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;执行了第1个\u0026#34;); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;第1个执行结束啦\u0026#34;); return \u0026#34;hello!\u0026#34;; }) .thenCombine(CompletableFuture.supplyAsync( () -\u0026gt; { System.out.println(\u0026#34;执行了第2个\u0026#34;); System.out.println(\u0026#34;第2个执行结束啦\u0026#34;); return \u0026#34;world!\u0026#34;; }), (s1, s2) -\u0026gt; s1 + s2); System.out.println(completableFuture.get()); /* 执行了第1个 执行了第2个 第2个执行结束啦 第1个执行结束啦 hello!world! */ 并行运行多个CompletableFuture # 通过CompletableFuture的allOf()这个静态方法并行运行多个CompletableFuture\n实际项目中，我们经常需要并行运行多个互不相关的任务，这些任务没有依赖关系\n比如读取处理6个文件，没有顺序依赖关系 但我们需要返回给用户的时候将这几个文件的处理结果统计整理，示例：\nCompletableFuture\u0026lt;Void\u0026gt; task1 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作 }); ...... CompletableFuture\u0026lt;Void\u0026gt; task6 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作 }); ...... CompletableFuture\u0026lt;Void\u0026gt; headerFuture=CompletableFuture.allOf(task1,.....,task6); try { headerFuture.join(); } catch (Exception ex) { ...... } System.out.println(\u0026#34;all done. \u0026#34;); 调用join()可以让程序等future1和future2都运行完后继续执行\nCompletableFuture\u0026lt;Void\u0026gt; completableFuture = CompletableFuture.allOf(future1, future2); completableFuture.join(); assertTrue(completableFuture.isDone()); System.out.println(\u0026#34;all futures done...\u0026#34;); /**--- future1 done... future2 done... all futures done... */ anyOf则其中一个执行完就立马返回\nCompletableFuture\u0026lt;Object\u0026gt; f = CompletableFuture.anyOf(future1, future2); System.out.println(f.get()); /* future2 done... efg */ //或 /* future1 done... abc */ 例子2\nCompletableFuture\u0026lt;Object\u0026gt; a = CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;a\u0026#34;); //执行了3s的任务 try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;a-hello\u0026#34;; }); CompletableFuture\u0026lt;Object\u0026gt; b = CompletableFuture.supplyAsync(() -\u0026gt; { System.out.println(\u0026#34;b\u0026#34;); //执行了3s的任务 try { TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;b-hello\u0026#34;; }); /* //会等两个任务都执行完才继续 CompletableFuture\u0026lt;Void\u0026gt; voidCompletableFuture = CompletableFuture.allOf(a, b); voidCompletableFuture.join(); //停顿10s System.out.println(\u0026#34;主线程继续执行\u0026#34;);*/ //任何一个任务执行完就会继续执行 CompletableFuture\u0026lt;Object\u0026gt; objectCompletableFuture = CompletableFuture.anyOf(a, b); objectCompletableFuture.join(); //会得到最快返回值的那个CompletableFuture的值 System.out.println(objectCompletableFuture.get()); //停顿3s System.out.println(\u0026#34;主线程继续执行\u0026#34;); 后记 # 京东的aysncTool框架\nhttps://gitee.com/jd-platform-opensource/asyncTool#%E5%B9%B6%E8%A1%8C%E5%9C%BA%E6%99%AF%E4%B9%8B%E6%A0%B8%E5%BF%83%E4%BB%BB%E6%84%8F%E7%BC%96%E6%8E%92\n"},{"id":136,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0310lythreadlocal/","title":"ThreadLocal详解","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n本文来自一枝花算不算浪漫投稿， 原文地址：https://juejin.cn/post/6844904151567040519open in new window。 感谢作者!\n思维导图\n目录 # ThreadLocal代码演示 # 简单使用\npublic class ThreadLocalTest { private List\u0026lt;String\u0026gt; messages = Lists.newArrayList(); public static final ThreadLocal\u0026lt;ThreadLocalTest\u0026gt; holder = ThreadLocal.withInitial(ThreadLocalTest::new); public static void add(String message) { holder.get().messages.add(message); } public static List\u0026lt;String\u0026gt; clear() { List\u0026lt;String\u0026gt; messages = holder.get().messages; holder.remove(); System.out.println(\u0026#34;size: \u0026#34; + holder.get().messages.size()); return messages; } public static void main(String[] args) { ThreadLocalTest.add(\u0026#34;一枝花算不算浪漫\u0026#34;); System.out.println(holder.get().messages); ThreadLocalTest.clear(); } } /* 结果 [一枝花算不算浪漫] size: 0 */ 简单使用2\n@Data class LyTest{ private ThreadLocal\u0026lt;String\u0026gt; threadLocal=ThreadLocal.withInitial(()-\u0026gt;{ return \u0026#34;hello\u0026#34;; }); } public class ThreadLocalTest { public static void main(String[] args) throws InterruptedException { CountDownLatch countDownLatch=new CountDownLatch(2); LyTest lyTest=new LyTest(); ThreadLocal\u0026lt;String\u0026gt; threadLocal = lyTest.getThreadLocal(); new Thread(()-\u0026gt;{ String name = Thread.currentThread().getName(); threadLocal.set(name+ \u0026#34;-ly\u0026#34;); System.out.println(name+\u0026#34;：threadLocal当前值\u0026#34;+threadLocal.get()); countDownLatch.countDown(); },\u0026#34;线程1\u0026#34;).start(); new Thread(()-\u0026gt;{ String name = Thread.currentThread().getName(); threadLocal.set(name+ \u0026#34;-ly\u0026#34;); System.out.println(name+\u0026#34;：threadLocal当前值\u0026#34;+threadLocal.get()); countDownLatch.countDown(); },\u0026#34;线程2\u0026#34;).start(); /*while (true){}*/ countDownLatch.await(); System.out.println(Thread.currentThread().getName()+\u0026#34;：threadLocal当前值\u0026#34;+threadLocal.get()); } } /* 线程1：threadLocal当前值线程1-ly 线程2：threadLocal当前值线程2-ly main：threadLocal当前值hello */ ThreadLocal对象可以提供线程局部变量，每个线程Thread拥有一份自己的副本变量，多个线程互不干扰。\n回顾之前的知识点\npublic void set(T value) { //获取当前请求的线程 Thread t = Thread.currentThread(); //取出 Thread 类内部的 threadLocals 变量(哈希表结构) ThreadLocalMap map = getMap(t); if (map != null) // 将需要存储的值放入到这个哈希表中 //★★实际使用的方法 map.set(this, value); else //★★实际使用的方法 createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } 如上，实际存取都是从Thread的threadLocals （ThreadLocalMap类）中，并不是存在ThreadLocal上，ThreadLocal用来传递了变量值，只是ThreadLocalMap的封装 ThreadLocal类中通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t) 可以访问到该线程的ThreadLocalMap对象 每个Thread中具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key，Object对象为value的键值对 ThreadLocal的数据结构 # 由上面回顾的知识点可知，value实际上都是保存在**线程类(Thread类)中的某个属性(ThreadLocalMap类)**中\nThreadLocalMap的底层是一个数组（map的底层是数组）\nThread类有一个类型为**ThreadLocal.ThreadLocalMap**的实例变量threadLocals，也就是说每个线程有一个自己的ThreadLocalMap。 ThreadLocalMap是一个静态内部类\n没有修饰符，为包可见。比如父类有一个protected修饰的方法f()，不同包下存在子类A和其他类X，在子类中可以访问方法f()，即使在其他类X创建子类A实例a1，也不能调用a1.f()\u0026ndash;\u0026gt; 其他包不可见\nThreadLocalMap有自己独立实现，简单地将它的key视作ThreadLocal，value为代码中放入的值，（看底层代码可知，实际key不是ThreadLocal本身，而是它的一个弱引用）\n★每个线程在往ThreadLocal里放值的时候，都会往自己的ThreadLocalMap里存，读也是以ThreadLocal作为引用，在自己的map里找对应的key，从而实现了线程隔离。\nThreadLocalMap有点类似HashMap的结构，只是HashMap是由数组+链表实现的，而ThreadLocalMap中并没有链表结构。其中，还要注意Entry类， 它的key是ThreadLocal\u0026lt;?\u0026gt; k ，(Entry类)继承自WeakReference， 也就是我们常说的弱引用类型。\n如下，有个数组存放Entry(弱引用类，且有属性value)，且\nstatic class ThreadLocalMap { static class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } //..... } 为上面的知识点总结一张图 # GC之后key是否为null # WeakReference的使用\nWeakReference\u0026lt;Car\u0026gt; weakCar = new WeakReference(Car)(car); weakCar.get(); //如果值为null表示已经被回收了 问题： ThreadLocal的key为弱引用，那么在ThreadLocal.get()的时候，发生GC之后，key是否为null\nJava的四种引用类型 强引用：通常情况new出来的为强引用，只要强引用存在，垃圾回收器永远不会回收被引用的对象（即使内存不足） 软引用：使用SoftReference修饰的对象称软引用，软引用指向的对象在内存要溢出的时候被回收 弱引用：使用WeakReference修饰的对象称为弱引用，只要发生垃圾回收，如果这个对象只被弱引用指向，那么就会被回收 虚引用：虚引用是最弱的引用，用PhantomReference定义。唯一的作用就是用队列接收对象即将死亡的通知 使用反射方式查看GC后ThreadLocal中的数据情况\nimport java.lang.reflect.Field; /* t.join()方法阻塞调用此方法的线程(calling thread)进入 TIMED_WAITING 状态，直到线程t完成，此线程再继续 */ public class ThreadLocalDemo { public static void main(String[] args) throws NoSuchFieldException, IllegalAccessException, InterruptedException { Thread t = new Thread(()-\u0026gt;test(\u0026#34;abc\u0026#34;,false)); t.start(); t.join(); System.out.println(\u0026#34;--gc后--\u0026#34;); Thread t2 = new Thread(() -\u0026gt; test(\u0026#34;def\u0026#34;, true)); t2.start(); t2.join(); } private static void test(String s,boolean isGC) { try { //注意这一行,这个ThreadLocal对象是不存在任何强引用的 new ThreadLocal\u0026lt;\u0026gt;().set(s);//当前线程设置了一个值 s if (isGC) { System.gc(); } Thread t = Thread.currentThread(); Class\u0026lt;? extends Thread\u0026gt; clz = t.getClass(); Field field = clz.getDeclaredField(\u0026#34;threadLocals\u0026#34;); field.setAccessible(true); Object threadLocalMap = field.get(t);//得到当前线程的ThreadLocalMap Class\u0026lt;?\u0026gt; tlmClass = threadLocalMap.getClass(); Field tableField = tlmClass.getDeclaredField(\u0026#34;table\u0026#34;); tableField.setAccessible(true); //注意：这里获取的是threadLocalMap内部的(维护)数组 private Entry[] table; Object[] arr = (Object[]) tableField.get(threadLocalMap); for (Object o : arr) { if (o != null) { Class\u0026lt;?\u0026gt; entryClass = o.getClass(); /* Entry结构 static class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { //The value associated with this ThreadLocal. Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } */ //获取Entry中的值（键值对的“值”） Field valueField = entryClass.getDeclaredField(\u0026#34;value\u0026#34;); //Entry extends WeakReference //WeakReference\u0026lt;T\u0026gt; extends Reference\u0026lt;T\u0026gt; //Reference 里面有一个属性 referent ，指向实际的对象，即key实际的对象 Field referenceField = entryClass.getSuperclass().getSuperclass().getDeclaredField(\u0026#34;referent\u0026#34;); valueField.setAccessible(true); referenceField.setAccessible(true); System.out.println(String.format(\u0026#34;弱引用key:%s,值:%s\u0026#34;, referenceField.get(o), valueField.get(o))); } } } catch (Exception e) { e.printStackTrace(); } } } /* 结果如下 弱引用key:java.lang.ThreadLocal@433619b6,值:abc 弱引用key:java.lang.ThreadLocal@418a15e3,值:java.lang.ref.SoftReference@bf97a12 --gc后-- 弱引用key:null,值:def */ gc之后的图：\nnew ThreadLocal\u0026lt;\u0026gt;().set(s); GC之后，key就会被回收，我们看到上面的debug中referent=null\n如果这里修改代码，\nThreadLocal\u0026lt;Object\u0026gt; threadLocal=new ThreadLocal\u0026lt;\u0026gt;(); threadLocal.set(s); 使用弱引用+垃圾回收\n如上，垃圾回收前，ThreadLoal是存在强引用的，因此如果如上修改代码，则key不为null\n当不存在强引用时，key会被回收，即出现value没被回收，key被回收，导致key永远存在，内存泄漏\nThreadLocal.set()方法源码详解 # 如图所示\nThreadLocal中的set()方法原理如上，先取出线程Thread中的threadLocals，判断是否存在，然后使用ThreadLocal中的set方法进行数据处理\npublic void set(T value) { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } ThreadLocalMap Hash算法 # ThreadLocalMap实现了自己的hash算法来解决散列表数组冲突问题：\n//i为当前key在散列表中对应的数组下标位置 //即(len-1)和和斐波那契数做 与运算 int i = key.threadLocalHashCode \u0026amp; (len-1); threadLocalHashCode值的计算，ThreadLocal中有一个属性为HASH_INCREMENT = 0x61c88647\n0x61c88647，又称为斐波那契数也叫黄金分割数，hash增量为这个数，好处是hash 分布非常均匀\npublic class ThreadLocal\u0026lt;T\u0026gt; { private final int threadLocalHashCode = nextHashCode(); private static AtomicInteger nextHashCode = new AtomicInteger(); private static final int HASH_INCREMENT = 0x61c88647; //hashCode增加 private static int nextHashCode() { return nextHashCode.getAndAdd(HASH_INCREMENT); } static class ThreadLocalMap { ThreadLocalMap(ThreadLocal\u0026lt;?\u0026gt; firstKey, Object firstValue) { table = new Entry[INITIAL_CAPACITY]; int i = firstKey.threadLocalHashCode \u0026amp; (INITIAL_CAPACITY - 1); table[i] = new Entry(firstKey, firstValue); size = 1; setThreshold(INITIAL_CAPACITY); } } } 例子如下，产生的哈希码分布十分均匀\n★★ 说明，下面的所有示例图中，绿色块Entry代表为正常数据，灰色块代表Entry的key为null，已被垃圾回收。白色块代表Entry为null（或者说数组那个位置为null(没有指向)）\nThreadLocalMap Hash冲突 # ThreadLocalMap 中使用黄金分割数作为hash计算因子，大大减少Hash冲突的概率 HashMap中解决冲突的方法，是在数组上构造一个链表结构，冲突的数据挂载到链表上，如果链表长度超过一定数量则会转化为红黑树 ThreadLocalMap中没有链表结构（使用线性向后查找） 如图 假设需要插入value = 27 的数据，hash后应该落入槽位4，而槽位已经有了Entry数据 此时线性向后查找，一直找到Entry为null的操作才会停止查找，将当前元素放入该槽位中 线性向后查找迭代中，会遇到Entry不为null且key值相等，以及**Entry中的key为null（图中Entry 为 2）**的情况，处理方式不同 set过程中如果遇到了key过期(key为null)的Entry数据，实际上会进行一轮探测式清理操作 ThreadLocalMap.set() 详解 # ThreadLocalMap.set() 原理图解\n往ThreadLocalMap中set数据（新增或更新数据）分为好几种\n通过hash计算后的槽位对应的Entry数据为空 直接将数据放到该槽位即可\n槽位数据不为空，key值与当前ThreadLocal通过hash计算获取的key值一致 直接更新该槽位的数据\n槽位数据不为空，往后遍历过程中，在找到Entry为null的槽位之前，没有遇到过期的Entry 遍历散列数组的过程中，线性往后查找，如果找到Entry为null的槽位则将数据放入槽位中；或者往后遍历过程中遇到key值相等的数据则更新\n槽位数据不为空，在找到Entry为null的槽位之前，遇到了过期的Entry，如下图 此时会执行replaceStableEntry()方法，该方法含义是替换过期数据的逻辑\n\u0026hellip; 以下省略，太复杂\n替换完成后也是进行过期元素清理工作，清理工作主要是有两个方法：expungeStaleEntry()和cleanSomeSlots()\n经过迭代处理后，有过Hash冲突数据的Entry位置会更靠近正确位置，这样的话，查询的时候 效率才会更高。\nThreadLocalMap过期 key 的探测式清理流程(略过) # ThreadLocalMap扩容机制 # 在ThreadLocalMap.set()方法的最后，如果执行完启发式清理工作后，未清理到任何数据，且当前散列数组中Entry的数量已经达到了列表的扩容阈值(len*2/3)，就开始执行rehash()逻辑：\nif (!cleanSomeSlots(i, sz) \u0026amp;\u0026amp; sz \u0026gt;= threshold) rehash(); rehash()的具体实现\nprivate void rehash() { expungeStaleEntries(); if (size \u0026gt;= threshold - threshold / 4) resize(); } private void expungeStaleEntries() { Entry[] tab = table; int len = tab.length; for (int j = 0; j \u0026lt; len; j++) { Entry e = tab[j]; if (e != null \u0026amp;\u0026amp; e.get() == null) expungeStaleEntry(j); } } 注意：\nthreshold [ˈθreʃhəʊld], 门槛 = length * 2/3\nrehash之前进行一次容量判断( 是否 \u0026gt; threshold , 是则rehash)\nrehash时先进行expungeStaleEntries() （探索式清理，从table起始为止）\n这里首先是会进行探测式清理工作，从table的起始位置往后清理，上面有分析清理的详细流程。清理完成之后，table中可能有一些key为null的Entry数据被清理掉，所以此时通过判断size \u0026gt;= threshold - threshold / 4 也就是size \u0026gt;= threshold * 3/4 来决定是否扩容。\n清理后如果大于 threshold 的3/4 ，则进行扩容 具体的resize()方法 以oldTab .len = 8\n容后的tab的大小为oldLen * 2 =16\n遍历老的散列表，重新计算hash位置，然后放到新的tab数组中，如果出现hash冲突则往后寻找最近的entry为null的槽位\n遍历完成之后，oldTab中所有的entry数据都已经放入到新的tab中了。重新计算tab下次扩容的阈值 代码如下\nprivate void resize() { Entry[] oldTab = table; int oldLen = oldTab.length; int newLen = oldLen * 2; Entry[] newTab = new Entry[newLen]; int count = 0; for (int j = 0; j \u0026lt; oldLen; ++j) { Entry e = oldTab[j]; if (e != null) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == null) { e.value = null; } else { int h = k.threadLocalHashCode \u0026amp; (newLen - 1); while (newTab[h] != null) h = nextIndex(h, newLen); newTab[h] = e; count++; } } } setThreshold(newLen); size = count; table = newTab; } ThreadLocalMap.get() 详解 # 通过查找key值计算出散列表中slot位置，然后该slot位置中的Entry.key和查找的key一致，则直接返回 slot位置中的Entry.key和要查找的key不一致，之后清理+遍历 我们以get(ThreadLocal1)为例，通过hash计算后，正确的slot位置应该是 4，而index=4的槽位已经有了数据，且key值不等于ThreadLocal1，所以需要继续往后迭代查找。\n迭代到index=5的数据时，此时Entry.key=null，触发一次探测式数据回收操作，执行expungeStaleEntry()方法，执行完后，index 5,8的数据都会被回收，而index 6,7的数据都会前移。index 6,7前移之后，继续从 index=5 往后迭代，于是就在 index=5 找到了key值相等的Entry数据，如下图所示： ThreadLocalMap.get()源码详解\nprivate Entry getEntry(ThreadLocal\u0026lt;?\u0026gt; key) { int i = key.threadLocalHashCode \u0026amp; (table.length - 1); Entry e = table[i]; if (e != null \u0026amp;\u0026amp; e.get() == key) return e; else return getEntryAfterMiss(key, i, e); } private Entry getEntryAfterMiss(ThreadLocal\u0026lt;?\u0026gt; key, int i, Entry e) { Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == key) return e; if (k == null) expungeStaleEntry(i); else i = nextIndex(i, len); e = tab[i]; } return null; } ThreadLocalMap过期key的启发式清理流程(略过，跟移位运算符有关) # 上面多次提及到ThreadLocalMap过期key的两种清理方式：探测式清理(expungeStaleEntry())、启发式清理(cleanSomeSlots())\n探测式清理是以当前Entry 往后清理，遇到值为null则结束清理，属于线性探测清理。\n而启发式清理被作者定义为：Heuristically scan some cells looking for stale entries.\nInheritable ThreadLocal # 使用ThreadLocal的时候，在异步场景下是无法给子线程共享父线程中创建的线程副本数据的。JDK中存在InheritableThreadLocal类可以解决处理这个问题\n原理： 子线程是通过在父线程中通过new Thread()方法创建子线程，Thread#init 方法在Thread的构造方法中被调用，init方法中拷贝父线程数据到子线程中\nprivate void init(ThreadGroup g, Runnable target, String name, long stackSize, AccessControlContext acc, boolean inheritThreadLocals) { if (name == null) { throw new NullPointerException(\u0026#34;name cannot be null\u0026#34;); } if (inheritThreadLocals \u0026amp;\u0026amp; parent.inheritableThreadLocals != null) this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); this.stackSize = stackSize; tid = nextThreadID(); } public class InheritableThreadLocalDemo { public static void main(String[] args) { ThreadLocal\u0026lt;String\u0026gt; ThreadLocal = new ThreadLocal\u0026lt;\u0026gt;(); ThreadLocal\u0026lt;String\u0026gt; inheritableThreadLocal = new InheritableThreadLocal\u0026lt;\u0026gt;(); ThreadLocal.set(\u0026#34;父类数据:threadLocal\u0026#34;); inheritableThreadLocal.set(\u0026#34;父类数据:inheritableThreadLocal\u0026#34;); new Thread(new Runnable() { @Override public void run() { System.out.println(\u0026#34;子线程获取父类ThreadLocal数据：\u0026#34; + ThreadLocal.get()); System.out.println(\u0026#34;子线程获取父类inheritableThreadLocal数据：\u0026#34; + inheritableThreadLocal.get()); } }).start(); } } /*结果 子线程获取父类ThreadLocal数据：null 子线程获取父类inheritableThreadLocal数据：父类数据:inheritableThreadLocal */ 但是如果不是直接new()，也就是实际中我们都是通过使用线程池来获取新线程的，那么可以使用阿里开源的一个组件解决这个问题 TransmittableThreadLocal\nThreadLocal项目中使用实战 # 这里涉及到requestId，没用过，不是很懂，略过\nThreadLocal使用场景 # Feign远程调用解决方案 # 线程池异步调用,requestId 传递 # 使用MQ发送消息给第三方系统 # "},{"id":137,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0309lyatomic-classes/","title":"Atomic原子类介绍","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n文章开头先用例子介绍几种类型的api使用\npackage com.aqs; import lombok.*; import java.util.concurrent.atomic.*; @Data @Getter @Setter @AllArgsConstructor @ToString class User { private String name; //如果要为atomicReferenceFieldUpdater服务,必须加上volatile修饰 public volatile Integer age; } public class AtomicTest { public static void main(String[] args) { System.out.println(\u0026#34;原子更新数值---------------\u0026#34;); AtomicInteger atomicInteger = new AtomicInteger(); int i1 = atomicInteger.incrementAndGet(); System.out.println(\u0026#34;原子增加后为\u0026#34; + i1); System.out.println(\u0026#34;原子更新数组---------------\u0026#34;); int[] a = new int[3]; AtomicIntegerArray atomicIntegerArray = new AtomicIntegerArray(a); int i = atomicIntegerArray.addAndGet(1, 3); System.out.println(\u0026#34;数组元素[\u0026#34; + 1 + \u0026#34;]增加后为\u0026#34; + i); System.out.println(\u0026#34;数组为\u0026#34; + atomicIntegerArray); System.out.println(\u0026#34;原子更新对象---------------\u0026#34;); User user1 = new User(\u0026#34;ly1\u0026#34;, 10); User user2 = new User(\u0026#34;ly2\u0026#34;, 20); User user3 = new User(\u0026#34;ly3\u0026#34;, 30); AtomicReference\u0026lt;User\u0026gt; atomicReference = new AtomicReference\u0026lt;\u0026gt;(user1); boolean b = atomicReference.compareAndSet(user2, user3); System.out.println(\u0026#34;更新\u0026#34; + (b ? \u0026#34;成功\u0026#34; : \u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里值为\u0026#34;+atomicReference.get()); boolean b1 = atomicReference.compareAndSet(user1, user3); System.out.println(\u0026#34;更新\u0026#34; + (b1 ? \u0026#34;成功\u0026#34; : \u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里值为\u0026#34;+atomicReference.get()); System.out.println(\u0026#34;原子更新对象属性---------------\u0026#34;); User user4=new User(\u0026#34;ly4\u0026#34;,40); AtomicReferenceFieldUpdater\u0026lt;User, Integer\u0026gt; atomicReferenceFieldUpdater = AtomicReferenceFieldUpdater.newUpdater(User.class, Integer.class, \u0026#34;age\u0026#34;); boolean b2 = atomicReferenceFieldUpdater.compareAndSet(user4, 41, 400); System.out.println(\u0026#34;更新\u0026#34;+(b2?\u0026#34;成功\u0026#34;:\u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里user4值为\u0026#34;+atomicReferenceFieldUpdater.get(user4)); boolean b3 = atomicReferenceFieldUpdater.compareAndSet(user4, 40, 400); System.out.println(\u0026#34;更新\u0026#34;+(b3?\u0026#34;成功\u0026#34;:\u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里user4值为\u0026#34;+atomicReferenceFieldUpdater.get(user4)); System.out.println(\u0026#34;其他使用---------------\u0026#34;); User user5=new User(\u0026#34;ly5\u0026#34;,50); User user6=new User(\u0026#34;ly6\u0026#34;,60); User user7=new User(\u0026#34;ly7\u0026#34;,70); AtomicMarkableReference\u0026lt;User\u0026gt; userAtomicMarkableReference=new AtomicMarkableReference\u0026lt;\u0026gt;(user5,true); boolean b4 = userAtomicMarkableReference.weakCompareAndSet(user6, user7, true, false); System.out.println(\u0026#34;更新\u0026#34;+(b4?\u0026#34;成功\u0026#34;:\u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里值为\u0026#34;+userAtomicMarkableReference.getReference()); boolean b5 = userAtomicMarkableReference.weakCompareAndSet(user5, user7, false, true); System.out.println(\u0026#34;更新\u0026#34;+(b5?\u0026#34;成功\u0026#34;:\u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里值为\u0026#34;+userAtomicMarkableReference.getReference()); boolean b6 = userAtomicMarkableReference.weakCompareAndSet(user5, user7, true, false); System.out.println(\u0026#34;更新\u0026#34;+(b6?\u0026#34;成功\u0026#34;:\u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里值为\u0026#34;+userAtomicMarkableReference.getReference()); System.out.println(\u0026#34;AtomicStampedReference使用---------------\u0026#34;); User user80=new User(\u0026#34;ly8\u0026#34;,80); User user90=new User(\u0026#34;ly9\u0026#34;,90); User user100=new User(\u0026#34;ly10\u0026#34;,100); AtomicStampedReference\u0026lt;User\u0026gt; userAtomicStampedReference=new AtomicStampedReference\u0026lt;\u0026gt;(user80,80);//版本80 //...每次更改stamp都加1 //这里假设中途被改成81了 boolean b7 = userAtomicStampedReference.compareAndSet(user80, user100,81,90); System.out.println(\u0026#34;更新\u0026#34;+(b7?\u0026#34;成功\u0026#34;:\u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里值为\u0026#34;+userAtomicStampedReference.getReference()); boolean b8 = userAtomicStampedReference.compareAndSet(user80, user100,80,90); System.out.println(\u0026#34;更新\u0026#34;+(b8?\u0026#34;成功\u0026#34;:\u0026#34;失败\u0026#34;)); System.out.println(\u0026#34;引用里值为\u0026#34;+userAtomicStampedReference.getReference()); } } /* 原子更新数值--------------- 原子增加后为1 原子更新数组--------------- 数组元素[1]增加后为3 数组为[0, 3, 0] 原子更新对象--------------- 更新失败 引用里值为User(name=ly1, age=10) 更新成功 引用里值为User(name=ly3, age=30) 原子更新对象属性--------------- 更新失败 引用里user4值为40 更新成功 引用里user4值为400 其他使用--------------- 更新失败 引用里值为User(name=ly5, age=50) 更新失败 引用里值为User(name=ly5, age=50) 更新成功 引用里值为User(name=ly7, age=70) AtomicStampedReference使用--------------- 更新失败 引用里值为User(name=ly8, age=80) 更新成功 引用里值为User(name=ly10, age=100) Process finished with exit code 0 */ 原子类介绍 # 在化学上，原子是构成一般物质的最小单位，化学反应中是不可分割的，Atomic指一个操作是不可中断的，即使在多个线程一起执行时，一个操作一旦开始就不会被其他线程干扰 原子类\u0026ndash;\u0026gt;具有原子/原子操作特征的类 并发包java.util.concurrent 的原子类都放着java.util.concurrent.atomic中 根据操作的数据类型，可以将JUC包中的原子类分为4类（基本类型、数组类型、引用类型、对象的属性修改类型） 基本类型 使用原子方式更新基本类型，包括AtomicInteger 整型原子类，AtomicLong 长整型原子类，AtomicBoolean 布尔型原子类\n数组类型 使用原子方式更新数组里某个元素，包括AtomicIntegerArray 整型数组原子类，AtomicLongArray 长整型数组原子类，AtomicReferenceArray 引用类型数组原子类\n引用类型 AtomicReference 引用类型原子类，AtomicMarkableReference 原子更新带有标记的引用类型，该类将boolean标记与引用关联（不可解决CAS进行原子操作出现的ABA问题），AtomicStampedReference 原子更新带有版本号的引用类型 该类将整数值与引用关联，可用于解决原子更新数据和数据的版本号(解决使用CAS进行原子更新时可能出现的ABA问题)\n对象的属性修改类型 AtomicIntegerFieldUpdater 原子更新整型字段的更新器，AtomicLongFieldUpdater 原子更新长整型字段的更新器， AtomicReferenceFieldUpdater 原子更新引用类型里的字段\nAtomicMarkableReference 不能解决 ABA 问题\npublic class SolveABAByAtomicMarkableReference { private static AtomicMarkableReference atomicMarkableReference = new AtomicMarkableReference(100, false); public static void main(String[] args) { Thread refT1 = new Thread(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } atomicMarkableReference.compareAndSet(100, 101, atomicMarkableReference.isMarked(), !atomicMarkableReference.isMarked());//根据期望值100和false 修改为101和true atomicMarkableReference.compareAndSet(101, 100, atomicMarkableReference.isMarked(), !atomicMarkableReference.isMarked());//根据期望值101和true 修改为100和false }); Thread refT2 = new Thread(() -\u0026gt; { //获取原来的marked标记(false) boolean marked = atomicMarkableReference.isMarked(); //2s之后进行替换,不应该替换成功 try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } boolean c3 = atomicMarkableReference.compareAndSet(100, 101, marked, !marked); System.out.println(c3); // 返回true,实际应该返回false }); //导致了ABA问题 refT1.start(); refT2.start(); } } CAS ABA问题\n描述: 第一个线程取到了变量 x 的值 A，然后巴拉巴拉干别的事，总之就是只拿到了变量 x 的值 A。这段时间内第二个线程也取到了变量 x 的值 A，然后把变量 x 的值改为 B，然后巴拉巴拉干别的事，最后又把变量 x 的值变为 A （相当于还原了）。在这之后第一个线程终于进行了变量 x 的操作，但是此时变量 x 的值还是 A，所以 compareAndSet 操作是成功。\n也就是说，线程一无法保证自己操作期间，该值被修改了\n例子描述(可能不太合适，但好理解): 年初，现金为零，然后通过正常劳动赚了三百万，之后正常消费了（比如买房子）三百万。年末，虽然现金零收入（可能变成其他形式了），但是赚了钱是事实，还是得交税的！\n代码描述\nimport java.util.concurrent.atomic.AtomicInteger; public class AtomicIntegerDefectDemo { public static void main(String[] args) { defectOfABA(); } static void defectOfABA() { final AtomicInteger atomicInteger = new AtomicInteger(1); Thread coreThread = new Thread( () -\u0026gt; { final int currentValue = atomicInteger.get(); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue); // 这段目的：模拟处理其他业务花费的时间 //也就是说，在差值300-100=200ms内，值被操作了两次(但又改回去了)，然后线程coreThread并没有感知到，当作没有修改过来处理 try { Thread.sleep(300); } catch (InterruptedException e) { e.printStackTrace(); } boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue + \u0026#34;, finalValue=\u0026#34; + atomicInteger.get() + \u0026#34;, compareAndSet Result=\u0026#34; + casResult); } ); coreThread.start(); // 这段目的：为了让 coreThread 线程先跑起来 try { Thread.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } Thread amateurThread = new Thread( () -\u0026gt; { int currentValue = atomicInteger.get(); boolean casResult = atomicInteger.compareAndSet(1, 2); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue + \u0026#34;, finalValue=\u0026#34; + atomicInteger.get() + \u0026#34;, compareAndSet Result=\u0026#34; + casResult); currentValue = atomicInteger.get(); casResult = atomicInteger.compareAndSet(2, 1); System.out.println(Thread.currentThread().getName() + \u0026#34; ------ currentValue=\u0026#34; + currentValue + \u0026#34;, finalValue=\u0026#34; + atomicInteger.get() + \u0026#34;, compareAndSet Result=\u0026#34; + casResult); } ); amateurThread.start(); } } /*输出内容 Thread-0 ------ currentValue=1 Thread-1 ------ currentValue=1, finalValue=2, compareAndSet Result=true Thread-1 ------ currentValue=2, finalValue=1, compareAndSet Result=true Thread-0 ------ currentValue=1, finalValue=2, compareAndSet Result=true */ 基本类型原子类 # 使用原子方式更新基本类型：AtomicInteger 整型原子类，AtomicLong 长整型原子类 ，AtomicBoolean 布尔型原子类，下文以AtomicInteger为例子来介绍 常用方法：\npublic final int get() //获取当前的值 public final int getAndSet(int newValue)//获取当前的值，并设置新的值 public final int getAndIncrement()//获取当前的值，并自增 public final int getAndDecrement() //获取当前的值，并自减 public final int getAndAdd(int delta) //获取当前的值，并加上预期的值 boolean compareAndSet(int expect, int update) //如果输入的数值等于预期值，则以原子方式将该值设置为输入值（update） public final void lazySet(int newValue)//最终设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 常见方法使用\nimport java.util.concurrent.atomic.AtomicInteger; public class AtomicIntegerTest { public static void main(String[] args) { // TODO Auto-generated method stub int temvalue = 0; AtomicInteger i = new AtomicInteger(0); temvalue = i.getAndSet(3); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i);//temvalue:0; i:3 temvalue = i.getAndIncrement(); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i);//temvalue:3; i:4 temvalue = i.getAndAdd(5); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i);//temvalue:4; i:9 } } 基本数据类型原子类的优势 # 多线程环境不使用原子类保证线程安全（基本数据类型）\nclass Test { private volatile int count = 0; //若要线程安全执行执行count++，需要加锁 public synchronized void increment() { count++; } public int getCount() { return count; } } 多线程环境使用原子类保证线程安全(基本数据类型)\nclass Test2 { private AtomicInteger count = new AtomicInteger(); //使用AtomicInteger之后，不需要加锁，也可以实现线程安全。 public void increment() { count.incrementAndGet(); } public int getCount() { return count.get(); } } AtomicInteger线程安全原理简单分析 # 部分源码：\n/ setup to use Unsafe.compareAndSwapInt for updates（更新操作时提供“比较并替换”的作用） private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static { try { valueOffset = unsafe.objectFieldOffset (AtomicInteger.class.getDeclaredField(\u0026#34;value\u0026#34;)); } catch (Exception ex) { throw new Error(ex); } } private volatile int value; AtomicInteger类主要利用CAS（compare and swap) + volatile 和 native方法来保证原子操作，从而避免synchronized高开销，提高执行效率 CAS的原理是拿期望的值和原本的值做比较，如果相同则更新成新值 UnSafe类的objectFieldOffset()方法是一个本地方法，这个方法用来拿到**\u0026ldquo;原来的值\u0026quot;的内存地址** value是一个volatile变量，在内存中可见，因此JVM可以保证任何时刻任何线程总能拿到该变量的最新值 数组类型原子类 # 使用原子的方式更新数组里的某个元素\nAtomicIntegerArray 整型数组原子类，AtomicLongArray 长整型数组原子类，AtomicReferenceArray 引用类型数组原子类\n常用方法：\npublic final int get(int i) //获取 index=i 位置元素的值 public final int getAndSet(int i, int newValue)//返回 index=i 位置的当前的值，并将其设置为新值：newValue public final int getAndIncrement(int i)//获取 index=i 位置元素的值，并让该位置的元素自增 public final int getAndDecrement(int i) //获取 index=i 位置元素的值，并让该位置的元素自减 public final int getAndAdd(int i, int delta) //获取 index=i 位置元素的值，并加上预期的值 boolean compareAndSet(int i, int expect, int update) //如果输入的数值等于预期值，则以原子方式将 index=i 位置的元素值设置为输入值（update） public final void lazySet(int i, int newValue)//最终 将index=i 位置的元素设置为newValue,使用 lazySet 设置之后可能导致其他线程在之后的一小段时间内还是可以读到旧的值。 常见方法使用\nimport java.util.concurrent.atomic.AtomicIntegerArray; public class AtomicIntegerArrayTest { public static void main(String[] args) { // TODO Auto-generated method stub int temvalue = 0; int[] nums = { 1, 2, 3, 4, 5, 6 }; AtomicIntegerArray i = new AtomicIntegerArray(nums); for (int j = 0; j \u0026lt; nums.length; j++) { System.out.println(i.get(j)); } temvalue = i.getAndSet(0, 2); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i); temvalue = i.getAndIncrement(0); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i); temvalue = i.getAndAdd(0, 5); System.out.println(\u0026#34;temvalue:\u0026#34; + temvalue + \u0026#34;; i:\u0026#34; + i); } } 引用类型原子类 # 基本类型原子类只能更新一个变量，如果需要原子更新多个变量，则需要使用引用类型原子类\nAtomicReference 引用类型原子类；\nAtomicStampedReference 原子更新带有版本号的引用类型，该类将整数值与引用关联起来，可用于解决原子的更新数据和数据的版本号，可以解决使用CAS进行原子更新时可能出现的ABA问题；\nAtomicMarkableReference：原子更新带有标记的引用类型。该类将boolean标记与引用关联**（注：无法解决ABA问题）**\n下面以AtomicReference为例介绍\nimport java.util.concurrent.atomic.AtomicReference; public class AtomicReferenceTest { public static void main(String[] args) { AtomicReference\u0026lt;Person\u0026gt; ar = new AtomicReference\u0026lt;Person\u0026gt;(); Person person = new Person(\u0026#34;SnailClimb\u0026#34;, 22); ar.set(person); Person updatePerson = new Person(\u0026#34;Daisy\u0026#34;, 20); ar.compareAndSet(person, updatePerson);//如果期望值为person，则替换成updatePerson System.out.println(ar.get().getName()); System.out.println(ar.get().getAge()); } } class Person { private String name; private int age; public Person(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 上述代码首先创建了一个 Person 对象，然后把 Person 对象设置进 AtomicReference 对象中，然后调用 compareAndSet 方法，该方法就是通过 CAS 操作设置 ar。如果 ar 的值为 person 的话，则将其设置为 updatePerson。实现原理与 AtomicInteger 类中的 compareAndSet 方法相同。运行上面的代码后的输出结果如下\nDaisy 20 AtomicStampedReference类使用示例\nimport java.util.concurrent.atomic.AtomicStampedReference; public class AtomicStampedReferenceDemo { public static void main(String[] args) { // 实例化、取当前值和 stamp 值 final Integer initialRef = 0, initialStamp = 0; final AtomicStampedReference\u0026lt;Integer\u0026gt; asr = new AtomicStampedReference\u0026lt;\u0026gt;(initialRef, initialStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp()); // compare and set final Integer newReference = 666, newStamp = 999; final boolean casResult = asr.compareAndSet(initialRef, newReference, initialStamp, newStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp() + \u0026#34;, casResult=\u0026#34; + casResult); // 获取当前的值和当前的 stamp 值 int[] arr = new int[1]; final Integer currentValue = asr.get(arr); final int currentStamp = arr[0]; System.out.println(\u0026#34;currentValue=\u0026#34; + currentValue + \u0026#34;, currentStamp=\u0026#34; + currentStamp); // 单独设置 stamp 值 final boolean attemptStampResult = asr.attemptStamp(newReference, 88); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp() + \u0026#34;, attemptStampResult=\u0026#34; + attemptStampResult); // 重新设置当前值和 stamp 值 asr.set(initialRef, initialStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp()); // [不推荐使用，除非搞清楚注释的意思了] weak compare and set // 困惑！weakCompareAndSet 这个方法最终还是调用 compareAndSet 方法。[版本: jdk-8u191] // 但是注释上写着 \u0026#34;May fail spuriously and does not provide ordering guarantees, // so is only rarely an appropriate alternative to compareAndSet.\u0026#34; // todo 感觉有可能是 jvm 通过方法名在 native 方法里面做了转发 final boolean wCasResult = asr.weakCompareAndSet(initialRef, newReference, initialStamp, newStamp); System.out.println(\u0026#34;currentValue=\u0026#34; + asr.getReference() + \u0026#34;, currentStamp=\u0026#34; + asr.getStamp() + \u0026#34;, wCasResult=\u0026#34; + wCasResult); } } /* 结果 currentValue=0, currentStamp=0 currentValue=666, currentStamp=999, casResult=true currentValue=666, currentStamp=999 currentValue=666, currentStamp=88, attemptStampResult=true currentValue=0, currentStamp=0 currentValue=666, currentStamp=999, wCasResult=true */ AtomicMarkableReference 类使用示例\nimport java.util.concurrent.atomic.AtomicMarkableReference; public class AtomicMarkableReferenceDemo { public static void main(String[] args) { // 实例化、取当前值和 mark 值 final Boolean initialRef = null, initialMark = false; final AtomicMarkableReference\u0026lt;Boolean\u0026gt; amr = new AtomicMarkableReference\u0026lt;\u0026gt;(initialRef, initialMark); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked()); // compare and set final Boolean newReference1 = true, newMark1 = true; final boolean casResult = amr.compareAndSet(initialRef, newReference1, initialMark, newMark1); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked() + \u0026#34;, casResult=\u0026#34; + casResult); // 获取当前的值和当前的 mark 值 boolean[] arr = new boolean[1]; final Boolean currentValue = amr.get(arr); final boolean currentMark = arr[0]; System.out.println(\u0026#34;currentValue=\u0026#34; + currentValue + \u0026#34;, currentMark=\u0026#34; + currentMark); // 单独设置 mark 值 final boolean attemptMarkResult = amr.attemptMark(newReference1, false); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked() + \u0026#34;, attemptMarkResult=\u0026#34; + attemptMarkResult); // 重新设置当前值和 mark 值 amr.set(initialRef, initialMark); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked()); // [不推荐使用，除非搞清楚注释的意思了] weak compare and set // 困惑！weakCompareAndSet 这个方法最终还是调用 compareAndSet 方法。[版本: jdk-8u191] // 但是注释上写着 \u0026#34;May fail spuriously and does not provide ordering guarantees, // so is only rarely an appropriate alternative to compareAndSet.\u0026#34; // todo 感觉有可能是 jvm 通过方法名在 native 方法里面做了转发 final boolean wCasResult = amr.weakCompareAndSet(initialRef, newReference1, initialMark, newMark1); System.out.println(\u0026#34;currentValue=\u0026#34; + amr.getReference() + \u0026#34;, currentMark=\u0026#34; + amr.isMarked() + \u0026#34;, wCasResult=\u0026#34; + wCasResult); } } /* 结果 currentValue=null, currentMark=false currentValue=true, currentMark=true, casResult=true currentValue=true, currentMark=true currentValue=true, currentMark=false, attemptMarkResult=true currentValue=null, currentMark=false currentValue=true, currentMark=true, wCasResult=true */ 对象的属性修改类型原子类 # 对象的属性修改类型原子类，用来原子更新某个类里的某个字段\n包括： AtomicIntegerFieldUpdater 原子更新整型字段的更新器，AtomicLongFieldUpdater 原子更新长整型字段的更新器，AtomicReferenceFieldUpdater 原子更新引用类型里的字段的更新器\n原子地更新对象属性需要两步骤：\n对象的属性修改类型原子类都是抽象类，所以每次使用都必须使用静态方法newUpdater()创建一个更新器，且设置想要更新的类和属性 更新的对象属性必须使用public volatile修饰符 下面以AtomicIntegerFieldUpdater为例子来介绍\nimport java.util.concurrent.atomic.AtomicIntegerFieldUpdater; public class AtomicIntegerFieldUpdaterTest { public static void main(String[] args) { AtomicIntegerFieldUpdater\u0026lt;User\u0026gt; a = AtomicIntegerFieldUpdater.newUpdater(User.class, \u0026#34;age\u0026#34;); User user = new User(\u0026#34;Java\u0026#34;, 22); System.out.println(a.getAndIncrement(user));// 22 System.out.println(a.get(user));// 23 } } class User { private String name; public volatile int age; public User(String name, int age) { super(); this.name = name; this.age = age; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } /* 结果 22 33 */ "},{"id":138,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0308lyaqs-details/","title":"aqs详解","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nSemaphore [ˈseməfɔː(r)]\n何为 AQS？AQS 原理了解吗？ CountDownLatch 和 CyclicBarrier 了解吗？两者的区别是什么？ 用过 Semaphore 吗？应用场景了解吗？ \u0026hellip;\u0026hellip; AQS简单介绍 # AQS,AbstractQueueSyschronizer，即抽象队列同步器，这个类在java.util.concurrent.locks包下面\nAQS是一个抽象类，主要用来构建锁和同步器\npublic abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { } AQS 为构建锁和同步器提供了一些通用功能的实现，因此，使用 AQS 能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的 ReentrantLock，Semaphore，其他的诸如 ReentrantReadWriteLock，SynchronousQueue，FutureTask(jdk1.7) 等等皆是基于 AQS 的。\nAQS原理 # AQS核心思想 # 面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来\nAQS 核心思想是，如果被请求的共享资源（AQS内部）空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。\nCLH(Craig,Landin and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。\n[ 搜索了一下，CLH好像是人名 ] 在 CLH 同步队列中，一个节点表示一个线程，它保存着线程的引用（thread）、 当前节点在队列中的状态（waitStatus）、前驱节点（prev）、后继节点（next）。\nCLH队列结构\nAQS（AbstractQueuedSynchronized）原理图\nAQS使用一个int成员变量来表示同步状态，通过内置的线程等待队列来获取资源线程的排队工作。\nstate 变量由 volatile 修饰，用于展示当前临界资源的获锁情况。\nprivate volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息的操作\n通过 protected 类型的getState()、setState()和compareAndSetState() 进行操作。并且，这几个方法都是 final 修饰的，在子类中无法被重写。\n//返回同步状态的当前值 protected final int getState() { return state; } //设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 以 ReentrantLock 为例，state 初始值为 0，表示未锁定状态。A 线程 lock() 时，会调用 tryAcquire() 独占该锁并将 state+1 。此后，其他线程再 tryAcquire() 时就会失败，直到 A 线程 unlock() 到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多少次，这样才能保证 state 是能回到零态的。\n再以 CountDownLatch 以例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后countDown() 一次，state 会 CAS(Compare and Swap) 减 1。等到所有子线程都执行完后(即 state=0 )，会 unpark() 主调用线程，然后主调用线程就会从 await() 函数返回，继续后余动作。\n​\nAQS资源共享方式 # 包括Exclusive（独占，只有一个线程能执行，如ReentrantLock）和Share（共享，多个线程可同时执行，如Semaphore/CountDownLatch）\n从另一个角度讲，就是只有一个线程能操作state变量以及有n个线程能操作state变量的区别\n一般来说，自定义同步器的共享方式要么是独占，要么是共享，他们也只需实现**tryAcquire-tryRelease、tryAcquireShared-tryReleaseShared中的一种即可。但 AQS 也支持自定义同步器同时实现独占和共享两种方式，如ReentrantReadWriteLock**。\n自定义同步器 # 同步器的设计是基于模板方法模式的，如果需要自定义同步器一般的方式是这样（模板方法模式很经典的一个应用）：\n使用者继承 AbstractQueuedSynchronizer 并重写指定的方法。【使用者】 将 AQS 组合在自定义同步组件的实现中，并调用其模板方法，而这些模板方法会调用使用者重写的方法。【AQS内部】 这和我们以往通过实现接口的方式有很大区别，这是模板方法模式很经典的一个运用。\n//独占方式。尝试获取资源，成功则返回true，失败则返回false。 protected boolean tryAcquire(int) //独占方式。尝试释放资源，成功则返回true，失败则返回false。 protected boolean tryRelease(int) //共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。 protected int tryAcquireShared(int) //共享方式。尝试释放资源，成功则返回true，失败则返回false。 protected boolean tryReleaseShared(int) //该线程是否正在独占资源。只有用到condition才需要去实现它。 protected boolean isHeldExclusively() 什么是钩子方法呢？ 钩子方法是一种被声明在抽象类中的方法，一般使用 protected 关键字修饰，它可以是空方法（由子类实现），也可以是默认实现的方法。模板设计模式通过钩子方法控制固定步骤的实现。\n篇幅问题，这里就不详细介绍模板方法模式了，不太了解的小伙伴可以看看这篇文章：用 Java8 改造后的模板方法模式真的是 yyds!open in new window。\n除了上面提到的钩子方法之外，AQS 类中的其他方法都是 final ，所以无法被其他类重写。\n常见同步类 # Semaphore # Semaphore（信号量）可以指定多个线程同时访问某个资源\n/** * * @author Snailclimb * @date 2018年9月30日 * @Description: 需要一次性拿一个许可的情况 */ public class SemaphoreExample1 { // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); // 一次只能允许执行的线程数量。 final Semaphore semaphore = new Semaphore(20); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -\u0026gt; {// Lambda 表达式的运用 try { //通行证发了20个之后，就不能再发放了 semaphore.acquire();// 获取一个许可，所以可运行线程数量为20/1=20 test(threadnum); semaphore.release();// 释放一个许可 } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); System.out.println(\u0026#34;finish\u0026#34;); } //拿了通行证之后，处理2s钟后才释放 public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 } } //另一个例子\npublic static void main(String[] args) throws InterruptedException{ AtomicInteger atomicInteger=new AtomicInteger(); ExecutorService executorService = Executors.newCachedThreadPool(); Semaphore semaphore=new Semaphore(3); for(int i=0;i\u0026lt;8;i++) { int finalI = i; executorService.submit(()-\u0026gt;{ try { semaphore.acquire(); int i1 = atomicInteger.incrementAndGet(); log.info(\u0026#34;获取一个通行证\u0026#34;+ finalI); TimeUnit.SECONDS.sleep(finalI+1); } catch (InterruptedException e) { e.printStackTrace(); }finally { log.info(\u0026#34;通行证\u0026#34;+ finalI +\u0026#34;释放完毕\u0026#34;); semaphore.release(); } }); } log.info(\u0026#34;全部获取完毕\u0026#34;); //这个方法不会导致线程立即结束 executorService.shutdown(); log.info(\u0026#34;线程池shutdown\u0026#34;); } /* 结果 2022-12-01 14:21:31 下午 [Thread: pool-1-thread-3] INFO:获取一个通行证2 2022-12-01 14:21:31 下午 [Thread: main] INFO:全部获取完毕 2022-12-01 14:21:31 下午 [Thread: main] INFO:线程池shutdown 2022-12-01 14:21:31 下午 [Thread: pool-1-thread-2] INFO:获取一个通行证1 2022-12-01 14:21:31 下午 [Thread: pool-1-thread-1] INFO:获取一个通行证0 2022-12-01 14:21:32 下午 [Thread: pool-1-thread-1] INFO:通行证0释放完毕 2022-12-01 14:21:32 下午 [Thread: pool-1-thread-4] INFO:获取一个通行证3 2022-12-01 14:21:33 下午 [Thread: pool-1-thread-2] INFO:通行证1释放完毕 2022-12-01 14:21:33 下午 [Thread: pool-1-thread-5] INFO:获取一个通行证4 2022-12-01 14:21:34 下午 [Thread: pool-1-thread-3] INFO:通行证2释放完毕 2022-12-01 14:21:34 下午 [Thread: pool-1-thread-6] INFO:获取一个通行证5 2022-12-01 14:21:36 下午 [Thread: pool-1-thread-4] INFO:通行证3释放完毕 2022-12-01 14:21:36 下午 [Thread: pool-1-thread-7] INFO:获取一个通行证6 2022-12-01 14:21:38 下午 [Thread: pool-1-thread-5] INFO:通行证4释放完毕 2022-12-01 14:21:38 下午 [Thread: pool-1-thread-8] INFO:获取一个通行证7 2022-12-01 14:21:40 下午 [Thread: pool-1-thread-6] INFO:通行证5释放完毕 2022-12-01 14:21:43 下午 [Thread: pool-1-thread-7] INFO:通行证6释放完毕 2022-12-01 14:21:46 下午 [Thread: pool-1-thread-8] INFO:通行证7释放完毕 Process finished with exit code 0 如上所示，先是获取了210，之后释放一个获取一个(最多获取3个)， 3+n*2 =10 ，之后陆续释放0获取3，释放1获取4，释放2获取5 之后 释放3获取6，释放4获取7； 这是还有5,7,6拿着通行证 之后随机将5，7，6释放掉即可。 */ //如上，shutdown不会立即停止，而是：\n线程池shutdown之后不再接收新任务\nsutdown只是将线程池的状态设置为SHUTWDOWN状态，正在执行的任务会继续执行下去，没有被执行的则中断。而shutdownNow则是将线程池的状态设置为STOP，正在执行的任务则被停止，没被执行任务的则返回。如果是shutdownNow,则会报这个问题\njava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at java.lang.Thread.sleep(Thread.java:340) at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386) at com.ly.SemaphoreExample2.lambda$main$0(SemaphoreExample2.java:45) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 解释最上面的例子：\n执行acquire()方法会导致阻塞，直到有一个许可证可以获得然后拿走一个许可证 每个release()方法增加一个许可证，这**可能会释放一个阻塞的acquire()**方法 Semaphore只是维持了一个可以获得许可证的数量，没有实际的许可证这个对象 Semaphore经常用于限制获取某种资源的线程数量 可以一次性获取或释放多个许可，不过没必要\nsemaphore.acquire(5);// 获取5个许可，所以可运行线程数量为20/5=4 test(threadnum); semaphore.release(5);// 释放5个许可 除了 acquire() 方法之外，另一个比较常用的与之对应的方法是 tryAcquire() 方法，该方法如果获取不到许可就立即返回 false\n介绍\nsynchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，而Semaphore(信号量)可以用来控制同时访问特定资源的线程数量。\nSemaphore 的使用简单，我们这里假设有 N(N\u0026gt;5) 个线程来获取 Semaphore 中的共享资源，下面的代码表示同一时刻 N 个线程中只有 5 个线程能获取到共享资源，其他线程都会阻塞，只有获取到共享资源的线程才能执行。等到有线程释放了共享资源，其他阻塞的线程才能获取到。\n// 初始共享资源数量 final Semaphore semaphore = new Semaphore(5); // 获取1个许可 semaphore.acquire(); // 释放1个许可 semaphore.release(); /* 当初始的资源个数为 1 的时候，Semaphore 退化为排他锁。 */ Semaphore有两种模式，公平模式和非公平模式\n公平模式：调用acquire()方法的顺序，就是获取许可证的顺序，遵循FIFO 非公平模式：抢占式的 两个构造函数，必须提供许可数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式\npublic Semaphore(int permits) { sync = new NonfairSync(permits); } public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits); } Semaphore 通常用于那些资源有明确访问数量限制的场景比如限流（仅限于单机模式，实际项目中推荐使用 Redis +Lua 来做限流）\n原理\nSemaphore 是共享锁的一种实现，它默认构造 AQS 的 state 值为 permits，你可以将 permits 的值理解为许可证的数量，只有拿到许可证的线程才能执行。\n调用semaphore.acquire() ，线程尝试获取许可证，如果 state \u0026gt;= 0 的话，则表示可以获取成功。如果获取成功的话，使用 CAS 操作去修改 state 的值 state=state-1。如果 state\u0026lt;0 的话，则表示许可证数量不足。此时会创建一个 Node 节点加入阻塞队列，挂起当前线程。\n/** * 获取1个许可证 */ public void acquire() throws InterruptedException { sync.acquireSharedInterruptibly(1); } /** * 共享模式下获取许可证，获取成功则返回，失败则加入阻塞队列，挂起线程 */ public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); // 尝试获取许可证，arg为获取许可证个数，当可用许可证数减当前获取的许可证数结果小于0,则创建一个节点加入阻塞队列，挂起当前线程。 if (tryAcquireShared(arg) \u0026lt; 0) doAcquireSharedInterruptibly(arg); } 调用semaphore.release(); ，线程尝试释放许可证，并使用 CAS 操作去修改 state 的值 state=state+1。释放许可证成功之后，同时会唤醒同步队列中的一个线程。被唤醒的线程会重新尝试去修改 state 的值 state=state-1 ，如果 state\u0026gt;=0 则获取令牌成功，否则重新进入阻塞队列，挂起线程。\n// 释放一个许可证 public void release() { sync.releaseShared(1); } // 释放共享锁，同时会唤醒同步队列中的一个线程。 public final boolean releaseShared(int arg) { //释放共享锁 if (tryReleaseShared(arg)) { //唤醒同步队列中的一个线程 doReleaseShared(); return true; } return false; } 补充\nSemaphore 与 CountDownLatch 一样，也是共享锁的一种实现。它默认构造 AQS 的 state 为 permits。当执行任务的线程数量超出 permits，那么多余的线程将会被放入阻塞队列 Park,并自旋判断 state 是否大于 0。只有当 state 大于 0 的时候，阻塞的线程才能继续执行,此时先前执行任务的线程继续执行 release() 方法，release() 方法使得 state 的变量会加 1，那么自旋的线程便会判断成功。 如此，每次只有最多不超过 permits 数量的线程能自旋成功，便限制了执行任务线程的数量。\nCountDownLatch(倒计时) # CountDown 倒计时器；Latch 门闩 允许count个线程阻塞在一个地方，直至所有线程的任务都执行完毕 CountDownLatch 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 CountDownLatch 使用完毕后，它不能再次被使用。 原理 CountDownLatch是共享锁的一种实现（我的理解是 本质上是说AQS内部的state变量可以被多个线程同时修改，所以是\u0026quot;共享\u0026quot;），默认构造AQS的state值为count。当线程使用countDown()方法时，其实是使用了tryReleaseShared方法以CAS操作来减少state，直至state为0 当调用await()方法时，如果state不为0，那就证明任务还没有执行完毕,await()方法会一直阻塞，即await()方法之后的语句不会被执行。之后CountDownLatch会自旋CAS判断state==0，如果state == 0就会释放所有等待线程，await()方法之后的语句得到执行 CountDownLatch的两种典型用法 # 其实就是n个线程等待其他m个线程执行完毕后唤醒，只有n为1时是第一种情况，只有m为1时是第二种情况\n某线程在开始运行前等待n个线程执行完毕\n将 CountDownLatch 的计数器初始化为 n （new CountDownLatch(n)），每当一个任务线程执行完毕，就将计数器减 1 （countdownlatch.countDown()），当计数器的值变为 0 时，在 CountDownLatch 上 await() 的线程就会被唤醒。一个典型应用场景就是启动一个服务时，主线程需要等待多个组件加载完毕，之后再继续执行。\n实现多个线程开始执行任务的最大并行性\n注意是并行性，不是并发，强调的是多个线程在某一时刻同时开始执行。类似于赛跑，将多个线程放到起点，等待发令枪响，然后同时开跑。\n做法是初始化一个共享的 CountDownLatch 对象，将其计数器初始化为 1 （new CountDownLatch(1)），多个线程在开始执行任务前首先 coundownlatch.await()，当主线程调用 countDown() 时，计数器变为 0，多个线程同时被唤醒。\nCountDownLatch使用示例\n300个线程(说的是线程池有300个核心线程，而不是CountDown300次)，550个请求（及count = 550）。启动线程后，主线程阻塞。当所有请求都countDown，主线程恢复运行\n/** * * @author SnailClimb * @date 2018年10月1日 * @Description: CountDownLatch 使用方法示例 */ public class CountDownLatchExample1 { // 请求的数量 private static final int threadCount = 550; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（如果这里线程池的线程数量给太少的话你会发现执行的很慢） ExecutorService threadPool = Executors.newFixedThreadPool(300); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -\u0026gt; {// Lambda 表达式的运用 try { test(threadnum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } finally { countDownLatch.countDown();// 表示一个请求已经被完成 } }); } countDownLatch.await(); threadPool.shutdown(); System.out.println(\u0026#34;finish\u0026#34;); } public static void test(int threadnum) throws InterruptedException { Thread.sleep(1000);// 模拟请求的耗时操作 System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum); Thread.sleep(1000);// 模拟请求的耗时操作 } } 与CountDownLatch的第一次交互是主线程等待其他线程\n主线程必须在启动其他线程后立即调用CountDownLatch.await()方法，这样主线程的操作就会在这个方法阻塞，直到其他线程完成各自任务\n其他 N 个线程必须引用闭锁对象（说的是CountDownLoatch对象），因为他们需要通知 CountDownLatch 对象，他们已经完成了各自的任务。这种通知机制是通过 CountDownLatch.countDown()方法来完成的；每调用一次这个方法，在构造函数中初始化的 count 值就减 1。所以当 N 个线程都调 用了这个方法，count 的值等于 0，然后主线程就能通过 await()方法，恢复执行自己的任务。\nCountDownLatch 的 await() 方法使用不当很容易产生死锁，比如我们上面代码中的 for 循环改为：\nfor (int i = 0; i \u0026lt; threadCount-1; i++) { ....... } //这样就导致 count 的值没办法等于 0（最终为1），然后就会导致一直等待。 CountDownLatch 的不足 # CountDownLatch 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 CountDownLatch 使用完毕后，它不能再次被使用。\nCountDownLatch 相常见面试题（改版后没了） # CountDownLatch 怎么用？应用场景是什么？ CountDownLatch 和 CyclicBarrier 的不同之处？ CountDownLatch 类中主要的方法？ CyclicBarrier # CyclicBarrier和CountDownLatch类似，可以实现线程间的技术等待，主要应用场景和CountDownLatch类似，但更复杂强大 主要应用场景和 CountDownLatch 类似。\nCountDownLatch基于AQS，而CycliBarrier基于ReentrantLock（ReentrantLock属于AQS同步器）和Condition\nCyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是：让一组线程(中的一个)到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。\n原理 # CyclicBarrier 内部通过一个 count 变量作为计数器，count 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减 1。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务（之后再释放所有阻塞的线程）。\n//每次拦截的线程数 private final int parties; //计数器 private int count; CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await() 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。\npublic CyclicBarrier(int parties) { this(parties, null); } public CyclicBarrier(int parties, Runnable barrierAction) { if (parties \u0026lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; } 先看一个例子\n/** * * @author Snailclimb * @date 2018年10月1日 * @Description: 测试 CyclicBarrier 类中带参数的 await() 方法 */ public class CyclicBarrierExample2 { // 请求的数量 private static final int threadCount = 550; // 需要同步的线程数量 private static final CyclicBarrier cyclicBarrier = new CyclicBarrier(5, () -\u0026gt; { System.out.println(\u0026#34;------当线程数达到之后，优先执行------\u0026#34;); }); public static void main(String[] args) throws InterruptedException { // 创建线程池 ExecutorService threadPool = Executors.newFixedThreadPool(10); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadNum = i; Thread.sleep(1000); ///注意这行 threadPool.execute(() -\u0026gt; { try { test(threadNum); } catch (InterruptedException e) { // TODO Auto-generated catch block e.printStackTrace(); } catch (BrokenBarrierException e) { // TODO Auto-generated catch block e.printStackTrace(); } }); } threadPool.shutdown(); } public static void test(int threadnum) throws InterruptedException, BrokenBarrierException { System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum + \u0026#34;is ready\u0026#34;); try { /**等待60秒，保证子线程完全执行结束*/ //如果等待的时间，超过了60秒，那么就会抛出异常，而且还会进行重置(变为0个线程再等待) cyclicBarrier.await(60, TimeUnit.SECONDS); //最后一个(第5个到达后，count会重置为0) } catch (Exception e) { System.out.println(\u0026#34;-----CyclicBarrierException------\u0026#34;); } System.out.println(\u0026#34;threadnum:\u0026#34; + threadnum + \u0026#34;is finish\u0026#34;); } } /* 结果 threadnum:0is ready threadnum:1is ready threadnum:2is ready threadnum:3is ready threadnum:4is ready threadnum:4is finish threadnum:0is finish threadnum:1is finish threadnum:2is finish threadnum:3is finish threadnum:5is ready threadnum:6is ready threadnum:7is ready threadnum:8is ready threadnum:9is ready threadnum:9is finish threadnum:5is finish threadnum:8is finish threadnum:7is finish threadnum:6is finish ...... */ /* 1.可以看到当线程数量也就是请求数量达到我们定义的 5 个的时候， await() 方法之后的方法才被执行。 2.另外，CyclicBarrier 还提供一个更高级的构造函数 CyclicBarrier(int parties, Runnable barrierAction)，用于在线程到达屏障时，优先执行 barrierAction，方便处理更复杂的业务场景。 */ //注意这里，如果把Thread.sleep(1000)去掉，顺序(情况之一)为： //也就是说，上面的代码，导致的现象：所有的ready都挤在一起了(而且不分先后，随时执行，而某5个的finish，会等待那5个的ready执行完才会执行，且finish没有顺序的) //★如上，ready也是没有顺序的 /*threadnum:0is ready threadnum:5is ready threadnum:9is ready threadnum:7is ready threadnum:3is ready threadnum:8is ready threadnum:4is ready threadnum:2is ready threadnum:1is ready threadnum:6is ready ------当线程数达到之后，优先执行------ 当ready数量为5的倍数时（栅栏是5个，就会执行这个） threadnum:3is finish threadnum:10is ready ------当线程数达到之后，优先执行------ threadnum:10is finish threadnum:11is ready threadnum:0is finish threadnum:5is finish threadnum:4is finish threadnum:1is finish threadnum:8is finish threadnum:12is ready threadnum:9is finish threadnum:7is finish threadnum:16is ready threadnum:15is ready ------当线程数达到之后，优先执行------ threadnum:14is ready threadnum:6is finish threadnum:13is ready threadnum:2is finish threadnum:19is ready threadnum:16is finish threadnum:12is finish threadnum:18is ready threadnum:11is finish threadnum:23is ready ------当线程数达到之后，优先执行------ threadnum:17is ready threadnum:19is finish threadnum:15is finish threadnum:25is ready threadnum:24is ready threadnum:18is finish threadnum:26is ready threadnum:13is finish threadnum:14is finish threadnum:23is finish threadnum:22is ready threadnum:21is ready threadnum:20is ready ------当线程数达到之后，优先执行------ threadnum:29is ready threadnum:28is ready threadnum:27is ready threadnum:22is finish threadnum:24is finish threadnum:25is finish threadnum:32is ready ..... */ 在看一个例子：\npublic class BarrierTest1 { public static void main(String[] args) throws InterruptedException, TimeoutException, BrokenBarrierException { CyclicBarrier cyclicBarrier = new CyclicBarrier(3); ExecutorService executorService = Executors.newFixedThreadPool(10); executorService.submit(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } try { cyclicBarrier.await( ); System.out.println(\u0026#34;数量11====\u0026#34;+cyclicBarrier.getNumberWaiting()); System.out.println(\u0026#34;111\u0026#34;); } catch (Exception e) { System.out.println(\u0026#34;数量异常1111===\u0026#34;+cyclicBarrier.getNumberWaiting()); // e.printStackTrace(); System.out.println(\u0026#34;报错1\u0026#34;); } }); executorService.submit(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } try { System.out.println(\u0026#34;数量2222====\u0026#34;+cyclicBarrier.getNumberWaiting()); cyclicBarrier.await(111,TimeUnit.SECONDS); System.out.println(\u0026#34;222\u0026#34;); } catch (Exception e) { System.out.println(\u0026#34;数量异常2222====\u0026#34;+cyclicBarrier.getNumberWaiting()); System.out.println(\u0026#34;报错2\u0026#34;); } }); executorService.submit(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } try { System.out.println(\u0026#34;数量33 await前====\u0026#34;+cyclicBarrier.getNumberWaiting()); cyclicBarrier.await(); System.out.println(\u0026#34;数量33 await后====\u0026#34;+cyclicBarrier.getNumberWaiting()); System.out.println(\u0026#34;333\u0026#34;); } catch (Exception e) { System.out.println(\u0026#34;数量异常333====\u0026#34;+cyclicBarrier.getNumberWaiting()); System.out.println(\u0026#34;报错3\u0026#34;); } }); } } /* 数量2222====1 数量33 await前====2 (第1、2个处于wait状态) 数量33 await后====0 （得到栅栏数量3，wait线程数重置为0） 333 数量11====0 （此时第1、2个线程都会释放，且数量重置为0） 111 222 */ CyclicBarrier源码分析 # 当调用CyclicBarrier对象调用await() 方法时，实际上调用的是dowait(false,0L )方法【主要用到false】\nawait() 方法就像树立起一个栅栏的行为一样，将线程挡住了，当拦住的线程数量达到 parties 的值时，栅栏才会打开，线程才得以通过执行。\npublic int await() throws InterruptedException, BrokenBarrierException { try { return dowait(false, 0L); } catch (TimeoutException toe) { throw new Error(toe); // cannot happen } } dowait(false,0L)方法\n// 当线程数量或者请求数量达到 count 时 await 之后的方法才会被执行。上面的示例中 count 的值就为 5。 private int count; /** * Main barrier code, covering the various policies. */ private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 锁住 lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); // 如果线程中断了，抛出异常 if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } // cout减1 //★前面锁住了，所以不需要CAS int index = --count; //★★ 当 count 数量减为 0 之后说明最后一个线程已经到达栅栏了，也就是达到了可以执行await 方法之后的条件 if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; // 将 count 重置为 parties 属性的初始化值 // 唤醒之前等待的线程 // 下一波执行开始 nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out for (;;) { try { if (!timed) trip.await(); else if (nanos \u0026gt; 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation \u0026amp;\u0026amp; ! g.broken) { breakBarrier(); throw ie; } else { // We\u0026#39;re about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \u0026#34;belong\u0026#34; to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed \u0026amp;\u0026amp; nanos \u0026lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); } } 总结：CyclicBarrier 内部通过一个 count 变量作为计数器，count 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减一。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务\nCyclicBarrier和CountDownLatch区别 # CountDownLatch 是计数器，只能使用一次，而 CyclicBarrier 的计数器提供 reset 功能，可以多次使用。\n从jdk作者设计的目的来看，javadoc是这么描述他们的\nCountDownLatch: A synchronization aid that allows one or more threads to wait until a set of operations being performed in other threads completes.(CountDownLatch: 一个或者多个线程，等待其他多个线程完成某件事情之后才能执行；) CyclicBarrier : A synchronization aid that allows a set of threads to all wait for each other to reach a common barrier point.(CyclicBarrier : 多个线程互相等待，直到到达同一个同步点，再继续一起执行。)\n需要结合上面的代码示例，CyclicBarrier示例是这个意思\n对于 CountDownLatch 来说，重点是“一个线程（多个线程）等待”，而其他的 N 个线程在完成“某件事情”之后，可以终止，也可以等待。【强调的是某个(组)等另一组线程完成】\n而对于 CyclicBarrier，重点是多个线程，在任意一个线程没有完成，所有的线程都必须等待。【强调的是互相】\nCountDownLatch 是计数器，线程完成一个记录一个，只不过计数不是递增而是递减，而 CyclicBarrier 更像是一个阀门，需要所有线程都到达，阀门才能打开，然后继续执行。\nReentrantLock和ReentrantReadWriteLock # 读写锁 ReentrantReadWriteLock 可以保证多个线程可以同时读，所以在读操作远大于写操作的时候，读写锁就非常有用了。\n"},{"id":139,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0307lyconcurrent-collections/","title":"java常见并发容器","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nJDK提供的容器，大部分在java.util.concurrent包中\nConcurrentHashMap：线程安全的HashMap CopyOnWriteArrayList：线程安全的List，在读多写少的场合性能非常好，远好于Vector ConcurrentLinkedQueue：高效的并发队列，使用链表实现，可以看作一个线程安全的LinkedList，是一个非阻塞队列 BlockingQueue：这是一个接口，JDK内部通过链表、数组等方式实现了该接口。表示阻塞队列，非常适合用于作为数据共享的通道 ConcorrentSkipListMap：跳表的实现，是一个Map，使用跳表的数据结构进行快速查找 ConcurrentHashMap # HashMap是线程不安全的，并发场景下要保证线程安全，可以使用Collections.synchronizedMap()方法来包装HashMap，但这是通过使用一个全局的锁来同步不同线程间的并发访问，因此会带来性能问题 建议使用ConcurrentHashMap，不论是读操作还是写操作都能保证高性能：读操作（几乎）不需要加锁，而写操作时通过锁分段(这里说的是JDK1.7？)技术，只对所操作的段加锁而不影响客户端对其他段的访问 CopyOnWriteArrayList # //源码 public class CopyOnWriteArrayList\u0026lt;E\u0026gt; extends Object implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, Serializable 在很多应用场景中，读操作可能会远远大于写操作 我们应该允许多个线程同时访问List内部数据（针对读） 与ReentrantReadWriteLock读写锁思想非常类似，即读读共享、写写互斥、读写互斥、写读互斥 不一样的是，CopyOnWriteArrayList读取时完全不需要加锁，且写入也不会阻塞读取操作，只有写入和写入之间需要同步等待。 CopyOnWriteArrayList是如何做到的 # CopyOnWriteArrayList 类的所有可变操作（add，set 等等）都是通过创建底层数组的新副本来实现的。当 List 需要被修改的时候，并不修改原有内容，而是对原有数据进行一次复制，将修改的内容写入副本。写完之后，再将修改完的副本替换原来的数据，这样就可以保证写操作不会影响读操作了。 从 CopyOnWriteArrayList 的名字就能看出 CopyOnWriteArrayList 是满足 CopyOnWrite 的 在计算机，如果你想要对一块内存进行修改时，我们不在原有内存块中进行写操作，而是将内存拷贝一份，在新的内存中进行写操作，写完之后呢，就将指向原来内存指针指向新的内存(注意，是指向，而不是重新拷贝★重要★)，原来的内存就可以被回收掉了 CopyOnWriteArrayList 读取和写入源码简单分析 # CopyOnWriteArrayList读取操作的实现 读取操作没有任何同步控制和锁操作，理由就是内部数组array不会发生修改，只会被另一个array替换，因此可以保证数据安全\n/** The array, accessed only via getArray/setArray. */ private transient volatile Object[] array; public E get(int index) { return get(getArray(), index); } @SuppressWarnings(\u0026#34;unchecked\u0026#34;) private E get(Object[] a, int index) { return (E) a[index]; } final Object[] getArray() { return array; } CopyOnWriteArrayList写入操作的实现 在添加集合的时候加了锁，保证同步，避免多线程写的时候会copy出多个副本\n/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return {@code true} (as specified by {@link Collection#add}) */ public boolean add(E e) { final ReentrantLock lock = this.lock; lock.lock();//加锁 try { Object[] elements = getArray(); int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len + 1);//拷贝新数组 newElements[len] = e; setArray(newElements); return true; } finally { lock.unlock();//释放锁 } } ConcurrentLinkedQueue # Java提供的线程安全的Queue分为阻塞队列和非阻塞队列 阻塞队列的典型例子是BlockingQueue，非阻塞队列的典型例子是ConcurrentLinkedQueue 阻塞队列通过锁来实现，非阻塞队列通过CAS实现 ConcurrentLinkedQueue使用链表作为数据结构，是高并发环境中性能最好的队列 ConcurrentLinkedQueue 适合在对性能要求相对较高，同时对队列的读写存在多个线程同时进行的场景，即如果对队列加锁的成本较高则适合使用无锁的 ConcurrentLinkedQueue，即CAS 来替代 BlockingQueue # 阻塞队列（BlockingQueue）被广泛使用在“生产者-消费者”问题中，其原因是 BlockingQueue 提供了可阻塞的插入和移除的方法。当队列容器已满，生产者线程会被阻塞，直到队列未满；当队列容器为空时，消费者线程会被阻塞，直至队列非空时为止\nBlockingQueue是一个接口，继承自Queue，而Queue又继承自Collection接口，下面是BlockingQueue的相关实现类：\n代码例子（主要是**put()和take()**两个方法）：\npublic class TestBlockingQueue { public static void main(String[] args) { BlockingQueue\u0026lt;String\u0026gt; blockingQueue = new ArrayBlockingQueue\u0026lt;\u0026gt;(2); for (int i = 10; i \u0026lt; 20; i++) { int finalI = i; new Thread(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(finalI); } catch (InterruptedException e) { e.printStackTrace(); } try { blockingQueue.put(finalI + \u0026#34;\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() .getName() + \u0026#34;放入了元素[\u0026#34; + finalI + \u0026#34;\u0026#34;); }, \u0026#34;线程\u0026#34; + i).start(); } for (int i = 20; i \u0026lt; 30; i++) { int finalI = i; new Thread(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(finalI); } catch (InterruptedException e) { e.printStackTrace(); } String remove = null; try { remove = blockingQueue.take(); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() .getName() + \u0026#34;取出了元素[\u0026#34; + remove + \u0026#34;\u0026#34;); }, \u0026#34;线程\u0026#34; + i).start(); } } } /* 由下可以知道，放入了两个元素之后，需要等待取出后，才能继续放入 线程10放入了元素[10 线程11放入了元素[11 ----\u0026gt; 之后这里发生了停顿 线程20取出了元素[10 线程12放入了元素[12 线程21取出了元素[11 线程13放入了元素[13 线程22取出了元素[12 线程14放入了元素[14 线程23取出了元素[13 线程15放入了元素[15 线程24取出了元素[14 线程16放入了元素[16 线程25取出了元素[15 线程17放入了元素[17 线程26取出了元素[16 线程18放入了元素[18 线程27取出了元素[17 线程19放入了元素[19 线程28取出了元素[18 线程29取出了元素[19 Process finished with exit code 0 */ ArrayBockingQueue # ArrayBlockingQueue是BlockingQueue接口的有界队列实现类，底层采用数组来实现\npublic class ArrayBlockingQueue\u0026lt;E\u0026gt; extends AbstractQueue\u0026lt;E\u0026gt; implements BlockingQueue\u0026lt;E\u0026gt;, Serializable{} ArrayBlockingQueue 一旦创建，容量不能改变。其并发控制采用可重入锁 ReentrantLock ，不管是插入操作还是读取操作，都需要获取到锁才能进行操作。当队列容量满时，尝试将元素放入队列将导致操作阻塞;尝试从一个空队列中取一个元素也会同样阻塞。\nArrayBlockingQueue 默认情况下不能保证线程访问队列的公平性，所谓公平性是指严格按照线程等待的绝对时间顺序，即最先等待的线程能够最先访问到 ArrayBlockingQueue。而非公平性则是指访问 ArrayBlockingQueue 的顺序不是遵守严格的时间顺序，有可能存在，当 ArrayBlockingQueue 可以被访问时，长时间阻塞的线程依然无法访问到 ArrayBlockingQueue。如果保证公平性，通常会降低吞吐量。如果需要获得公平性的 ArrayBlockingQueue，可采用如下代码(主要是第二个参数)\nprivate static ArrayBlockingQueue\u0026lt;Integer\u0026gt; blockingQueue = new ArrayBlockingQueue\u0026lt;Integer\u0026gt;(10,true); LinkedBlockingQueue # 底层基于单向链表实现阻塞队列，可以当作无界队列也可以当作有界队列\n满足FIFO特性，与ArrayBlockingQueue相比有更高吞吐量，为防止LinkedBlockingQueue容量迅速增加，损耗大量内存，一般创建LinkedBlockingQueue对象时会指定大小****；如果未指定则容量等于Integer.MAX_VALUE\n相关构造方法\n/** *某种意义上的无界队列 * Creates a {@code LinkedBlockingQueue} with a capacity of * {@link Integer#MAX_VALUE}. */ public LinkedBlockingQueue() { this(Integer.MAX_VALUE); } /** *有界队列 * Creates a {@code LinkedBlockingQueue} with the given (fixed) capacity. * * @param capacity the capacity of this queue * @throws IllegalArgumentException if {@code capacity} is not greater * than zero */ public LinkedBlockingQueue(int capacity) { if (capacity \u0026lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node\u0026lt;E\u0026gt;(null); } PriorityBlockingQueue # 支持优先级的无界阻塞队列，默认情况元素采用自然顺序进行排序，或通过自定义类实现compareTo()方法指定元素排序，或初始化时通过构造器参数Comparator来指定排序规则 PriorityBlockingQueue 并发控制采用的是可重入锁 ReentrantLock，队列为无界队列（ArrayBlockingQueue 是有界队列，LinkedBlockingQueue 也可以通过在构造函数中传入 capacity 指定队列最大的容量，但是 PriorityBlockingQueue 只能指定初始的队列大小，后面插入元素的时候，如果空间不够的话会自动扩容） 它就是 PriorityQueue 的线程安全版本。不可以插入 null 值，同时，插入队列的对象必须是可比较大小的（comparable），否则报 ClassCastException 异常。它的插入操作 put 方法不会 block(是block 阻塞，不是lock 锁)，因为它是无界队列（take 方法在队列为空的时候会阻塞） ConcurrentSkipListMap # 对于一个单链表，即使链表是有序的，如果我们想要在其中查找某个数据，也只能从头到尾遍历链表，这样效率自然就会很低，跳表就不一样了。跳表是一种可以用来快速查找的数据结构，有点类似于平衡树。它们都可以对元素进行快速的查找。但一个重要的区别是：对平衡树的插入和删除往往很可能导致平衡树进行一次全局的调整。而对跳表的插入和删除只需要对整个数据结构的局部进行操作即可。这样带来的好处是：在高并发的情况下，你会需要一个全局锁来保证整个平衡树的线程安全。而对于跳表，你只需要部分锁即可。这样，在高并发环境下，你就可以拥有更好的性能。而就查询的性能而言，跳表的时间复杂度也是 O(logn) 所以在并发数据结构中，JDK 使用跳表来实现一个 Map。\n跳表的本质是维护多个链表，且链表是分层的 最低层的链表维护跳表内所有元素，每上面一层链表都是下面一层的子集 跳表内所有链表的元素都是排序的 查找时，可以从顶级链表开始找。一旦发现被查找的元素大于当前链表中的取值（这里应该加上一句，小于前一个节点，比如下面如果是查找3，那么就从1跳下去），就会转入下一层链表继续找。这也就是说在查找过程中，搜索是跳跃式的。如上图所示，在跳表中查找元素 18。 查找 18 的时候原来需要遍历 18 次，现在只需要 7 次即可。针对链表长度比较大的时候，构建索引查找效率的提升就会非常明显 （这里好像不太对，原来也不需要遍历18次,反正大概率是说效率高就是了）\n使用跳表实现 Map 和使用哈希算法实现 Map 的另外一个不同之处是：哈希并不会保存元素的顺序，而跳表内所有的元素都是排序的。因此在对跳表进行遍历时，你会得到一个有序的结果。所以，如果你的应用需要有序性，那么跳表就是你不二的选择。JDK 中实现这一数据结构的类是 ConcurrentSkipListMap。\n"},{"id":140,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0306lythread-pool-best/","title":"线程池最佳实践","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n线程池知识回顾 # 1. 为什么要使用线程池 # 池化技术的思想，主要是为了减少每次获取资源（线程资源）的消耗，提高对资源的利用率 线程池提供了一种限制和管理资源（包括执行一个任务）的方法，每个线程池还维护一些基本统计信息，例如已完成任务的数量 好处：\n降低资源消耗 提高响应速度 提高线程的可管理性 2. 线程池在实际项目的使用场景 # 线程池一般用于执行多个不相关联的耗时任务，没有多线程的情况下，任务顺序执行，使用了线程池的话可让多个不相关联的任务同时执行。\n3. 如何使用线程池 # 一般是通过 ThreadPoolExecutor 的构造函数来创建线程池，然后提交任务给线程池执行就可以了。构造函数如下：\n/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 使用代码：\nprivate static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i \u0026lt; 10; i++) { executor.execute(() -\u0026gt; { try { Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;CurrentThread name:\u0026#34; + Thread.currentThread().getName() + \u0026#34;date：\u0026#34; + Instant.now()); }); } //终止线程池 executor.shutdown(); try { /* awaitTermination()方法的作用: 当前线程阻塞，直到 1. 等所有已提交的任务（包括正在跑的和队列中等待的）执行完 2. 或者等超时时间到 3. 或者线程被中断，抛出InterruptedException 然后返回true（shutdown请求后所有任务执行完毕）或false（已超时） */ executor.awaitTermination(5, TimeUnit.SECONDS); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;Finished all threads\u0026#34;); } /*输出 CurrentThread name:pool-1-thread-5date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-3date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-1date：2020-06-06T11:45:31.636Z CurrentThread name:pool-1-thread-4date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-2date：2020-06-06T11:45:31.639Z CurrentThread name:pool-1-thread-2date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-4date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-1date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-3date：2020-06-06T11:45:33.656Z CurrentThread name:pool-1-thread-5date：2020-06-06T11:45:33.656Z Finished all threads */ 线程池最佳实践 # 1. 使用ThreadPoolExecutor的构造函数声明线程池 # 线程池必须手动通过 ThreadPoolExecutor 的构造函数来声明，避免使用Executors 类的 newFixedThreadPool 和 newCachedThreadPool ，因为可能会有 OOM 的风险。\nFixedThreadPool 和 SingleThreadExecutor ： 允许请求的队列长度为 Integer.MAX_VALUE,可能堆积大量的请求，从而导致 OOM。\nCachedThreadPool 和 ScheduledThreadPool ： 允许创建的线程数量为 Integer.MAX_VALUE ，可能会创建大量线程，从而导致 OOM。\n总结：使用有界队列，控制线程创建数量\n其他原因：\n实际中要根据自己机器的性能、业务场景来手动配置线程池参数，比如核心线程数、使用的任务队列、饱和策略 给线程池命名，方便定位问题 2. 监测线程池运行状态 # 可以通过一些手段检测线程池运行状态，比如SpringBoot中的Actuator组件\n或者利用ThreadPoolExecutor相关的API做简陋监控，ThreadPoolExecutor提供了获取线程池当前的线程数和活跃线程数、已经执行完成的任务数，正在排队中的任务数等\n简单的demo，使用ScheduleExecutorService定时打印线程池信息\n/** * 打印线程池的状态 * * @param threadPool 线程池对象 */ public static void printThreadPoolStatus(ThreadPoolExecutor threadPool) { ScheduledExecutorService scheduledExecutorService = new ScheduledThreadPoolExecutor(1, createThreadFactory(\u0026#34;print-images/thread-pool-status\u0026#34;, false)); scheduledExecutorService.scheduleAtFixedRate(() -\u0026gt; { log.info(\u0026#34;=========================\u0026#34;); log.info(\u0026#34;ThreadPool Size: [{}]\u0026#34;, threadPool.getPoolSize()); log.info(\u0026#34;Active Threads: {}\u0026#34;, threadPool.getActiveCount()); log.info(\u0026#34;Number of Tasks : {}\u0026#34;, threadPool.getCompletedTaskCount()); log.info(\u0026#34;Number of Tasks in Queue: {}\u0026#34;, threadPool.getQueue().size()); log.info(\u0026#34;=========================\u0026#34;); }, 0, 1, TimeUnit.SECONDS); } 3. 建议不同类别的业务用不同的线程池 # 建议是不同的业务使用不同的线程池，配置线程池的时候根据当前业务的情况对当前线程池进行配置，因为不同的业务的并发以及对资源的使用情况都不同，重心优化系统性能瓶颈相关的业务\n极端情况导致死锁：\n假如我们线程池的核心线程数为 n，父任务（扣费任务）数量为 n，父任务下面有两个子任务（扣费任务下的子任务），其中一个已经执行完成，另外一个被放在了任务队列中。由于父任务把线程池核心线程资源用完，所以子任务因为无法获取到线程资源无法正常执行，一直被阻塞在队列中。父任务等待子任务执行完成，而子任务等待父任务释放线程池资源，这也就造成了 \u0026ldquo;死锁\u0026rdquo;。\n4. 别忘记给线程池命名 # 初始化线程池时显示命名（设置线程池名称前缀），有利于定位问题\n利用guava的ThreadFactoryBuilder\nThreadFactory threadFactory = new ThreadFactoryBuilder() .setNameFormat(threadNamePrefix + \u0026#34;-%d\u0026#34;) .setDaemon(true).build(); ExecutorService threadPool = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, TimeUnit.MINUTES, workQueue, threadFactory) 自己实现ThreadFactor\nimport java.util.concurrent.Executors; import java.util.concurrent.ThreadFactory; import java.util.concurrent.atomic.AtomicInteger; /** * 线程工厂，它设置线程名称，有利于我们定位问题。 */ public final class NamingThreadFactory implements ThreadFactory { private final AtomicInteger threadNum = new AtomicInteger(); private final ThreadFactory delegate; private final String name; /** * 创建一个带名字的线程池生产工厂 */ public NamingThreadFactory(ThreadFactory delegate, String name) { this.delegate = delegate; this.name = name; // TODO consider uniquifying this } @Override public Thread newThread(Runnable r) { Thread t = delegate.newThread(r); t.setName(name + \u0026#34; [#\u0026#34; + threadNum.incrementAndGet() + \u0026#34;]\u0026#34;); return t; } } 5. 正确配置线程池参数 # 如果线程池中的线程太多，就会增加上下文切换的成本\n多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\n上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\nLinux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n过大跟过小都不行\n如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM 设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率 简单且适用面较广的公式\nCPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。\nI/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。\n如何判断是CPU密集任务还是IO密集任务\nCPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。\n美团线程池的处理 主要对线程池的核心参数实现自定义可配置\ncorePoolSize : 核心线程数线程数定义了最小可以同时运行的线程数量。 maximumPoolSize : 当队列中存放的任务达到队列容量的时候，当前可以同时运行的线程数量变为最大线程数。 workQueue: 当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中 参数动态配置 格外需要注意的是corePoolSize， 程序运行期间的时候，我们调用 setCorePoolSize（）这个方法的话，线程池会首先判断当前工作线程数是否大于corePoolSize，如果大于的话就会回收工作线程。【ThreadPoolExecutor里面的】 另外，你也看到了上面并没有动态指定队列长度的方法，美团的方式是自定义了一个叫做 ResizableCapacityLinkedBlockIngQueue 的队列（主要就是把LinkedBlockingQueue的 capacity 字段的 final 关键字修饰给去掉了，让它变为可变的） "},{"id":141,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0305lyjava-thread-pool/","title":"java线程池详解","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n一 使用线程池的好处 # 池化技术：减少每次获取资源的消耗，提高对资源的利用率 线程池提供一种限制和管理资源（包括执行一个任务）的方式，每个线程池还维护一些基本统计信息，例如已完成任务的数量 线程池的好处 降低资源消耗（重复利用，降低线程创建和销毁造成的消耗） 提高响应速度（任务到达直接执行，无需等待线程创建） 提高线程可管理性（避免无休止创建，使用线程池统一分配、调优、监控） 二 Executor框架 # Java5之后，通过Executor启动线程，比使用Thread的start方法更好，更易于管理，效率高，还能有助于避免this逃逸的问题\nthis逃逸，指的是构造函数返回之前，其他线程就持有该对象的引用，会导致调用尚未构造完全的对象\n例子：\npublic class ThisEscape { public ThisEscape() { new Thread(new EscapeRunnable()).start(); // ... } private class EscapeRunnable implements Runnable { @Override public void run() { // 通过ThisEscape.this就可以引用外围类对象, 但是此时外围类对象可能还没有构造完成, 即发生了外围类的this引用的逃逸 } } } 处理办法 //不要在构造函数中运行线程\npublic class ThisEscape { private Thread t; public ThisEscape() { t = new Thread(new EscapeRunnable()); // ... } public void init() { //也就是说对象没有构造完成前，不要调用ThisEscape.this即可 t.start(); } private class EscapeRunnable implements Runnable { @Override public void run() { // 通过ThisEscape.this就可以引用外围类对象, 此时可以保证外围类对象已经构造完成 } } } Executor框架不仅包括线程池的管理，提供线程工厂、队列以及拒绝策略。\nExecutor框架结构 # 主要是三大部分：任务（Runnable/Callable），任务的执行(Executor)，异步计算的结果Future\n任务 执行的任务需要的Runnable/Callable接口，他们的实现类，都可以被ThreadPoolExecutor或ScheduleThreadPoolExecutor执行\n任务的执行 我们更多关注的，是ThreadPoolExecutor类。另外，ScheduledThreadPoolExecutor类，继承了ThreadPoolExecutor类，并实现了ScheduledExecutorService接口\n//ThreadPoolExecutor类描述 //AbstractExecutorService实现了ExecutorService接口 public class ThreadPoolExecutor extends AbstractExecutorService{} //ScheduledThreadPoolExecutor类描述 //ScheduledExecutorService继承ExecutorService接口 public class ScheduledThreadPoolExecutor extends ThreadPoolExecutor implements ScheduledExecutorService {} 异步计算的结果 Future接口以及其实现类FutueTask类都可以代表异步计算的结果(下面就是Future接口) 当我们把Runnable接口（结果为null）或Callable接口的实现类提交给ThreadPoolExecutor或ScheduledThreadPoolExecutor执行（）\nExecutorService executorService = Executors.newCachedThreadPool(); Callable\u0026lt;MyClass\u0026gt; myClassCallable = new Callable\u0026lt;MyClass\u0026gt;() { public MyClass call() throws Exception { MyClass myClass1 = new MyClass(); myClass1.setName(\u0026#34;ly-callable-测试\u0026#34;); TimeUnit.SECONDS.sleep(2); return myClass1; } }; Future\u0026lt;?\u0026gt; submit = executorService.submit(myClassCallable); //这里会阻塞 Object o = submit.get(); log.info(\u0026#34;ly-callable-打印结果1:\u0026#34; + o); FutureTask\u0026lt;MyClass\u0026gt; futureTask = new FutureTask\u0026lt;\u0026gt;(() -\u0026gt; { MyClass myClass1 = new MyClass(); myClass1.setName(\u0026#34;ly-FutureTask-测试\u0026#34;); TimeUnit.SECONDS.sleep(2); return myClass1; }); Future\u0026lt;?\u0026gt; submit2 = executorService.submit(futureTask); //这里会阻塞 Object o2 = submit2.get(); log.info(\u0026#34;ly-callable-打印结果2:\u0026#34; + o2); executorService.shutdown(); /*结果 2022-11-09 10:19:10 上午 [Thread: main] INFO:ly-callable-打印结果1:MyClass(name=ly-callable-测试) 2022-11-09 10:19:12 上午 [Thread: main] INFO:ly-callable-打印结果2:null */ Executor框架的使用示意图 # 主线程首先要创建实现 Runnable 或者 Callable 接口的任务对象。\n把创建完成的实现 Runnable/Callable接口的 对象直接交给 ExecutorService 执行: ExecutorService.execute（Runnable command））或者也可以把 Runnable 对象或Callable 对象提交给 ExecutorService 执行（ExecutorService.submit（Runnable task）或 ExecutorService.submit（Callable \u0026lt;T\u0026gt; task））。\n如果执行 ExecutorService.submit（…），ExecutorService 将返回一个实现Future接口的对象（我们刚刚也提到过了执行 execute()方法和 submit()方法的区别，submit()会返回一个 FutureTask 对象）。由于 FutureTask 实现了 Runnable，我们也可以创建 FutureTask，然后直接交给 ExecutorService 执行（FutureTask实现了Runnable，不是一个Callable 所以直接使用future.get()获取的是null）。\npublic class MyMain { private byte[] x = new byte[10 * 1024 * 1024];//10M public static void main(String[] args) throws Exception { Callable\u0026lt;Object\u0026gt; abc = Executors.callable(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;aaa\u0026#34;);//输出aaa }, \u0026#34;abcccc\u0026#34;);//如果没有\u0026#34;abcccc\u0026#34;，则下面输出null FutureTask\u0026lt;Object\u0026gt; futureTask = new FutureTask\u0026lt;\u0026gt;(abc); /*new Thread(futureTask).start(); Object o = futureTask.get(); System.out.println(\u0026#34;获取值：\u0026#34;+o); //输出abc */ ExecutorService executorService = Executors.newSingleThreadExecutor(); Future\u0026lt;?\u0026gt; future = executorService.submit(futureTask); Future\u0026lt;?\u0026gt; future1 = executorService.submit(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;hello\u0026#34;; } }); /*System.out.println(future.get());//输出null*/ System.out.println(future1.get()); //输出hello //System.out.println(futureTask.get());//输出abcccc System.out.println(\u0026#34;阻塞结束\u0026#34;); executorService.shutdown(); } } 最后，主线程可以执行 FutureTask.get()方法来等待任务执行完成。主线程也可以执行 FutureTask.cancel（boolean mayInterruptIfRunning）来取消此任务的执行。\n三 (重要)ThreadPoolExecutor类简单介绍 # 线程池实现类 ThreadPoolExecutor 是 Executor 框架最核心的类。\nThreadPoolExecutor类分析 # 这里看最长的那个，其余三个都是在该构造方法的基础上产生，即给定某些默认参数的构造方法，比如默认的拒绝策略\n/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } ThreadPoolExecutor中，3个最重要的参数\ncorePoolSize：核心线程数，定义了最小可以同时运行的线程数量 maximumPoolSize：当队列中存放的任务达到队列容量时，当前可以同时运行的线程数量变为最大线程数 workQueue：当新任务来的时候，会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中 ThreadPoolExecutor其他常见参数\nkeepAliveTime：当线程池中的线程数量大于corePoolSize时，如果此时没有新任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待时间超过了keepAliveTime才会被回收销毁 unit：keepAliveTime参数的时间单位 threadFactory：executor创建新线程的时候会用到 handler：饱和策略 线程池各个参数的相互关系的理解\nThreadPoolExecutor饱和策略定义 如果当前同时运行的线程数量达到最大线程数量并且队列也已经被放满了任务时，ThreadPoolTaskExecutor定义了一些策略：\nThreadPoolExecutor.AbortPolicy ：抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy ：调用执行自己的线程运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy ：不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy ： 此策略将丢弃最早的未处理的任务请求。 举例： Spring 通过 ThreadPoolTaskExecutor 或者我们直接通过 ThreadPoolExecutor 的构造函数创建线程池的时候，当我们不指定 RejectedExecutionHandler 饱和策略的话来配置线程池的时候默认使用的是 ThreadPoolExecutor.AbortPolicy。在默认情况下，ThreadPoolExecutor 将抛出 RejectedExecutionException 来拒绝新来的任务 ，这代表你将丢失对这个任务的处理。 对于可伸缩的应用程序，建议使用 ThreadPoolExecutor.CallerRunsPolicy。当最大池被填满时，此策略为我们提供可伸缩队列。（这个直接查看 ThreadPoolExecutor 的构造函数源码就可以看出，比较简单的原因，这里就不贴代码了。\n推荐使用 ThreadPoolExecutor 构造函数创建线程池 # 阿里巴巴Java开发手册\u0026quot;并发处理\u0026quot;这一章节，明确指出，线程资源必须通过线程池提供，不允许在应用中自行显示创建线程\n原因：使用线程池的好处是减少在创建和销毁线程上所消耗的时间以及系统资源开销，解决资源不足的问题。如果不使用线程池，有可能会造成系统创建大量同类线程而导致消耗完内存或者“过度切换”的问题。也不允许使用Executors去创建，而是通过ThreadPoolExecutor构造方式\nExecutors返回线程池对象的弊端：\nFixedThreadPool和SingleThreadExecutor：允许请求的队列长度为Integer.MAV_VALUE 可能堆积大量请求，导致OOM CachedThreadPool和ScheduledThreadPool，允许创建的线程数量为Integer.MAX_VALUE 可能创建大量线程，从而导致OOM 创建线程的几种方法\n通过ThreadPoolExecutor构造函数实现（推荐） 通过Executors框架的工具类Executors来实现，我们可以创建三红类型的ThreadPoolExecutor FixedThreadPool、SingleThreadExecutor、CachedThreadPool 四 ThreadPoolExecutor使用+原理分析 # 示例代码：Runnable+ThreadPoolExecutor # 先创建一个Runnable接口的实现类\n//MyRunnable.java import java.util.Date; /** * 这是一个简单的Runnable类，需要大约5秒钟来执行其任务。 * @author shuang.kou */ public class MyRunnable implements Runnable { private String command; public MyRunnable(String s) { this.command = s; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; Start. Time = \u0026#34; + new Date()); processCommand(); System.out.println(Thread.currentThread().getName() + \u0026#34; End. Time = \u0026#34; + new Date()); } private void processCommand() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public String toString() { return this.command; } } 使用自定义的线程池\nimport java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, //5 MAX_POOL_SIZE, //10 KEEP_ALIVE_TIME, //1L TimeUnit.SECONDS, //单位 new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY),//100 new ThreadPoolExecutor.CallerRunsPolicy()); //主线程中运行 for (int i = 0; i \u0026lt; 10; i++) { //创建WorkerThread对象（WorkerThread类实现了Runnable 接口） Runnable worker = new MyRunnable(\u0026#34;\u0026#34; + i); //执行Runnable executor.execute(worker); } //终止线程池 executor.shutdown(); // isTerminated 判断所有提交的任务是否完成(保证之前调用过shutdown方法) while (!executor.isTerminated()) { } System.out.println(\u0026#34;Finished all threads\u0026#34;); } } //结果： /* corePoolSize: 核心线程数为 5。 maximumPoolSize ：最大线程数 10 keepAliveTime : 等待时间为 1L。 unit: 等待时间的单位为 TimeUnit.SECONDS。 workQueue：任务队列为 ArrayBlockingQueue，并且容量为 100; handler:饱和策略为 CallerRunsPolicy ---output--- pool-1-thread-3 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-3 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:47 CST 2020 ------ */ 线程池原理分析 # 如上，线程池首先会先执行 5 个任务，然后这些任务有任务被执行完的话，就会去拿新的任务执行\nexecute方法源码\n// 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount) private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } //任务队列 private final BlockingQueue\u0026lt;Runnable\u0026gt; workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中执行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 2.如果当前执行的任务数量大于等于 corePoolSize 的时候就会走到这里 // 通过 isRunning 方法判断线程池状态，线程池处于 RUNNING 状态并且队列可以加入任务，该任务才会被加入进去 if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { int recheck = ctl.get(); // 再次获取线程池状态，如果线程池状态不是 RUNNING 状态就需要从任务队列中移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。 if (!isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); // 如果当前线程池为空就新创建一个线程并执行。 else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 通过addWorker(command, false)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 //如果addWorker(command, false)执行失败，则通过reject()执行相应的拒绝策略的内容。 else if (!addWorker(command, false)) reject(command); } ------ 图示：\n源码\n// 全局锁，并发操作必备 private final ReentrantLock mainLock = new ReentrantLock(); // 跟踪线程池的最大大小，只有在持有全局锁mainLock的前提下才能访问此集合 private int largestPoolSize; // 工作线程集合，存放线程池中所有的（活跃的）工作线程，只有在持有全局锁mainLock的前提下才能访问此集合 private final HashSet\u0026lt;Worker\u0026gt; workers = new HashSet\u0026lt;\u0026gt;(); //获取线程池状态 private static int runStateOf(int c) { return c \u0026amp; ~CAPACITY; } //判断线程池的状态是否为 Running private static boolean isRunning(int c) { return c \u0026lt; SHUTDOWN; } /** * 添加新的工作线程到线程池 * @param firstTask 要执行 * @param core参数为true的话表示使用线程池的基本大小，为false使用线程池最大大小 * @return 添加成功就返回true否则返回false */ private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { //这两句用来获取线程池的状态 int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; ! (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; ! workQueue.isEmpty())) return false; for (;;) { //获取线程池中工作的线程的数量 int wc = workerCountOf(c); // core参数为false的话表明队列也满了，线程池大小变为 maximumPoolSize if (wc \u0026gt;= CAPACITY || wc \u0026gt;= (core ? corePoolSize : maximumPoolSize)) return false; //原子操作将workcount的数量加1 if (compareAndIncrementWorkerCount(c)) break retry; // 如果线程的状态改变了就再次执行上述操作 c = ctl.get(); if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } } // 标记工作线程是否启动成功 boolean workerStarted = false; // 标记工作线程是否创建成功 boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { // 加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { //获取线程池状态 int rs = runStateOf(ctl.get()); //rs \u0026lt; SHUTDOWN 如果线程池状态依然为RUNNING,并且线程的状态是存活的话，就会将工作线程添加到工作线程集合中 //(rs=SHUTDOWN \u0026amp;\u0026amp; firstTask == null)如果线程池状态小于STOP，也就是RUNNING或者SHUTDOWN状态下，同时传入的任务实例firstTask为null，则需要添加到工作线程集合和启动新的Worker // firstTask == null证明只新建线程而不执行任务 if (rs \u0026lt; SHUTDOWN || (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null)) { if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); //更新当前工作线程的最大容量 int s = workers.size(); if (s \u0026gt; largestPoolSize) largestPoolSize = s; // 工作线程是否启动成功 workerAdded = true; } } finally { // 释放锁 mainLock.unlock(); } //// 如果成功添加工作线程，则调用Worker内部的线程实例t的Thread#start()方法启动真实的线程实例 if (workerAdded) { t.start(); /// 标记线程启动成功 workerStarted = true; } } } finally { // 线程启动失败，需要从工作线程中移除对应的Worker if (! workerStarted) addWorkerFailed(w); } return workerStarted; } ------ 完整源码分析 https://www.throwx.cn/2020/08/23/java-concurrency-thread-pool-executor/\n对于代码中，进行分析：\n我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的 5 个任务中如果有任务被执行完了，线程池就会去拿新的任务执行。\n几个常见的对比 # Runnable VS Callable Runnable Java 1.0，不会返回结果或抛出检查异常\nCallable Java 1.5 可以\n工具类Executors可以实现，将Runnable对象转换成Callable对象( Executors.callable(Runnable task)或Executors.callable(Runnable task, Object result) )\n//Runnable @FunctionalInterface public interface Runnable { /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run(); } ------ //Callable @FunctionalInterface public interface Callable\u0026lt;V\u0026gt; { /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception; } execute() VS submit()\nexecute()方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功与否； submit()方法用于提交需要返回值的任务。线程池会返回一个 Future 类型的对象，通过这个 Future 对象可以判断任务是否执行成功，并且可以通过 Future 的 get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成，而使用 get（long timeout，TimeUnit unit）方法的话，如果在 timeout 时间内任务还没有执行完，就会抛出 java.util.concurrent.TimeoutException。 //真实使用，建议使用ThreadPoolExecutor构造方法 ExecutorService executorService = Executors.newFixedThreadPool(3); Future\u0026lt;String\u0026gt; submit = executorService.submit(() -\u0026gt; { try { Thread.sleep(5000L); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;abc\u0026#34;; }); String s = submit.get(); System.out.println(s); executorService.shutdown(); /* abc */ 使用抛异常的方法\nExecutorService executorService = Executors.newFixedThreadPool(3); Future\u0026lt;String\u0026gt; submit = executorService.submit(() -\u0026gt; { try { Thread.sleep(5000L); } catch (InterruptedException e) { e.printStackTrace(); } return \u0026#34;abc\u0026#34;; }); String s = submit.get(3, TimeUnit.SECONDS); System.out.println(s); executorService.shutdown(); /* 控制台输出 Exception in thread \u0026#34;main\u0026#34; java.util.concurrent.TimeoutException at java.util.concurrent.FutureTask.get(FutureTask.java:205) */ shutdown() VS shutdownNow() shutdown（） :关闭线程池，线程池的状态变为 SHUTDOWN。线程池不再接受新任务了，但是队列里的任务得执行完毕。 shutdownNow（） :关闭线程池，线程的状态变为 STOP。线程池会终止当前正在运行的任务，并停止处理排队的任务并返回正在等待执行的 List。 isTerminated() VS isshutdown()\nisShutDown 当调用 shutdown() 方法后返回为 true。 isTerminated 当调用 shutdown() 方法后，并且所有提交的任务完成后返回为 true callable+ThreadPoolExecutor示例代码 源代码 //MyCallable.java\nimport java.util.ArrayList; import java.util.Date; import java.util.List; import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.Callable; import java.util.concurrent.ExecutionException; import java.util.concurrent.Future; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class CallableDemo { private static final int CORE_POOL_SIZE = 5; private static final int MAX_POOL_SIZE = 10; private static final int QUEUE_CAPACITY = 100; private static final Long KEEP_ALIVE_TIME = 1L; public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME, TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); List\u0026lt;Future\u0026lt;String\u0026gt;\u0026gt; futureList = new ArrayList\u0026lt;\u0026gt;(); Callable\u0026lt;String\u0026gt; callable = new MyCallable(); for (int i = 0; i \u0026lt; 10; i++) { //提交任务到线程池 Future\u0026lt;String\u0026gt; future = executor.submit(callable); //将返回值 future 添加到 list，我们可以通过 future 获得 执行 Callable 得到的返回值 futureList.add(future); } for (Future\u0026lt;String\u0026gt; fut : futureList) { try { System.out.println(new Date() + \u0026#34;::\u0026#34; + fut.get()); } catch (InterruptedException | ExecutionException e) { e.printStackTrace(); } } //关闭线程池 executor.shutdown(); } } /* 运行结果 Wed Nov 13 13:40:41 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-5 Wed Nov 13 13:40:42 CST 2019::pool-1-thread-3 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-2 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-1 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-4 Wed Nov 13 13:40:43 CST 2019::pool-1-thread-5 ------ */ 几种常见的线程池详解 # FixedThreadPool 称之为可重用固定线程数的线程池，Executors类中源码：\n/** * 创建一个可重用固定数量线程的线程池 */ public static ExecutorService newFixedThreadPool(int nThreads, ThreadFactory threadFactory) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } //================或================ public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } 如上得知，新创建的FixedThreadPool的corePoolSize和maximumPoolSize都被设置为nThreads\n执行任务过程介绍 FixedThreadPool的execute()方法运行示意图\n上图分析 如果当前运行的线程数小于 corePoolSize， 如果再来新任务的话，就创建新的线程来执行任务； 当前运行的线程数等于 corePoolSize 后， 如果再来新任务的话，会将任务加入 LinkedBlockingQueue； 线程池中的线程执行完 手头的任务后，会在循环中反复从 LinkedBlockingQueue 中获取任务来执行； 为什么不推荐使用FixedThreadPool 主要原因，FixedThreadPool使用无界队列LinkedBlockingQueue（队列容量为Integer.MAX_VALUE)作为线程池的工作队列 线程池的线程数达到corePoolSize后，新任务在无界队列中等待，因此线程池中线程数不超过corePoolSize 由于使用无界队列时 maximumPoolSize 将是一个无效参数，因为不可能存在任务队列满的情况。所以，【不需要空闲线程，因为corePool，然后Queue，最后才是空闲线程】通过创建 FixedThreadPool的源码可以看出创建的 FixedThreadPool 的 corePoolSize 和 maximumPoolSize 被设置为同一个值。 又由于1、2原因，使用无界队列时，keepAliveTime将是无效参数 运行中的FixedThreadPool（如果未执行shutdown()或shutdownNow()）则不会拒绝任务，因此在任务较多时会导致OOM（内存溢出,Out Of Memory） SingleThreadExecutor\nSingleThreadExecutor是只有一个线程的线程池，源码：\n/** *返回只有一个线程的线程池 */ public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory)); } //另一种构造函数 public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;())); } 新创建的 SingleThreadExecutor 的 corePoolSize 和 maximumPoolSize 都被设置为 1.其他参数和 FixedThreadPool 相同\n执行过程 如果当前运行线程数少于corePoolSize（1），则创建一个新的线程执行任务；当前线程池有一个运行的线程后，将任务加入LinkedBlockingQueue；线程执行完当前的任务后，会在循环中反复从LinkedBlockingQueue中获取任务执行\n为什么不推荐使用SingleThreadExecutor SingleThreadExecutor使用无界队列LinkedBlockingQueue作为线程池的工作队列（容量为Integer.MAX_VALUE) 。SingleThreadExecutor使用无界队列作为线程池的工作队列会对线程池带来的影响与FixedThreadPoll相同，即导致OOM\nCachedThreadPool CachedThreadPool是一个会根据需要创建新线程的线程池，源码：\n/** * 创建一个线程池，根据需要创建新线程，但会在先前构建的线程可用时重用它。 */ public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } //其他构造函数 public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;()); } CachedThreadPool 的**corePoolSize 被设置为空（0），maximumPoolSize被设置为 Integer.MAX.VALUE，即它是无界的，这也就意味着如果主线程提交任务的速度高于 maximumPool 中线程处理任务的速度时，CachedThreadPool 会不断创建新的线程**。极端情况下，这样会导致耗尽 cpu 和内存资源\n★：SynchronousQueue队列只能容纳零个元素 执行过程（execute()示意图）\n上图说明：\n首先执行 SynchronousQueue.offer(Runnable task) 提交任务到任务队列。如果当前 maximumPool 中有闲线程正在执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)，那么主线程执行 offer 操作与空闲线程执行的 poll 操作配对成功，主线程把任务交给空闲线程执行，execute()方法执行完成，否则执行下面的步骤 2； 当初始 maximumPool 为空，或者 maximumPool 中没有空闲线程时，将没有线程执行 SynchronousQueue.poll(keepAliveTime,TimeUnit.NANOSECONDS)。这种情况下，步骤 1 将失败，此时 CachedThreadPool 会创建新线程执行任务，execute 方法执行完成； 不推荐使用CachedThreadPool? 因为它允许创建的线程数量为Integer.MAX_VALUE,可能创建大量线程，从而导致OOM\nScheduledThreadPoolExecutor详解 # 项目中基本不会用到，主要用来在给定的延迟后运行任务，或者定期执行任务 它使用的任务队列DelayQueue封装了一个PriorityQueue，PriorityQueue会对队列中的任务进行排序，执行所需时间（第一次执行的时间）短的放在前面先被执行(ScheduledFutureTask的time变量小的先执行)，如果一致则先提交的先执行(ScheduleFutureTask的sequenceNumber变量)\nScheduleFutureTask\n/** * 其中, triggerTime(initialDelay, unit) 的结果即上面说的time，说的应该是第一次执行的时间，而不是整个任务的执行时间 * @throws RejectedExecutionException {@inheritDoc} * @throws NullPointerException {@inheritDoc} * @throws IllegalArgumentException {@inheritDoc} */ public ScheduledFuture\u0026lt;?\u0026gt; scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) { if (command == null || unit == null) throw new NullPointerException(); if (period \u0026lt;= 0) throw new IllegalArgumentException(); ScheduledFutureTask\u0026lt;Void\u0026gt; sft = new ScheduledFutureTask\u0026lt;Void\u0026gt;(command, null, triggerTime(initialDelay, unit), unit.toNanos(period)); RunnableScheduledFuture\u0026lt;Void\u0026gt; t = decorateTask(command, sft); sft.outerTask = t; delayedExecute(t); return t; } 代码，TimerTask\n@Slf4j class MyTimerTask extends TimerTask{ @Override public void run() { log.info(\u0026#34;hello\u0026#34;); } } public class TimerTaskTest { public static void main(String[] args) { Timer timer = new Timer(); Calendar calendar = Calendar.getInstance(); calendar.set(Calendar.HOUR_OF_DAY, 17);//控制小时 calendar.set(Calendar.MINUTE, 1);//控制分钟 calendar.set(Calendar.SECOND, 0);//控制秒 Date time = calendar.getTime();//执行任务时间为17:01:00 //每天定时17:02执行操作，每5秒执行一次 timer.schedule(new MyTimerTask(), time, 5000 ); } } 代码，ScheduleThreadPoolExecutor\n@Slf4j public class ScheduleTask { public static void main(String[] args) throws InterruptedException { ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(3); scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { log.info(\u0026#34;hello world!\u0026#34;); } }, 3, 5, TimeUnit.SECONDS);//10表示首次执行任务的延迟时间，5表示每次执行任务的间隔时间，Thread.sleep(10000); System.out.println(\u0026#34;Shutting down executor...\u0026#34;); TimeUnit.SECONDS.sleep(4); //线程池一关闭，定时器就不会再执行 scheduledExecutorService.shutdown(); while (true){} } } /*结果 Shutting down executor... 2022-11-28 17:25:06 下午 [Thread: pool-1-thread-1] INFO:hello world! 不会再执行定时任务，因为线程池已经关了*/ ScheduleThreadPoolExecutor和Timer的比较 Timer对系统时钟变化敏感，ScheduledThreadPoolExecutor不是\nTimer使用的是System.currentTime()，而ScheduledThreadPoolExecutor使用的是System.nanoTime()\nTimer只有一个线程（导致长时间运行的任务延迟其他任务），ScheduleThreadPoolExecutor可以配置任意数量线程\nTimerTask中抛出运行时异常会杀死一个线程，从而导致Timer死机（即计划任务将不在运行）；而ScheduleThreadExecutor不仅捕获运行时异常，还允许需要时处理（afterExecute方法），抛出异常的任务会被取消而其他任务将继续运行\nJDK1.5 之后，没有理由再使用Timer进行任务调度\n运行机制 //下面这块内容后面更新后原作者删除了 ScheduledThreadPoolExecutor的执行分为：\n当调用scheduleAtFixedRate()或scheduleWithFixedDelay()方法时，会向ScheduleThreadPoolExector的DelayQueue添加一个实现了RunnableScheduleFuture接口的ScheduleFutureTask(私有内部类) 线程池中的线程从DelayQueue中获取ScheduleFutureTask，然后执行任务 为了执行周期性任务，对ThreadPoolExecutor做了如下修改：\n使用DelayQueue作为任务队列 获取任务的方式不同 获取周期任务后做了额外处理 获取任务，执行任务，修改任务(time)，回放(添加)任务\n线程 1 从 DelayQueue 中获取已到期的 ScheduledFutureTask（DelayQueue.take()）。到期任务是指 ScheduledFutureTask的 time 大于等于当前系统的时间； 线程 1 执行这个 ScheduledFutureTask； 线程 1 修改 ScheduledFutureTask 的 time 变量为下次将要被执行的时间； 线程 1 把这个修改 time 之后的 ScheduledFutureTask 放回 DelayQueue 中（DelayQueue.add())。 线程池大小确定 # 如果线程池中的线程太多，就会增加上下文切换的成本\n多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\n上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\nLinux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n过大跟过小都不行\n如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM 设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率 简单且适用面较广的公式\nCPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。\nI/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。\n如何判断是CPU密集任务还是IO密集任务\nCPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。\n"},{"id":142,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0304lyjmm/","title":"java内存模型","section":"并发","content":" 引用自https://github.com/Snailclimb/JavaGuide\n从CPU缓存模型说起 # redis是为了解决程序处理速度和访问常规关系型数据库速度不对等的问题，CPU缓存则是为了解决CPU处理速度和内存处理速度不对等的问题\n我们把内存看作外存的高速缓存，程序运行时把外存的数据复制到内存，由于内存的处理速度远高于外存，这样提高了处理速度\n总结，CPU Cache缓存的是内存数据，用于解决CPU处理速度和内存不匹配的问题，内存缓存的是硬盘数据用于解决硬盘访问速度过慢的问题 CPU Cache示意图：\nCPU Cache通常分为三层，分别叫L1，L2，L3 Cache 工作方式： 先复制一份数据到CPUCache中，当CPU需要用的时候就可以从CPUCache中读取数据，运算完成后，将运算得到的数据，写回MainMemory中，此时，会出现内存缓存不一致的问题，例子：执行了i++，如果两个线程同时执行，假设两个线程从CPUCach中读取的i=1，两个线程做了1++运算完之后再写回MainMemory，此时i=2 而正确结果为3\nCPU为了解决内存缓存不一致问题，可以通过制定缓存一致协议（比如MESI协议）或其他手段。这个缓存一致协议，指的是在 CPU 高速缓存与主内存交互的时候需要遵守的原则和规范 操作系统，通过内存模型MemoryModel定义一系列规范来解决这个问题\nJava内存模型 # 指令重排序 # 什么是指令重排序？ 简单来说就是系统在执行代码的时候并不一定是按照你写的代码的顺序依次执行\n指令重排有下面2种\n编译器优化重排：编译器（包括 JVM、JIT 编译器等）在不改变单线程程序语义的前提下，重新安排语句的执行顺序。 指令并行重排：现代处理器采用了指令级并行技术(Instruction-Level Parallelism，ILP)来将多条指令重叠执行。如果不存在数据依赖性，处理器可以改变语句对应机器指令的执行顺序 另外，内存系统也会有“重排序”，但又不是真正意义上的重排序。在 JMM 里表现为主存和本地内存的内容可能不一致，进而导致程序在多线程下执行可能出现问题。\n即Java源代码会经历 编译器优化重排\u0026mdash;\u0026gt;指令并行重排\u0026mdash;\u0026gt;内存系统重排，最终编程操作系统可执行的指令序列\n极其重要★：指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致，所以在多线程下指令重排可能导致一些问题\n编译器和处理器的指令重排序的处理方式不一样。对于编译器，通过禁止特定类型的编译器重排序的方式来禁止重排序。对于处理器，通过插入内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）的方式来禁止特定类型的处理器重排序。指令并行重排和内存系统重排都属于是处理器级别的指令重排序。\n内存屏障（Memory Barrier，或有时叫做内存栅栏，Memory Fence）是一种 CPU 指令，用来禁止处理器指令发生重排序（像屏障一样），从而保障指令执行的有序性。另外，为了达到屏障的效果，它也会使处理器写入、读取值之前，将主内存的值写入高速缓存，清空无效队列，从而保障变量的可见性。\nJMM（JavaMemoryMode） # 什么是 JMM？为什么需要 JMM？ # 一般来说，编程语言也可以直接复用操作系统层面的内存模型。不过，不同的操作系统内存模型不同。如果直接复用操作系统层面的内存模型，就可能会导致同样一套代码换了一个操作系统就无法执行了。Java 语言是跨平台的，它需要自己提供一套内存模型以屏蔽系统差异。\n实际上，对于Java来说，可以把JMM看作是Java定义的并发编程相关的一组规范，除了抽象了线程和主内存之间的关系之外，还规定了从Java源代码到CPU可执行指令的转化过程要遵守哪些和并发相关的原则和规范，主要目的是为了简化多线程编程，增强程序可移植性。\n为什么要遵守这些并发相关的原则和规范呢？因为在并发编程下，CPU多级缓存和指令重排这类设计会导致程序运行出问题，比如指令重排，为此JMM抽象了happens-before原则\nJMM 说白了就是定义了一些规范来解决这些问题，开发者可以利用这些规范更方便地开发多线程程序。对于 Java 开发者说，你不需要了解底层原理，直接使用并发相关的一些关键字和类（比如 volatile、synchronized、各种 Lock）即可开发出并发安全的程序。\nJMM 是如何抽象线程和主内存之间的关系？ # Java内存模型(JMM)，抽象了线程和主内存之间的关系，比如线程之间的共享变量必须存储在主内存中\nJDK1.2之前，Java内存模型总是从主存(共享内存)读取变量；而当前的Java内存模型下，线程可以把变量保存本地内存（机器的寄存器）中，而不直接在主存中读写。这可能造成，一个线程在主存中修改了一个变量的值，而在另一个线程继续使用它在寄存器中的变量值的拷贝，造成数据不一致\n上面所述跟CPU缓存模型非常相似\n什么是主内存？什么是本地内存？\n主内存：★重要！！★所有线程创建的实例对象都存放在主内存中（感觉这里说的是堆？），不管该实例对象是成员变量还是方法中的本地变量（也称局部变量）\n本地内存 ：每个线程都有一个私有的本地内存来存储共享变量的副本，并且，每个线程只能访问自己的本地内存，无法访问其他线程的本地内存。本地内存是 JMM 抽象出来的一个概念，存储了主内存中的共享变量副本\nJava 内存模型的抽象示意图如下：\n如上，若线程1和线程2之间要通信，则\n线程1把本地内存中修改过的共享变量副本的值，同步到主内存中 线程2到主存中，读取对应的共享变量的值 即，JMM为共享变量提供了可见性的保障\n多线程下，主内存中一个共享变量进行操作引发的线程安全问题：\n线程1、2分别对同一个共享变量操作，一个执行修改，一个执行读取 线程2读取到的是线程1修改之前的还是修改之后的值，不确定 关于主内存和工作内存直接的具体交互协议，即一个变量，如何从主内存拷贝到工作内存，如何从工作内存同步到主内存，JMM定义八种同步操作：\n锁定（lock）: 作用于主内存中的变量，将他标记为一个线程独享变量。\n解锁（unlock）: 作用于主内存中的变量，解除变量的锁定状态，被解除锁定状态的变量才能被其他线程锁定。\nread（读取）：作用于主内存的变量，它把一个变量的值从主内存传输到线程的工作内存中，以便随后的 load 动作使用。\nload(载入)：把 read 操作从主内存中得到的变量值放入工作内存的变量的副本中。\nuse(使用)：把工作内存中的一个变量的值传给执行引擎，每当虚拟机遇到一个使用到变量的指令时都会使用该指令。\nassign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。\nstore（存储）：作用于工作内存的变量，它把工作内存中一个变量的值传送到主内存中，以便随后的 write 操作使用。\nwrite（写入）：作用于主内存的变量，它把 store 操作从工作内存中得到的变量的值放入主内存的变量中\n下面的同步规则，保证这些同步操作的正确执行： (没看懂)\n不允许一个线程无原因地（没有发生过任何 assign 操作）把数据从线程的工作内存同步回主内存中。\n一个新的变量只能在主内存中 “诞生”，不允许在工作内存中直接使用一个未被初始化（load 或 assign）的变量，换句话说就是对一个变量实施 use 和 store 操作之前，必须先执行过了 assign 和 load 操作。\n一个变量在同一个时刻只允许一条线程对其进行 lock 操作，但 lock 操作可以被同一条线程重复执行多次，多次执行 lock 后，只有执行相同次数的 unlock 操作，变量才会被解锁。\n如果对一个变量执行 lock 操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前，需要重新执行 load 或 assign 操作初始化变量的值。\n如果一个变量事先没有被 lock 操作锁定，则不允许对它执行 unlock 操作，也不允许去 unlock 一个被其他线程锁定住的变量。\n\u0026hellip;\u0026hellip;\nJava 内存区域和 JMM 有何区别？ # JVM 内存结构和 Java 虚拟机的运行时区域相关，定义了 JVM 在运行时如何分区存储程序数据，就比如说堆主要用于存放对象实例。 Java 内存模型和 Java 的并发编程相关，抽象了线程和主内存之间的关系就比如说线程之间的共享变量必须存储在主内存中，规定了从 Java 源代码到 CPU 可执行指令的这个转化过程要遵守哪些和并发相关的原则和规范，其主要目的是为了简化多线程编程，增强程序可移植性的。 happens-before 原则是什么？ # 通过逻辑时钟能对分布式系统中的事件的先后关系进行判断\n逻辑时钟并不度量时间本身，仅区分事件发生的前后顺序，其本质就是定义了一种 happens-before 关系。\nJSR 133引入happens-before这个概念来描述两个操作之间的内存可见性\n为什么需要 happens-before 原则？ happens-before 原则的诞生是为了程序员和编译器、处理器之间的平衡。程序员追求的是易于理解和编程的强内存模型，遵守既定规则编码即可。编译器和处理器追求的是较少约束的弱内存模型，让它们尽己所能地去优化性能，让性能最大化。\nhappens-before原则的设计思想\n为了对编译器和处理器的约束尽可能少，只要不改变程序的执行结果（单线程程序和正确执行的多线程程序），编译器和处理器怎么进行重排序优化都行。 对于会改变程序执行结果的重排序，JMM 要求编译器和处理器必须禁止这种重排序。 JSR-133对happens-before原则的定义：\n如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，并且第一个操作的执行顺序排在第二个操作之前 这是 JMM 对程序员强内存模型的承诺。从程序员的角度来说，可以这样理解 Happens-before 关系：如果 A Happens-before B，那么 JMM 将向程序员保证 — A 操作的结果将对 B 可见，且 A 的执行顺序排在 B 之前。注意，这只是 Java内存模型向程序员做出的保证，即Happens-before提供跨线程的内存可见性保证\n对于这条定义，举个例子（不代表代码就是这样的，这是一个概括性的假设情况）\n// 以下操作在线程 A 中执行 i = 1; // a // 以下操作在线程 B 中执行 j = i; // b // 以下操作在线程 C 中执行 i = 2; // c 假设线程 A 中的操作 a Happens-before 线程 B 的操作 b，那我们就可以确定操作 b 执行后，变量 j 的值一定是等于 1。\n得出这个结论的依据有两个：一是根据 Happens-before 原则，a 操作的结果对 b 可见，即 “i=1” 的结果可以被观察到；二是线程 C 还没运行，线程 A 操作结束之后没有其他线程会修改变量 i 的值。\n现在再来考虑线程 C，我们依然保持 a Happens-before b ，而 c 出现在 a 和 b 的操作之间，但是 c 与 b 没有 Happens-before 关系，也就是说 b 并不一定能看到 c 的操作结果。那么 b 操作的结果也就是 j 的值就不确定了，可能是 1 也可能是 2，那这段代码就是线程不安全的。\n两个操作之间存在 happens-before 关系，并不意味着 Java 平台的具体实现必须要按照 happens-before 关系指定的顺序来执行。如果重排序之后的执行结果，与按 happens-before 关系来执行的结果一致，那么 JMM 也允许这样的重排序 例子：\nint userNum = getUserNum(); // 1 int teacherNum = getTeacherNum();\t// 2 int totalNum = userNum + teacherNum;\t// 3 如上，1 happens-before 2，2 happens-before 3，1 happens-before 3\n虽然 1 happens-before 2，但对 1 和 2 进行重排序不会影响代码的执行结果，所以 JMM 是允许编译器和处理器执行这种重排序的。但 1 和 2 必须是在 3 执行之前，也就是说 1,2 happens-before 3 。\nhappens-before 原则表达的意义其实并不是一个操作发生在另外一个操作的前面，虽然这从程序员的角度上来说也并无大碍。更准确地来说，它更想表达的意义是前一个操作的结果对于后一个操作是可见的，无论这两个操作是否在同一个线程里。\n举个例子：操作 1 happens-before 操作 2，即使操作 1 和操作 2 不在同一个线程内，JMM 也会保证操作 1 的结果对操作 2 是可见的。\nhappens-before 常见规则有哪些？谈谈你的理解？ # 主要的5条规则：\n程序顺序规则：一个线程内，按照代码顺序，书写在前面的操作happens-before于书写在后面的操作 解锁规则：解锁happens-before于加锁 volatile变量规则：对一个 volatile 变量的写操作 happens-before 于后面对这个 volatile 变量的读操作。说白了就是对 volatile 变量的写操作的结果对于发生于其后的任何操作都是可见的。 传递规则：如果A happens-before B，且B happens-before C ，那么A happens-before C 线程启动规则：Thread对象的start() 方法 happens-before 于此线程的每一个操作 如果两个操作，不满足于上述任何一个happens-before规则，那么这两个操作就没有顺序的保障，JVM可以对这两个操作进行重排序\nhappens-before 和JMM什么关系 # 根据happens-before规则，告诉程序员，有哪些happens-before规则（哪些情况不会被重排序）\n为了避免 Java 程序员为了理解 JMM 提供的内存可见性保证而去学习复杂的重排序规则以及这些规则的具体实现方法，JMM 就出了这么一个简单易懂的 Happens-before 原则，一个 Happens-before 规则就对应于一个或多个编译器和处理器的重排序规则\nas-if-serial 语义保证单线程内程序的执行结果不被改变，Happens-before 关系保证正确同步的多线程程序的执行结果不被改变。 as-if-serial 语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。Happens-before 关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按 Happens-before 指定的顺序来执行的。 JMM定义的\n再看并发编程三个重要特性 # 原子性，可见性，有序性\n原子性 一次操作或多次操作，要么所有的操作，全部都得到执行并且不会受到任何因素的干扰而中断，要么都不执行\nJava中，使用synchronized、各种Lock以及各种原子类实现原子性(AtomicInteger等)\nsynchronized 和各种 Lock 可以保证任一时刻只有一个线程访问该代码块，因此可以保障原子性。各种原子类是利用 CAS (compare and swap) 操作（可能也会用到 volatile或者final关键字）来保证原子操作。\n可见性 当一个线程对共享变量进行了修改，那么另外的线程都是立即可以看到修改后的最新值。\n在 Java 中，可以借助synchronized 、volatile 以及各种 Lock 实现可见性。\n如果我们将变量声明为 volatile ，这就指示 JVM，这个变量是共享且不稳定的，每次使用它都到主存中进行读取。\n有序性 由于指令重排序问题，代码的执行顺序未必就是编写代码时候的顺序\n指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致 ，所以在多线程下，指令重排序可能会导致一些问题。\nJava中，volatile关键字可以禁止指令进行重排序优化（注意，synchronized也可以）\n总结 # 补充：线程join()方法，导致调用线程暂停，直到xx.join()中的xx线程执行完，调用join方法的线程才继续执行\nThread thread1 = new Thread(new Runnable() { @Override public void run() { log.info(\u0026#34;暂停5s\u0026#34;); try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } } }); thread1.start(); Thread thread2 = new Thread(new Runnable() { @Override public void run() { log.info(\u0026#34;暂停3s\u0026#34;); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } } }); thread2.start(); thread1.join(); thread2.join(); log.info(\u0026#34;主线程执行\u0026#34;); /*结果 2022-11-23 13:57:06 下午 [Thread: Thread-1] INFO:暂停5s 2022-11-23 13:57:06 下午 [Thread: Thread-2] INFO:暂停3s 2022-11-23 13:57:11 下午 [Thread: main] INFO:主线程执行 */ 指令重排的影响，举例：【★很重要★】\npublic class Test { private static int x = 0, y = 0; private static int a = 0, b = 0; public static void main(String[] args) throws InterruptedException { for (int i = 0; ; i++) { x = 0; y = 0; a = 0; b = 0; Thread one = new Thread(() -\u0026gt; { a = 1; x = b; }); Thread other = new Thread(() -\u0026gt; { b = 1; y = a; }); one.start(); other.start();; one.join(); other.join(); if (x == 0 \u0026amp;\u0026amp; y == 0) { String result = \u0026#34;第\u0026#34; + i + \u0026#34;次（\u0026#34; + x + \u0026#34;, \u0026#34; + y + \u0026#34;）\u0026#34;; System.out.println(result); } } } } /* 因为线程one中，a和x并不存在依赖关系，因此可能会先执行x=b;而这个时候，b=0。因此x会被赋值为0，而a=1这条语句还没有被执行的时候，线程other先执行了y=a这条语句，这个时候a还是a=0;因此y被赋值为了0。所以存在情况x=0;y=0。这就是指令重排导致的多线程问题。 原文链接：https://blog.csdn.net/qq_45948401/article/details/124973903 */ Java 是最早尝试提供内存模型的语言，其主要目的是为了简化多线程编程，增强程序可移植性的。 CPU 可以通过制定缓存一致协议（比如 MESI 协议open in new window）来解决内存缓存不一致性问题。 为了提升执行速度/性能，计算机在执行程序代码的时候，会对指令进行重排序。 简单来说就是系统在执行代码的时候并不一定是按照你写的代码的顺序依次执行。指令重排序可以保证串行语义一致，但是没有义务保证多线程间的语义也一致 ，所以在多线程下，指令重排序可能会导致一些问题。 你可以把 JMM 看作是 Java 定义的并发编程相关的一组规范，除了抽象了线程和主内存之间的关系之外，其还规定了从 Java 源代码到 CPU 可执行指令的这个转化过程要遵守哪些和并发相关的原则和规范，其主要目的是为了简化多线程编程，增强程序可移植性的。 JSR 133 引入了 happens-before 这个概念来**（极其重要又精简的话）描述两个操作之间的内存可见性**。 "},{"id":143,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0303lyconcurrent-03/","title":"并发03","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n线程池 # 为什么要使用线程池\n池化技术：线程池、数据库连接池、Http连接池 池化技术思想意义：为了减少每次获取资源的消耗，提高对资源的利用率 线程池提供了限制和管理 资源(包括执行一个任务)的方式 每个线程池还维护基本统计信息，例如已完成任务的数量 好处： 降低资源消耗 重复利用已创建线程降低线程创建和销毁造成的消耗 提高响应速度 任务到达时，任务可以不需等到线程创建就能继续执行 提高线程的可管理性 线程是稀缺资源，如果无限制创建，不仅消耗系统资源，还会降低系统的稳定性，使用线程池统一管理分配、调优和监控。 实现Runnable接口和Callable接口的区别\n//Callable的用法 public class TestLy { //如果加上volatile,就能保证可见性，线程1 才能停止 boolean stop = false;//对象属性 public static void main(String[] args) throws InterruptedException, ExecutionException { FutureTask\u0026lt;String\u0026gt; futureTask=new FutureTask\u0026lt;\u0026gt;(new Callable\u0026lt;String\u0026gt;() { @Override public String call() throws Exception { System.out.println(\u0026#34;等3s再把结果给你\u0026#34;); TimeUnit.SECONDS.sleep(3); return \u0026#34;hello world\u0026#34;; } }); new Thread(futureTask).start(); String s = futureTask.get(); System.out.println(\u0026#34;3s后获取到了结果\u0026#34;+s); new Thread(new Runnable() { @Override public void run() { System.out.println(\u0026#34;abc\u0026#34;); } }).start(); } } /* 等3s再把结果给你 3s后获取到了结果hello world abc */ Runnable接口不会返回结果或抛出检查异常，Callable接口可以\nExecutors可以实现将Runnable对象转换成Callable对象\nExecutors.callable(Runnable task)或Executors.callable(Runnable task, Object result) //则两个方法，运行的结果是 Callable\u0026lt;Object\u0026gt;\n//一个不指定结果，另一个指定结果 public static void main(String[] args) throws Exception { Callable\u0026lt;Object\u0026gt; abc = Executors.callable(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;abc\u0026#34;); },\u0026#34;abcccc\u0026#34;);//如果没有\u0026#34;abcccc\u0026#34;，则下面输出null FutureTask\u0026lt;Object\u0026gt; futureTask = new FutureTask\u0026lt;\u0026gt;(abc); new Thread(futureTask).start(); Object o = futureTask.get(); System.out.println(\u0026#34;获取值：\u0026#34;+o); } Runnable和Callable：\n//Runnable.java @FunctionalInterface public interface Runnable { /** * 被线程执行，没有返回值也无法抛出异常 */ public abstract void run(); } //========================================= //Callable.java @FunctionalInterface public interface Callable\u0026lt;V\u0026gt; { /** * 计算结果，或在无法这样做时抛出异常。 * @return 计算得出的结果 * @throws 如果无法计算结果，则抛出异常 */ V call() throws Exception; } 执行execute()和submit()方法的区别是什么\nexecute() 方法用于提交不需要返回值的任务，所以无法判断任务是否被线程池执行成功\nsubmit()方法用于提交需要返回值的任务，线程池会返回一个Future类型的对象，通过这个Future对象可以判断任务是否返回成功\n这个Future的get()方法来获取返回值，get()方法会阻塞当前线程直到任务完成； 使用get(long timeout,TimeUnit unit) 方法会在阻塞当前线程一段时间后立即返回(此时任务不一定已经执行完) 注意: 这里的get()不一定会有返回值的，例子如下\nExecutorService executorService = Executors.newCachedThreadPool(); Callable\u0026lt;MyClass\u0026gt; myClassCallable = new Callable\u0026lt;MyClass\u0026gt;() { @Override public MyClass call() throws Exception { MyClass myClass1 = new MyClass(); myClass1.setName(\u0026#34;ly-callable-测试\u0026#34;); TimeUnit.SECONDS.sleep(2); return myClass1; } }; Future\u0026lt;?\u0026gt; submit = executorService.submit(myClassCallable); //这里会阻塞 Object o = submit.get(); log.info(\u0026#34;ly-callable-打印结果1:\u0026#34; + o); FutureTask\u0026lt;MyClass\u0026gt; futureTask = new FutureTask\u0026lt;\u0026gt;(() -\u0026gt; { MyClass myClass1 = new MyClass(); myClass1.setName(\u0026#34;ly-FutureTask-测试\u0026#34;); TimeUnit.SECONDS.sleep(2); return myClass1; }); Future\u0026lt;?\u0026gt; submit2 = executorService.submit(futureTask); //这里会阻塞 Object o2 = submit2.get(); log.info(\u0026#34;ly-callable-打印结果2:\u0026#34; + o2); executorService.shutdown(); /*结果 2022-11-09 10:19:10 上午 [Thread: main] INFO:ly-callable-打印结果1:MyClass(name=ly-callable-测试) 2022-11-09 10:19:12 上午 [Thread: main] INFO:ly-callable-打印结果2:null */ 当submit一个Callable对象的时候，能从submit返回的Future.get到返回值；当submit一个FutureTask对象（FutureTask有参构造函数包含Callable对象，但它本身不是Callable）时，没法获取返回值，因为会被当作Runnable对象submit进来\n虚线是实现，实线是继承。\n而入参为Runnable时返回值里是get不到结果的\n下面这段源码，解释了为什么当传入的类型是Runnable对象时，结果为null\n只要是submit（Runnable ），就会返回null\n//源码AbstractExecutorService 接口中的一个submit方法 public Future\u0026lt;?\u0026gt; submit(Runnable task) { if (task == null) throw new NullPointerException(); RunnableFuture\u0026lt;Void\u0026gt; ftask = newTaskFor(task, null); execute(ftask); return ftask; } //其中的newTaskFor方法 protected \u0026lt;T\u0026gt; RunnableFuture\u0026lt;T\u0026gt; newTaskFor(Runnable runnable, T value) { return new FutureTask\u0026lt;T\u0026gt;(runnable, value); } //execute()方法 public void execute(Runnable command) { ... } FutureTask、Thread、Callable、Executors\n如何创建线程池\nexecutor [ɪɡˈzekjətə(r)] 遗嘱执行人(或银行等)\n关于SynchronousQueue（具有0个元素的阻塞队列）：\nSynchronousQueue\u0026lt;String\u0026gt; synchronousQueue =new SynchronousQueue\u0026lt;\u0026gt;(); new Thread(()-\u0026gt;{ try { log.info(\u0026#34;放入数据A\u0026#34;); synchronousQueue.put(\u0026#34;A\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } log.info(\u0026#34;继续执行\u0026#34;); },\u0026#34;子线程1\u0026#34;).start(); new Thread(()-\u0026gt;{ try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } String poll = null; try { poll = synchronousQueue.take(); } catch (InterruptedException e) { e.printStackTrace(); } log.info(poll); },\u0026#34;子线程2\u0026#34;).start(); /**输出 2023-03-07 15:20:17 下午 [Thread: 子线程1] INFO:放入数据A ---这里会等待3s(等子线程2 task()消费掉) 2023-03-07 15:20:20 下午 [Thread: 子线程2] INFO:A 2023-03-07 15:20:20 下午 [Thread: 子线程1] INFO:继续执行 */ 不允许使用Executors去创建，而是通过new ThreadPoolExecutor的方式：能让写的同学明确线程池运行规则，规避资源耗尽\n/* 工具的方式创建线程池 */ void test(){ ExecutorService executorService = Executors.newCachedThreadPool(); Callable\u0026lt;MyClass\u0026gt; myClassCallable = new Callable\u0026lt;MyClass\u0026gt;() { @Override public MyClass call() throws Exception { MyClass myClass1 = new MyClass(); myClass1.setName(\u0026#34;ly-callable-测试\u0026#34;); TimeUnit.SECONDS.sleep(2); return myClass1; } }; Future\u0026lt;?\u0026gt; submit = executorService.submit(myClassCallable); //这里会阻塞 Object o = submit.get(); log.info(\u0026#34;ly-callable-打印结果1:\u0026#34; + o); } 使用Executors返回线程池对象的弊端：\nThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue\u0026lt;Runnable\u0026gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler){} //#####时间表示keepAliveTime##### //########线程数量固定，队列长度为Integer.MAX################ Executors.newFixedThreadPool(3); public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;()); } //############线程数量固定，队列长度为Integer.MAX############## Executors.newSingleThreadExecutor(Executors.defaultThreadFactory()); public static ExecutorService newSingleThreadExecutor(ThreadFactory threadFactory) { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue\u0026lt;Runnable\u0026gt;(), threadFactory)); //############线程数量为Integer.MAX############# Executors.newCachedThreadPool(Executors.defaultThreadFactory()); public static ExecutorService newCachedThreadPool(ThreadFactory threadFactory) { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue\u0026lt;Runnable\u0026gt;(), threadFactory); } //#############线程数量为Integer.MAX############# Executors.newScheduledThreadPool(3, Executors.defaultThreadFactory()); public static ScheduledExecutorService newScheduledThreadPool( int corePoolSize, ThreadFactory threadFactory) { return new ScheduledThreadPoolExecutor(corePoolSize, threadFactory); } ====================\u0026gt; public ScheduledThreadPoolExecutor(int corePoolSize, ThreadFactory threadFactory) { super(corePoolSize, Integer.MAX_VALUE, 0, NANOSECONDS, new DelayedWorkQueue(), threadFactory); } FixedThreadPool和SingleThreadExecutor：这两个方案允许请求的队列长度为Integer.MAX_VALUE，可能堆积大量请求，导致OOM CachedThreadPool和ScheduledThreadPool：允许创建的线程数量为Integer.MAX_VALUE，可能创建大量线程，导致OOM 通过构造方法实现 通过Executor框架的工具类Executors来实现 以下三个方法，返回的类型都是ThreadPoolExecutor\nFixedThreadPool : 该方法返回固定线程数量的线程池，线程数量始终不变。当有新任务提交时，线程池中若有空闲线程则立即执行；若没有，则新任务被暂存到任务队列中，待有线程空闲时，则处理在任务队列中的任务 SingleThreadExecutor：方法返回一个只有一个线程的线程池。若多余一个任务被提交到该线程池，任务被保存到一个任务队列中，待线程空闲，按先进先出的顺序执行队列中任务 CachedThreadPool：该方法返回一个根据实际情况调整线程数量的线程池。 数量不固定，若有空闲线程可以复用则优先使用可复用线程。若所有线程均工作，此时又有新任务提交，则创建新线程处理任务。所有线程在当前任务执行完毕后返回线程池进行复用 Executors工具类中的方法\n核心线程数和最大线程数有什么区别？ 该类提供四个构造方法，看最长那个，其余的都是（给定默认值后）调用这个方法\n/** * 用给定的初始参数创建一个新的ThreadPoolExecutor。 */ public ThreadPoolExecutor(int corePoolSize,//线程池的核心线程数量 int maximumPoolSize,//线程池的最大线程数 long keepAliveTime,//当线程数大于核心线程数时，多余的空闲线程存活的最长时间 TimeUnit unit,//时间单位 BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,//任务队列，用来储存等待执行任务的队列 ThreadFactory threadFactory,//线程工厂，用来创建线程，一般默认即可 RejectedExecutionHandler handler//拒绝策略，当提交的任务过多而不能及时处理时，我们可以定制策略来处理任务 ) { if (corePoolSize \u0026lt; 0 || maximumPoolSize \u0026lt;= 0 || maximumPoolSize \u0026lt; corePoolSize || keepAliveTime \u0026lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler; } 构造函数重要参数分析\ncorePoolSize : 核心线程数定义最小可以运行的线程数量\nmaximumPoolSize: 当队列中存放的任务达到队列容量时，当前可以同时运行的线程数量变为最大线程数\nworkQueue：当新线程来的时候先判断当前运行线程数量是否达到核心线程数，如果达到的话，新任务就会被存放在队列中 ThreadPoolExecutor其他常见参数：\nkeepAliveTime：如果线程池中的线程数量大于corePoolSize时，如果这时没有新任务提交，核心线程外的线程不会立即销毁，而是等待，等待的时间超过了keepAliveTime就会被回收\nunit: keepAliveTime参数的时间单位\nthreadFactory: executor创建新线程的时候会用到\nhandle: 饱和策略\n如果同时运行的线程数量达到最大线程数，且队列已经被放满任务，ThreadPoolTaskExecutor定义该情况下的策略：\nThreadPoolExecutor.AbortPolicy： 抛出 RejectedExecutionException来拒绝新任务的处理。 ThreadPoolExecutor.CallerRunsPolicy： 调用执行自己的线程(如果在main方法中，那就是main线程)运行任务，也就是直接在调用execute方法的线程中运行(run)被拒绝的任务，如果执行程序已关闭，则会丢弃该任务。因此这种策略会降低对于新任务提交速度，影响程序的整体性能。如果您的应用程序可以承受此延迟并且你要求任何一个任务请求都要被执行的话，你可以选择这个策略。 ThreadPoolExecutor.DiscardPolicy： 不处理新任务，直接丢弃掉。 ThreadPoolExecutor.DiscardOldestPolicy： 此策略将丢弃最早的未处理的任务请求。 举个例子：如果在Spring中通过ThreadPoolTaskExecutor或直接通过ThreadPoolExecutor构造函数创建线程池时，若不指定RejectExcecutorHandler饱和策略则默认使用ThreadPoolExecutor.AbortPolicy，即抛出RejectedExecution来拒绝新来的任务；对于可伸缩程序，建议使用ThreadPoolExecutor.CallerRunsPolicy，\n一个简单的线程池Demo\n//定义一个Runnable接口实现类 import java.util.Date; /** * 这是一个简单的Runnable类，需要大约5秒钟来执行其任务。 * @author shuang.kou */ public class MyRunnable implements Runnable { private String command; public MyRunnable(String s) { this.command = s; } @Override public void run() { System.out.println(Thread.currentThread().getName() + \u0026#34; Start. Time = \u0026#34; + new Date()); processCommand(); System.out.println(Thread.currentThread().getName() + \u0026#34; End. Time = \u0026#34; + new Date()); } private void processCommand() { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } } @Override public String toString() { return this.command; } } //实际执行 import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit; public class ThreadPoolExecutorDemo { private static final int CORE_POOL_SIZE = 5;//核心线程数5 private static final int MAX_POOL_SIZE = 10;//最大线程数10 private static final int QUEUE_CAPACITY = 100;//队列容量100 private static final Long KEEP_ALIVE_TIME = 1L;//等待时间 public static void main(String[] args) { //使用阿里巴巴推荐的创建线程池的方式 //通过ThreadPoolExecutor构造函数自定义参数创建 ThreadPoolExecutor executor = new ThreadPoolExecutor( CORE_POOL_SIZE, MAX_POOL_SIZE, KEEP_ALIVE_TIME,8 TimeUnit.SECONDS, new ArrayBlockingQueue\u0026lt;\u0026gt;(QUEUE_CAPACITY), new ThreadPoolExecutor.CallerRunsPolicy()); for (int i = 0; i \u0026lt; 10; i++) { //创建 MyRunnable 对象（MyRunnable 类实现了Runnable 接口） Runnable worker = new MyRunnable(\u0026#34;\u0026#34; + i); //执行Runnable executor.execute(worker); } //终止线程池 executor.shutdown(); while (!executor.isTerminated()) { } System.out.println(\u0026#34;Finished all threads\u0026#34;); } } /*------输出 pool-1-thread-3 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:37 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-5 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-4 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-3 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-2 Start. Time = Sun Apr 12 11:14:42 CST 2020 pool-1-thread-1 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-4 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-5 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-3 End. Time = Sun Apr 12 11:14:47 CST 2020 pool-1-thread-2 End. Time = Sun Apr 12 11:14:47 CST 2020 ------ */ 线程池原理是什么？\n由结果可以看出，线程池先执行5个任务，此时多出的任务会放到队列，那5个任务中有任务执行完的话，会拿新的任务执行\n为了搞懂线程池的原理，我们需要首先分析一下 execute方法。\n我们可以使用 executor.execute(worker)来提交一个任务到线程池中去，这个方法非常重要，下面我们来看看它的源码：\n//源码分析 // 存放线程池的运行状态 (runState) 和线程池内有效线程的数量 (workerCount) private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); private static int workerCountOf(int c) { return c \u0026amp; CAPACITY; } private final BlockingQueue\u0026lt;Runnable\u0026gt; workQueue; public void execute(Runnable command) { // 如果任务为null，则抛出异常。 if (command == null) throw new NullPointerException(); // ctl 中保存的线程池当前的一些状态信息 int c = ctl.get(); // 下面会涉及到 3 步 操作 // 1.首先判断当前线程池中执行的任务数量是否小于 corePoolSize // 如果小于的话，通过addWorker(command, true)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 if (workerCountOf(c) \u0026lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } // 2.如果当前执行的任务数量大于等于 corePoolSize 的时候就会走到这里 // 通过 isRunning 方法判断线程池状态，线程池处于 RUNNING 状态并且队列可以加入任务，该任务才会被加入进去 if (isRunning(c) \u0026amp;\u0026amp; workQueue.offer(command)) { int recheck = ctl.get(); // 再次获取线程池状态，如果线程池状态不是 RUNNING 状态就需要从任务队列中移除任务，并尝试判断线程是否全部执行完毕。同时执行拒绝策略。 if (!isRunning(recheck) \u0026amp;\u0026amp; remove(command)) reject(command); // 如果当前线程池为空就新创建一个线程并执行。 else if (workerCountOf(recheck) == 0) addWorker(null, false); } //3. 通过addWorker(command, false)新建一个线程，并将任务(command)添加到该线程中；然后，启动该线程从而执行任务。 //如果addWorker(command, false)执行失败，则通过reject()执行相应的拒绝策略的内容。 else if (!addWorker(command, false)) reject(command); } ------ 如图 分析上面的例子，\n我们在代码中模拟了 10 个任务，我们配置的核心线程数为 5 、等待队列容量为 100 ，所以每次只可能存在 5 个任务同时执行，剩下的 5 个任务会被放到等待队列中去。当前的5个任务中如果有任务被执行完了，线程池就会去拿新的任务执行。\naddWorker 这个方法主要用来创建新的工作线程，如果返回 true 说明创建和启动工作线程成功，否则的话返回的就是 false。\n// 全局锁，并发操作必备 private final ReentrantLock mainLock = new ReentrantLock(); // 跟踪线程池的最大大小，只有在持有全局锁mainLock的前提下才能访问此集合 private int largestPoolSize; // 工作线程集合，存放线程池中所有的（活跃的）工作线程，只有在持有全局锁mainLock的前提下才能访问此集合 private final HashSet\u0026lt;Worker\u0026gt; workers = new HashSet\u0026lt;\u0026gt;(); //获取线程池状态 private static int runStateOf(int c) { return c \u0026amp; ~CAPACITY; } //判断线程池的状态是否为 Running private static boolean isRunning(int c) { return c \u0026lt; SHUTDOWN; } /** * 添加新的工作线程到线程池 * @param firstTask 要执行 * @param core参数为true的话表示使用线程池的基本大小，为false使用线程池最大大小 * @return 添加成功就返回true否则返回false */ private boolean addWorker(Runnable firstTask, boolean core) { retry: for (;;) { //这两句用来获取线程池的状态 int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs \u0026gt;= SHUTDOWN \u0026amp;\u0026amp; ! (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null \u0026amp;\u0026amp; ! workQueue.isEmpty())) return false; for (;;) { //获取线程池中工作的线程的数量 int wc = workerCountOf(c); // core参数为false的话表明队列也满了，线程池大小变为 maximumPoolSize if (wc \u0026gt;= CAPACITY || wc \u0026gt;= (core ? corePoolSize : maximumPoolSize)) return false; //原子操作将workcount的数量加1 if (compareAndIncrementWorkerCount(c)) break retry; // 如果线程的状态改变了就再次执行上述操作 c = ctl.get(); if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop } } // 标记工作线程是否启动成功 boolean workerStarted = false; // 标记工作线程是否创建成功 boolean workerAdded = false; Worker w = null; try { w = new Worker(firstTask); final Thread t = w.thread; if (t != null) { // 加锁 final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { //获取线程池状态 int rs = runStateOf(ctl.get()); //rs \u0026lt; SHUTDOWN 如果线程池状态依然为RUNNING,并且线程的状态是存活的话，就会将工作线程添加到工作线程集合中 //(rs=SHUTDOWN \u0026amp;\u0026amp; firstTask == null)如果线程池状态小于STOP，也就是RUNNING或者SHUTDOWN状态下，同时传入的任务实例firstTask为null，则需要添加到工作线程集合和启动新的Worker // firstTask == null证明只新建线程而不执行任务 if (rs \u0026lt; SHUTDOWN || (rs == SHUTDOWN \u0026amp;\u0026amp; firstTask == null)) { if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); //更新当前工作线程的最大容量 int s = workers.size(); if (s \u0026gt; largestPoolSize) largestPoolSize = s; // 工作线程是否启动成功 workerAdded = true; } } finally { // 释放锁 mainLock.unlock(); } //// 如果成功添加工作线程，则调用Worker内部的线程实例t的Thread#start()方法启动真实的线程实例 if (workerAdded) { t.start(); /// 标记线程启动成功 workerStarted = true; } } } finally { // 线程启动失败，需要从工作线程中移除对应的Worker if (! workerStarted) addWorkerFailed(w); } return workerStarted; } 如何设定线程池的大小\n如果线程池中的线程太多，就会增加上下文切换的成本\n多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。\n上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。\nLinux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。\n过大跟过小都不行\n如果我们设置的线程池数量太小的话，如果同一时间有大量任务/请求需要处理，可能会导致大量的请求/任务在任务队列中排队等待执行，甚至会出现任务队列满了之后任务/请求无法处理的情况，或者大量任务堆积在任务队列导致 OOM 设置线程数量太大，大量线程可能会同时在争取 CPU 资源，这样会导致大量的上下文切换，从而增加线程的执行时间，影响了整体执行效率（对于CPU密集型任务不能使用这个，因为本来CPU资源就紧张，需要设置小一点，减小上下文切换） 简单且适用面较广的公式\nCPU 密集型任务(N+1)： 这种任务消耗的主要是 CPU 资源，可以将线程数设置为 N（CPU 核心数）+1，比 CPU 核心数多出来的一个线程是为了防止线程偶发的缺页中断，或者其它原因导致的任务暂停而带来的影响。一旦任务暂停，CPU 就会处于空闲状态，而在这种情况下多出来的一个线程就可以充分利用 CPU 的空闲时间。\nI/O 密集型任务(2N)： 这种任务应用起来，系统会用大部分的时间来处理 I/O 交互，而线程在处理 I/O 的时间段内不会占用 CPU 来处理，这时就可以将 CPU 交出给其它线程使用。因此在 I/O 密集型任务的应用中，我们可以多配置一些线程，具体的计算方法是 2N。\n如何判断是CPU密集任务还是IO密集任务\nCPU 密集型简单理解就是利用 CPU 计算能力的任务比如你在内存中对大量数据进行排序。但凡涉及到网络读取，文件读取这类都是 IO 密集型，这类任务的特点是 CPU 计算耗费时间相比于等待 IO 操作完成的时间来说很少，大部分时间都花在了等待 IO 操作完成上。\nAtomic原子类 # Atomic 英[əˈtɒmɪk]原子，即不可分割\n线程中，Atomic，指一个操作是不可中断的，即使在多线程一起执行时，一个操作一旦开始，就不会被其他线程干扰\n原子类，即具有原子/原子操作特性的类。并发包java.util.concurrent原子类都放在java.util.concurrent.atomit Java中存在四种原子类（基本、数组、引用、对象属性）\n基本类型：AtomicInteger，AtomicLong，AtomicBoolean 数组类型：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray 引用类型：AtomicReference，AtomicStampedReference（[原子更新] 带有版本号的引用类型。该类将整数值与引用关联，解决原子的更新数据和数据的版本号，解决使用CAS进行原子更新可能出现的ABA问题），AtomicMarkableReference（原子更新带有标记位的引用类型） 对象属性修改类型：AtomicIntegerFiledUpdater原子更新整型字段的更新器；AtomicLongFiledUpdater；AtomicReferenceFieldUpdater 详见\nAQS # AQS介绍 全程，AbstractQueuedSynchronizer抽象队列同步器，在java.util.concurrent.locks包下 AQS是一个抽象类，主要用来构建锁和同步器\npublic abstract class AbstractQueuedSynchronizer extends AbstractOwnableSynchronizer implements java.io.Serializable { } AQS 为构建锁和同步器提供了一些通用功能的实现，能简单且高效地构造出大量应用广泛的同步器，例如ReentrantLock，Semaphore[ˈseməfɔː(r)]以及ReentrantReadWriteLock，SynchronousQueue 等等都基于AQS\nAQS原理分析\n面试不是背题，大家一定要加入自己的思想，即使加入不了自己的思想也要保证自己能够通俗的讲出来而不是背出来\nAQS 核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制 AQS 是用 CLH 队列锁实现的，即将暂时获取不到锁的线程加入到队列中。\nCLH(Craig,Landin and Hagersten)队列是一个虚拟的双向队列（虚拟的双向队列即不存在队列实例，仅存在结点之间的关联关系）。AQS 是将每条请求共享资源的线程封装成一个 CLH 锁队列的一个结点（Node）来实现锁的分配。 搜索了一下，CLH好像是人名\nCLH队列结构如下图所示\nAQS（AbstractQueuedSynchronized）原理图\nAQS 使用 int 成员变量 state 表示同步状态，通过内置的 线程等待队列 来完成获取资源线程的排队工作。\n//state 变量由 volatile 修饰，用于展示当前临界资源的获锁情况。 private volatile int state;//共享变量，使用volatile修饰保证线程可见性 状态信息的操作\n//返回同步状态的当前值 protected final int getState() { return state; } //设置同步状态的值 protected final void setState(int newState) { state = newState; } //原子地（CAS操作）将同步状态值设置为给定值update如果当前同步状态的值等于expect（期望值） protected final boolean compareAndSetState(int expect, int update) { return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 以 ReentrantLock 为例，state 初始值为 0，表示未锁定状态。A 线程 lock() 时，会调用 tryAcquire() 独占该锁并将 state+1 。此后，其他线程再 tryAcquire() 时就会失败，直到 A 线程 unlock() 到 state=0（即释放锁）为止，其它线程才有机会获取该锁。当然，释放锁之前，A 线程自己是可以重复获取此锁的（state 会累加），这就是可重入的概念。但要注意，获取多少次就要释放多少次，这样才能保证 state 是能回到零态的。\n以 CountDownLatch 为例，任务分为 N 个子线程去执行，state 也初始化为 N（注意 N 要与线程个数一致）。这 N 个子线程是并行执行的，每个子线程执行完后countDown() 一次，state 会 CAS(Compare and Swap) 减 1。等到所有子线程都执行完后(即 state=0 )，会 unpark() 主调用线程，然后主调用线程就会从 await() 函数返回，继续后余动作\n//例子 public class TestCountDownLatch { public static void main(String[] args) { CountDownLatch countDownLatch=new CountDownLatch(3); new Thread(()-\u0026gt;{ try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\u0026#34;执行完毕\u0026#34;); countDownLatch.countDown(); },\u0026#34;线程1\u0026#34;).start(); new Thread(()-\u0026gt;{ try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\u0026#34;执行完毕\u0026#34;); countDownLatch.countDown(); },\u0026#34;线程2\u0026#34;).start(); new Thread(()-\u0026gt;{ try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName()+\u0026#34;执行完毕\u0026#34;); countDownLatch.countDown(); },\u0026#34;线程3\u0026#34;).start(); try { System.out.println(Thread.currentThread().getName()+\u0026#34;等待中....\u0026#34;); countDownLatch.await();//阻塞 System.out.println(Thread.currentThread().getName()+\u0026#34;等待完毕，继续执行\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } } } /* main等待中.... 线程1执行完毕 线程2执行完毕 线程3执行完毕 main等待完毕，继续执行 */ Semaphore # Semaphore 有什么用？ synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，而Semaphore(信号量)可以用来控制同时访问特定资源的线程数量。\n//使用 public class TestSemaphore { public static void main(String[] args) { Semaphore semaphore = new Semaphore(3);//能同时运行3个 for (int i = 0; i \u0026lt; 15; i++) { int finalI = i; new Thread(() -\u0026gt; { try { semaphore.acquire();//获取通行证 System.out.println(Thread.currentThread().getName() + \u0026#34;执行中...\u0026#34;); TimeUnit.SECONDS.sleep(finalI); System.out.println(Thread.currentThread().getName() + \u0026#34;释放了通行证\u0026#34;); semaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } },\u0026#34;线程\u0026#34;+finalI).start(); } } } /*结果 线程0执行中... 线程2执行中... 线程1执行中... 线程0释放了通行证 线程3执行中... 线程1释放了通行证 线程4执行中... 线程2释放了通行证 线程5执行中... 线程3释放了通行证 线程6执行中... 线程4释放了通行证 线程7执行中... 线程5释放了通行证 线程8执行中... 线程6释放了通行证 线程10执行中... 线程7释放了通行证 线程11执行中... 线程8释放了通行证 线程9执行中... 线程10释放了通行证 线程12执行中... 线程11释放了通行证 线程13执行中... 线程9释放了通行证 线程14执行中... 线程12释放了通行证 线程13释放了通行证 线程14释放了通行证 */ Semaphore 的使用简单，我们这里假设有 N(N\u0026gt;5) 个线程来获取 Semaphore 中的共享资源，下面的代码表示同一时刻 N 个线程中只有 5 个线程能获取到共享资源，其他线程都会阻塞，只有获取到共享资源的线程才能执行。等到有线程释放了共享资源，其他阻塞的线程才能获取到\n// 初始共享资源数量 final Semaphore semaphore = new Semaphore(5); // 获取1个许可 semaphore.acquire(); // 释放1个许可 semaphore.release(); 当初始的资源个数为 1 的时候，Semaphore 退化为排他锁。\nSemaphore对应的两个构造方法\npublic Semaphore(int permits) { sync = new NonfairSync(permits); } public Semaphore(int permits, boolean fair) { sync = fair ? new FairSync(permits) : new NonfairSync(permits); } 这两个构造方法，都必须提供许可的数量，第二个构造方法可以指定是公平模式还是非公平模式，默认非公平模式。\nSemaphore 通常用于那些资源有明确访问数量限制的场景比如限流（仅限于单机模式，实际项目中推荐使用 Redis +Lua 来做限流）。\nSemaphore 的原理是什么？\nSemaphore 是共享锁的一种实现，它默认构造 AQS 的 state 值为 permits，你可以将 permits 的值理解为许可证的数量，只有拿到许可证的线程才能执行。\n调用semaphore.acquire() ，线程尝试获取许可证，如果 state \u0026gt;= 0 的话，则表示可以获取成功。如果获取成功的话，使用 CAS 操作去修改 state 的值 state=state-1。如果 state\u0026lt;0 的话，则表示许可证数量不足。此时会创建一个 Node 节点加入阻塞队列，挂起当前线程。\n/** * 获取1个许可证 */ public void acquire() throws InterruptedException { sync.acquireSharedInterruptibly(1); } /** * 共享模式下获取许可证，获取成功则返回，失败则加入阻塞队列，挂起线程 */ public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); // 尝试获取许可证，arg为获取许可证个数，当可用许可证数减当前获取的许可证数结果小于0,则创建一个节点加入阻塞队列，挂起当前线程。 if (tryAcquireShared(arg) \u0026lt; 0) doAcquireSharedInterruptibly(arg); } 调用semaphore.release(); ，线程尝试释放许可证，并使用 CAS 操作去修改 state 的值 state=state+1。释放许可证成功之后，同时会唤醒同步队列中的一个线程。被唤醒的线程会重新尝试去修改 state 的值 state=state-1 ，如果 state\u0026gt;=0 则获取令牌成功，否则重新进入阻塞队列，挂起线程。\n// 释放一个许可证 public void release() { sync.releaseShared(1); } // 释放共享锁，同时会唤醒同步队列中的一个线程。 public final boolean releaseShared(int arg) { //释放共享锁 if (tryReleaseShared(arg)) { //唤醒同步队列中的一个线程 doReleaseShared(); return true; } return false; } CountDownLatch # CountDownLatch有什么用\nCountDownLatch 允许 count 个线程阻塞在一个地方(一般例子是阻塞在主线程中 countDownLatch.await())，直至所有线程的任务都执行完毕**(再从阻塞的地方继续执行)**。 CountDownLatch 是一次性的，计数器的值只能在构造方法中初始化一次，之后没有任何机制再次对其设置值，当 CountDownLatch 使用完毕后，它不能再次被使用。 CountDownLatch的原理是什么 CountDownLatch 是共享锁的一种实现,它默认构造 AQS 的 state 值为 count。当线程使用 countDown() 方法时,其实使用了**tryReleaseShared方法以 CAS 的操作来减少 state,直至 state 为 0 。当调用 await() 方法的时候，如果 state 不为 0，那就证明任务还没有执行完毕，await() 方法就会一直阻塞**，也就是说 await() 方法之后的语句不会被执行。然后，CountDownLatch 会自旋 CAS 判断 state == 0，如果 state == 0 的话，就会释放所有等待的线程，await() 方法之后的语句得到执行。\n用过 CountDownLatch 么？什么场景下用的？\nCountDownLatch 的作用就是 允许 count 个线程阻塞在一个地方，直至所有线程的任务都执行完毕。之前在项目中，有一个使用多线程读取多个文件处理的场景，我用到了 CountDownLatch 。具体场景是下面这样的：\n我们要读取处理 6 个文件，这 6 个任务都是没有执行顺序依赖的任务，但是我们需要返回给用户的时候将这几个文件的处理的结果进行统计整理。\n为此我们定义了一个线程池和 count 为 6 的CountDownLatch对象 。使用线程池处理读取任务，每一个线程处理完之后就将 count-1，调用CountDownLatch对象的 await()方法，直到所有文件读取完之后，才会接着执行后面的逻辑。\n//伪代码 public class CountDownLatchExample1 { // 处理文件的数量 private static final int threadCount = 6; public static void main(String[] args) throws InterruptedException { // 创建一个具有固定线程数量的线程池对象（推荐使用构造方法创建） ExecutorService threadPool = Executors.newFixedThreadPool(10); final CountDownLatch countDownLatch = new CountDownLatch(threadCount); for (int i = 0; i \u0026lt; threadCount; i++) { final int threadnum = i; threadPool.execute(() -\u0026gt; { try { //处理文件的业务操作 //...... } catch (InterruptedException e) { e.printStackTrace(); } finally { //表示一个文件已经被完成 countDownLatch.countDown(); } }); } countDownLatch.await(); //这里应该是要对threadCound个线程的结果，进行汇总 threadPool.shutdown(); System.out.println(\u0026#34;finish\u0026#34;); } } 上面的例子，也可以用CompletableFuture进行改进\nJava8 的 CompletableFuture 提供了很多对多线程友好的方法，使用它可以很方便地为我们编写多线程程序，什么异步、串行、并行或者等待所有线程执行完任务什么的都非常方便。\nCompletableFuture\u0026lt;Void\u0026gt; task1 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作 }); ...... CompletableFuture\u0026lt;Void\u0026gt; task6 = CompletableFuture.supplyAsync(()-\u0026gt;{ //自定义业务操作 }); ...... CompletableFuture\u0026lt;Void\u0026gt; headerFuture=CompletableFuture.allOf(task1,.....,task6); try { headerFuture.join(); } catch (Exception ex) { //...... } System.out.println(\u0026#34;all done. \u0026#34;); 通过循环添加任务\n//文件夹位置 List\u0026lt;String\u0026gt; filePaths = Arrays.asList(...) // 异步处理所有文件 List\u0026lt;CompletableFuture\u0026lt;String\u0026gt;\u0026gt; fileFutures = filePaths.stream() .map(filePath -\u0026gt; doSomeThing(filePath)) .collect(Collectors.toList()); // 将他们合并起来 CompletableFuture\u0026lt;Void\u0026gt; allFutures = CompletableFuture.allOf( fileFutures.toArray(new CompletableFuture[fileFutures.size()]) ); CyclicBarrier # //使用场景，不太一样的是，它一般是让子任务阻塞后，到时候一起执行 public class TestCyclicBarrier { public static void main(String[] args) { CyclicBarrier cyclicBarrier = new CyclicBarrier(3, () -\u0026gt; { try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026#34;执行咯\u0026#34;); }); for (int n = 0; n \u0026lt; 15; n++) { int finalN = n; new Thread(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(finalN); System.out.println(Thread.currentThread().getName() + \u0026#34;数据都准备好了,等待中....\u0026#34;); cyclicBarrier.await(); } catch (InterruptedException | BrokenBarrierException e) { e.printStackTrace(); } System.out.println(Thread.currentThread().getName() + \u0026#34;出发咯!\u0026#34;); }, \u0026#34;线程\u0026#34; + n).start(); } } } /* 线程0数据都准备好了,等待中.... 线程1数据都准备好了,等待中.... 线程2数据都准备好了,等待中.... 线程3数据都准备好了,等待中.... 线程4数据都准备好了,等待中.... 线程5数据都准备好了,等待中.... 线程2执行咯 线程2出发咯! 线程6数据都准备好了,等待中.... 线程7数据都准备好了,等待中.... 线程8数据都准备好了,等待中.... 线程5执行咯 线程5出发咯! 线程0出发咯! 线程1出发咯! 线程9数据都准备好了,等待中.... 线程10数据都准备好了,等待中.... 线程11数据都准备好了,等待中.... 线程8执行咯 线程3出发咯! 线程8出发咯! 线程4出发咯! 线程12数据都准备好了,等待中.... 线程13数据都准备好了,等待中.... 线程14数据都准备好了,等待中.... 线程11执行咯 线程11出发咯! 线程6出发咯! 线程7出发咯! 线程14执行咯 线程14出发咯! 线程12出发咯! 线程10出发咯! 线程9出发咯! 线程13出发咯! Process finished with exit code 0 */ CyclicBarrier 有什么用？\nCyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。\nCountDownLatch 的实现是基于 AQS 的，而 CycliBarrier 是基于 **ReentrantLock(ReentrantLock 也属于 AQS 同步器)**和 Condition 的。\nCyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是：让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。\nCyclicBarrier的原理\nCyclicBarrier 内部通过一个 count 变量作为计数器，count 的初始值为 parties 属性的初始化值，每当一个线程到了栅栏这里了，那么就将计数器减 1。如果 count 值为 0 了，表示这是这一代最后一个线程到达栅栏，就尝试执行我们构造方法中输入的任务，之后再从线程阻塞的位置继续执行。\n//每次拦截的线程数, 注意：这个是不可变的哦 private final int parties; //计数器 private int count; 结合源码\nCyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await() 方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。\npublic CyclicBarrier(int parties) { this(parties, null); } public CyclicBarrier(int parties, Runnable barrierAction) { if (parties \u0026lt;= 0) throw new IllegalArgumentException(); this.parties = parties; this.count = parties; this.barrierCommand = barrierAction; } 其中，parties 就代表了有（需要）拦截的线程的数量，当拦截的线程数量达到这个值的时候就打开栅栏，让所有线程通过。\n当调用 CyclicBarrier 对象调用 await() 方法时，实际上调用的是 dowait(false, 0L)方法。 await() 方法就像树立起一个栅栏的行为一样，将线程挡住了，当拦住的线程数量达到 parties 的值时，栅栏才会打开，线程才得以通过执行\nublic int await() throws InterruptedException, BrokenBarrierException { try { return dowait(false, 0L); } catch (TimeoutException toe) { throw new Error(toe); // cannot happen } } dowait(false,0L)方法源码如下\n// 当线程数量或者请求数量达到 count 时 await 之后的方法才会被执行。上面的示例中 count 的值就为 5。 private int count; /** * Main barrier code, covering the various policies. */ private int dowait(boolean timed, long nanos) throws InterruptedException, BrokenBarrierException, TimeoutException { final ReentrantLock lock = this.lock; // 锁住 lock.lock(); try { final Generation g = generation; if (g.broken) throw new BrokenBarrierException(); // 如果线程中断了，抛出异常 if (Thread.interrupted()) { breakBarrier(); throw new InterruptedException(); } // cout减1 int index = --count; // 当 count 数量减为 0 之后说明最后一个线程已经到达栅栏了，也就是达到了可以执行await 方法之后的条件 if (index == 0) { // tripped boolean ranAction = false; try { final Runnable command = barrierCommand; if (command != null) command.run(); ranAction = true; // 将 count 重置为 parties 属性的初始化值 // 唤醒之前等待的线程 // 下一波执行开始 nextGeneration(); return 0; } finally { if (!ranAction) breakBarrier(); } } // loop until tripped, broken, interrupted, or timed out for (;;) { try { if (!timed) trip.await(); else if (nanos \u0026gt; 0L) nanos = trip.awaitNanos(nanos); } catch (InterruptedException ie) { if (g == generation \u0026amp;\u0026amp; ! g.broken) { breakBarrier(); throw ie; } else { // We\u0026#39;re about to finish waiting even if we had not // been interrupted, so this interrupt is deemed to // \u0026#34;belong\u0026#34; to subsequent execution. Thread.currentThread().interrupt(); } } if (g.broken) throw new BrokenBarrierException(); if (g != generation) return index; if (timed \u0026amp;\u0026amp; nanos \u0026lt;= 0L) { breakBarrier(); throw new TimeoutException(); } } } finally { lock.unlock(); } } 三者区别 # CountDownLatch也能实现CyclicBarrier类似功能，不过它的栅栏被推到后就不会再重新存在了(CyclicBarrier会重新建立栅栏)\nCountDownLatch countDownLatch=new CountDownLatch(5); new Thread(()-\u0026gt;{ try { countDownLatch.await(); log.info(\u0026#34;栅栏被推开了\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } },\u0026#34;线程A\u0026#34;).start(); new Thread(()-\u0026gt;{ try { countDownLatch.await(); log.info(\u0026#34;栅栏被推开了\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } },\u0026#34;线程B\u0026#34;).start(); new Thread(()-\u0026gt;{ try { countDownLatch.await(); log.info(\u0026#34;栅栏被推开了\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } },\u0026#34;线程C\u0026#34;).start(); new Thread(()-\u0026gt;{ try { countDownLatch.await(); log.info(\u0026#34;栅栏被推开了\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } },\u0026#34;线程D\u0026#34;).start(); new Thread(()-\u0026gt;{ try { countDownLatch.await(); log.info(\u0026#34;栅栏被推开了\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } },\u0026#34;线程E\u0026#34;).start(); new Thread(()-\u0026gt;{ //countDownLatch.await(); for(int i=0;i\u0026lt;5;i++) { //每隔一秒钟推开一个栅栏 try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } log.info(\u0026#34;推开一个栅栏\u0026#34;); countDownLatch.countDown(); } },\u0026#34;线程F\u0026#34;).start(); while (true){} /**输出 2023-03-07 23:23:19 下午 [Thread: 线程F] INFO:推开一个栅栏 2023-03-07 23:23:20 下午 [Thread: 线程F] INFO:推开一个栅栏 2023-03-07 23:23:21 下午 [Thread: 线程F] INFO:推开一个栅栏 2023-03-07 23:23:22 下午 [Thread: 线程F] INFO:推开一个栅栏 2023-03-07 23:23:24 下午 [Thread: 线程F] INFO:推开一个栅栏 ///////////////////////////////////////////推开5个栅栏后(这里是一个线程推开五个，也可以5个线程-\u0026gt;每个各推开一个)，5个被阻塞的线程一起执行了 2023-03-07 23:23:24 下午 [Thread: 线程A] INFO:栅栏被推开了 2023-03-07 23:23:24 下午 [Thread: 线程E] INFO:栅栏被推开了 2023-03-07 23:23:24 下午 [Thread: 线程C] INFO:栅栏被推开了 2023-03-07 23:23:24 下午 [Thread: 线程D] INFO:栅栏被推开了 2023-03-07 23:23:24 下午 [Thread: 线程B] INFO:栅栏被推开了 */ "},{"id":144,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly03122lylock_escalation/","title":"锁升级","section":"并发","content":" 以下内容均转自 https://www.cnblogs.com/wuqinglong/p/9945618.html，部分疑惑参考自另一作者 https://github.com/farmerjohngit/myblog/issues/12 ，感谢原作者。\n【目前还是存有部分疑虑（轻量级锁那块），可能需要详细看源码才能释疑】\n概述 # 传统的synchronized为重量级锁（使用操作系统互斥量（mutex）来实现的传统锁），但是随着JavaSE1.6对synchronized优化后，部分情况下他就没有那么重了。本文介绍了JavaSE1.6为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁，以及锁结构、及锁升级过程\n实现同步的基础 # Java中每个对象都可以作为锁，具体变现形式\n对于普通同步方法，锁是当前实例对象 对于静态同步方法，锁是当前类的Class对象 对于同步方法块，锁是synchronized括号里配置的对象 一个线程试图访问同步代码块时，必须获取锁；在退出或者抛出异常时，必须释放锁\n实现方式 # JVM 基于进入和退出 Monitor 对象来实现方法同步和代码块同步，但是两者的实现细节不一样\n代码块同步：通过使用 monitorenter 和 monitorexit 指令实现的 同步方法：ACC_SYNCHRONIZED 修饰 monitorenter 指令是在编译后插入到同步代码块的开始位置，而 monitorexit 指令是在编译后插入到同步代码块的结束处或异常处\n对于同步方法，进入方法前添加一个 monitorenter 指令，退出方法后添加一个 monitorexit 指令。\ndemo：\npublic class Demo { public void f1() { synchronized (Demo.class) { System.out.println(\u0026#34;Hello World.\u0026#34;); } } public synchronized void f2() { System.out.println(\u0026#34;Hello World.\u0026#34;); } } 编译之后的字节码（使用 javap )\npublic void f1(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=3, args_size=1 0: ldc #2 // class me/snail/base/Demo 2: dup 3: astore_1 4: monitorenter 5: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 8: ldc #4 // String Hello World. 10: invokevirtual #5 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 13: aload_1 14: monitorexit 15: goto 23 18: astore_2 19: aload_1 20: monitorexit 21: aload_2 22: athrow 23: return Exception table: from to target type 5 15 18 any 18 21 18 any LineNumberTable: line 6: 0 line 7: 5 line 8: 13 line 9: 23 StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 18 locals = [ class me/snail/base/Demo, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4 public synchronized void f2(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED Code: stack=2, locals=1, args_size=1 0: getstatic #3 // Field java/lang/System.out:Ljava/io/PrintStream; 3: ldc #4 // String Hello World. 5: invokevirtual #5 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 8: return LineNumberTable: line 12: 0 line 13: 8 先说 f1() 方法，发现其中一个 monitorenter 对应了两个 monitorexit，这是不对的。但是仔细看 #15: goto 语句，直接跳转到了 #23: return 处，再看 #22: athrow 语句发现，原来第二个 monitorexit 是保证同步代码块抛出异常时锁能得到正确的释放而存在的，这就理解了。\nJava对象头（存储锁类型） # HotSpot虚拟机中，对象在内存中的布局分为三块区域：对象头、实例数据、对齐填充\n对象头又包括两部分：MarkWord和类型指针，对于数组对象，对象头中还有一部分时存储数组的长度\n多线程下synchronized的加锁，就是对同一个对象的对象头中的MarkWord中的变量进行CAS操作\nMarkWord\n类型指针 虚拟机通过这个指针确定该对象是哪个类的实例\n对象头的长度\n长度 内容 说明 32/64bit MarkWord 存储对象的hashCode或锁信息等 32/64bit Class Metadada Address 存储对象类型数据的指针 32/64bit Array Length 数组的长度(如果当前对象是数组) 如果是数组对象的话，虚拟机用3个字宽(32/64bit + 32/64bit + 32/64bit)存储对象头，如果是普通对象的话，虚拟机用2字宽存储对象头(32/64bit + 32/64bit)。\n32位的字宽为32bit，64位的字宽位64bit\n优化后synchronized锁的分类 # 级别从低到高依次是：无锁状态 -\u0026gt; 偏向锁状态 -\u0026gt; 轻量级锁状态 -\u0026gt; 重量级锁状态\n锁可以升级，但不能降级，即顺序为单向\n下面以32位系统为例，每个锁状态下，每个字宽中的内容\n无锁状态\n25bit 4bit 1bit(是否是偏向锁) 2bit(锁标志位) 对象的hashCode 对象分代年龄 0 01 这里的 hashCode 是 Object#hashCode 或者 System#identityHashCode 计算出来的值，不是用户覆盖产生的 hashCode。\n偏向锁状态\n25bit 4bit 1bit(是否是偏向锁) 2bit(锁标志位) 线程ID epoch 1 01 这里 线程ID 和 epoch 占用了 hashCode 的位置，所以，如果对象如果计算过 identityHashCode 后，便无法进入偏向锁状态，反过来，如果对象处于偏向锁状态，并且需要计算其 identityHashCode 的话，则偏向锁会被撤销，升级为重量级锁。 对于偏向锁，如果线程ID=0 表示为加锁\n什么时候会计算 HashCode 呢？比如：将对象作为 Map 的 Key 时会自动触发计算，List 就不会计算，日常创建一个对象，持久化到库里，进行 json 序列化，或者作为临时对象等，这些情况下，并不会触发计算 hashCode，所以大部分情况不会触发计算 hashCode。\nIdentity hash code是未被覆写的 java.lang.Object.hashCode() 或者 java.lang.System.identityHashCode(Object) 所返回的值。\n轻量级锁状态\n30bit 2bit 指向 线程栈 锁记录的指针 00 这里指向栈帧中的LockRecord记录，里面当然可以记录对象的identityHashCode\n重量级锁状态\n30bit 2bit 指向锁监视器的指针 10 这里指向了内存中对象的 ObjectMonitor 对象，而 ObectMontitor 对象可以存储对象的 identityHashCode 的值。\n锁的升级 # 偏向锁 # 偏向锁是针对于一个线程而言的，线程获得锁之后就不会再有解锁等操作了，这样可以省略很多开销。假如有两个线程来竞争该锁话，那么偏向锁就失效了，进而升级成轻量级锁了【注意这段解释，网上很多都错了，没有什么CAS失败才升级，只要有线程来抢，就直接升级为轻量级锁】\n为什么要这样做呢？因为经验表明，其实大部分情况下，都会是同一个线程进入同一块同步代码块的。这也是为什么会有偏向锁出现的原因。\n如果支持偏向锁（没有计算 hashCode），那么在分配(创建)对象时，分配一个可偏向而未偏向的对象（MarkWord的最后 3 位为 101，并且 Thread Id 字段的值为 0）\n1. 偏向锁的加锁 # 偏向锁标志是未偏向状态，使用 CAS 将 MarkWord 中的线程ID设置为自己的线程ID\n如果成功，则获取偏向锁成功 如果失败，则进行锁升级（也就是被别人抢了，没抢过） 偏向锁状态是已偏向状态\nMarkWord中的线程ID是自己的线程ID，则成功获取锁\nMarkWord中的线程ID不是自己的线程ID，则需要进行锁升级\n注意，这里说的锁升级，需要进行偏向锁的撤销\n2. 偏向锁的撤销 # 前提：撤销偏向的操作需要在全局检查点执行 。我们假设线程A曾经拥有锁（不确定是否释放锁）， 线程B来竞争锁对象，如果当线程A不再拥有锁时或者死亡时，线程B直接去尝试获得锁（根据是否 允许重偏向（rebiasing），获得偏向锁或者轻量级锁）；如果线程A仍然拥有锁，那么锁 升级为轻量级锁，线程B自旋请求获得锁。\n对象是不可偏向状态 不需要撤销\n对象是可偏向状态\n如果MarkWord中指向的线程不存活 （这里说的是拥有偏向锁的线程正常执行完毕后释放锁，不存活那一定要释放锁咯） 如果允许重偏向（rebiasing），则退回到可偏向但未偏向的状态；如果不允许重偏向，则变为无锁状态 如果MarkWord中的线程仍然存活（注意，关注的是存活，不是是否拥有锁） （这里说的是拥有偏向锁的线程未执行完毕但进行了锁撤销：（包括释放锁及未释放锁(有线程来抢)两种情形）） 如果线程ID指向的线程仍然拥有锁，则**★★升级为轻量级锁，MarkWord复制到线程栈中（很重要）★★；如果线程ID不再拥有锁**（那个线程已经释放了锁），则同样是退回到可偏向(如果允许)但未偏向的状态（即线程ID未空），如果不允许重偏向，则变为无锁状态 偏向锁的撤销流程如图：\n轻量级锁 # 之所以称为轻量级，是因为它仅仅使用CAS进行操作，实现获取锁\n1. 加锁流程 # 如果线程发现对象头中Mark Word已经存在指向自己栈帧的指针，即线程已经获得轻量级锁，那么只需要将0存储在自己的栈帧中（此过程称为递归加锁）；在解锁的时候，如果发现锁记录的内容为0， 那么只需要移除栈帧中的锁记录即可，而不需要更新Mark Word。\n线程尝试使用 CAS 将对象头中的 Mark Word 替换为指向锁记录（Lock Record）的指针（★如果当前锁的状态不是无锁状态，则CAS失败★很重要，不然后面有一堆疑问），如果成功当前线程获得轻量级锁， 如上图所示。（我觉得**★这里的CAS，原值为原来的markword，而不是指向其他线程的线程栈地址，否则这样意义就不对了，会导致别的线程执行到一半失去锁【注意：要结合下面的撤销流程看，锁是不会降级的，但是会撤销。撤销后对象头就变为加锁前了(但不是01哦，轻量级锁是00)】★**）\n如果成功，当前线程获得轻量级锁 如果失败，虚拟机先检查当前对象头的 Mark Word 是否指向当前线程的栈帧 如果指向，则说明当前线程已经拥有这个对象的锁，则可以直接进入同步块 执行操作 否则表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁。当竞争线程的自旋次数 达到界限值（threshold），轻量级锁将会膨胀为重量级锁。 2. 撤销流程 # 轻量级锁解锁时，如果对象的Mark Word仍然指向着线程的锁记录（LockRecord），会使用CAS操作， 将Dispalced Mark Word替换到对象头，如果成功，则表示没有竞争发生。如果失败， 表示当前锁存在锁竞争，锁就会膨胀为重量级锁。\n重量级锁 # 重量级锁（heavy weight lock），是使用操作系统互斥量（mutex）来实现的传统锁。 当所有对锁的优化都失效时，将退回到重量级锁。它与轻量级锁不同竞争的线程不再通过自旋来竞争线程， 而是直接进入堵塞状态，此时不消耗CPU，然后等拥有锁的线程释放锁后，唤醒堵塞的线程， 然后线程再次竞争锁。但是注意，当锁膨胀（inflate）为重量锁时，就不能再退回到轻量级锁。\n总结 # 首先要明确一点是引入这些锁是为了提高获取锁的效率, 要明白每种锁的使用场景, 比如偏向锁适合一个线程对一个锁的多次获取的情况; 轻量级锁适合锁执行体比较简单(即减少锁粒度或时间), 自旋一会儿就可以成功获取锁的情况.\n要明白MarkWord中的内容表示的含义.\n"},{"id":145,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/lock_escalation_deprecated2/","title":"(该文弃用)锁升级","section":"并发","content":"本文主要讲解synchronized原理和偏向锁、轻量级锁、重量级锁的升级过程，基本都转自\nhttps://blog.csdn.net/MariaOzawa/article/details/107665689 原作者:MariaOzawa\n简介 # 为什么需要锁\n并发编程中，多个线程访问同一共享资源时，必须考虑如何维护数据的原子性 历史 JDK1.5之前，Java依靠Synchronized关键字实现锁功能，Synchronized是Jvm实现的内置锁，锁的获取与释放由JVM隐式实现 JDK1.5，并发包新增Lock接口实现锁功能，提供同步功能，使用时显式获取和释放锁 区别 Lock同步锁基于Java实现，Synchronized基于底层操作系统的MutexLock实现 /ˈmjuːtɛks/ ，每次获取和释放锁都会带来用户态和内核态的切换，从而增加系统性能开销，性能糟糕，又称重量级锁 JDK1.6之后，对Synchronized同步锁做了充分优化 Synchronized同步锁实现原理 # Synchronized实现同步锁的两种方式：修饰方法；修饰方法块\n// 关键字在实例方法上，锁为当前实例 public synchronized void method1() { // code } // 关键字在代码块上，锁为括号里面的对象 public void method2() { Object o = new Object(); synchronized (o) { // code } } 这里使用编译\u0026ndash;及javap 打印字节文件\njavac -encoding UTF-8 SyncTest.java //先运行编译class文件命令 javap -v SyncTest.class //再通过javap打印出字节文件 结果如下，Synchronized修饰代码块时，由monitorenter和monitorexist指令实现同步。进入monitorenter指令后线程持有Monitor对象；退出monitorenter指令后，线程释放该Monitor对象\npublic void method2(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=4, args_size=1 0: new #2 3: dup 4: invokespecial #1 7: astore_1 8: aload_1 9: dup 10: astore_2 11: monitorenter //monitorenter 指令 12: aload_2 13: monitorexit //monitorexit 指令 14: goto 22 17: astore_3 18: aload_2 19: monitorexit 20: aload_3 21: athrow 22: return Exception table: from to target type 12 14 17 any 17 20 17 any LineNumberTable: line 18: 0 line 19: 8 line 21: 12 line 22: 22 StackMapTable: number_of_entries = 2 frame_type = 255 /* full_frame */ offset_delta = 17 locals = [ class com/demo/io/SyncTest, class java/lang/Object, class java/lang/Object ] stack = [ class java/lang/Throwable ] frame_type = 250 /* chop */ offset_delta = 4 如果Synchronized修饰同步方法，代替monitorenter和monitorexit的是 ACC_SYNCHRONIZED标志，即：JVM使用该访问标志区分方法是否为同步方法。方法调用时，调用指令检查是否设置ACC_SYNCHRONIZED标志，如有，则执行线程先持有该Monitor对象，再执行该方法；运行期间，其他线程无法获取到该Monitor对象；方法执行完成后，释放该Monitor对象 javap -v xx.class 字节文件查看\npublic synchronized void method1(); descriptor: ()V flags: ACC_PUBLIC, ACC_SYNCHRONIZED // ACC_SYNCHRONIZED 标志 Code: stack=0, locals=1, args_size=1 0: return LineNumberTable: line 8: 0 Monitor：JVM中的同步是基于进入和退出管程（Monitor）对象实现的。每个对象实例都会有一个Monitor，Monitor可以和对象一起创建、销毁。Monitor由ObjectMonitor实现，而ObjectMonitor由C++的ObjectMonitor.hpp文件实现，如下：\nObjectMonitor() { _header = NULL; _count = 0; //记录个数 _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; //处于wait状态的线程，会被加入到_WaitSet _WaitSetLock = 0 ; _Responsible = NULL ; _succ = NULL ; _cxq = NULL ; FreeNext = NULL ; _EntryList = NULL ; //处于等待锁block状态的线程，会被加入到该列表(Contention List中那些有资格成为候选资源的线程被移动到Entry List中；) _SpinFreq = 0 ; _SpinClock = 0 ; OwnerIsThread = 0 ; } //Contention List：竞争队列，所有请求锁的线程首先被放在这个竞争队列中 如上，多个线程同时访问一段同步代码时，多个线程会先被存放在ContentionList和**_EntryList**集合中，处于block状态的线程都会加入该列表。 当线程获取到对象的Monitor时，Monitor依靠底层操作系统的MutexLock实现互斥，线程申请Mutex成功，则持有该Mutex，其他线程无法获取；竞争失败的线程再次进入ContentionList被挂起 如果线程调用wait()方法，则会释放当前持有的Mutex，并且该线程进入WaitSet集合中，等待下一次被唤醒（或者顺利执行完方法也会释放Mutex） 锁升级 # 为了提升性能，Java1.6，引入了偏向锁、轻量级锁、重量级锁，来减少锁竞争带来的上下文切换，由新增的Java对象头实现了锁升级。锁只能升级不能降级，目的是提高获得锁和释放锁的效率 当Java对象被Synchronized关键字修饰为同步锁后，围绕这个锁的一系列升级操作都和Java对象头有关 JDK1.6 JVM中，对象实例在堆内存中被分为三个部分：对象头、实例数据和对齐填充。其中对象头由MarkWord、指向类的指针以及数组长度三部分组成 MarkWord记录了对象和锁相关的信息，它在64为JVM的长度是64bit，下图为64位JVM的存储结构： 32位如下 锁标志位是两位，无锁和偏向锁的锁标志位实际为01，轻量级锁的锁标志位为00 锁升级功能，主要依赖于MarkWord中的锁标志位和释放偏向锁标志位，Synchronized同步锁，是从偏向锁开始的，随着竞争越来越激烈，偏向锁升级到轻量级锁，最终升级到重量级锁 =================================从这之后往下，是有误的的============================= # 偏向锁 # JVM会为每个当前线程的栈帧中，创建用于存储锁记录的空间，官方称为Displaced Mark Word（轻量级锁会用到）\n为什么引入偏向锁\n多数情况，锁不仅不存在多线程竞争，且经常由同一线程获得，为了在这种情况让线程获得锁的代价更低而引入了偏向锁。例如：线程操作一个线程安全集合时，同一线程每次都需要获取和释放锁，则每次操作都会发生用户态和内核态的切换（重量级锁）\n解决方案（偏向锁的作用）\n当一个线程再次访问这个同步代码或方法时，该线程只需去对象头的MarkWord中，判断一下是否有偏向锁指向该线程的ID，而无需再进入Monitor去竞争对象 当对象被当作同步锁并有一个线程抢到了锁，锁标志位还是01，是否偏向锁标志位为1，并且记录抢到锁的线程ID，表示进入偏向锁状态 偏向锁的撤销 一旦出现其他线程竞争锁资源（竞争且CAS失败）时，偏向锁就会被撤销。偏向锁的撤销需要等待全局安全点，暂停持有该锁的线程，同时检查该线程是否还在执行该方法，如果是升级锁，反之(该锁)被其他线程抢占\n注：对于“CAS操作替换线程ID”这个解释，我的理解是：\n偏向锁是不会被主动释放的 偏向锁默认开启（JDK15默认关闭)，如果应用程序里所有的锁通常情况下处于竞争状态，此时可以添加JVM参数关闭偏向锁来调优系统性能\n-XX:-UseBiasedLocking //关闭偏向锁（默认打开） 轻量级锁 # 何时升级为轻量级锁 当有另外一个线程获取这个锁，由于该锁已经是偏向锁，当发现对象头MarkWord中的线程ID不是自己的线程ID，就会进行CAS操作获取锁 如果获取成功，直接替换MarkWord中的线程ID为自己ID，该锁把持偏向锁状态 如果获取失败，代表当前锁有一定的竞争，偏向锁将升级为轻量级锁 适用场景 **”绝大部分的锁，在整个同步周期内都不存在长时间的竞争“**的场景 "},{"id":146,"href":"/zh/docs/problem/Linux/20221101/","title":"post","section":"Linux","content":" 在安装可视化的时候，出现需要libmysqlclient.so.18()(64bit)解决方案\n将mysql卸载即可 http://wenfeifei.com/art/detail/yGM1BG4\n"},{"id":147,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/lock_escalation_deprecated/","title":"(该文弃用)锁升级","section":"并发","content":" 简介 # 无锁 =\u0026gt; 偏向锁 =\u0026gt; 轻量锁 =\u0026gt; 重量锁\n复习Class类锁和实例对象锁，说明Class类锁和实例对象锁不是同一把锁，互相不影响\npublic static void main(String[] args) throws InterruptedException { Object object=new Object(); new Thread(()-\u0026gt;{ synchronized (Customer.class){ System.out.println(Thread.currentThread().getName()+\u0026#34;Object.class类锁\u0026#34;); try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } } System.out.println(Thread.currentThread().getName()+\u0026#34;结束并释放锁\u0026#34;); },\u0026#34;线程1\u0026#34;).start(); //保证线程1已经获得类锁 try { TimeUnit.SECONDS.sleep(2); } catch (InterruptedException e) { e.printStackTrace(); } new Thread(()-\u0026gt;{ synchronized (object){ System.out.println(Thread.currentThread().getName()+\u0026#34;获得object实例对象锁\u0026#34;); } System.out.println(Thread.currentThread().getName()+\u0026#34;结束并释放锁\u0026#34;); },\u0026#34;线程2\u0026#34;).start(); } /* 输出 线程1Object.class类锁 线程2获得object实例对象锁 线程2结束并释放锁 线程1结束并释放锁 */ 总结图 , 00 , 01 , 10 ，没有11\n001（无锁）和101（偏向锁），00（轻量级锁），10（重量级锁）\n背景 # 下面这部分，其实在io模块有提到过\n为了保证系统稳定性和安全性，一个进程的地址空间划分为用户空间User space和内核空间Kernel space 平常运行的应用程序都运行在用户空间，只有内核空间才能进行系统态级别的资源有关操作\u0026mdash;文件管理、进程通信、内存管理 如果直接synchronized加锁，会有下面图的流程出现，频繁进行用户态和内核态的切换(阻塞和唤醒线程[线程通信]，需要频繁切换cpu的状态)\n为什么每一个对象都可以成为一个锁 markOop.hpp （对应对象标识） 每一个java对象里面，有一个Monitor对象（ObjectMonitor.cpp)关联 如图，_owner指向持有ObjectMonitor对象的线程 Monitor本质依赖于底层操作系统的MutexLock实现，操作系统实现线程之间的切换，需要从用户态到内核态的切换，成本极高 ★★ 重点：Monitor与Java对象以及线程是如何关联 如果一个java对象被某个线程锁住，则该对象的MarkWord字段中，LockWord指向monitor的起始地址（这里说的应该是重量级锁） Monitor的Owner字段会存放拥有相关联对象锁的线程id 图 锁升级 # synchronized用的锁，存在Java对象头里的MarkWord中，锁升级功能主要依赖MarkWord中锁标志位(后2位)和释放偏向锁标志位(无锁和偏向锁，倒数第3位)\n对于锁的指向\n无锁情况：（放hashcode(调用了Object.hashcode才有)) 偏向锁：MarkWord存储的是偏向的线程ID 轻量锁：MarkWord存储的是指向线程栈中LockRecord的指针 重量锁：MarkWord存储的是指向堆中的monitor对象的指针 =================================从这之后往下，是有误的的============================= # 无锁状态 初始状态，一个对象被实例化后，如果还没有任何线程竞争锁，那么它就为无锁状态（001）\npublic static void main(String[] args) { Object o = new Object(); System.out.println(ClassLayout.parseInstance(o).toPrintable()); //16字节 } /* 输出( 这里的mark,VALUE为0x0000000000000001，没有hashCode的值): java.lang.Object object internals: OFF SZ TYPE DESCRIPTION VALUE 0 8 (object header: mark) 0x0000000000000001 (non-biasable; age: 0) 8 4 (object header: class) 0xf80001e5 12 4 (object alignment gap) Instance size: 16 bytes Space losses: 0 bytes internal + 4 bytes external = 4 bytes total */ 下面是调用了hashCode()这个方法的情形:\npublic static void main(String[] args) { Object o = new Object(); System.out.println(Integer.toHexString(o.hashCode())); System.out.println(ClassLayout.parseInstance(o).toPrintable()); //16字节 } /**输出: 74a14482 java.lang.Object object internals: OFF SZ TYPE DESCRIPTION VALUE 0 8 (object header: mark) 0x00000074a1448201 (hash: 0x74a14482; age: 0) 8 4 (object header: class) 0xf80001e5 12 4 (object alignment gap) Instance size: 16 bytes Space losses: 0 bytes internal + 4 bytes external = 4 bytes total */ 偏向锁：单线程竞争\n当线程A第一次竞争到锁时，通过操作修改MarkWord中的偏向线程ID、偏向模式。如果不存在其他线程竞争，那么持有偏向锁的线程将永远不需要同步\n如果没有偏向锁，那么就会频繁出现用户态到内核态的切换\n意义：当一段同步代码，一直被同一个线程多次访问，由于只有一个线程那么该线程在后续访问时便会自动获得锁 锁在第一次被拥有的时候，记录下偏向线程ID（后续这个线程进入和退出这段加了同步锁的代码块时，不需要再次加锁和释放锁，只需要直接检查锁的MarkWord是不是放的自己的线程ID）\n如果相等，表示偏向锁是偏向于当前线程的，不需要再尝试获得锁，直到竞争才会释放锁；以后每次同步，检查锁的偏向线程ID与当前线程ID是否一致，若一致则进入同步，无需每次都加锁解锁去CAS更新对象头；如果自始至终使用锁的线程只有一个，很明显偏向锁几乎没有额外开销 如果不等，表示发生了竞争，锁已经不偏向于同一个线程，此时会尝试使用CAS来替换MarkWord里面的线程ID为新线程的ID 竞争成功，说明之前线程不存在了，MarkWord里的线程ID为新线程ID，所不会升级，仍然为偏向锁 竞争失败，需要升级为轻量级锁，才能保证线程间公平竞争锁 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程是不会主动释放锁的（尽量不会涉及用户到内核态转换）\n一个synchronized方法被一个线程抢到锁时，这个方法所在的对象，就会在其所在的MarkWord中**将偏向锁修改状态位\n如图\nJVM不用和操作系统协商设置Mutex（争取内核），不需要操作系统介入\n偏向锁相关参数\njava -XX:+PrintFlagsInitial | grep BiasedLock* intx BiasedLockingBulkRebiasThreshold = 20 {product} intx BiasedLockingBulkRevokeThreshold = 40 {product} intx BiasedLockingDecayTime = 25000 {product} intx BiasedLockingStartupDelay = 4000 #偏向锁启动延迟 4s {product} bool TraceBiasedLocking = false {product} bool UseBiasedLocking = true #默认开启偏向锁 {product} # 使用-XX:UseBiasedLocking 关闭偏向锁 例子：\npublic static void main(String[] args) throws InterruptedException { TimeUnit.SECONDS.sleep(5); //1 如果1跟下面的2兑换，则就不是偏向锁，是否是偏向锁，在创建对象的时候，就已经确认了 Object o = new Object(); //2 //System.out.println(Integer.toHexString(o.hashCode())); synchronized (o){ } System.out.println(ClassLayout.parseInstance(o).toPrintable()); //16字节 } //延迟5秒(\u0026gt;4)后，就会看到偏向锁 /* 打印，005，即二进制101 java.lang.Object object internals: OFF SZ TYPE DESCRIPTION VALUE 0 8 (object header: mark) 0x0000000002f93005 (biased: 0x000000000000be4c; epoch: 0; age: 0) 8 4 (object header: class) 0xf80001e5 12 4 (object alignment gap) Instance size: 16 bytes Space losses: 0 bytes internal + 4 bytes external = 4 bytes total */ 偏向锁的升级\n是一种等到竞争出现才释放锁的机制，只有当其他线程竞争锁时，持有偏向锁的原来线程才会被撤销；撤销需要等待全局安全点（该时间点没有字节码在执行），同时检查持有偏向锁的线程是否还在执行 如果此时第一个线程正在执行synchronized方法（处于同步块），还没执行完其他线程来抢，该偏向锁被取消并出现锁升级；此时轻量级锁由原持有偏向锁的线程持有，继续执行其同步代码，而正在竞争的线程会进入自旋等待获得该轻量级锁 如果第一个线程执行完成synchronized方法（退出同步块），而将对象头设置成无锁状态并撤销偏向锁，重新偏向 Java15之后，HotSpot不再默认开启偏向锁，使用+XX:UseBiasedLocking手动开启\n偏向锁流程总结 (转自https://blog.csdn.net/MariaOzawa/article/details/107665689) 轻量级锁 主要是为了在线程近乎交替执行同步块时提高性能 升级时机，当关闭偏向锁或多线程竞争偏向锁会导致偏向锁升级为轻量级锁 标志位为00\n"},{"id":148,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly03121lyobject-concurrent/","title":"对象内存布局和对象头","section":"并发","content":" 对象布局 # heap （where）: new (eden ,s0 ,s1) ,old, metaspace\n对象的构成元素（what） HotSpot虚拟机里，对象在堆内存中的存储布局分为三个部分 对象头（Header） 对象标记 MarkWord 类元信息（类型指针 Class Pointer，指向方法区的地址） 对象头多大 length（数组才有） 实例数据（Instance Data） 对其填充（Padding，保证整个对象大小，是8个字节的倍数） 对象头 # 对象标记\nObject o= new Object(); //new一个对象，占内存多少 o.hashCode() //hashCode存在对象哪个地方 synchronized(o){ } //对象被锁了多少次（可重入锁） System.gc(); //躲过了几次gc（次数） 上面这些，哈希码、gc标记、gc次数、同步锁标记、偏向锁持有者，都保存在对象标记里面 如果在64位系统中，对象头中，**mark word（对象标记）**占用8个字节（64位）；**class pointer（类元信息）**占用8个字节，总共16字节（忽略压缩指针） 无锁的时候， 类型指针 注意下图，指向方法区中（模板）的地址 实例数据和对齐填充 # 实例数据\n用来存放类的属性（Filed）数据信息，包括父类的属性信息\n对齐填充\n填充到长度为8字节，因为虚拟机要求对象起始地址必须是8字节的整数倍（对齐填充不一定存在）\n示例\nclass Customer{ int id;//4字节 boolean flag=false; //1字节 } //Customer customer=new Customer(); //该对象大小：对象头（对象标记8+类型指针8）+实例数据（4+1）=21字节 ===\u0026gt; 为了对齐填充，则为24字节 源码查看 # 具体的（64位虚拟机为主） # 无锁和偏向锁的锁标志位(最后2位)都是01 无锁的倒数第3位，为0，表示非偏向锁 偏向锁的倒数第3位，为1，表示偏向锁 轻量级锁的锁标志位（最后2位）是00 重量级锁的锁标志位（最后2位）是10 GC标志（最后2位）是11 如上所示，对象分代年龄4位，即最大值为15（十进制）\n源码中\n使用代码演示上述理论（JOL) # \u0026lt;!--引入依赖，用来分析对象在JVM中的大小和分布--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.openjdk.jol\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jol-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.16\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; //使用\n//VM的细节详细情况 System.out.println(VM.current().details()); //所有对象分配字节都是8的整数倍 System.out.println(VM.current().objectAlignment()); /* 输出： # Running 64-bit HotSpot VM. # Using compressed oop with 3-bit shift. # Using compressed klass with 3-bit shift. # Objects are 8 bytes aligned. # Field sizes by type: 4, 1, 1, 2, 2, 4, 4, 8, 8 [bytes] # Array element sizes: 4, 1, 1, 2, 2, 4, 4, 8, 8 [bytes] 8 */ 简单的情形 注意，下面的8 4 (object header: class) 0xf80001e5，由于开启了类型指针压缩，只用了4个字节\npublic class Hello4 { public static void main(String[] args) throws InterruptedException { Object o = new Object(); System.out.println(ClassLayout.parseInstance(o).toPrintable()); //16字节 Customer customer = new Customer(); System.out.println(ClassLayout.parseInstance(customer).toPrintable()); //16字节 } } class Customer{ } /*输出 java.lang.Object object internals: OFF SZ TYPE DESCRIPTION VALUE 0 8 (object header: mark) 0x0000000000000001 (non-biasable; age: 0) 8 4 (object header: class) 0xf80001e5 12 4 (object alignment gap) Instance size: 16 bytes Space losses: 0 bytes internal + 4 bytes external = 4 bytes total com.ly.Customer object internals: OFF SZ TYPE DESCRIPTION VALUE 0 8 (object header: mark) 0x0000000000000001 (non-biasable; age: 0) 8 4 (object header: class) 0xf800cc94 12 4 (object alignment gap) Instance size: 16 bytes Space losses: 0 bytes internal + 4 bytes external = 4 bytes total Process finished with exit code 0 */ 带有实例数据\npublic class Hello4 { public static void main(String[] args) throws InterruptedException { Customer customer = new Customer(); System.out.println(ClassLayout.parseInstance(customer).toPrintable()); //16字节 } } class Customer{ private int a; private boolean b; } /*输出 com.ly.Customer object internals: OFF SZ TYPE DESCRIPTION VALUE 0 8 (object header: mark) 0x0000000000000001 (non-biasable; age: 0) 8 4 (object header: class) 0xf800cc94 12 4 int Customer.a 0 16 1 boolean Customer.b false 17 7 (object alignment gap) Instance size: 24 bytes Space losses: 0 bytes internal + 7 bytes external = 7 bytes total */ java 运行中添加参数 -XX:MaxTenuringThreshold = 16 ，则会出现下面错误，即分代gc最大年龄为15 压缩指针的相关说明\n使用 java -XX:+PrintComandLineFlags -version ，打印参数\n其中有一个, -XX:+UseCompressedClassPointers ，即开启了类型指针压缩，只需要4字节\n当使用了类型指针压缩（默认）时，一个无任何属性对象是 8字节(markWord) + 4字节（classPointer) + 4字节(对齐填充) = 16字节\n下面代码，使用了 -XX:-UseCompressedClassPointers进行关闭压缩指针 一个无任何属性对象是 8字节(markWord) + 8字节（classPointer) = 16字节\npublic class Hello4 { public static void main(String[] args) throws InterruptedException { Object o = new Object(); System.out.println(ClassLayout.parseInstance(o).toPrintable()); //16字节 //16字节 } } /*输出 java.lang.Object object internals: OFF SZ TYPE DESCRIPTION VALUE 0 8 (object header: mark) 0x0000000000000001 (non-biasable; age: 0) 8 8 (object header: class) 0x000000001dab1c00 Instance size: 16 bytes Space losses: 0 bytes internal + 0 bytes external = 0 bytes total */ "},{"id":149,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0302lyconcurrent-02/","title":"并发02","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nJMM（JavaMemoryModel) # 详见-知识点 volatile关键字 # 保证变量可见性\n使用volatile关键字保证变量可见性，如果将变量声明为volatile则指示JVM该变量是共享且不稳定的，每次使用它都到主存中读取\nvolatile关键字并非Java语言特有，在C语言里也有，它最原始的意义就是禁用CPU缓存。\nvolatile关键字只能保证数据可见性，不能保证数据原子性。synchronized关键字两者都能保证\n不可见的例子\npackage com.concurrent; import java.util.concurrent.TimeUnit; public class TestLy { //如果加上volatile,就能保证可见性，线程1 才能停止 boolean stop = false;//对象属性 public static void main(String[] args) throws InterruptedException { TestLy atomicTest = new TestLy(); new Thread(() -\u0026gt; { while (!atomicTest.stop) { //这里不能加System.out.println ,因为这个方法内部用了synchronized修饰,会导致获取主内存的值， //就没法展示效果了 /*System.out.println(\u0026#34;1还没有停止\u0026#34;);*/ } System.out.println(Thread.currentThread().getName()+\u0026#34;停止了\u0026#34;); },\u0026#34;线程1\u0026#34;).start(); new Thread(() -\u0026gt; { try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException e) { e.printStackTrace(); } atomicTest.stop= true; System.out.println(Thread.currentThread().getName()+\u0026#34;让线程1停止\u0026#34;); },\u0026#34;线程2\u0026#34;).start(); while (true){} } } 如何禁止指令重排 使用volatile关键字，除了可以保证变量的可见性，还能防止JVM指令重排。当我们对这个变量进行读写操作的时候，-会通过插入特定的内存屏障来禁止指令重排\nJava中，Unsafe类提供了三个开箱即用关于内存屏障相关的方法，屏蔽了操作系统底层的差异\n可以用来实现和volatile禁止重排序的效果\npublic native void loadFence(); //读指令屏障 public native void storeFence(); //写指令屏障 public native void fullFence(); //读写指令屏障 例子（通过双重校验锁实现对象单例），保证线程安全\npublic class Singleton { private volatile static Singleton uniqueInstance; private Singleton() { } public static Singleton getUniqueInstance() { //先判断对象是否已经实例过，没有实例化过才进入加锁代码(第3、4次 //就不需要再进来(synchronized了)) //避免了不论如何都进行加锁的情况 if (uniqueInstance == null) { //...一些其他代码 //加锁，并判断如果未初始化则进行初始化 synchronized (Singleton.class) { //别晕了，这个是一定要判断的【判断是否已经初始化， //如果还未初始化才进行new对象】 if (uniqueInstance == null) { uniqueInstance = new Singleton(); } } } return uniqueInstance; } } 这里，uniqueInstance采用volatile的必要性：主要分析``` uniqueInstance = new Singleton(); ```分三步（正常情况） 1. 为uniqueInstance**分配内存空间** 2. **初始化** uniqueInstance 3. 将uniqueInstance**指向**被分配的空间 由于指令重排的关系，可能会编程1-\u0026gt;3-\u0026gt;2 ，指令重排在单线程情况下不会出现问题，而多线程， - 就会导致可能指针非空的时候，实际该指针所指向的对象（实例）并还没有初始化 - 例如，线程 T1 执行了 1 和 3，此时 T2 调用 `getUniqueInstance`() 后发现 `uniqueInstance` 不为空，因此返回 `uniqueInstance`，但此时 `uniqueInstance` 还未被初始化**（就会造成一些问题）** - 即可能存在1，3已经完成，2还未完成 volatile不能保证原子性\n下面的代码，输出结果小于2500\npublic class VolatoleAtomicityDemo { public volatile static int inc = 0; public void increase() { inc++; } public static void main(String[] args) throws InterruptedException { ExecutorService threadPool = Executors.newFixedThreadPool(5); VolatoleAtomicityDemo volatoleAtomicityDemo = new VolatoleAtomicityDemo(); for (int i = 0; i \u0026lt; 5; i++) { threadPool.execute(() -\u0026gt; { for (int j = 0; j \u0026lt; 500; j++) { volatoleAtomicityDemo.increase(); } }); } // 等待1.5秒，保证上面程序执行完成 Thread.sleep(1500); System.out.println(inc); threadPool.shutdown(); } } 对于上面例子, 很多人会误以为inc++ 是原子性的，实际上inc ++ 是一个复合操作，即\n读取inc的值**（到线程内存）** 对inc加1 将加1后的值写回内存（主内存） 这三部操作并不是原子性的，有可能出现：\n线程1对inc读取后，尚未修改 线程2又读取了，并对他进行+1，然后将+1后的值写回主存 此时线程2操作完毕后，线程1在之前读取的基础上进行一次自增，这将覆盖第2步操作的值，导致inc只增加了1（实际两个线程处理了，应该加2才对） 如果要保证上面代码运行正确，可以使用synchronized、Lock或者AtomicInteger，如\n//synchronized public synchronized void increase() { inc++; } //或者AtomicInteger public AtomicInteger inc = new AtomicInteger(); public void increase() { inc.getAndIncrement(); } //或者ReentrantLock改进 Lock lock = new ReentrantLock(); public void increase() { lock.lock(); try { inc++; } finally { lock.unlock(); } } synchronized关键字 # 说一说自己对synchronized的理解\n翻译成中文是同步的意思，主要解决的是多个线程之间访问资源的同步性，保证被它修饰的方法/代码块，在任一时刻只有一个线程执行 Java早期版本中，synchronized属于重量级锁；监视器锁（monitor）依赖底层操作系统的Mutex Lock来实现，Java线程映射到操作系统的原生线程上 挂起或唤醒线程，都需要操作系统帮忙完成，即操作系统实现线程之间切换，需要从用户态转换到内核态，这个转换时间成本高 Java 6 之后，Java官方对synchronized较大优化，引入了大量优化：自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等减少所操作的开销 如何使用synchronized关键字\n修饰实例方法 修饰静态方法 修饰代码块 修饰实例方法（锁当前对象实例） 给当前对象实例加锁，进入同步代码前要获得当前对象实例的锁\nsynchronized void method() { //业务代码 } 修饰静态方法（锁当前类） 给当前类枷锁，会作用于类的所有对象实例，进入同步代码前要获得当前class的锁; 这是因为静态成员归整个类所有，而不属于任何一个实例对象，不依赖于类的特定实例，被类所有实例共享\nsynchronized static void method() { //业务代码 } 静态synchronized方法和非静态synchronized方法之间的调用互斥吗：不互斥\n如果线程A调用实例对象的非静态方法，而线程B调用这个实例所属类的静态synchronized方法，是允许的，不会发生互斥；因为访问静态synchronized方法占用的锁是当前类的锁；非静态synchronized方法占用的是当前实例对象的锁\n修饰代码块（锁指定对象/类）\nsynchronized(object) 表示进入同步代码库前要获得 给定对象的锁。 synchronized(类.class) 表示进入同步代码前要获得 给定 Class 的锁 synchronized(this) { //业务代码 } 总结\nsynchronized 关键字加到 static 静态方法和 synchronized(class) 代码块上都是是给 Class 类上锁； synchronized 关键字加到实例方法上是给对象实例上锁； 尽量不要使用 synchronized(String a) 因为 JVM 中，字符串常量池具有缓存功能。(所以就会导致，容易**和其他地方的代码（同样的值的字符串）**互斥，因为是缓冲池的同一个对象) 讲一下synchronized关键字的底层原理 synchronized底层原理是属于JVM层面的\nsynchronized + 代码块 例子：\npublic class SynchronizedDemo { public void method() { synchronized (this) { System.out.println(\u0026#34;synchronized 代码块\u0026#34;); } } } 使用javap命令查看SynchronizedDemo类相关字节码信息：对编译后的SynchronizedDemo.class文件，使用javap -c -s -v -l SynchronizedDemo.class\n同步代码块的实现，使用的是monitorenter和monitorexit指令，其中monitorenter指令指向同步代码块开始的地方，monitorexit指向同步代码块结束的结束位置 执行monitorenter指令就是获取对象监视器monitor的持有权\n在HotSport虚拟机中，Monitor基于C++实现，由ObjectMonitor实现：每个对象内置了ObjectMonitor对象。wait/notify等方法也基于monitor对象，所以只有在同步块或者方法中（获得锁）才能调用wait/notify方法，否则会抛出java.lang.IllegalMonitorStateException异常的原因\nnotify()仅仅是通知，并不会释放锁；wait()会立即释放锁，例子：\nObject obj = new Object(); new Thread(() -\u0026gt; { synchronized (obj) { try { log.info(\u0026#34;运行中\u0026#34;); TimeUnit.SECONDS.sleep(3); log.info(\u0026#34;3s后释放锁\u0026#34;); obj.wait();//会释放锁 log.info(\u0026#34;完成执行\u0026#34;); } catch (InterruptedException e) { e.printStackTrace(); } } }, \u0026#34;线程1\u0026#34;).start(); //保证线程2在线程1之后启动 TimeUnit.SECONDS.sleep(1); new Thread(() -\u0026gt; { synchronized (obj) { log.info(\u0026#34;获得锁\u0026#34;); try { TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } log.info(\u0026#34;5s后唤醒线程1\u0026#34;); obj.notify(); try { TimeUnit.SECONDS.sleep(3); } catch (InterruptedException e) { e.printStackTrace(); } log.info(\u0026#34;完成执行\u0026#34;); } }, \u0026#34;线程2\u0026#34;).start(); /**打印 2023-03-07 11:33:00 上午 [Thread: 线程1] INFO:运行中 2023-03-07 11:33:03 上午 [Thread: 线程1] INFO:3s后释放锁 2023-03-07 11:33:03 上午 [Thread: 线程2] INFO:获得锁 2023-03-07 11:33:08 上午 [Thread: 线程2] INFO:5s后唤醒线程1 2023-03-07 11:33:21 上午 [Thread: 线程2] INFO:完成执行 2023-03-07 11:33:21 上午 [Thread: 线程1] //这段输出永远会在最后（线程2释放锁才会输出） INFO:完成执行 Process finished with exit code 0 */ 执行monitorenter时，**尝试获取**对象的锁，如果锁计数器为0则表示所可以被获取，获取后锁计数器设为1，简单的流程 只有拥有者线程才能执行monitorexit来释放锁，执行monitorexit指令后，锁计数器设为0（应该是减一，与可重入锁有关），当计数器为0时，表明锁被释放，其他线程可以尝试获得锁(如果某个线程获取锁失败，那么该线程就会阻塞等待，直到锁被（另一个线程）释放) synchronized修饰方法\npublic class SynchronizedDemo2 { public synchronized void method() { System.out.println(\u0026#34;synchronized 方法\u0026#34;); } } 如图 : 对比（下面是对synchronized代码块）：\nsynchronized修饰的方法没有monitorenter和monitorexit指令，而是ACC_SYNCHRONIZED标识（flags），该标识指明方法是一个同步方法（JVM通过访问标志判断方法是否声明为同步方法），从而执行同步调用 如果是实例方法，JVM 会尝试获取实例对象的锁。如果是静态方法，JVM 会尝试获取当前 class 的锁。\n总结\nsynchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。\nsynchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。\n不过两者的本质都是对对象监视器 monitor 的获取。\nJava1.6之后的synchronized关键字底层做了哪些优化 这是一个链接 详情见另一个文章\nJDK1.6对锁的实现，引入了大量的优化，如偏向锁、轻量级锁、自旋锁、适应性自旋锁、锁消除、锁粗化等技术来减少操作的开销 锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。锁可以升级但不可以降级，这种策略是为了提高获得锁和释放锁的效率 synchronized和volatile的区别 synchronized和volatile是互补的存在，而非对立\nvolatile关键字是线程同步的轻量级实现，所以volatile性能肯定比synchronized关键字好，但volatile用于变量而synchronized关键字修饰方法及代码块 volatile关键字能保证数据的可见性、有序性，但无法保证原子性；synchronized三者都能保证 volatile主要还是用于解决变量在线程之间的可见性，而synchronized关键字解决的是多个线程之间访问资源的同步性 synchronized 和 ReentrantLock 的区别\n两者都是可重入锁 ”可重入锁“指的是，自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的\n反之，如果是不可重入锁的话，就会造成死锁。 同一个线程，每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁 synchronized依赖于JVM，而ReentrantLock依赖于API synchronized为虚拟机在JDK1.6进行的优化，但这些优化是在虚拟机层面实现的；ReentrantLock是JDK层面实现的，使用时，使用lock()和unlock()并配合try/finally语句块来完成 （Java代码） ReentrantLock 比 synchronized 增加了一些高级功能 ReentrantLock增加了一些高级功能，主要有\n等待可中断，提供了能够中断等待锁的线程的机制，通过lock.lockInterruptibly()来实现该机制。即正在等待的线程可以放弃等待，改为处理其他事情\n可实现公平锁：可以指定是公平锁还是非公平锁，而synchronized只能是非公平锁。 所谓公平锁就是先等待的线程先获得锁。ReentrantLock默认是非公平的，可以通过构造方法指定是否公平\n可实现选择性的通知（锁可以绑定多个条件） synchronized关键字与wait()和notify()/notifyAll()方法相结合可以实现等待/通知机制。**ReentrantLock类当然也可以实现，但是需要借助于Condition接口与newCondition()**方法。\nReentrantLock reentrantLock=new ReentrantLock(); Condition condition = reentrantLock.newCondition(); condition.await(); condition.signal(); Condition是 JDK1.5 之后才有的，它具有很好的灵活性，比如可以实现多路通知功能也就是在一个Lock对象中可以创建多个Condition实例（即对象监视器），**线程对象可以注册在指定的Condition中，从而可以有选择性的进行线程通知，在调度线程上更加灵活。 ** 在使用notify()/notifyAll()方法进行通知时，被通知的线程是由 JVM 选择的，用ReentrantLock类结合Condition实例可以实现“选择性通知” ，这个功能非常重要，而且是 Condition 接口默认提供的。 synchronized关键字就相当于整个 Lock 对象中只有一个Condition实例，所有的线程都注册在它一个身上。如果执行notifyAll()方法的话就会通知所有处于等待状态的线程这样会造成很大的效率问题， Condition实例的signalAll()方法 只会唤醒注册在该Condition实例中的所有等待线程。 ThreadLocal # ThreadLocal有什么用\n通常情况下，创建的变量是可以被任何一个线程访问并修改的 JDK自带的ThreadLocal类，该类主要解决的就是让每个线程绑定自己的值，可以将ThreadLocal类形象的比喻成存放数据的盒子，盒子中可以存储每个线程的私有数据 对于ThreadLocal变量，访问这个变量的每个线程都会有这个变量的本地副本。使用get()和set()来获取默认值或将其值更改为当前线程所存的副本的值 如图\n如何使用ThreadLocal Demo演示实际中如何使用ThreadLocal\nimport java.text.SimpleDateFormat; import java.util.Random; public class ThreadLocalExample implements Runnable{ // SimpleDateFormat 不是线程安全的，所以每个线程都要有自己独立的副本 private static final ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; formatter = ThreadLocal.withInitial(() -\u0026gt; new SimpleDateFormat(\u0026#34;yyyyMMdd HHmm\u0026#34;)); /* 非lambda写法 private static final ThreadLocal\u0026lt;SimpleDateFormat\u0026gt; formatter = new ThreadLocal\u0026lt;SimpleDateFormat\u0026gt;(){ @Override protected SimpleDateFormat initialValue(){ return new SimpleDateFormat(\u0026#34;yyyyMMdd HHmm\u0026#34;); } }; */ public static void main(String[] args) throws InterruptedException { ThreadLocalExample obj = new ThreadLocalExample(); for(int i=0 ; i\u0026lt;10; i++){ Thread t = new Thread(obj, \u0026#34;\u0026#34;+i); Thread.sleep(new Random().nextInt(1000)); t.start(); } } //formatter.get().toPattern() 同一个对象的线程变量formatter(里面封装了一个simpleDateFormate对象，具有初始值) //每个线程访问时，先打印它的初始值，然后休眠1s（1s内的随机数），反正每个线程随机数不同，然后修改它 //结果：虽然前面执行的线程，修改值，但是后面执行的线程打印的值还是一样的 没有修改 @Override public void run() { System.out.println(\u0026#34;Thread Name= \u0026#34;+Thread.currentThread().getName()+\u0026#34; default Formatter = \u0026#34;+formatter.get().toPattern()); try { Thread.sleep(new Random().nextInt(1000)); } catch (InterruptedException e) { e.printStackTrace(); } //formatter pattern is changed here by thread, but it won\u0026#39;t reflect to other threads formatter.set(new SimpleDateFormat());//new SimpleDateFormat().toPattern()默认值为\u0026#34;yy-M-d ah:mm\u0026#34; System.out.println(\u0026#34;Thread Name= \u0026#34;+Thread.currentThread().getName()+\u0026#34; formatter = \u0026#34;+formatter.get().toPattern()); } } /*虽然前面执行的线程，修改值，但是后面执行的线程打印的值还是一样的 没有修改 , 结果如下： Thread Name= 0 default Formatter = yyyyMMdd HHmm Thread Name= 0 formatter = yy-M-d ah:mm Thread Name= 1 default Formatter = yyyyMMdd HHmm Thread Name= 2 default Formatter = yyyyMMdd HHmm Thread Name= 1 formatter = yy-M-d ah:mm Thread Name= 3 default Formatter = yyyyMMdd HHmm Thread Name= 2 formatter = yy-M-d ah:mm Thread Name= 4 default Formatter = yyyyMMdd HHmm Thread Name= 3 formatter = yy-M-d ah:mm Thread Name= 4 formatter = yy-M-d ah:mm Thread Name= 5 default Formatter = yyyyMMdd HHmm Thread Name= 5 formatter = yy-M-d ah:mm Thread Name= 6 default Formatter = yyyyMMdd HHmm Thread Name= 6 formatter = yy-M-d ah:mm Thread Name= 7 default Formatter = yyyyMMdd HHmm Thread Name= 7 formatter = yy-M-d ah:mm Thread Name= 8 default Formatter = yyyyMMdd HHmm Thread Name= 9 default Formatter = yyyyMMdd HHmm Thread Name= 8 formatter = yy-M-d ah:mm Thread Name= 9 formatter = yy-M-d ah:mm */ ThreadLocal原理了解吗\n从Thread类源代码入手\npublic class Thread implements Runnable { //...... //与此线程有关的ThreadLocal值。由ThreadLocal类维护 ThreadLocal.ThreadLocalMap threadLocals = null; //与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护 ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; //...... } Thread类中有一个threadLocals和一个inheritableThreadLocals变量，它们都是ThreadLocalMap类型的变量，ThreadLocalMap可以理解为ThreadLocal类实现的定制化HashMap ( key为threadLocal , value 为值) 默认两个变量都是null，当调用set或get时会创建，实际调用的是ThreadLocalMap类对应的get()、set()方法\n//★★ThreadLocal类的set() 方法 public void set(T value) { //获取当前请求的线程 Thread t = Thread.currentThread(); //取出 Thread 类内部的 threadLocals 变量(哈希表结构) ThreadLocalMap map = getMap(t); if (map != null) // 将需要存储的值放入到这个哈希表中 //★★实际使用的方法 map.set(this, value); else //★★实际使用的方法 createMap(t, value); } ThreadLocalMap getMap(Thread t) { return t.threadLocals; } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } public T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T result = (T)e.value; return result; } } return setInitialValue(); } /** * Set the value associated with key. * * @param key the thread local object * @param value the value to be set */ private void set(ThreadLocal\u0026lt;?\u0026gt; key, Object value) { // We don\u0026#39;t use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode \u0026amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == key) { e.value = value; return; } if (k == null) { replaceStaleEntry(key, value, i); return; } } tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) \u0026amp;\u0026amp; sz \u0026gt;= threshold) rehash(); } 如上，实际存取都是从Thread的threadLocals （ThreadLocalMap类）中，并不是存在ThreadLocal上，ThreadLocal用来传递了变量值，只是ThreadLocalMap的封装\nThreadLocal类中通过Thread.currentThread()获取到当前线程对象后，直接通过getMap(Thread t) 可以访问到该线程的ThreadLocalMap对象\n【★★最重要★★】每个Thread中具备一个ThreadLocalMap，而ThreadLocalMap可以存储以ThreadLocal为key，Object对象为value的键值对\nThreadLocalMap(ThreadLocal\u0026lt;?\u0026gt; firstKey, Object firstValue) { //...... } 比如我们在同一个线程中声明了两个 ThreadLocal 对象的话， Thread内部都是使用仅有的那个ThreadLocalMap 存放数据的，ThreadLocalMap的 key 就是 ThreadLocal对象，value 就是 ThreadLocal 对象调用set方法设置的值\nThreadLocal数据结构如下图所示 ThreadLocalMap是ThreadLocal的静态内部类。 ThreadLocal内存泄露问题时怎么导致的\n前提知识：强引用、软引用、弱引用和虚引用的区别\n强引用StrongReference\n是最普遍的一种引用方式，只要强引用存在，则垃圾回收器就不会回收这个对象\n软引用 SoftReference\n如果内存足够不回收，如果内存不足则回收\n弱引用WeakReference 如果一个对象只具有弱引用，那就类似于可有可无的生活用品。弱引用与软引用的区别在于：只具有弱引用的对象拥有更短暂的生命周期。在垃圾回收器线程扫描它 所管辖的内存区域的过程中，一旦发现了只具有弱引用的对象，不管当前内存空间足够与否，都会回收它的内存。不过，由于垃圾回收器是一个优先级很低的线程， 因此不一定会很快发现那些只具有弱引用的对象。\n弱引用可以和一个引用队列（ReferenceQueue）联合使用，如果弱引用所引用的对象被垃圾回收，Java 虚拟机就会把这个弱引用加入到与之关联的引用队列中。\n虚引用PhantomReference [ˈfæntəm] 幻影\n如果一个对象仅持有虚引用，那么它就和没有任何引用一样，在任何时候都可能被垃圾回收器回收。虚引用主要用来跟踪对象被垃圾回收器回收的活动 虚引用与软引用和弱引用的一个区别在于：虚引用必须和引用队列 （ReferenceQueue）联合使用。当垃圾回收器准备回收一个对象时，如果发现它还有虚引，就会在回收对象的内存之前，把这个虚引用加入到与之关联的引用队列中。 ThreadLocalMap中，使用的key为ThreadLocal的弱引用（源码中，即Entry），而value是强引用\n//注意看ThreadLocal的set()方法 /** * Set the value associated with key. * * @param key the thread local object * @param value the value to be set */ private void set(ThreadLocal\u0026lt;?\u0026gt; key, Object value) { // We don\u0026#39;t use a fast path as with get() because it is at // least as common to use set() to create new entries as // it is to replace existing ones, in which case, a fast // path would fail more often than not. Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode \u0026amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == key) { e.value = value; return; } if (k == null) { replaceStaleEntry(key, value, i); return; } } //★★注意看这行，结合下面 tab[i] = new Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) \u0026amp;\u0026amp; sz \u0026gt;= threshold) rehash(); } 所以，ThreadLocal没有被外部强引用的情况下，垃圾回收的时候 key会被清理掉，而value不会 ```java static class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } ``` 此时，ThreadLocalMap中就会出现key为null的Entry，如果不做任何措施，value永远无法被GC回收，此时会产生内存泄漏。ThreadLocaMap实现中已经考虑了这种情况，在调用set()、get()、**remove()**方法时，清理掉key为null的记录 所以使用完ThreadLocal的方法后，最好手动调用remove()方法\nset()方法中的cleanSomeSlots() 已经清除了部分key为null的记录。但是还不完整，还要依赖 expungeStaleEntry() 方法（在remove中）\n//remove()方法 public void remove() { ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); } /** * Remove the entry for key. */ private void remove(ThreadLocal\u0026lt;?\u0026gt; key) { Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode \u0026amp; (len-1); for (Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { if (e.get() == key) { e.clear(); expungeStaleEntry(i); return; } } } /** * Expunge a stale entry by rehashing any possibly colliding entries * lying between staleSlot and the next null slot. This also expunges * any other stale entries encountered before the trailing null. See * Knuth, Section 6.4 * * @param staleSlot index of slot known to have null key * @return the index of the next null slot after staleSlot * (all between staleSlot and this slot will have been checked * for expunging). */ private int expungeStaleEntry(int staleSlot) { Entry[] tab = table; int len = tab.length; // expunge entry at staleSlot tab[staleSlot].value = null; tab[staleSlot] = null; size--; // Rehash until we encounter null Entry e; int i; for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal\u0026lt;?\u0026gt; k = e.get(); if (k == null) { e.value = null; tab[i] = null; size--; } else { int h = k.threadLocalHashCode \u0026amp; (len - 1); if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i; } "},{"id":150,"href":"/zh/docs/technology/springCloud/bl_zhouyang/base/","title":"基础","section":"基础(尚硅谷)_","content":" springCloud涉及到的技术有哪些 约定 \u0026gt; 配置 \u0026gt; 编码 "},{"id":151,"href":"/zh/docs/technology/Review/java_guide/java/Concurrent/ly0301lyconcurrent-01/","title":"并发01","section":"并发","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n什么是进程和线程\n进程：是程序的一次执行过程，是系统运行程序的基本单位 系统运行一个程序，即一个进程从创建、运行到消亡的过程\n启动main函数则启动了一个JVM进程，main函数所在线程为进程中的一个线程，也称主线程\n以下为一个个的进程\n查看java进程\njps -l 32 org.jetbrains.jps.cmdline.Launcher 10084 16244 com.Test 17400 sun.tools.jps.Jps 杀死进程\ntaskkill /f /pid 16244 何为线程\n线程，比进程更小的执行单位\n同类的多个线程共享进程的堆和方法区资源，但每个线程有自己的程序计数器、虚拟机栈、本地方法栈，又被称为轻量级进程\nJava天生就是多线程程序，如：\npublic class MultiThread { public static void main(String[] args) { // 获取 Java 线程管理 MXBean ThreadMXBean threadMXBean = ManagementFactory.getThreadMXBean(); // 不需要获取同步的 monitor 和 synchronizer 信息，仅获取线程和线程堆栈信息 ThreadInfo[] threadInfos = threadMXBean.dumpAllThreads(false, false); // 遍历线程信息，仅打印线程 ID 和线程名称信息 for (ThreadInfo threadInfo : threadInfos) { System.out.println(\u0026#34;[\u0026#34; + threadInfo.getThreadId() + \u0026#34;] \u0026#34; + threadInfo.getThreadName()); } } } //输出 [5] Attach Listener //添加事件 [4] Signal Dispatcher // 分发处理给 JVM 信号的线程 [3] Finalizer //调用对象 finalize 方法的线程 [2] Reference Handler //清除 reference 线程 [1] main //main 线程,程序入口 也就是说，一个Java程序的运行，是main线程和多个其他线程同时运行\n请简要描述线程与进程的关系，区别及优缺点\n从JVM角度说明 Java内存区域 一个进程拥有多个线程，多个线程共享进程的堆和方法区（JDK1.8: 元空间），每个线程拥有自己的程序计数器、虚拟机栈、本地方法栈 总结\n线程是进程划分成的更小运行单位 线程和进程最大不同在于各进程基本独立，而各线程极有可能互相影响 线程开销小，但不利于资源保护；进程反之 程序计数器为什么是私有\n程序计数器的作用\n单线程情况下，字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。 在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。 如果执行的是native方法，则程序计数器记录的是undefined地址；执行Java方法则记录的是下一条指令的地址\n私有，是为了线程切换后能恢复到正确的执行位置\n虚拟机栈和本地方法栈为什么私有\n虚拟机栈：每个Java方法执行时同时会创建一个栈帧用于存储局部变量表、操作数栈、常量池引用等信息 本地方法栈：和虚拟机栈类似，区别是虚拟机栈为虚拟机执行Java方法 （字节码）服务，本地方法栈则为虚拟机使用到的Native方法服务。HotSpot虚拟机中和Java虚拟机栈合二为一 为了保证线程中局部变量不被别的线程访问到，虚拟机栈和本地方法栈是私有的 堆和方法区是所有线程共享的资源，堆是进程中最大一块内存，用于存放新创建的对象（几乎所有对象都在这分配内存）; 方法区则存放**已被加载的 ** 类信息、常量、静态变量、即时编译器编译后的代码等数据\n并发与并行的区别\n并发：两个及两个以上的作业在同一时间段内执行（线程，同一个代码同一秒只能由一个线程访问） 并行：两个及两个以上的作业同一时刻执行 关键点：是否同时执行，只有并行才能同时执行 同步和异步\n同步：发出调用后，没有得到结果前，该调用不能返回，一直等待 异步：发出调用后，不用等返回结果，该调用直接返回 为什么要使用多线程\n从计算机底层来说：线程是轻量级进程，程序执行最小单位，线程间切换和调度 成本远小于进程。多核CPU时代意味着多个线程可以同时运行，减少线程上下文切换 从当代互联网发展趋势：如今系统并发量大，利用多线程机制可以大大提高系统整体并发能力及性能 深入计算机底层 单核时代：提高单进程利用CPU和IO系统的效率。当请求IO的时候，如果Java进程中只有一个线程，此线程被IO阻塞则整个进程被阻塞，CPU和IO设备只有一个运行，系统整体效率50%；而多线程时，如果一个线程被IO阻塞，其他线程还可以继续使用CPU 多核时代：多核时代多线程主要是提高进程利用多核CPU的能力，如果要计算复杂任务，只有一个线程的话，不论系统几个CPU核心，都只有一个CPU核心被利用；而创建多个线程，这些线程可以被映射到底层多个CPU上执行，如果任务中的多个线程没有资源竞争，那么执行效率会显著提高 多线程带来的问题：内存泄漏（对象，没有释放）、死锁、线程不安全等\n说说线程的声明周期和状态 Java线程在运行的生命周期中的指定时刻，只可能处于下面6种不同状态中的一个\nNEW：初始状态，线程被创建出来但没有调用start()\nRUNNABLE：运行状态，线程被调用了start() 等待运行的状态\nBLOCKED：阻塞状态，需要等待锁释放\nWAITING：等待状态，表示该线程需要等待其他线程做出一些特定动作（通知或中断）\nTIME_WAITING：超时等待状态，在指定的时间后自行返回而不是像WAITING一直等待\nTERMINATED：终止状态，表示该线程已经运行完毕 如图\n对于该图有以下几点要注意：\n线程创建后处于NEW状态，之后调用start()方法运行，此时线程处于READY，可运行的线程获得CPU时间片（timeslice）后处于RUNNING状态\n操作系统中有READY和RUNNING两个状态，而JVM中只有RUNNABLE状态 现在的操作系统通常都是**“时间分片“方法进行抢占式 轮转调度**“，一个线程最多只能在CPU上运行10-20ms的时间（此时处于RUNNING)状态，时间过短，时间片之后放入调度队列末尾等待再次调度（回到READY状态），太快所以不区分两种状态 线程执行wait()方法后，进入WAITING(等待 )状态，进入等待状态的线程需要依靠其他线程通知才能回到运行状态\nTIMED_WAITING(超时等待)状态，在等待状态的基础上增加超时限制，通过sleep(long millis)或wait(long millis) 方法可以将线程置于TIMED_WAITING状态，超时结束后返回到RUNNABLE状态（注意，不是RUNNING）\n当线程进入synchronized方法/块或者调用wait后(被notify)重新进入synchronized方法/块，但是锁被其他线程占有，这个时候线程就会进入BLOCKED（阻塞）状态\n线程在执行完了**run()方法之后就会进入到TERMINATED（终止）**状态\n注意上述，阻塞和等待的区别\n什么是上下文切换\n线程在执行过程中会有自己的运行条件和状态（也称上下文），比如上文提到的程序计数器，栈信息等。当出现下面情况时，线程从占用CPU状态中退出：\n主动让出CPU，如sleep(),wait()等 时间片用完了 调用了阻塞类型的系统中断（请求IO，线程被阻塞） 被终止或结束运行 前3种会发生线程切换：需要保存当前线程上下文，留待线程下次占用CPU的时候恢复，并加载下一个将要占用CPU的线程上下文，即所谓的上下文切换\n是现代系统基本功能，每次都要保存信息恢复信息，将会占用CPU，内存等系统资源，即效率有一定损耗，频繁切换会造成整体效率低下\n线程死锁是什么？如何避免?\n多个线程同时被阻塞，它们中的一个或者全部，都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止\n前提：线程A持有资源2，线程B持有资源1。现象：线程A在等待申请资源1，线程B在等待申请资源2，所以这两个线程就会互相等待而进入死锁状态 使用代码描述上述问题\npublic class DeadLockDemo { private static Object resource1 = new Object();//资源 1 private static Object resource2 = new Object();//资源 2 public static void main(String[] args) { new Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 1\u0026#34;).start(); new Thread(() -\u0026gt; { synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource1\u0026#34;); synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); } } /* - 线程A通过synchronized(resource1)获得resource1的监视器锁，然后休眠1s（是为了保证线程B获得执行然后拿到resource2监视器锁） - 休眠结束了两线程都企图请求获得对方的资源，陷入互相等待的状态，于是产生了死锁 */ 死锁产生条件\n互斥：该资源任意一个时刻只由一个线程占有 请求与保持：一线程因请求资源而阻塞时，对已获得的资源保持不放 不剥夺条件：线程已获得的资源未使用完之前不能被其他线程强行剥夺，只有自己使用完才释放（资源） 循环等待：若干线程之间形成头尾相接的循环等待资源关系 如何预防死锁\u0026mdash;\u0026gt;破坏死锁的必要条件\n破坏请求与保持条件：一次性申请所有资源 破坏不剥夺条件：占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源 破坏循环等待条件：靠按需申请资源来预防（按某顺序申请资源，释放资源时反序） 如何将避免死锁\n在资源分配时，借助于算法（银行家算法)对资源分配计算评估，使其进入安全状态\n安全状态 指的是系统能够按照某种线程推进顺序（P1、P2、P3\u0026hellip;..Pn）来为每个线程分配所需资源，直到满足每个线程对资源的最大需求，使每个线程都可顺利完成。称 \u0026lt;P1、P2、P3.....Pn\u0026gt; 序列为安全序列\n修改线程2的代码 原线程1代码不变\nnew Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 1\u0026#34;).start(); 线程2代码修改：\nnew Thread(() -\u0026gt; { synchronized (resource1) { System.out.println(Thread.currentThread() + \u0026#34;get resource1\u0026#34;); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(Thread.currentThread() + \u0026#34;waiting get resource2\u0026#34;); synchronized (resource2) { System.out.println(Thread.currentThread() + \u0026#34;get resource2\u0026#34;); } } }, \u0026#34;线程 2\u0026#34;).start(); /* 输出 Thread[线程 1,5,main]get resource1 Thread[线程 1,5,main]waiting get resource2 Thread[线程 1,5,main]get resource2 Thread[线程 2,5,main]get resource1 Thread[线程 2,5,main]waiting get resource2 Thread[线程 2,5,main]get resource2\nProcess finished with exit code 0 */\n分析 \u0026gt; 线程 1 首先获得到 resource1 的监视器锁,这时候线程 2 就获取不到了。然后线程 1 再去获取 resource2 的监视器锁，可以获取到。然后**线程 1 释放了对 resource1、resource2 的监视器锁的占用，线程 2 获取到（resource1）就可以执行了**。这样就破坏了破坏循环等待条件，因此避免了死锁。 sleep()方法和wait()方法对比\n共同点： 两者都可暂停线程执行 区别 seep() 方法没有释放锁，wait() 方法释放了锁 wait() 通常用于线程间交互/通信，sleep()用于暂停执行 wait()方法被调用后，线程不会自动苏醒，需要别的线程调用同一对象（监视器monitor）的notify()或者notifyAll()方法；sleep()方法执行完成后/或者wait(long timeout)超时后，线程会自动苏醒 sleep时Thread类的静态本地方法，wait()则是Object类的本地方法 为什么wait()方法不定义在Thread中\nwait() 目的是让获得对象锁的线程实现等待，会自动释放当前线程占有的对象锁 每个对象(Object)都拥有对象锁，既然是让获得对象锁的线程等待，所以方法应该出现在对象Object上 sleep()是让当前线程暂停执行，不涉及对象类，也不需要获得对象锁 可以直接调用Thread类的run方法吗\nnew一个Thread之后，线程进入新建状态 调用start()，会启动线程并使他进入就绪状态（Runable，可运行状态，又分为Ready和Running），分配到时间片后就开始运行 start()执行线程相应准备工作，之后**自动执行run()**方法的内容 如果直接执行run()方法，则会把run()方法当作main线程下普通方法去执行，并不会在某个线程中执行它 只有调用start()方法才可以启动新的线程使他进入就绪状态，等待获取时间片后运行 "},{"id":152,"href":"/zh/docs/technology/Review/java_guide/java/IO/ly0203lyio-model/","title":"io模型","section":"IO","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nhttps://zhuanlan.zhihu.com/p/360878783 IO多路复用讲解，这是一个与系统底层有关的知识点，需要一些操作系统调用代码才知道IO多路复用省的时间。\nI/O # 何为I/O # I/O(Input/Output)，即输入/输出 从计算机结构的角度来解读一下I/O，根据冯诺依曼结构，计算机结构分为5大部分：运算器、控制器、存储器、输入设备、输出设备 其中，输入设备：键盘；输出设备：显示器 网卡、硬盘既属于输入设备也属于输出设备 输入设备向计算机输入（内存）数据，输出设备接收计算机（内存）输出的数据，即I/O描述了计算机系统与外部设备之间通信的过程 从应用程序的角度解读I/O 为了保证系统稳定性和安全性，一个进程的地址空间划分为用户空间User space和内核空间Kernel space kernel\t英[ˈkɜːnl] 平常运行的应用程序都运行在用户空间，只有内核空间才能进行系统态级别的资源有关操作\u0026mdash;文件管理、进程通信、内存管理 如果要进行IO操作，就得依赖内核空间的能力，用户空间的程序不能直接访问内核空间 用户进程要想执行IO操作，必须通过系统调用来间接访问内核空间 对于磁盘IO（读写文件）和网络IO（网络请求和响应），从应用程序视角来看，应用程序对操作系统的内核发起IO调用（系统调用），操作系统负责的内核执行具体IO操作 应用程序只是发起了IO操作调用，而具体的IO执行则由操作系统内核完成 应用程序发起I/O后，经历两个步骤 内核等待I/O设备准备好数据 内核将数据从内核空间拷贝到用户空间 有哪些常见的IO模型 # UNIX系统下，包括5种：同步阻塞I/O，同步非阻塞I/O，I/O多路复用、信号驱动I/O和异步I/O\nJava中3中常见I/O模型 # BIO (Blocking I/O ) # 应用程序发起read调用后，会一直阻塞，直到内核把数据拷贝到用户空间 NIO (Non-blocking/New I/O) # 对于java.nio包，提供了Channel、Selector、Buffer等抽象概念，对于高负载高并发，应使用NIO NIO是I/O多路复用模型，属于同步非阻塞IO模型 一般的同步非阻塞 IO 模型中，应用程序会一直发起 read 调用。\n等待数据从内核空间拷贝到用户空间的这段时间里，线程依然是阻塞的**，**直到在内核把数据拷贝到用户空间。\n相比于同步阻塞 IO 模型，同步非阻塞 IO 模型确实有了很大改进。通过轮询操作，避免了一直阻塞。\n但是，这种 IO 模型同样存在问题：应用程序不断进行 I/O 系统调用轮询数据是否已经准备好的过程是十分消耗 CPU 资源的。\n★★ 也就是说，【准备数据，数据就绪】是不阻塞的。而【拷贝数据】是阻塞的 I/O多路复用 线程首先发起select调用，询问内核数据是否准备就绪，等准备好了，用户线程再发起read调用，r**ead调用的过程（数据从内核空间\u0026ndash;\u0026gt;用户空间）**还是阻塞的\nIO 多路复用模型，通过减少无效的系统调用，减少了对 CPU 资源的消耗。\nJava 中的 NIO ，有一个非常重要的选择器 ( Selector ) 的概念，也可以被称为 多路复用器。通过它，只需要一个线程便可以管理多个客户端连接。当客户端数据到了之后，才会为其服务。\nSelector，即多路复用器，一个线程管理多个客户端连接 AIO(Asynchronous I/O ) # 异步 IO 是基于事件和回调机制实现的，也就是应用操作之后会直接返回，不会堵塞在那里，当后台处理完成，操作系统会通知相应的线程进行后续的操作 如图\n三者区别 # "},{"id":153,"href":"/zh/docs/technology/Review/java_guide/java/IO/ly0202lyio-design-patterns/","title":"io设计模式","section":"IO","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n装饰器模式 # ​\t类图：\n​\t装饰器，Decorator，装饰器模式可以在不改变原有对象的情况下拓展其功能\n★装饰器模式，通过组合替代继承来扩展原始类功能，在一些继承关系较复杂的场景（IO这一场景各种类的继承关系就比较复杂）下更加实用\n对于字节流，FilterInputStream（对应输入流）和FilterOutputStream（对应输出流）是装饰器模式的核心，分别用于增强（继承了）InputStream和OutputStream子类对象的功能 Filter （过滤的意思），中间（Closeable）下面这两条虚线代表实现；最下面的实线代表继承 其中BufferedInputStream（字节缓冲输入流）、DataInputStream等等都是FilterInputStream的子类，对应的BufferedOutputStream和DataOutputStream都是FilterOutputStream的子类\n例子，使用BufferedInputStream（字节缓冲输入流）来增强FileInputStream功能\nBufferedInputStream源码（构造函数）\nprivate static int DEFAULT_BUFFER_SIZE = 8192; public BufferedInputStream(InputStream in) { this(in, DEFAULT_BUFFER_SIZE); } public BufferedInputStream(InputStream in, int size) { super(in); if (size \u0026lt;= 0) { throw new IllegalArgumentException(\u0026#34;Buffer size \u0026lt;= 0\u0026#34;); } buf = new byte[size]; } 使用\ntry (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;input.txt\u0026#34;))) { int content; long skip = bis.skip(2); while ((content = bis.read()) != -1) { System.out.print((char) content); } } catch (IOException e) { e.printStackTrace(); } ZipInputStream和ZipOutputStream还可以用来增强BufferedInputStream和BufferedOutputStream的能力\n//使用 BufferedInputStream bis = new BufferedInputStream(new FileInputStream(fileName)); ZipInputStream zis = new ZipInputStream(bis); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(fileName)); ZipOutputStream zipOut = new ZipOutputStream(bos); 装饰器模式重要的一点，就是可以对原始类嵌套使用多个装饰器，所以装饰器需要跟原始类继承相同的抽象类或实现相同接口，上面介绍的IO相关装饰器和原始类共同父类都是InputStream和OutputStream 而对于字符流来说，BufferedReader用来增强Reader（字符输入流）子类功能，BufferWriter用来增加Writer（字符输出流）子类功能\nBufferedWriter bw = new BufferedWriter(new OutputStreamWriter(new FileOutputStream(fileName), \u0026#34;UTF-8\u0026#34;)); IO流中大量使用了装饰器模式，不需要特意记忆\n适配器模式 # 适配器（Adapter Pattern）模式：主要用于接口互不兼容的类的协调工作，你可以将其联想到我们日常使用的电源适配器\n其中被适配的对象/类称为适配者（Adaptee），作用于适配者的对象或者类称为适配器（Adapter）。对象适配器使用组合关系实现，类适配器使用继承关系实现 IO中字符流和字节流接口不同，而他们能协调工作就是基于适配器模式来做的，具体的，是对象适配器：将字节流对象适配成字符流对象，然后通过字节流对象，读取/写入字符数据\nInputStreamReader和OutputStreamWriter为两个适配器，也是字节流和字符流之间的桥梁\nInputStreamReader使用StreamDecode（流解码器）对字节进行解码，实现字节流到字符流的转换\nOutputStreamWriter使用StreamEncoder（流编码器）对字符进行编码，实现字符到字节流的转换\nInputStream和OutputStream的子类是被适配者，InputStreamReader和OutputStreamWriter是适配器 使用：\n// InputStreamReader 是适配器，FileInputStream 是被适配的类 InputStreamReader isr = new InputStreamReader(new FileInputStream(fileName), \u0026#34;UTF-8\u0026#34;); // BufferedReader 增强 InputStreamReader 的功能（装饰器模式） BufferedReader bufferedReader = new BufferedReader(isr); fileReader的源码：\npublic class FileReader extends InputStreamReader { public FileReader(String fileName) throws FileNotFoundException { super(new FileInputStream(fileName)); } } //其父类InputStreamReader public class InputStreamReader extends Reader { //用于解码的对象 private final StreamDecoder sd; public InputStreamReader(InputStream in) { super(in); try { // 获取 StreamDecoder 对象 sd = StreamDecoder.forInputStreamReader(in, this, (String)null); } catch (UnsupportedEncodingException e) { throw new Error(e); } } // 使用 StreamDecoder 对象做具体的读取工作 public int read() throws IOException { return sd.read(); } } 同理，java.io.OutputStreamWriter部分源码：\npublic class OutputStreamWriter extends Writer { // 用于编码的对象 private final StreamEncoder se; public OutputStreamWriter(OutputStream out) { super(out); try { // 获取 StreamEncoder 对象 se = StreamEncoder.forOutputStreamWriter(out, this, (String)null); } catch (UnsupportedEncodingException e) { throw new Error(e); } } // 使用 StreamEncoder 对象做具体的写入工作 public void write(int c) throws IOException { se.write(c); } } 适配器模式和装饰器模式区别\n装饰器模式更侧重于动态增强原始类的功能，（为了嵌套）装饰器类需要跟原始类继承相同抽象类/或实现相同接口。装饰器模式支持对原始类嵌套\n适配器模式侧重于让接口不兼容而不能交互的类一起工作，当调用适配器方法时，适配器内部会调用适配者类或者和适配者类相关类的方法，这个过程透明的。就比如说 StreamDecoder （流解码器）和StreamEncoder（流编码器）就是分别基于 InputStream 和 OutputStream 来获取 FileChannel对象并调用对应的 read 方法和 write 方法进行字节数据的读取和写入。\nStreamDecoder(InputStream in, Object lock, CharsetDecoder dec) { // 省略大部分代码 // 根据 InputStream 对象获取 FileChannel 对象 ch = getChannel((FileInputStream)in); } 适配器和适配者（注意，这里说的都是适配器模式）两者不需要继承相同抽象类/不需要实现相同接口 FutureTask使用了适配器模式 直接调用(构造器)\npublic FutureTask(Runnable runnable, V result) { // 调用 Executors 类的 callable 方法 this.callable = Executors.callable(runnable, result); this.state = NEW; } 间接：\n// 实际调用的是 Executors 的内部类 RunnableAdapter 的构造方法 public static \u0026lt;T\u0026gt; Callable\u0026lt;T\u0026gt; callable(Runnable task, T result) { if (task == null) throw new NullPointerException(); return new RunnableAdapter\u0026lt;T\u0026gt;(task, result); } // 适配器 static final class RunnableAdapter\u0026lt;T\u0026gt; implements Callable\u0026lt;T\u0026gt; { final Runnable task; final T result; RunnableAdapter(Runnable task, T result) { this.task = task; this.result = result; } public T call() { task.run(); return result; } } 工厂模式 # NIO中大量出现，例如Files类的newInputStream，Paths类中的get方法，ZipFileSystem类中的getPath\nInputStream is = Files.newInputStream(Paths.get(generatorLogoPath)) 观察者模式 # 比如NIO中的文件目录监听服务 该服务基于WatchService接口（观察者）和Watchable接口（被观察者）\nWatchable接口其中有一个register方法，用于将对象注册到WatchService（监控服务）并绑定监听事件的方法\n例子\n// 创建 WatchService 对象 WatchService watchService = FileSystems.getDefault().newWatchService(); // 初始化一个被监控文件夹的 Path 类: Path path = Paths.get(\u0026#34;workingDirectory\u0026#34;); // 将这个 path 对象注册到 WatchService（监控服务） 中去 WatchKey watchKey = path.register( watchService, StandardWatchEventKinds...); 可以通过WatchKey对象获取事件具体信息\nWatchKey key; while ((key = watchService.take()) != null) { for (WatchEvent\u0026lt;?\u0026gt; event : key.pollEvents()) { // 可以调用 WatchEvent 对象的方法做一些事情比如输出事件的具体上下文信息 } key.reset(); } 完整的代码应该是如下\n@Test public void myTest() throws IOException, InterruptedException { // 创建 WatchService 对象 WatchService watchService = FileSystems.getDefault().newWatchService(); // 初始化一个被监控文件夹的 Path 类: Path path = Paths.get(\u0026#34;F:\\\\java_test\\\\git\\\\hexo\\\\review_demo\\\\src\\\\com\\\\hp\u0026#34;); // 将这个 path 对象注册到 WatchService（监控服务） 中去 WatchKey key = path.register( watchService, StandardWatchEventKinds.ENTRY_CREATE,StandardWatchEventKinds.ENTRY_DELETE ,StandardWatchEventKinds.ENTRY_MODIFY); while ((key = watchService.take()) != null) { System.out.println(\u0026#34;检测到了事件--start--\u0026#34;); for (WatchEvent\u0026lt;?\u0026gt; event : key.pollEvents()) { // 可以调用 WatchEvent 对象的方法做一些事情比如输出事件的具体上下文信息 System.out.println(\u0026#34;event.kind().name()\u0026#34;+event.kind().name()); } key.reset(); System.out.println(\u0026#34;检测到了事件--end--\u0026#34;); } } public interface Path extends Comparable\u0026lt;Path\u0026gt;, Iterable\u0026lt;Path\u0026gt;, Watchable{ } public interface Watchable { WatchKey register(WatchService watcher, WatchEvent.Kind\u0026lt;?\u0026gt;[] events, WatchEvent.Modifier... modifiers) throws IOException; //events，需要监听的事件，包括创建、删除、修改。 @Override WatchKey register(WatchService watcher, WatchEvent.Kind\u0026lt;?\u0026gt;... events) throws IOException; } 其中events包括下面3种:\nStandardWatchEventKinds.ENTRY_CREATE ：文件创建。\nStandardWatchEventKinds.ENTRY_DELETE : 文件删除。\nStandardWatchEventKinds.ENTRY_MODIFY : 文件修改。\nWatchService内部通过一个daemon thread （守护线程），采用定期轮询的方式检测文件变化\nclass PollingWatchService extends AbstractWatchService { // 定义一个 daemon thread（守护线程）轮询检测文件变化 private final ScheduledExecutorService scheduledExecutor; PollingWatchService() { scheduledExecutor = Executors .newSingleThreadScheduledExecutor(new ThreadFactory() { @Override public Thread newThread(Runnable r) { Thread t = new Thread(r); t.setDaemon(true); return t; }}); } void enable(Set\u0026lt;? extends WatchEvent.Kind\u0026lt;?\u0026gt;\u0026gt; events, long period) { synchronized (this) { // 更新监听事件 this.events = events; // 开启定期轮询 Runnable thunk = new Runnable() { public void run() { poll(); }}; this.poller = scheduledExecutor .scheduleAtFixedRate(thunk, period, period, TimeUnit.SECONDS); } } } "},{"id":154,"href":"/zh/docs/technology/Review/java_guide/java/IO/ly0201lyio/","title":"io基础","section":"IO","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n简介 # IO，即Input/Output，输入和输出，输入就是数据输入到计算机内存；输出则是输出到外部存储（如数据库、文件、远程主机）\n根据数据处理方式，又分为字节流和字符流\n基类\n字节输入流 InputStream，字符输入流 Reader 字节输出流 OutputStream, 字符输出流 Writer 字节流 # 字节输入流 InputStream InputStream用于从源头（通常是文件）读取数据（字节信息）到内存中，java.io.InputStream抽象类是所有字节输入流的父类\n常用方法\nread() ：返回输入流中下一个字节的数据。返回的值介于 0 到 255 之间。如果未读取任何字节，则代码返回 -1 ，表示文件结束。 read(byte b[ ]) : 从输入流中读取一些字节存储到数组 b 中。如果数组 b 的长度为零，则不读取。如果没有可用字节读取，返回 -1。如果有可用字节读取，则最多读取的字节数最多等于 b.length ， 返回读取的字节数。这个方法等价于 read(b, 0, b.length)。 read(byte b[], int off, int len) ：在read(byte b[ ]) 方法的基础上增加了 off 参数（偏移量）和 len 参数（要读取的最大字节数）。 skip(long n) ：忽略输入流中的 n 个字节 ,返回实际忽略的字节数。 available() ：返回输入流中可以读取的字节数。 close() ：关闭输入流释放相关的系统资源。 Java9 新增了多个实用方法\nreadAllBytes() ：读取输入流中的所有字节，返回字节数组。 readNBytes(byte[] b, int off, int len) ：阻塞直到读取 len 个字节。 transferTo(OutputStream out) ： 将所有字节从一个输入流传递到一个输出流。 FileInputStream \u0026ndash;\u0026gt; 字节输入流对象，可直接指定文件路径：用来读取单字节数据/或读取至字节数组中，示例如下：\ninput.txt中的字符为LLJavaGuide\ntry (InputStream fis = new FileInputStream(\u0026#34;input.txt\u0026#34;)) { System.out.println(\u0026#34;Number of remaining bytes:\u0026#34; + fis.available()); int content; long skip = fis.skip(2); System.out.println(\u0026#34;The actual number of bytes skipped:\u0026#34; + skip); System.out.print(\u0026#34;The content read from file:\u0026#34;); while ((content = fis.read()) != -1) { System.out.print((char) content); } } catch (IOException e) { e.printStackTrace(); } //输出 /**Number of remaining bytes:11 The actual number of bytes skipped:2 The content read from file:JavaGuide **/ 一般不会单独使用FileInputStream，而是配合BufferdInputStream(字节缓冲输入流)，下面代码转为String 较为常见：\n// 新建一个 BufferedInputStream 对象 BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\u0026#34;input.txt\u0026#34;)); // 读取文件的内容并复制到 String 对象中 String result = new String(bufferedInputStream.readAllBytes()); System.out.println(result); DataInputStream 用于读取指定类型数据，不能单独使用，必须结合FileInputStream\nFileInputStream fileInputStream = new FileInputStream(\u0026#34;input.txt\u0026#34;); //必须将fileInputStream作为构造参数才能使用 DataInputStream dataInputStream = new DataInputStream(fileInputStream); //可以读取任意具体的类型数据 dataInputStream.readBoolean(); dataInputStream.readInt(); dataInputStream.readUTF(); ObjectInputStream 用于从输入流读取Java对象（一般是被反序列化到文件中，或者其他介质的数据），ObjectOutputStream用于将对象写入到输出流（[将对象]序列化）\nObjectInputStream input = new ObjectInputStream(new FileInputStream(\u0026#34;object.data\u0026#34;)); MyClass object = (MyClass) input.readObject(); input.close(); 用于序列化和反序列化的类必须实现Serializable接口，不想被序列化的属性用**transizent**修饰\n字节输出流 OutputStream\nOutputStream用于将字节数据（字节信息）写入到目的地（通常是文件），java.io.OutputStream抽象类是所有字节输出流的父类\n//常用方法\nwrite(int b) ：将特定字节写入输出流。 write(byte b[ ]) : 将数组b 写入到输出流，等价于 write(b, 0, b.length) 。 write(byte[] b, int off, int len) : 在write(byte b[ ]) 方法的基础上增加了 off 参数（偏移量）和 len 参数（要读取的最大字节数）。 flush() ：刷新此输出流并强制写出所有缓冲的输出字节。 //相比输入流多出的方法 close() ：关闭输出流释放相关的系统资源。 示例代码：\ntry (FileOutputStream output = new FileOutputStream(\u0026#34;output.txt\u0026#34;)) { byte[] array = \u0026#34;JavaGuide\u0026#34;.getBytes(); output.write(array); } catch (IOException e) { e.printStackTrace(); } //结果 /**output.txt文件中内容为: JavaGuide **/ FileOutputStream一般也是配合BufferedOutputStream （字节缓冲输出流）： ```java FileOutputStream fileOutputStream = new FileOutputStream(\u0026#34;output.txt\u0026#34;); BufferedOutputStream bos = new BufferedOutputStream(fileOutputStream) DataOutputStream用于写入指定类型数据，不能单独使用，必须结合FileOutputStream\n// 输出流 FileOutputStream fileOutputStream = new FileOutputStream(\u0026#34;out.txt\u0026#34;); DataOutputStream dataOutputStream = new DataOutputStream(fileOutputStream); // 输出任意数据类型 dataOutputStream.writeBoolean(true); dataOutputStream.writeByte(1); ObjectInputStream用于从输入流中读取Java对象（ObjectInputStream，反序列化）；ObjectOutputStream用于将对象写入到输出流（ObjectOutputStream，序列化）\nObjectOutputStream output = new ObjectOutputStream(new FileOutputStream(\u0026#34;file.txt\u0026#34;) Person person = new Person(\u0026#34;Guide哥\u0026#34;, \u0026#34;JavaGuide作者\u0026#34;); output.writeObject(person); 字符流 # 简介 文件读写或者网络发送接收，信息的最小存储单元都是字节，为什么I/O流操作要分为字节流操作和字符流操作呢\n字符流是由Java虚拟机将字节转换得到的，过程相对耗时\n如果不知道编码类型，容易出现乱码 如上面的代码，将文件内容改为 ： 你好，我是Guide\ntry (InputStream fis = new FileInputStream(\u0026#34;input.txt\u0026#34;)) { System.out.println(\u0026#34;Number of remaining bytes:\u0026#34; + fis.available()); int content; long skip = fis.skip(2); System.out.println(\u0026#34;The actual number of bytes skipped:\u0026#34; + skip); System.out.print(\u0026#34;The content read from file:\u0026#34;); while ((content = fis.read()) != -1) { System.out.print((char) content); } } catch (IOException e) { e.printStackTrace(); } //输出 /**Number of remaining bytes:9 The actual number of bytes skipped:2 The content read from file:§å®¶å¥½ **/ 为了解决乱码问题，I/O流提供了一个直接操作字符的接口，方便对字符进行流操作；但如果音频文件、图片等媒体文件用字节流比较好，涉及字符的话使用字符流\n★ 重要：\n字符流默认采用的是 Unicode 编码，我们可以通过构造方法自定义编码。顺便分享一下之前遇到的笔试题：常用字符编码所占字节数？\nutf8 :英文占 1 字节，中文占 3 字节，\nunicode：任何字符都占 2 个字节，\ngbk：英文占 1 字节，中文占 2 字节。\nReader（字符输入流）\n用于从源头（通常是文件）读取数据（字符信息）到内存中，java.io.Reader抽象类是所有字符输入流的父类\n注意：InputStream和Reader都是类，再往上就是接口了；Reader用于读取文本，InputStream用于读取原始字节 常用方法：\nread() : 从输入流读取一个字符。 read(char[] cbuf) : 从输入流中读取一些字符，并将它们存储到字符数组 cbuf中，等价于 read(cbuf, 0, cbuf.length) 。 read(char[] cbuf, int off, int len) ：在read(char[] cbuf) 方法的基础上增加了 off 参数（偏移量）和 len 参数（要读取的最大字节数）。 skip(long n) ：忽略输入流中的 n 个字符 ,返回实际忽略的字符数。 close() : 关闭输入流并释放相关的系统资源。 InputStreamReader是字节流转换为字符流的桥梁，子类FileReader基于该基础上的封装，可以直接操作字符文件\n// 字节流转换为字符流的桥梁 public class InputStreamReader extends Reader { } // 用于读取字符文件 public class FileReader extends InputStreamReader { } 示例：input.txt中内容为\u0026quot;你好，我是Guide\u0026quot;\ntry (FileReader fileReader = new FileReader(\u0026#34;input.txt\u0026#34;);) { int content; long skip = fileReader.skip(3); System.out.println(\u0026#34;The actual number of bytes skipped:\u0026#34; + skip); System.out.print(\u0026#34;The content read from file:\u0026#34;); while ((content = fileReader.read()) != -1) { System.out.print((char) content); } } catch (IOException e) { e.printStackTrace(); } /*输出 The actual number of bytes skipped:3 The content read from file:我是Guide。 */ Write（字符输出流） 用于将数据（字符信息）写到目的地（通常是文件），java.io.Writer抽象类是所有字节输出流的父类\nwrite(int c) : 写入单个字符。 write(char[] cbuf) ：写入字符数组 cbuf，等价于write(cbuf, 0, cbuf.length)。 write(char[] cbuf, int off, int len) ：在write(char[] cbuf) 方法的基础上增加了 off 参数（偏移量）和 len 参数（要读取的最大字节数）。 write(String str) ：写入字符串，等价于 write(str, 0, str.length()) 。 write(String str, int off, int len) ：在write(String str) 方法的基础上增加了 off 参数（偏移量）和 len 参数（要读取的最大字节数）。 append(CharSequence csq) ：将指定的字符序列附加到指定的 Writer 对象并返回该 Writer 对象。 append(char c) ：将指定的字符附加到指定的 Writer 对象并返回该 Writer 对象。 flush() ：刷新此输出流并强制写出所有缓冲的输出字符。//相对于Reader增加的 close():关闭输出流释放相关的系统资源。 OutputStreamWriter是字符流转换为字节流的桥梁（注意，这里没有错），其子类FileWriter是基于该基础上的封装，可以直接将字符写入到文件\n// 字符流转换为字节流的桥梁 public class OutputStreamWriter extends Writer { } // 用于写入字符到文件 public class FileWriter extends OutputStreamWriter { } FileWriter代码示例：\ntry (Writer output = new FileWriter(\u0026#34;output.txt\u0026#34;)) { output.write(\u0026#34;你好，我是Guide。\u0026#34;); //字符流，转为字节流 } catch (IOException e) { e.printStackTrace(); } /*结果：output.txt中 你好，我是Guide */ InputStreamWriter和OutputStreamWriter 比较\n前者InputStreamWriter，是需要从文件中读数据出来（读到内存中），而文件是通过二进制（字节）保存的，所以InputStreamWriter是将（看不懂的）字节流转换为（看得懂的）字符流 后者OutputStreamWriter，是需要**将（看得懂的）字符流转换为（看不懂的）字节流（然后从内存读出）**并保存到介质中 字节缓冲流 # 简介\nIO操作是很消耗性能的，缓冲流将数据加载至缓冲区，一次性读取/写入多个字节，从而避免频繁的IO操作，提高流的效率\n采用装饰器模式来增强InputStream和OutputStream子类对象的功能\n例子：\n// 新建一个 BufferedInputStream 对象 BufferedInputStream bufferedInputStream = new BufferedInputStream(new FileInputStream(\u0026#34;input.txt\u0026#34;)); 字节流和字节缓冲流的性能差别主要体现在：当使用两者时都调用的是write(int b)和read() 这两个一次只读取一个字节的方法的时候，由于字节缓冲流内部有缓冲区（字节数组），因此字节缓冲流会将读取到的字节存放在缓存区，大幅减少IO次数，提高读取效率\n对比：复制524.9mb文件，缓冲流15s，普通字节流2555s(30min)\n测试代码\n@Test void copy_pdf_to_another_pdf_buffer_stream() { // 记录开始时间 long start = System.currentTimeMillis(); try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;深入理解计算机操作系统.pdf\u0026#34;)); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;深入理解计算机操作系统-副本.pdf\u0026#34;))) { int content; while ((content = bis.read()) != -1) { bos.write(content); } } catch (IOException e) { e.printStackTrace(); } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\u0026#34;使用缓冲流复制PDF文件总耗时:\u0026#34; + (end - start) + \u0026#34; 毫秒\u0026#34;); } @Test void copy_pdf_to_another_pdf_stream() { // 记录开始时间 long start = System.currentTimeMillis(); try (FileInputStream fis = new FileInputStream(\u0026#34;深入理解计算机操作系统.pdf\u0026#34;); FileOutputStream fos = new FileOutputStream(\u0026#34;深入理解计算机操作系统-副本.pdf\u0026#34;)) { int content; while ((content = fis.read()) != -1) { fos.write(content); } } catch (IOException e) { e.printStackTrace(); } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\u0026#34;使用普通流复制PDF文件总耗时:\u0026#34; + (end - start) + \u0026#34; 毫秒\u0026#34;); } 但是如果是使用普通字节流的 read(byte b[] )和write(byte b[] , int off, int len) 这两个写入一个字节数组的方法的话，只要字节数组大小合适，差距性能不大 同理，使用read(byte b[]) 和write(byte b[] ,int off, int len)方法(字节流及缓冲字节流)，分别复制524mb文件，缓冲流需要0.7s , 普通字节流需要1s 代码如下：\n@Test void copy_pdf_to_another_pdf_with_byte_array_buffer_stream() { // 记录开始时间 long start = System.currentTimeMillis(); try (BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;深入理解计算机操作系统.pdf\u0026#34;)); BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;深入理解计算机操作系统-副本.pdf\u0026#34;))) { int len; byte[] bytes = new byte[4 * 1024]; while ((len = bis.read(bytes)) != -1) { bos.write(bytes, 0, len); } } catch (IOException e) { e.printStackTrace(); } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\u0026#34;使用缓冲流复制PDF文件总耗时:\u0026#34; + (end - start) + \u0026#34; 毫秒\u0026#34;); } @Test void copy_pdf_to_another_pdf_with_byte_array_stream() { // 记录开始时间 long start = System.currentTimeMillis(); try (FileInputStream fis = new FileInputStream(\u0026#34;深入理解计算机操作系统.pdf\u0026#34;); FileOutputStream fos = new FileOutputStream(\u0026#34;深入理解计算机操作系统-副本.pdf\u0026#34;)) { int len; byte[] bytes = new byte[4 * 1024]; while ((len = fis.read(bytes)) != -1) { fos.write(bytes, 0, len); } } catch (IOException e) { e.printStackTrace(); } // 记录结束时间 long end = System.currentTimeMillis(); System.out.println(\u0026#34;使用普通流复制PDF文件总耗时:\u0026#34; + (end - start) + \u0026#34; 毫秒\u0026#34;); } 字节缓冲输入流 BufferedInputStream\nBufferedInputStream 从源头（通常是文件）读取数据（字节信息）到内存的过程中不会一个字节一个字节的读取，而是会先将读取到的字节存放在缓存区，并从内部缓冲区中单独读取字节。这样大幅减少了 IO 次数，提高了读取效率。\nBufferedInputStream 内部维护了一个缓冲区，这个缓冲区实际就是一个字节数组，通过阅读 BufferedInputStream 源码即可得到这个结论。\n源码\npublic class BufferedInputStream extends FilterInputStream { // 内部缓冲区数组 protected volatile byte buf[]; // 缓冲区的默认大小 private static int DEFAULT_BUFFER_SIZE = 8192; // 使用默认的缓冲区大小 public BufferedInputStream(InputStream in) { this(in, DEFAULT_BUFFER_SIZE); } // 自定义缓冲区大小 public BufferedInputStream(InputStream in, int size) { super(in); if (size \u0026lt;= 0) { throw new IllegalArgumentException(\u0026#34;Buffer size \u0026lt;= 0\u0026#34;); } buf = new byte[size]; } } 字节缓冲输出流 BufferedOutputStream BufferedOutputStream 将数据（字节信息）写入到目的地（通常是文件）的过程中不会一个字节一个字节的写入，而是会先将要写入的字节存放在缓存区，并从内部缓冲区中单独写入字节。这样大幅减少了 IO 次数，提高了读取效率 使用\ntry (BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;output.txt\u0026#34;))) { byte[] array = \u0026#34;JavaGuide\u0026#34;.getBytes(); bos.write(array); } catch (IOException e) { e.printStackTrace(); } 字符缓冲流 # BufferedReader （字符缓冲输入流）和 BufferedWriter（字符缓冲输出流）类似于 BufferedInputStream（字节缓冲输入流）和BufferedOutputStream（字节缓冲输入流），内部都维护了一个字节数组作为缓冲区。不过，前者主要是用来操作字符信息。\n这里表述好像不太对，应该是维护了字符数组：\npublic class BufferedReader extends Reader { private Reader in; private char cb[]; } 打印流 # PrintStream属于字节打印流，对应的是PrintWriter（字符打印流）\nSystem.out 实际上获取了一个PrintStream，print方法调用的是PrintStream的write方法\nPrintStream 是 OutputStream 的子类，PrintWriter 是 Writer 的子类。\npublic class PrintStream extends FilterOutputStream implements Appendable, Closeable { } public class PrintWriter extends Writer { } 随机访问流 RandomAccessFile # 指的是支持随意跳转到文件的任意位置进行读写的RandomAccessFile 构造方法如下，可以指定mode (读写模式)\n// openAndDelete 参数默认为 false 表示打开文件并且这个文件不会被删除 public RandomAccessFile(File file, String mode) throws FileNotFoundException { this(file, mode, false); } // 私有方法 private RandomAccessFile(File file, String mode, boolean openAndDelete) throws FileNotFoundException{ // 省略大部分代码 } 读写模式主要有以下四种：\nr : 只读；rw：读写\nrws :相对于rw，rws同步更新对\u0026quot;文件内容\u0026quot;或元数据的修改到外部存储设备\nrwd:相对于rw,rwd同步更新对\u0026quot;文件内容\u0026quot;的修改到外部存储设备\n解释：\n文件内容指实际保存的数据，元数据则描述属性例如文件大小信息、创建和修改时间 默认情形下(rw模式下),是使用buffer的,只有cache满的或者使用RandomAccessFile.close()关闭流的时候儿才真正的写到文件。 调试麻烦的\u0026hellip;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;使用write方法修改byte的时候儿,只修改到个内存兰,还没到个文件,闪的调试麻烦的,不能使用notepad++工具立即看见修改效果.. 当系统halt的时候儿,不能写到文件\u0026hellip;安全性稍微差点儿\u0026hellip; rws：就是同步（synchronized）模式,每write修改一个byte,立马写到磁盘..当然中间性能走差点儿,适合小的文件\u0026hellip;and debug模式\u0026hellip;或者安全性高的需要的时候儿 rwd： 只对“文件的内容”同步更新到磁盘\u0026hellip;不对metadata同步更新 rwd介于rw和rws之间 RandomAccessFile：文件指针表示下一个将要被写入或读取的字节所处位置\n通过seek(long pos)方法设置文件指针偏移量（距离开头pos个字节处，从0开始）\n使用getFilePointer()方法获取文件指针当前位置\nRandomAccessFile randomAccessFile = new RandomAccessFile(new File(\u0026#34;input.txt\u0026#34;), \u0026#34;rw\u0026#34;); System.out.println(\u0026#34;读取之前的偏移量：\u0026#34; + randomAccessFile.getFilePointer() + \u0026#34;,当前读取到的字符\u0026#34; + (char) randomAccessFile.read() + \u0026#34;，读取之后的偏移量：\u0026#34; + randomAccessFile.getFilePointer()); // 指针当前偏移量为 6 randomAccessFile.seek(6); System.out.println(\u0026#34;读取之前的偏移量：\u0026#34; + randomAccessFile.getFilePointer() + \u0026#34;,当前读取到的字符\u0026#34; + (char) randomAccessFile.read() + \u0026#34;，读取之后的偏移量：\u0026#34; + randomAccessFile.getFilePointer()); // 从偏移量 7 的位置开始往后写入字节数据 randomAccessFile.write(new byte[]{\u0026#39;H\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;J\u0026#39;, \u0026#39;K\u0026#39;}); // 指针当前偏移量为 0，回到起始位置 randomAccessFile.seek(0); System.out.println(\u0026#34;读取之前的偏移量：\u0026#34; + randomAccessFile.getFilePointer() + \u0026#34;,当前读取到的字符\u0026#34; + (char) randomAccessFile.read() + \u0026#34;，读取之后的偏移量：\u0026#34; + randomAccessFile.getFilePointer()); input.txt文件内容： ABCDEFG\n输出\n读取之前的偏移量：0,当前读取到的字符A，读取之后的偏移量：1 读取之前的偏移量：6,当前读取到的字符G，读取之后的偏移量：7 读取之前的偏移量：0,当前读取到的字符A，读取之后的偏移量：1 文件内容： ABCDEFGHIJK\nwrite方法在写入对象时如果对应位置已有数据，会将其覆盖\nRandomAccessFile randomAccessFile = new RandomAccessFile(new File(\u0026#34;input.txt\u0026#34;), \u0026#34;rw\u0026#34;); randomAccessFile.write(new byte[]{\u0026#39;H\u0026#39;, \u0026#39;I\u0026#39;, \u0026#39;J\u0026#39;, \u0026#39;K\u0026#39;}); //如果程序之前input.txt内容为ABCD，则运行后变为HIJK 常见应用：解决断点续传：上传文件中途暂停或失败（网络问题），之后不需要重新上传，只需上传未成功上传的文件分片即可 分片（先将文件切分成多个文件分片）上传是断点续传的基础。 使用RandomAccessFile帮助我们合并文件分片（但是下面代码好像不是必须的，因为他是单线程连续写入？？，这里附上另一篇文章的另一段话：）\n但是由于 RandomAccessFile 可以自由访问文件的任意位置，所以如果需要访问文件的部分内容，而不是把文件从头读到尾，因此 RandomAccessFile 的一个重要使用场景就是网络请求中的多线程下载及断点续传。 https://blog.csdn.net/li1669852599/article/details/122214104\nly: 个人感觉，mysql数据库的写入可能也是依赖类似的规则，才能在某个位置读写\n"},{"id":155,"href":"/zh/docs/technology/Review/java_guide/java/Collection/ly0105lysource-code-concurrenthashmap/","title":"ConcurrentHashMap源码","section":"集合","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n总结 # Java7 中 ConcurrentHashMap 使用的分段锁，也就是每一个 Segment 上同时只有一个线程可以操作，每一个 Segment 都是一个类似 HashMap 数组的结构，每一个HashMap可以扩容，它的冲突会转化为链表。但是 Segment 的个数一但初始化就不能改变。\nJava8 中的 ConcurrentHashMap 使用的 Synchronized 锁加 CAS 的机制。结构也由 Java7 中的 Segment 数组 + HashEntry 数组 + 链表 进化成了 Node 数组 + 链表 / 红黑树，Node 是类似于一个 HashEntry 的结构。它的冲突再达到一定大小时会转化成红黑树，在冲突小于一定数量时又退回链表。\n源码 （略过） # ConcurrentHashMap1.7 # 存储结构 Segment数组（该数组用来加锁，每个数组元素是一个HashEntry数组（该数组可能包含链表） 如图，ConcurrentHashMap由多个Segment组合，每一个Segment是一个类似HashMap的结构，每一个HashMap内部可以扩容，但是Segment个数初始化后不能改变，默认16个（即默认支持16个线程并发） ConcurrentHashMap1.8 # 存储结构 可以发现 Java8 的 ConcurrentHashMap 相对于 Java7 来说变化比较大，不再是之前的 Segment 数组 + HashEntry 数组 + 链表，而是 Node 数组 + 链表 / 红黑树。当冲突链表达到一定长度时，链表会转换成红黑树。\n初始化 initTable\n/** * Initializes table, using the size recorded in sizeCtl. */ private final Node\u0026lt;K,V\u0026gt;[] initTable() { Node\u0026lt;K,V\u0026gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) { //　如果 sizeCtl \u0026lt; 0 ,说明另外的线程执行CAS 成功，正在进行初始化。 if ((sc = sizeCtl) \u0026lt; 0) // 让出 CPU 使用权 Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) { try { if ((tab = table) == null || tab.length == 0) { int n = (sc \u0026gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(\u0026#34;unchecked\u0026#34;) Node\u0026lt;K,V\u0026gt;[] nt = (Node\u0026lt;K,V\u0026gt;[])new Node\u0026lt;?,?\u0026gt;[n]; table = tab = nt; sc = n - (n \u0026gt;\u0026gt;\u0026gt; 2); } } finally { sizeCtl = sc; } break; } } return tab; } 是通过自旋和CAS操作完成的，注意的变量是sizeCtl，它的值决定着当前的初始化状态\n-1 说明正在初始化 -N 说明有N-1个线程正在进行扩容 表示 table 初始化大小，如果 table 没有初始化 表示 table 容量，如果 table　已经初始化。 put\n根据 key 计算出 hashcode 。\n判断是否需要进行初始化。\n即为当前 key 定位出的 Node，如果为空表示当前位置可以写入数据，利用 CAS 尝试写入，失败则自旋保证成功。\n如果当前位置的 hashcode == MOVED == -1,则需要进行扩容。\n如果都不满足，则利用 synchronized 锁写入数据。\n如果数量大于 TREEIFY_THRESHOLD 则要执行树化方法，在 treeifyBin 中会首先判断当前数组长度≥64时才会将链表转换为红黑树。\nget 流程比较简单\n根据 hash 值计算位置。 查找到指定位置，如果头节点就是要找的，直接返回它的 value. 如果头节点 hash 值小于 0 ，说明正在扩容或者是红黑树，查找之。 如果是链表，遍历查找之。 "},{"id":156,"href":"/zh/docs/technology/Review/java_guide/java/Collection/ly0106lysource-code-hashmap/","title":"HashMap源码","section":"集合","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nHashMap简介 # HashMap用来存放键值对，基于哈希表的Map接口实现，是非线程安全的 可以存储null的key和value，但null作为键只能有一个 JDK8之前，HashMap由数组和链表组成，链表是为了解决哈希冲突而存在；JDK8之后，当链表大于阈值（默认8），则会选择转为红黑树（当数组长度大于64则进行转换，否则只是扩容），以减少搜索时间 HashMap默认初始化大小为16，每次扩容为原容量2倍，且总是使用2的幂作为哈希表的大小 底层数据结构分析 # JDK8之前，HashMap底层是数组和链表，即链表散列；通过key的hashCode，经过扰动函数，获得hash值，然后再通过(n-1) \u0026amp; hash 判断当前元素存放位置（n指的是数组长度），如果当前位置存在元素，就判断元素与要存入的元素的hash值以及key是否相同，相同则覆盖，否则通过拉链法解决\n扰动函数，即hash(Object key)方法\n//JDK1.8 static final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // \u0026gt;\u0026gt;\u0026gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } JDK1.7\n//JDK1.7 , 则扰动了4次，性能较差 static int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } JDK1.8之后，当链表长度大于阈值（默认为 8）时，会首先调用 treeifyBin()方法。这个方法会根据 HashMap 数组来决定是否转换为红黑树。只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是执行 resize() 方法对数组扩容。相关源码这里就不贴了，重点关注 treeifyBin()方法即可！\nHashMap一些属性\npublic class HashMap\u0026lt;K,V\u0026gt; extends AbstractMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt;, Cloneable, Serializable { // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 \u0026lt;\u0026lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 \u0026lt;\u0026lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小容量 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍 transient Node\u0026lt;k,v\u0026gt;[] table; // 存放具体元素的集 transient Set\u0026lt;map.entry\u0026lt;k,v\u0026gt;\u0026gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值(容量*填充因子) 当实际大小超过临界值时，会进行扩容 int threshold; // 加载因子 final float loadFactor; } LoadFactor 加载因子\nloadFactor 加载因子是控制数组存放数据的疏密程度，loadFactor 越趋近于 1，那么 数组中存放的数据(entry)【说的就是数组个数】也就越多**，也就越密，也就是会让链表的长度增加，loadFactor 越小，也就是趋近于 0，数组中存放的数据(entry)也就越少，也就越稀疏。\nloadFactor 太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor 的默认值为 0.75f 是官方给出的一个比较好的临界值。\n给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。\nthreshold threshold 英[ˈθreʃhəʊld] threshold = capacity * loadFactor，即存放的元素Size 如果 \u0026gt; threshold ，即capacity * 0.75的时候，就要考虑扩容了\nNode类结点源码\n// 继承自 Map.Entry\u0026lt;K,V\u0026gt; static class Node\u0026lt;K,V\u0026gt; implements Map.Entry\u0026lt;K,V\u0026gt; { final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较 final K key;//键 V value;//值 // 指向下一个节点 Node\u0026lt;K,V\u0026gt; next; Node(int hash, K key, V value, Node\u0026lt;K,V\u0026gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \u0026#34;=\u0026#34; + value; } // 重写hashCode()方法 public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 重写 equals() 方法 public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry\u0026lt;?,?\u0026gt; e = (Map.Entry\u0026lt;?,?\u0026gt;)o; if (Objects.equals(key, e.getKey()) \u0026amp;\u0026amp; Objects.equals(value, e.getValue())) return true; } return false; } } 树节点类源码\nstatic final class TreeNode\u0026lt;K,V\u0026gt; extends LinkedHashMap.Entry\u0026lt;K,V\u0026gt; { TreeNode\u0026lt;K,V\u0026gt; parent; // 父 TreeNode\u0026lt;K,V\u0026gt; left; // 左 TreeNode\u0026lt;K,V\u0026gt; right; // 右 TreeNode\u0026lt;K,V\u0026gt; prev; // needed to unlink next upon deletion boolean red; // 判断颜色 TreeNode(int hash, K key, V val, Node\u0026lt;K,V\u0026gt; next) { super(hash, key, val, next); } // 返回根节点 final TreeNode\u0026lt;K,V\u0026gt; root() { for (TreeNode\u0026lt;K,V\u0026gt; r = this, p;;) { if ((p = r.parent) == null) return r; r = p; } HashMap源码分析 # 构造方法(4个，空参/Map/指定容量大小/容量大小及加载因子)\n// 默认构造函数。 public HashMap() { this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted } // 包含另一个“Map”的构造函数 public HashMap(Map\u0026lt;? extends K, ? extends V\u0026gt; m) { this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//下面会分析到这个方法 } // 指定“容量大小”的构造函数 public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } // 指定“容量大小”和“加载因子”的构造函数 public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } //putMapEntries方法 final void putMapEntries(Map\u0026lt;? extends K, ? extends V\u0026gt; m, boolean evict) { int s = m.size(); if (s \u0026gt; 0) { // 判断table是否已经初始化 if (table == null) { // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft \u0026lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t \u0026gt; threshold) threshold = tableSizeFor(t); } // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s \u0026gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry\u0026lt;? extends K, ? extends V\u0026gt; e : m.entrySet()) { K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); } } } put方法（对外只提供put，没有putVal) putVal方法添加元素分析\n如果定位到的数组位置没有元素直接插入\n如果有，则比较key，如果key相同则覆盖，不同则判断是否是否是一个树节点，如果是就调用e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value)将元素添加进入；如果不是，则遍历链表插入(链表尾部) ```java //源码 public V put(K key, V value) { return putVal(hash(key), key, value, false, true); }\nfinal V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; p; int n, i; // table未初始化或者长度为0，进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) \u0026amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) \u0026amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素（处理hash冲突） else { Node\u0026lt;K,V\u0026gt; e; K k; // 判断table[i]中的元素是否与插入的key一样，若相同那就直接使用插入的值p替换掉旧的值e。 if (p.hash == hash \u0026amp;\u0026amp; ((k = p.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) e = p; // 判断插入的是否是红黑树节点 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode\u0026lt;K,V\u0026gt;)p).putTreeVal(this, tab, hash, key, value); // 不是红黑树节点则说明为链表结点 else { // 在链表最末插入结点 for (int binCount = 0; ; ++binCount) { // 到达链表的尾部 if ((e = p.next) == null) { // 在尾部插入新结点 p.next = newNode(hash, key, value, null); // 结点数量达到阈值(默认为 8 )，执行 treeifyBin 方法 // 这个方法会根据 HashMap 数组来决定是否转换为红黑树。 // 只有当数组长度大于或者等于 64 的情况下，才会执行转换红黑树操作，以减少搜索时间。否则，就是只是对数组扩容。 if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; } // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; } } // 表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) { // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; } } // 结构性修改 ++modCount; // 实际大小大于阈值则扩容 if (++size \u0026gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null; } ``` 对比1.7中的put方法\n① 如果定位到的数组位置没有元素 就直接插入。\n② 如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的 key 比较，如果 key 相同就直接覆盖，不同就采用头插法插入元素。\n//源码 public V put(K key, V value) if (table == EMPTY_TABLE) { inflateTable(threshold); } if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry\u0026lt;K,V\u0026gt; e = table[i]; e != null; e = e.next) { // 先遍历 Object k; if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || key.equals(k))) { V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; } }\nmodCount++; addEntry(hash, key, value, i); // 再插入 return null; }\nget方法 //先算hash值，然后算出key在数组中的index下标，然后就要在数组中取值了（先判断第一个结点(链表/树))。如果相等，则返回，如果不相等则分两种情况：在（红黑树）树中get或者 链表中get（需要遍历）\npublic V get(Object key) { Node\u0026lt;K,V\u0026gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; }\nfinal Node\u0026lt;K,V\u0026gt; getNode(int hash, Object key) { Node\u0026lt;K,V\u0026gt;[] tab; Node\u0026lt;K,V\u0026gt; first, e; int n; K k; if ((tab = table) != null \u0026amp;\u0026amp; (n = tab.length) \u0026gt; 0 \u0026amp;\u0026amp; (first = tab[(n - 1) \u0026amp; hash]) != null) { // 数组元素相等 if (first.hash == hash \u0026amp;\u0026amp; // always check first node ((k = first.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return first; // 桶中不止一个节点 if ((e = first.next) != null) { // 在树中get if (first instanceof TreeNode) return ((TreeNode\u0026lt;K,V\u0026gt;)first).getTreeNode(hash, key); // 在链表中get do { if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; } ``` resize方法 每次扩容，都会进行一次重新hash分配，且会遍历所有元素（非常耗时）\nfinal Node\u0026lt;K,V\u0026gt;[] resize() { Node\u0026lt;K,V\u0026gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap \u0026gt; 0) { // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap \u0026gt;= MAXIMUM_CAPACITY) { threshold = Integer.MAX_VALUE; return oldTab; } // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap \u0026laquo; 1) \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; oldCap \u0026gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u0026laquo; 1; // double threshold } else if (oldThr \u0026gt; 0) // initial capacity was placed in threshold newCap = oldThr; else { // signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); } // 计算新的resize上限 if (newThr == 0) { float ft = (float)newCap * loadFactor; newThr = (newCap \u0026lt; MAXIMUM_CAPACITY \u0026amp;\u0026amp; ft \u0026lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); } threshold = newThr; @SuppressWarnings({\u0026ldquo;rawtypes\u0026rdquo;,\u0026ldquo;unchecked\u0026rdquo;}) Node\u0026lt;K,V\u0026gt;[] newTab = (Node\u0026lt;K,V\u0026gt;[])new Node[newCap]; table = newTab; if (oldTab != null) { // 把每个bucket都移动到新的buckets中 for (int j = 0; j \u0026lt; oldCap; ++j) { Node\u0026lt;K,V\u0026gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) newTab[e.hash \u0026amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode\u0026lt;K,V\u0026gt;)e).split(this, newTab, j, oldCap); else { Node\u0026lt;K,V\u0026gt; loHead = null, loTail = null; Node\u0026lt;K,V\u0026gt; hiHead = null, hiTail = null; Node\u0026lt;K,V\u0026gt; next; do { next = e.next; // 原索引 if ((e.hash \u0026amp; oldCap) == 0) { if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } // 原索引+oldCap else { if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) { loTail.next = null; newTab[j] = loHead; } // 原索引+oldCap放到bucket里 if (hiTail != null) { hiTail.next = null; newTab[j + oldCap] = hiHead; } } } } } return newTab; } ```\nHashMap常用方法测试 # package map; import java.util.Collection; import java.util.HashMap; import java.util.Set; public class HashMapDemo { public static void main(String[] args) { HashMap\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;String, String\u0026gt;(); // 键不能重复，值可以重复 map.put(\u0026#34;san\u0026#34;, \u0026#34;张三\u0026#34;); map.put(\u0026#34;si\u0026#34;, \u0026#34;李四\u0026#34;); map.put(\u0026#34;wu\u0026#34;, \u0026#34;王五\u0026#34;); map.put(\u0026#34;wang\u0026#34;, \u0026#34;老王\u0026#34;); map.put(\u0026#34;wang\u0026#34;, \u0026#34;老王2\u0026#34;);// 老王被覆盖 map.put(\u0026#34;lao\u0026#34;, \u0026#34;老王\u0026#34;); System.out.println(\u0026#34;-------直接输出hashmap:-------\u0026#34;); System.out.println(map); /** * 遍历HashMap */ // 1.获取Map中的所有键 System.out.println(\u0026#34;-------foreach获取Map中所有的键:------\u0026#34;); Set\u0026lt;String\u0026gt; keys = map.keySet(); for (String key : keys) { System.out.print(key+\u0026#34; \u0026#34;); } System.out.println();//换行 // 2.获取Map中所有值 System.out.println(\u0026#34;-------foreach获取Map中所有的值:------\u0026#34;); Collection\u0026lt;String\u0026gt; values = map.values(); for (String value : values) { System.out.print(value+\u0026#34; \u0026#34;); } System.out.println();//换行 // 3.得到key的值的同时得到key所对应的值 System.out.println(\u0026#34;-------得到key的值的同时得到key所对应的值:-------\u0026#34;); Set\u0026lt;String\u0026gt; keys2 = map.keySet(); for (String key : keys2) { System.out.print(key + \u0026#34;：\u0026#34; + map.get(key)+\u0026#34; \u0026#34;); } /** * 如果既要遍历key又要value，那么建议这种方式，因为如果先获取keySet然后再执行map.get(key)，map内部会执行两次遍历。 * 一次是在获取keySet的时候，一次是在遍历所有key的时候。 */ // 当我调用put(key,value)方法的时候，首先会把key和value封装到 // Entry这个静态内部类对象中，把Entry对象再添加到数组中，所以我们想获取 // map中的所有键值对，我们只要获取数组中的所有Entry对象，接下来 // 调用Entry对象中的getKey()和getValue()方法就能获取键值对了 Set\u0026lt;java.util.Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; entrys = map.entrySet(); for (java.util.Map.Entry\u0026lt;String, String\u0026gt; entry : entrys) { System.out.println(entry.getKey() + \u0026#34;--\u0026#34; + entry.getValue()); } /** * HashMap其他常用方法 */ System.out.println(\u0026#34;after map.size()：\u0026#34;+map.size()); System.out.println(\u0026#34;after map.isEmpty()：\u0026#34;+map.isEmpty()); System.out.println(map.remove(\u0026#34;san\u0026#34;)); System.out.println(\u0026#34;after map.remove()：\u0026#34;+map); System.out.println(\u0026#34;after map.get(si)：\u0026#34;+map.get(\u0026#34;si\u0026#34;)); System.out.println(\u0026#34;after map.containsKey(si)：\u0026#34;+map.containsKey(\u0026#34;si\u0026#34;)); System.out.println(\u0026#34;after containsValue(李四)：\u0026#34;+map.containsValue(\u0026#34;李四\u0026#34;)); System.out.println(map.replace(\u0026#34;si\u0026#34;, \u0026#34;李四2\u0026#34;)); System.out.println(\u0026#34;after map.replace(si, 李四2):\u0026#34;+map); } } 大部分转自https://github.com/Snailclimb/JavaGuide\n"},{"id":157,"href":"/zh/docs/technology/Review/java_guide/java/Collection/ly0104lysource-code-ArrayList/","title":"ArrayList源码","section":"集合","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n简介 # 底层是数组队列，相当于动态数组，能动态增长，可以在添加大量元素前先使用ensureCapacity来增加ArrayList容量，减少递增式再分配的数量 源码：\npublic class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable{ } Random Access，标志接口，表明这个接口的List集合支持快速随机访问，这里是指可通过元素序号快速访问 实现Cloneable接口，能被克隆 实现java.io.Serializable，支持序列化 ArrayList和Vector区别\nArrayList和Vector都是List的实现类，Vector出现的比较早，底层都是Object[] 存储 ArrayList线程不安全（适合频繁查找，线程不安全 ） Vector 线程安全的 ArrayList与LinkedList区别\n都是不同步的，即不保证线程安全\nArrayList底层为Object数组；LinkedList底层使用双向链表数据结构(1.6之前为循环链表，1.7取消了循环)\n插入和删除是否受元素位置影响\nArrayList采用数组存储，所以插入和删除元素的时间复杂度受元素位置影响[ 默认增加到末尾，O(1) ; 在指定位置，则O(n) , 要往后移动]\nLinkedList采用链表存储，所以对于add(E e)方法，还是O(1)；如果是在指定位置插入和删除，则为O(n) 因为需要遍历将指针移动到指定位置\n//LinkedList默认添加到最后 public boolean add(E e) { linkLast(e); return true; } LinkedList不支持高效随机元素访问，而ArrayList支持（通过get(int index))\n内存空间占用 ArrayList的空间浪费主要体现在list列表的结尾会预留一定的容量空间，而LinkedList的空间花费在，每个元素都需要比ArrayList更多空间（要存放直接前驱和直接后继以及(当前)数据)\n3. 扩容机制分析 ( JDK8 ) # ArrayList的构造函数\n三种方式初始化，构造方法源码 空参，指定大小，指定集合 （如果集合类型非Object[].class，则使用Arrays.copyOf转为Object[].class) 以无参构造方式创建ArrayList时，实际上初始化赋值的是空数组；当真正操作时才分配容量，即添加第一个元素时扩容为10 /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** *默认构造函数，使用初始容量10构造一个空列表(无参数构造) */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) { if (initialCapacity \u0026gt; 0) {//初始容量大于0 //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) {//初始容量等于0 //创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else {//初始容量小于0，抛出异常 throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); } } /** *构造包含指定collection元素的列表，这些元素利用该集合的迭代器按顺序返回 *如果指定的集合为null，throws NullPointerException。 */ public ArrayList(Collection\u0026lt;? extends E\u0026gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; } } 以无参构造参数函数为例 先看下面的 add()方法扩容\n得到最小扩容量( 如果空数组则为10，否则原数组大小+1 )\u0026mdash;\u0026gt;确定是否扩容【minCapacity \u0026gt; 此时的数组大小】\u0026mdash;\u0026gt; 真实进行扩容 【 grow(int minCapacity) 】\n扩容的前提是 数组最小扩容 \u0026gt; 数组实际大小\n几个名词：oldCapacity，newCapacity (oldCapacity * 1.5 )，minCapacity，MAX_ARRAY_SIZE ,INT_MAX\n对于MAX_ARRAY_SIZE的解释：\n/** 要分配的数组的最大大小。 一些 VM 在数组中保留一些标题字。 尝试分配更大的数组可能会导致 OutOfMemoryError：请求的数组大小超过 VM 限制**/ Integer.MAX_VALUE = Ingeger.MAX_VALUE - 8 ;\ncapacity 英[kəˈpæsəti] 这个方法最后是要用newCapacity扩容的，所以要给他更新可用的值，也就是：\n如果扩容后还比minCapacity 小，那就把newCapacity更新为minCapacity的值\n如果比MAX_ARRAY_SIZE还大，那就超过范围了\n得通过hugeCapacity(minCapcacity) ，即minCapacity和MAX_ARRAY_SIZE来设置newCapacity\n-\u0026gt; 这里有点绕，看了也记不住\u0026mdash;\u0026ndash;其实前面第1步，就是说我至少需要minCapcacity的数，但是如果newCapacity (1.5 * oldCapacity )比MAX_ARRAY_SIZE：如果实际需要的容量 (miniCapacity \u0026gt; MAX_ARRAY_SIZE , 那就直接取Integer.MAX_VALUE ；如果没有，那就取MAX_ARRAY_SIZE )\n//add方法，先扩容，再赋值（实际元素长度最后） /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { //添加元素之前，先调用ensureCapacityInternal方法 ensureCapacityInternal(size + 1); // Increments modCount!! //jdk11 移除了该方法，第一次进入时size为0 //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; } //ensureCapacityInternal,if语句说明第一次add时，取当前容量和默认容量的最大值作为扩容量 //**得到最小扩容量** private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取默认的容量和传入参数的较大值 //当 要 add(E) 进第 1 个元素时，minCapacity 为 1，在 Math.max()方法比较后，minCapacity 为 10。 //为什么不直接取DEFAULT_CAPACITY,因为这个方法不只是add(E )会用到， //其次addAll(Collection\u0026lt;? extends E\u0026gt; c)也用到了 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } //ensureExplicitCapacity 判断是否扩容 //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length \u0026gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); } /* if语句表示，当minCapacity（数组实际*需要*容量的大小）大于实际容量则进行扩容 添加第1个元素的时候，会进入grow方法，直到添加第10个元素 都不会再进入grow()方法 当添加第11个元素时，minCapacity(11)比elementData.length(10)大，进入扩容 */ // grow()方法 /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量[1.5倍扩容后还小于，说明一次添加的大于1.5倍扩容后的大小] if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; // 如果新容量大于 MAX_ARRAY_SIZE,进入(执行) `hugeCapacity()` 方法来比较 minCapacity 和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为`Integer.MAX_VALUE`，否则，新容量大小则为 MAX_ARRAY_SIZE 即为 `Integer.MAX_VALUE - 8`。 if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } /* 进入真正的扩容 int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1),所以 ArrayList 每次扩容之后容量都会变为原来的 1.5 倍左右（oldCapacity 为偶数就是 1.5 倍，否则是 1.5 倍左右）！ 奇偶不同，比如 ：10+10/2 = 15, 33+33/2=49。如果是奇数的话会丢掉小数；右移运算会比普通运算符快很多 */ 扩展\njava 中的 length属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的 length() 方法是针对字符串说的,如果想看这个字符串的长度则用到 length() 这个方法. java 中的 size() 方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! hugeCapacity 当新容量超过MAX_ARRAY_SIZE时，if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) 进入该方法\nprivate static int hugeCapacity(int minCapacity) { if (minCapacity \u0026lt; 0) // overflow throw new OutOfMemoryError(); //对minCapacity和MAX_ARRAY_SIZE进行比较 //若minCapacity大，将Integer.MAX_VALUE作为新数组的大小 //若MAX_ARRAY_SIZE大，将MAX_ARRAY_SIZE作为新数组的大小 //MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; return (minCapacity \u0026gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } System.arraycopy() 和 Arrays.copyOf()\n//System.arraycopy() 是一个native方法 // 我们发现 arraycopy 是一个 native 方法,接下来我们解释一下各个参数的具体意义 /** * 复制数组 * @param src 源数组 * @param srcPos 源数组中的起始位置 * @param dest 目标数组 * @param destPos 目标数组中的起始位置 * @param length 要复制的数组元素的数量 */ public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); 例子：\npublic class ArraycopyTest { public static void main(String[] args) { // TODO Auto-generated method stub int[] a = new int[10]; a[0] = 0; a[1] = 1; a[2] = 2; a[3] = 3; System.arraycopy(a, 2, a, 3, 3); a[2]=99; for (int i = 0; i \u0026lt; a.length; i++) { System.out.print(a[i] + \u0026#34; \u0026#34;); } } } //结果 0 1 99 2 3 0 0 0 0 0 Arrays.copyOf() 方法\npublic static int[] copyOf(int[] original, int newLength) { // 申请一个新的数组 int[] copy = new int[newLength]; // 调用System.arraycopy,将源数组中的数据进行拷贝,并返回新的数组 System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy; } //场景 /** 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; 返回的数组的运行时类型是指定数组的运行时类型。 */ public Object[] toArray() { //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); } Arrays.copypf() ： 用来扩容，或者缩短\npublic class ArrayscopyOfTest { public static void main(String[] args) { int[] a = new int[3]; a[0] = 0; a[1] = 1; a[2] = 2; int[] b = Arrays.copyOf(a, 10); System.out.println(\u0026#34;b.length\u0026#34;+b.length); } } //结果： 10 联系及区别\n看两者源代码可以发现 copyOf()内部实际调用了 System.arraycopy() 方法 arraycopy 更能实现自定义 ensureCapacity 方法 最好在向 ArrayList 添加大量元素之前用 ensureCapacity 方法，以减少增量重新分配的次数 向 ArrayList 添加大量元素之前使用ensureCapacity 方法可以提升性能。不过，这个性能差距几乎可以忽略不计。而且，实际项目根本也不可能往 ArrayList 里面添加这么多元素 2. 核心源码解读 # package java.util; import java.util.function.Consumer; import java.util.function.Predicate; import java.util.function.UnaryOperator; public class ArrayList\u0026lt;E\u0026gt; extends AbstractList\u0026lt;E\u0026gt; implements List\u0026lt;E\u0026gt;, RandomAccess, Cloneable, java.io.Serializable { private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = {}; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = {}; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数（用户可以在创建ArrayList对象时自己指定集合的初始大小） */ public ArrayList(int initialCapacity) { if (initialCapacity \u0026gt; 0) { //如果传入的参数大于0，创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { //如果传入的参数等于0，创建空数组 this.elementData = EMPTY_ELEMENTDATA; } else { //其他情况，抛出异常 throw new IllegalArgumentException(\u0026#34;Illegal Capacity: \u0026#34;+ initialCapacity); } } /** *默认无参构造函数 *DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; } /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection\u0026lt;? extends E\u0026gt; c) { //将指定集合转换为数组 elementData = c.toArray(); //如果elementData数组的长度不为0 if ((size = elementData.length) != 0) { // 如果elementData不是Object类型数据（c.toArray可能返回的不是Object类型的数组所以加上下面的语句用于判断） if (elementData.getClass() != Object[].class) //将原来不是Object类型的elementData数组的内容，赋值给新的Object类型的elementData数组 elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // 其他情况，用空数组代替 this.elementData = EMPTY_ELEMENTDATA; } } /** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */ public void trimToSize() { modCount++; if (size \u0026lt; elementData.length) { elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); } } //下面是ArrayList的扩容机制 //ArrayList的扩容机制提高了性能，如果每次只扩充一个， //那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) { //如果是true，minExpand的值为0，如果是false,minExpand的值为10 int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It\u0026#39;s already // supposed to be at default size. : DEFAULT_CAPACITY; //如果最小容量大于已有的最大容量 if (minCapacity \u0026gt; minExpand) { ensureExplicitCapacity(minCapacity); } } //1.得到最小扩容量 //2.通过最小容量扩容 private void ensureCapacityInternal(int minCapacity) { if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) { // 获取“默认的容量”和“传入参数”两者之间的最大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); } ensureExplicitCapacity(minCapacity); } //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) { modCount++; // overflow-conscious code if (minCapacity - elementData.length \u0026gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); } /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) { // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity \u0026gt;\u0026gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity \u0026lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于MAX_ARRAY_SIZE，则新容量则为Interger.MAX_VALUE，否则，新容量大小则为 MAX_ARRAY_SIZE。 if (newCapacity - MAX_ARRAY_SIZE \u0026gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); } //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) { if (minCapacity \u0026lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity \u0026gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; } /** *返回此列表中的元素数。 */ public int size() { return size; } /** * 如果此列表不包含元素，则返回 true 。 */ public boolean isEmpty() { //注意=和==的区别 return size == 0; } /** * 如果此列表包含指定的元素，则返回true 。 */ public boolean contains(Object o) { //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 return indexOf(o) \u0026gt;= 0; } /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */ public int indexOf(Object o) { if (o == null) { for (int i = 0; i \u0026lt; size; i++) if (elementData[i]==null) return i; } else { for (int i = 0; i \u0026lt; size; i++) //equals()方法比较 if (o.equals(elementData[i])) return i; } return -1; } /** * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。. */ public int lastIndexOf(Object o) { if (o == null) { for (int i = size-1; i \u0026gt;= 0; i--) if (elementData[i]==null) return i; } else { for (int i = size-1; i \u0026gt;= 0; i--) if (o.equals(elementData[i])) return i; } return -1; } /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() { try { ArrayList\u0026lt;?\u0026gt; v = (ArrayList\u0026lt;?\u0026gt;) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; } catch (CloneNotSupportedException e) { // 这不应该发生，因为我们是可以克隆的 throw new InternalError(e); } } /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() { return Arrays.copyOf(elementData, size); } /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public \u0026lt;T\u0026gt; T[] toArray(T[] a) { if (a.length \u0026lt; size) // 新建一个运行时类型的数组，但是ArrayList数组的内容 return (T[]) Arrays.copyOf(elementData, size, a.getClass()); //调用System提供的arraycopy()方法实现数组之间的复制 System.arraycopy(elementData, 0, a, 0, size); if (a.length \u0026gt; size) a[size] = null; return a; } // Positional Access Operations @SuppressWarnings(\u0026#34;unchecked\u0026#34;) E elementData(int index) { return (E) elementData[index]; } /** * 返回此列表中指定位置的元素。 */ public E get(int index) { rangeCheck(index); return elementData(index); } /** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) { //对index进行界限检查 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素 return oldValue; } /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) { ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; } /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; } /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) { rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; } /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) { if (o == null) { for (int index = 0; index \u0026lt; size; index++) if (elementData[index] == null) { fastRemove(index); return true; } } else { for (int index = 0; index \u0026lt; size; index++) if (o.equals(elementData[index])) { fastRemove(index); return true; } } return false; } /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) { modCount++; int numMoved = size - index - 1; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work } /** * 从列表中删除所有元素。 */ public void clear() { modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i \u0026lt; size; i++) elementData[i] = null; size = 0; } /** * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。 */ public boolean addAll(Collection\u0026lt;? extends E\u0026gt; c) { Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; } /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection\u0026lt;? extends E\u0026gt; c) { rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved \u0026gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; } /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) { modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work int newSize = size - (toIndex-fromIndex); for (int i = newSize; i \u0026lt; size; i++) { elementData[i] = null; } size = newSize; } /** * 检查给定的索引是否在范围内。 */ private void rangeCheck(int index) { if (index \u0026gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) { if (index \u0026gt; size || index \u0026lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); } /** * 返回IndexOutOfBoundsException细节信息 */ private String outOfBoundsMsg(int index) { return \u0026#34;Index: \u0026#34;+index+\u0026#34;, Size: \u0026#34;+size; } /** * 从此列表中删除指定集合中包含的所有元素。 */ public boolean removeAll(Collection\u0026lt;?\u0026gt; c) { Objects.requireNonNull(c); //如果此列表被修改则返回true return batchRemove(c, false); } /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection\u0026lt;?\u0026gt; c) { Objects.requireNonNull(c); return batchRemove(c, true); } /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator\u0026lt;E\u0026gt; listIterator(int index) { if (index \u0026lt; 0 || index \u0026gt; size) throw new IndexOutOfBoundsException(\u0026#34;Index: \u0026#34;+index); return new ListItr(index); } /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator\u0026lt;E\u0026gt; listIterator() { return new ListItr(0); } /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator\u0026lt;E\u0026gt; iterator() { return new Itr(); } "},{"id":158,"href":"/zh/docs/technology/Review/java_guide/java/Collection/ly0103lycollections-precautions-for-use/","title":"集合使用注意事项","section":"集合","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n集合判空 # //阿里巴巴开发手册\n判断所有集合内部的元素是否为空，使用 isEmpty() 方法，而不是 size()==0 的方式。\nisEmpty()可读性更好，且绝大部分情况下时间复杂度为O(1)\n有例外：ConcurrentHashMap的size()和isEmpty() 时间复杂度均不是O(1)\npublic int size() { long n = sumCount(); return ((n \u0026lt; 0L) ? 0 : (n \u0026gt; (long)Integer.MAX_VALUE) ? Integer.MAX_VALUE : (int)n); } final long sumCount() { CounterCell[] as = counterCells; CounterCell a; long sum = baseCount; if (as != null) { for (int i = 0; i \u0026lt; as.length; ++i) { if ((a = as[i]) != null) sum += a.value; } } return sum; } public boolean isEmpty() { return sumCount() \u0026lt;= 0L; // ignore transient negative values } 集合转Map # //阿里巴巴开发手册\n在使用 java.util.stream.Collectors 类的 toMap() 方法转为 Map 集合时，一定要注意当 value 为 null 时会抛 NPE 异常。\nclass Person { private String name; private String phoneNumber; // getters and setters } List\u0026lt;Person\u0026gt; bookList = new ArrayList\u0026lt;\u0026gt;(); bookList.add(new Person(\u0026#34;jack\u0026#34;,\u0026#34;18163138123\u0026#34;)); bookList.add(new Person(\u0026#34;martin\u0026#34;,null)); // 空指针异常 bookList.stream().collect(Collectors.toMap(Person::getName, Person::getPhoneNumber)); java.util.stream.Collections类的toMap() ，里面使用到了Map接口的merge()方法, 调用了Objects.requireNonNull()方法判断value是否为空\n集合遍历 # //阿里巴巴开发手册\n不要在 foreach 循环里进行元素的 remove/add 操作。remove 元素请使用 Iterator 方式，如果并发操作，需要对 Iterator 对象加锁。\nforeach语法底层依赖于Iterator （foreach是语法糖），不过remove/add 则是直接调用集合的方法，而不是Iterator的； 所以此时Iterator莫名发现自己元素被remove/add，就会抛出一个ConcurrentModificationException来提示用户发生了并发修改异常，即单线程状态下产生的fail-fast机制\njava8开始，可以使用Collection#**removeIf()**方法删除满足特定条件的元素，例子\nList\u0026lt;Integer\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (int i = 1; i \u0026lt;= 10; ++i) { list.add(i); } list.removeIf(filter -\u0026gt; filter % 2 == 0); /* 删除list中的所有偶数 */ System.out.println(list); /* [1, 3, 5, 7, 9] */ 其他的遍历数组的方法（注意是遍历，不是增加/删除）\n使用普通for循环\n使用fail-safe集合类，java.util包下面的所有集合类都是fail-fast，而java.util.concurrent包下面的所有类是fail-safe\n//ConcurrentHashMap源码 package java.util.concurrent; public class ConcurrentHashMap\u0026lt;K,V\u0026gt; extends AbstractMap\u0026lt;K,V\u0026gt; implements ConcurrentMap\u0026lt;K,V\u0026gt;, Serializable {} //List类源码 package java.util; public class HashMap\u0026lt;K,V\u0026gt; extends AbstractMap\u0026lt;K,V\u0026gt; implements Map\u0026lt;K,V\u0026gt;, Cloneable, Serializable { } 集合去重 # //阿里巴巴开发手册\n可以利用 Set 元素唯一的特性，可以快速对一个集合进行去重操作，避免使用 List 的 contains() 进行遍历去重或者判断包含操作。\n// Set 去重代码示例 public static \u0026lt;T\u0026gt; Set\u0026lt;T\u0026gt; removeDuplicateBySet(List\u0026lt;T\u0026gt; data) { if (CollectionUtils.isEmpty(data)) { return new HashSet\u0026lt;\u0026gt;(); } return new HashSet\u0026lt;\u0026gt;(data); } // List 去重代码示例 public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; removeDuplicateByList(List\u0026lt;T\u0026gt; data) { if (CollectionUtils.isEmpty(data)) { return new ArrayList\u0026lt;\u0026gt;(); } List\u0026lt;T\u0026gt; result = new ArrayList\u0026lt;\u0026gt;(data.size()); for (T current : data) { if (!result.contains(current)) { result.add(current); } } return result; } Set时间复杂度为 1 * n ，而List时间复杂度为 n * n\n//Set的Contains，底层依赖于HashMap,时间复杂度为 1 private transient HashMap\u0026lt;E,Object\u0026gt; map; public boolean contains(Object o) { return map.containsKey(o); } //ArrayList的Contains，底层则是遍历,时间复杂度为O(n) public boolean contains(Object o) { return indexOf(o) \u0026gt;= 0; } public int indexOf(Object o) { if (o == null) { for (int i = 0; i \u0026lt; size; i++) if (elementData[i]==null) return i; } else { for (int i = 0; i \u0026lt; size; i++) if (o.equals(elementData[i])) return i; } return -1; } 集合转数组 # //阿里巴巴开发手册\n使用集合转数组的方法，必须使用集合的 toArray(T[] array)，传入的是类型完全一致、长度为 0 的空数组。\n例子：\nString [] s= new String[]{ \u0026#34;dog\u0026#34;, \u0026#34;lazy\u0026#34;, \u0026#34;a\u0026#34;, \u0026#34;over\u0026#34;, \u0026#34;jumps\u0026#34;, \u0026#34;fox\u0026#34;, \u0026#34;brown\u0026#34;, \u0026#34;quick\u0026#34;, \u0026#34;A\u0026#34; }; List\u0026lt;String\u0026gt; list = Arrays.asList(s); Collections.reverse(list); //没有指定类型的话会报错 s=list.toArray(new String[0]); 对于 JVM 优化，new String[0]作为Collection.toArray()方法的参数现在使用更好，new String[0]就是起一个模板的作用，指定了返回数组的类型，0 是为了节省空间，因为它只是为了说明返回的类型\n数组转集合 # //阿里巴巴开发手册\n使用工具类 Arrays.asList() 把数组转换成集合时，不能使用其修改集合相关的方法， 它的 add/remove/clear 方法会抛出 UnsupportedOperationException 异常。\n例子及源码：\nString[] myArray = {\u0026#34;Apple\u0026#34;, \u0026#34;Banana\u0026#34;, \u0026#34;Orange\u0026#34;}; List\u0026lt;String\u0026gt; myList = Arrays.asList(myArray); //上面两个语句等价于下面一条语句 List\u0026lt;String\u0026gt; myList = Arrays.asList(\u0026#34;Apple\u0026#34;,\u0026#34;Banana\u0026#34;, \u0026#34;Orange\u0026#34;); //JDK源码说明[返回由指定数组支持的固定大小的列表] /** *返回由指定数组支持的固定大小的列表。此方法作为基于数组和基于集合的API之间的桥梁， * 与 Collection.toArray()结合使用。返回的List是可序列化并实现RandomAccess接口。 */ public static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; asList(T... a) { return new ArrayList\u0026lt;\u0026gt;(a); } 注意事项：\n1、Arrays.asList()是泛型方法，传递的数组必须是对象数组，而不是基本类型。 如果把原生数据类型数组传入，则传入的不是数组的元素，而是数组对象本身，可以使用包装类数组解决这个问题\nint[] myArray = {1, 2, 3}; List myList = Arrays.asList(myArray); System.out.println(myList.size());//1 System.out.println(myList.get(0));//数组地址值 System.out.println(myList.get(1));//报错：ArrayIndexOutOfBoundsException int[] array = (int[]) myList.get(0); System.out.println(array[0]);//1 2、使用集合的修改方法add()，remove()，clear()会抛出异常UnsupportedOperationException java.util.Arrays$ArrayList （Arrays里面有一个ArrayList类，该类继承了AbstractList）\n源码：\npublic E remove(int index) { throw new UnsupportedOperationException(); } public boolean add(E e) { add(size(), e); return true; } public void add(int index, E element) { throw new UnsupportedOperationException(); } public void clear() { removeRange(0, size()); } protected void removeRange(int fromIndex, int toIndex) { ListIterator\u0026lt;E\u0026gt; it = listIterator(fromIndex); for (int i=0, n=toIndex-fromIndex; i\u0026lt;n; i++) { it.next(); it.remove(); } } 如何转换成正常的ArraysList呢\n手动实现工具类\n//使用泛型 //JDK1.5+ static \u0026lt;T\u0026gt; List\u0026lt;T\u0026gt; arrayToList(final T[] array) { final List\u0026lt;T\u0026gt; l = new ArrayList\u0026lt;T\u0026gt;(array.length); for (final T s : array) { l.add(s); } return l; } Integer [] myArray = { 1, 2, 3 }; System.out.println(arrayToList(myArray).getClass());//class java.util.ArrayList 便捷的方法\n//再转一次 List list = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;)) 使用Java8的Stream（推荐），包括基本类型\nInteger [] myArray = { 1, 2, 3 }; List myList = Arrays.stream(myArray).collect(Collectors.toList()); //基本类型也可以实现转换（依赖boxed的装箱操作） int [] myArray2 = { 1, 2, 3 }; List myList = Arrays.stream(myArray2).boxed().collect(Collectors.toList()); 使用Apache Commons Colletions\nList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;(); CollectionUtils.addAll(list, str); 使用Java9的List.of()\nInteger[] array = {1, 2, 3}; List\u0026lt;Integer\u0026gt; list = List.of(array); 大部分转自https://github.com/Snailclimb/JavaGuide\n"},{"id":159,"href":"/zh/docs/technology/Review/java_guide/java/Collection/ly0102lycollection_2/","title":"集合_2","section":"集合","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nMap # HashMap和Hashtable的区别\nHashMap是非线程安全的，Hashtable是线程安全的，因为Hashtable内部方法都经过synchronized修饰（不过要保证线程安全一般用ConcurrentHashMap）\n由于加了synchronized修饰，HashTable效率没有HashMap高\nHashMap可以存储null的key和value，但null作为键只能有一个**；HashTable不允许有null键和null值**\n初始容量及每次扩容\nHashtable默认初始大小11，之后扩容为2n+1;HashMap初始大小16，之后扩容变为原来的2倍 如果指定初始大小，HashTable直接使用初始大小\n而HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的**tableSizeFor()**方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方 底层数据结构\nJDK1.8之后HashMap解决哈希冲突时，当链表大于阈值（默认8）时，将链表转为红黑树（转换前判断，如果当前数组长度小于64，则先进行数组扩容，而不转成红黑树），以减少搜索时间。 Hashtable没有上面的机制 /** HashMap 中带有初始容量的构造函数： */ public HashMap(int initialCapacity, float loadFactor) { if (initialCapacity \u0026lt; 0) throw new IllegalArgumentException(\u0026#34;Illegal initial capacity: \u0026#34; + initialCapacity); if (initialCapacity \u0026gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor \u0026lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(\u0026#34;Illegal load factor: \u0026#34; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); } public HashMap(int initialCapacity) { this(initialCapacity, DEFAULT_LOAD_FACTOR); } /*下面这个方法保证了 HashMap 总是使用 2 的幂作为哈希表的大小。*/ /** * Returns a power of two size for the given target capacity. */ static final int tableSizeFor(int cap) { int n = cap - 1; n |= n \u0026gt;\u0026gt;\u0026gt; 1; n |= n \u0026gt;\u0026gt;\u0026gt; 2; n |= n \u0026gt;\u0026gt;\u0026gt; 4; n |= n \u0026gt;\u0026gt;\u0026gt; 8; n |= n \u0026gt;\u0026gt;\u0026gt; 16; return (n \u0026lt; 0) ? 1 : (n \u0026gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; } HashMap和hashSet区别\n如果你看过 HashSet 源码的话就应该知道：HashSet 底层就是基于 HashMap 实现的。（HashSet 的源码非常非常少，因为除了 clone()、writeObject()、readObject()是 HashSet 自己不得不实现之外，其他方法都是直接调用 HashMap 中的方法\nHashSet底层就HashMap 会将其扩充为 2 的幂次方大小（HashMap 中的tableSizeFor()方法保证，下面给出了源代码）。也就是说 HashMap 总是使用 2 的幂作为哈希表的大小,后面会介绍到为什么是 2 的幂次方是HashMap实现的 HashMap：实现了Map接口；存储键值对；调用put()向map中添加元素；HashMap使用键（key）计算hashcode HashSet：实现Set接口；仅存储对象；调用add()方法向Set中添加元素；HashSet使用成员对象计算hashCode，对于不相等两个对象来说 hashcode也可能相同，所以**还要再借助equals()**方法判断对象相等性 HashMap和TreeMap navigable 英[ˈnævɪɡəbl] 通航的，可航行的\nHashMap和TreeMap都继承自AbstractMap\nTreeMap还实现了NavigableMap （对集合内元素搜索）和SortedMap（对集合内元素根据键排序，默认key升序，可指定排序的比较器）接口\n实现 NavigableMap 接口让 TreeMap 有了对集合内元素的搜索的能力。\n实现SortedMap接口让 TreeMap 有了对集合中的元素根据键排序的能力。默认是按 key 的升序排序，不过我们也可以指定排序的比较器。示例代码如下：\n/** * @author shuang.kou * @createTime 2020年06月15日 17:02:00 */ public class Person { private Integer age; public Person(Integer age) { this.age = age; } public Integer getAge() { return age; } public static void main(String[] args) { TreeMap\u0026lt;Person, String\u0026gt; treeMap = new TreeMap\u0026lt;\u0026gt;(new Comparator\u0026lt;Person\u0026gt;() { @Override public int compare(Person person1, Person person2) { int num = person1.getAge() - person2.getAge(); return Integer.compare(num, 0); } }); treeMap.put(new Person(3), \u0026#34;person1\u0026#34;); treeMap.put(new Person(18), \u0026#34;person2\u0026#34;); treeMap.put(new Person(35), \u0026#34;person3\u0026#34;); treeMap.put(new Person(16), \u0026#34;person4\u0026#34;); treeMap.entrySet().stream().forEach(personStringEntry -\u0026gt; { System.out.println(personStringEntry.getValue()); }); } } //输出 /**person1 person4 person2 person3 **/ HashSet如何检查重复\n当在HashSet加入对象时，先计算对象hashcode值判断加入位置，同时与其他加入对象的hashcode值比较，如果没有相同的，会假设对象没有重复出现；如果发现有相同的hashcode值的对象，则调用equals()方法检查hashcode相等的对象是否真的相等，如果相等则不会加入\nJDK1.8中，HashSet的add()方法调用了HashMap的put()方法，并判断是否有重复元素（返回值是否null)\n// Returns: true if this set did not already contain the specified element // 返回值：当 set 中没有包含 add 的元素时返回真 public boolean add(E e) { return map.put(e, PRESENT)==null; } //下面为HashMap的源代码 // Returns : previous value, or null if none // 返回值：如果插入位置没有元素返回null，否则返回上一个元素 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { ... } HashMap底层实现\nJDK1.8之前，底层是数组和链表结合在一起使用，即链表散列。通过key的hashcode 经过扰动函数处理后得到hash值，并通过 (n-1) \u0026amp; hash 判断当前元素存放的位置 （n为数组长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的hash值以及key是否相同，如果相同则覆盖，不同则通过拉链法解决冲突 扰动函数指的是HashMap的hash方法，是为了防止一些实现比较差的hashCode方法，减少碰撞 JDK1.8的hash：如果key为null则返回空，否则使用 (key的hash值) 与 (hash值右移16位) 做异或操作\nstatic final int hash(Object key) { int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // \u0026gt;\u0026gt;\u0026gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u0026gt;\u0026gt;\u0026gt; 16); } JDK1.7扰动次数更多\nstatic int hash(int h) { // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h \u0026gt;\u0026gt;\u0026gt; 20) ^ (h \u0026gt;\u0026gt;\u0026gt; 12); return h ^ (h \u0026gt;\u0026gt;\u0026gt; 7) ^ (h \u0026gt;\u0026gt;\u0026gt; 4); } 拉链法即链表和数组结合，也就是创建一个链表数组，数组每一格为一个链表，如果发生哈希冲突，就将冲突的值添加到链表中即可\nJDK8之后，解决冲突发生了较大变化，当链表长度大于阈值（默认是8）（如果数组小于64，则只会进行扩容；如果不是，才转成红黑树）时，将链表转换成红黑树，以减少搜索时间 二叉查找树，在某些情况下会退化成线性结构，时间复杂度为n ，而红黑树趋于log n 。TreeMap、TreeSet以及1.8之后的HashMap都用到了红黑树\n代码\n//当链表长度大于8时，执行treeifyBin（转换红黑树） // 遍历链表 for (int binCount = 0; ; ++binCount) { // 遍历到链表最后一个节点 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); // 如果链表元素个数大于等于TREEIFY_THRESHOLD（8） if (binCount \u0026gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 红黑树转换（并不会直接转换成红黑树） treeifyBin(tab, hash); break; } if (e.hash == hash \u0026amp;\u0026amp; ((k = e.key) == key || (key != null \u0026amp;\u0026amp; key.equals(k)))) break; p = e; } //判断是否会转成红黑树 final void treeifyBin(Node\u0026lt;K,V\u0026gt;[] tab, int hash) { int n, index; Node\u0026lt;K,V\u0026gt; e; // 判断当前数组的长度是否小于 64 if (tab == null || (n = tab.length) \u0026lt; MIN_TREEIFY_CAPACITY) // 如果当前数组的长度小于 64，那么会选择先进行数组扩容 resize(); else if ((e = tab[index = (n - 1) \u0026amp; hash]) != null) { // 否则才将列表转换为红黑树 TreeNode\u0026lt;K,V\u0026gt; hd = null, tl = null; do { TreeNode\u0026lt;K,V\u0026gt; p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) hd.treeify(tab); } } HashMap的长度为为什么是2的幂次方\n为了能让 HashMap 存取高效，尽量较少碰撞，也就是要尽量把数据分配均匀。我们上面也讲到了过了，Hash 值的范围值-2147483648 到 2147483647，前后加起来大概 40 亿的映射空间，只要哈希函数映射得比较均匀松散，一般应用是很难出现碰撞的。但问题是一个 40 亿长度的数组，内存是放不下的。所以这个散列值是不能直接拿来用的（也就是这个数组不能直接拿来用）。\n用之前还要先做对数组的长度取模运算，得到的余数才能用来要存放的位置也就是对应的数组下标。这个数组下标的计算方法是“ (n - 1) \u0026amp; hash”。（n 代表数组长度）。这也就解释了 HashMap 的长度为什么是 2 的幂次方。\n这个算法应该如何设计呢？\n我们首先可能会想到采用%取余的操作来实现。但是，重点来了：“取余(%)操作中如果除数是 2 的幂次则等价于与其除数减一的与(\u0026amp;)操作（也就是说 hash%length==hash\u0026amp;(length-1)的前提是 length 是 2 的 n 次方；）。” 并且 采用二进制位操作 \u0026amp;，相对于%能够提高运算效率，这就解释了 HashMap 的长度为什么是 2 的幂次方。\nHashMap多线程操作导致死循环问题 多线程下不建议使用HashMap，1.8之前并发下进行Rehash会造成元素之间形成循环链表，但是1.8之后还有其他问题（数据丢失），建议使用concurrentHashMap\nHashMap有哪几种常见的遍历方式\nhttps://mp.weixin.qq.com/s/zQBN3UvJDhRTKP6SzcZFKw\nConcurrentHashMap和Hashtable\n主要体现在，实现线程安全的方式上不同\n底层数据结构\nConcurrentHashMap：JDK1.7 底层采用分段数组+链表，JDK1.8 则是数组+链表/红黑二叉树（红黑树是1.8之后才出现的） HashTable采用 数组 (应该不是分段数组) + 链表 实现线程安全的方式\nConcurrentHashMap JDK1.7 时对整个桶数进行分割分段(Segment，分段锁)，每一把锁只锁容器其中一部分数据，当访问不同数据段的数据就不会存在锁竞争 ConcurrentHashMap JDK1.8摒弃Segment概念，直接用Node数组+链表+红黑树，并发控制使用synchronized和CAS操作 而Hashtable则是同一把锁，使用synchronized保证线程安全，效率低下。问题：当一个线程访问同步方式时，其他线程也访问同步方法，则可能进入阻塞/轮询状态，即如使用put添加元素另一个线程不能使用put和get 底层数据结构图\nHashTable：数组+链表 JDK1.7 的 ConcurrentHashMap（Segment数组，HashEntry数组，链表）\nSegment是用来加锁的 JDK1.8 的ConcurrentHashMap则是Node数组+链表/红黑树，不过红黑树时，不用Node，而是用TreeNode\nTreeNode，存储红黑树节点，被TreeBin包装\n/** root 维护红黑树根节点；waiter维护当前使用这颗红黑树的线程，防止其他线程进入 **/ static final class TreeBin\u0026lt;K,V\u0026gt; extends Node\u0026lt;K,V\u0026gt; { TreeNode\u0026lt;K,V\u0026gt; root; volatile TreeNode\u0026lt;K,V\u0026gt; first; volatile Thread waiter; volatile int lockState; // values for lockState static final int WRITER = 1; // set while holding write lock static final int WAITER = 2; // set when waiting for write lock static final int READER = 4; // increment value for setting read lock ... } ConcurrentHashMap线程安全的具体实现方式/底层具体实现\nJDK1.8之前的ConcurrentHashMap\nSegment 继承了 ReentrantLock,所以 Segment 是一种可重入锁，扮演锁的角色。HashEntry 用于存储键值对数据。\nstatic class Segment\u0026lt;K,V\u0026gt; extends ReentrantLock implements Serializable { } 一个 ConcurrentHashMap 里包含一个 Segment 数组，Segment 的个数一旦初始化就不能改变。 Segment 数组的大小默认是 16，也就是说默认可以同时支持 16 个线程并发写。 Segment 的结构和 HashMap 类似，是一种数组和链表结构，一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构的元素，每个 Segment 守护着一个 HashEntry 数组里的元素，当对 HashEntry 数组的数据进行修改时，必须首先获得对应的 Segment 的锁。也就是说，对同一 Segment 的并发写入会被阻塞，不同 Segment 的写入是可以并发执行的。\nJDK 1.8 之后 使用Node数组+链表/红黑树，几乎重写了ConcurrentHashMap，使用Node+CAS+Synchronized保证并发安全，数据结构跟HashMap1.8类似，超过一定阈值（默认8）将链表【O(N)】转成红黑树【O(log (N) )】 JDK8中，只锁定当前链表/红黑二叉树的首节点，这样只要hash不冲突就不会产生并发，不影响其他Node的读写，提高效率\nCollections工具类（不重要） # 包括 排序/查找/替换\n排序\nvoid reverse(List list)//反转 void shuffle(List list)//随机排序 void sort(List list)//按自然排序的升序排序 void sort(List list, Comparator c)//定制排序，由Comparator控制排序逻辑 void swap(List list, int i , int j)//交换两个索引位置的元素 void rotate(List list, int distance)//旋转。当distance为正数时，将list后distance个元素整体移到前面。当distance为负数时，将 list的前distance个元素整体移到后面 查找/替换\nint binarySearch(List list, Object key)//对List进行二分查找，返回索引，注意List必须是有序的 int max(Collection coll)//根据元素的自然顺序，返回最大的元素。 类比int min(Collection coll) int max(Collection coll, Comparator c)//根据定制排序，返回最大元素，排序规则由Comparatator类控制。类比int min(Collection coll, Comparator c) void fill(List list, Object obj)//用指定的元素代替指定list中的所有元素 int frequency(Collection c, Object o)//统计元素出现次数 int indexOfSubList(List list, List target)//统计target在list中第一次出现的索引，找不到则返回-1，类比int lastIndexOfSubList(List source, list target) boolean replaceAll(List list, Object oldVal, Object newVal)//用新元素替换旧元素 同步控制，Collections提供了多个synchronizedXxx()方法，该方法可以将指定集合包装成线程同步的集合，从而解决并发问题。 其中，HashSet、TreeSet、ArrayList、LinkedList、HashMap、TreeMap都是线程不安全的\n//不推荐，因为效率极低 建议使用JUC包下的并发集合 synchronizedCollection(Collection\u0026lt;T\u0026gt; c) //返回指定 collection 支持 的同步（线程安全的）collection。 synchronizedList(List\u0026lt;T\u0026gt; list)//返回指定列表支持的同步（线程安全的）List。 synchronizedMap(Map\u0026lt;K,V\u0026gt; m) //返回由指定映射支持的同步（线程安全的）Map。 synchronizedSet(Set\u0026lt;T\u0026gt; s) //返回指定 set 支持的同步（线程安全的）set。 "},{"id":160,"href":"/zh/docs/technology/Review/java_guide/java/Collection/ly0101lycollection_1/","title":"集合_1","section":"集合","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n集合包括Collection和Map，Collection 存放单一元素。Map 存放键值对 # List，Set，Queue，Map区别 # List(对付顺序的好帮手): 存储的元素是有序的、可重复的。 Set(注重独一无二的性质): 存储的元素是无序的、不可重复的。 Queue(实现排队功能的叫号机): 按特定的排队规则来确定先后顺序，存储的元素是有序的、可重复的。 Map(用 key 来搜索的专家): 使用键值对（key-value）存储，类似于数学上的函数 y=f(x)，\u0026ldquo;x\u0026rdquo; 代表 key，\u0026ldquo;y\u0026rdquo; 代表 value，key 是无序的、不可重复的，value 是无序的、可重复的，每个键最多映射到一个值。 各种集合框架\u0026ndash;底层数据结构 # List ArrayList、Vector \u0026mdash;-\u0026gt; Object[] 数组 LinkedList 双向链表 (jdk 1.6 之前为循环链表, 1.7 取消了循环) Set HashSet （无序，唯一），且基于HashMap LinkedHashSet 是HashSet的子类，基于LinkedHashMap (LinkedHashMap内部基于HashMap实现) TreeSet(有序，唯一) ：红黑树（自平衡的排序二叉树） Queue (队列) PriorityQueue：Object[] 数组来实现二叉堆 ArrayQueue：Object[] 数组+ 双指针 Map HashMap： JDK1.8 之前 HashMap 由数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）。JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）（将链表转换成红黑树前会判断，如果当前数组的长度小于 64，那么会选择先进行数组扩容，而不是转换为红黑树）时，将链表转化为红黑树，以减少搜索时间\nLinkedHashMap： LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构 即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。\n上图中，淡蓝色的箭头表示前驱引用，红色箭头表示后继引用。每当有新键值对节点插入，新节点最终会接在 tail 引用指向的节点后面（感觉这句话有问题，应该是head引用指向旧结点上）。而 tail 引用则会移动到新的节点上，这样一个双向链表就建立起来了。 作者：田小波 链接：https://www.imooc.com/article/22931\nHashtable： 数组+链表组成的，数组是 Hashtable 的主体，链表则是主要为了解决哈希冲突而存在的\nTreeMap： 红黑树（自平衡的排序二叉树）\n如何选用集合 # 当我们只需要存放元素值时，就选择实现Collection 接口的集合，需要保证元素唯一时选择实现 Set 接口的集合比如 TreeSet 或 HashSet，不需要就选择实现 List 接口的比如 ArrayList 或 LinkedList，然后再根据实现这些接口的集合的特点来选用 需要根据键值获取到元素值时就选用 Map 接口下的集合，需要排序时选择 TreeMap,不需要排序时就选择 HashMap,需要保证线程安全就选用 ConcurrentHashMap。 为什么需要集合 # 当需要保存一组类型相同的数据时，需要容器来保存，即数组，但实际中存储的类型多样， 而数组一旦声明则不可变长，同时数组数据类型也确定、数组有序可重复 集合可以存储不同类型不同数量的对象，还可以保存具有映射关系的数据 Collection 子接口 # List # ArrayList和Vector区别：ArrayList是List主要实现类，底层使用Object[]存储线程不安全；Vector是List古老实现类，底层使用Object[]存储，线程安全 （synchronized关键字）\nArrayList与LinkedList：\n都是线程不安全 ArrayList底层使用Object数组，LinkedList底层使用双向链表结构（JDK7以后非循环链表） ArrayList采用数组存储，所以插入和删除元素的时间复杂度受位置影响；LinkedList采用链表，所以在头尾插入或者删除元素不受元素位置影响，而如果需要插入或者删除中间指定位置，则时间复杂度为O(n) [主要是因为要遍历] LinkedList不支持高效的随机元素访问，而ArrayList支持（即通过元素的序号快速获取元素对象） 内存空间占用：ArrayList的空间浪费主要体现在List结尾会预留一定的容量空间（不是申请的所有容量都会用上），而LinkedList的空间花费则体现在它的每一个元素都需要消耗比ArrayList更多的空间（存放直接后继、直接前驱及数据） 实际项目中不怎么使用LinkedList，因为ArrayList性能通常会更好，LinkedList仅仅在头尾插入或者删除元素的时间复杂度近似O(1)\n双向链表与双向循环链表\n双向链表，包含两个指针，一个 prev 指向前一个节点，一个 next 指向后一个节点。 双向循环链表，首尾相连（头节点的前驱=尾结点，尾结点的后继=头节点） 补充：RandomAccess接口，这个接口只是用来标识：实现这个接口的类，具有随机访问功能，但并不是说因为实现了该接口才具有的快速随机访问机制\nCollections里面有这样一段代码\n在 binarySearch（) 方法中，它要判断传入的 list 是否 RandomAccess 的实例，如果是，调用indexedBinarySearch()方法，如果不是，那么调用iteratorBinarySearch()方法\npublic static \u0026lt;T\u0026gt; int binarySearch(List\u0026lt;? extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; list, T key) { if (list instanceof RandomAccess || list.size()\u0026lt;BINARYSEARCH_THRESHOLD) return Collections.indexedBinarySearch(list, key); else return Collections.iteratorBinarySearch(list, key); } ArrayList实现了RandomAccess方法，而LinkedList没有。是由于ArrayList底层是数组，支持快速随机访问，时间复杂度为O(1)，而LinkedList底层是链表，不支持快速随机访问，时间复杂度为O(n)\nSet # Coparable和Comparator的区别\nComparable实际出自java.lang包，有一个compareTo(Object obj)方法用来排序 Comparator实际出自java.util包，有一个compare(Object obj1,Object obj2)方法用来排序 Collections.sort(List\u0026lt;T\u0026gt; list, Comparator\u0026lt;? super T\u0026gt; c)默认是正序，T必须实现了Comparable，且Arrays.sort()方法中的部分代码如下：\n//使用插入排序 if (length \u0026lt; INSERTIONSORT_THRESHOLD) { for (int i=low; i\u0026lt;high; i++) for (int j=i; j\u0026gt;low \u0026amp;\u0026amp; c.compare(dest[j-1], dest[j])\u0026gt;0; j--) //如果前一个数跟后面的数相比大于零，则进行交换，即大的排后面 swap(dest, j, j-1); return; } //当比较结果\u0026gt;0时，调换数组前后两个元素的值，也就是后面的一定要比前面的大，即 public int compareTo(Person o) { if (this.age \u0026gt; o.getAge()) { return 1; } if (this.age \u0026lt; o.getAge()) { return -1; } return 0; } //下面这段代码，按照年龄降序（默认是升序） Collections.sort(arrayList, new Comparator\u0026lt;Integer\u0026gt;() { @Override public int compare(Integer o1, Integer o2) { //如果结果大于0，则两个数对调 //如果返回o2.compareTo(o1)，就是当o2\u0026gt;01时，两个结果对换，也就是降序 //如果返回o1.compareTo(o2)，就是当o1\u0026gt;o2时，两个结果对换，也就是升序 也就是当和参数顺序一致时，是升序；反之，则是降序 return o2.compareTo(o1); } }); //上面这段代码，标识 无序性和不可重复性\n无序性，指存储的数据，在底层数据结构中，并非按照数组索引的顺序添加（而是根据数据的哈希值决定） 不可重复性：指添加的元素按照equals()判断时，返回false。需同时重写equals()方法和hashCode() 方法 比较HashSet、LinkedHashSet和TreeSet三者异同\n都是Set实现类，保证元素唯一，且非线程安全 三者底层数据结构不同，HashSet底层为哈希表（HashMap）; LinkedHashSet底层为链表+哈希表 ，元素的插入和取出顺序满足FIFO。TreeSet底层为红黑树，元素有序，排序方式有自然排序和定制排序 Queue # Queue和Deque的区别 # Queue Queue为单端队列，只能从一端插入元素，另一端删除元素，实现上一般遵循先进先出（FIFO）规则【Dequeue为双端队列，在队列两端均可插入或删除元素】 Queue扩展了Collection接口，根据因容量问题而导致操作失败后的处理方式不同分两类，操作失败后抛异常或返回特殊值 Dequeue，双端队列，在队列两端均可插入或删除元素，也会根据失败后处理方式分两类 Deque还有push()和pop()等其他方法，可用于模拟栈 ArrayDeque与LinkedList区别 # ArrayDeque和LinkedList都实现了Deque接口，两者都具有队列功能 ArrayDeque基于可变长的数组和双指针来实现，而LinkedList则通过链表来实现 ArrayDeque不支持存储NULL数据，但LinkedList支持 ArrayDeque是后面（JDK1.6)引入的，而LinkedList在JDK1.2就存在 ArrayDeque 插入时可能存在扩容过程, 不过均摊后的插入操作依然为 O(1)。虽然 LinkedList 不需要扩容，但是每次插入数据时均需要申请新的堆空间，均摊性能相比更慢。 总的来说，ArrayDeque来实现队列要比LinkedList更好，此外，ArrayDeque也可以用于实现栈\n说一说PriorityQueue # PriorityQueue 是在 JDK1.5 中被引入的, 其与 Queue 的区别在于元素出队顺序是与优先级相关的，即总是优先级最高的元素先出队。\n这里列举其相关的一些要点：\nPriorityQueue 利用了二叉堆的数据结构来实现的，底层使用可变长的数组来存储数据 PriorityQueue 通过堆元素的上浮和下沉，实现了在 O(logn) 的时间复杂度内插入元素和删除堆顶元素。 PriorityQueue 是非线程安全的，且不支持存储 NULL 和 non-comparable 的对象。 PriorityQueue 默认是小顶堆，但可以接收一个 Comparator 作为构造参数，从而来自定义元素优先级的先后。 "},{"id":161,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0011lysyntactic_sugar/","title":"语法糖","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n简介 # 语法糖（Syntactic Sugar）也称糖衣语法，指的是在计算机语言中添加的某种语法，这种语法对语言的功能并没有影响，但是更方便程序员使用，简而言之，让程序更加简洁，有更高的可读性\nJava中有哪些语法糖 # Java虚拟机并不支持这些语法糖，这些语法糖在编译阶段就会被还原成简单的基础语法结构，这个过程就是解语法糖\njavac命令可以将后缀为.java的源文件编译为后缀名为.class的可以运行于Java虚拟机的字节码。其中，com.sun.tools.javac.main.JavaCompiler的源码中，compile()中有一个步骤就是调用desugar()，这个方法就是负责解语法糖的实现的 Java中的语法糖，包括 泛型、变长参数、条件编译、自动拆装箱、内部类等 switch支持String与枚举 # switch本身原本只支持基本类型，如int、char\nint是比较数值，而char则是比较其ascii码，所以其实对于编译器来说，都是int类型(整型)，比如byte。short，char(ackii 码是整型)以及int。 而对于enum类型，\n对于switch中使用String，则：\npublic class switchDemoString { public static void main(String[] args) { String str = \u0026#34;world\u0026#34;; switch (str) { case \u0026#34;hello\u0026#34;: System.out.println(\u0026#34;hello\u0026#34;); break; case \u0026#34;world\u0026#34;: System.out.println(\u0026#34;world\u0026#34;); break; default: break; } } } //反编译之后 public class switchDemoString { public switchDemoString() { } public static void main(String args[]) { String str = \u0026#34;world\u0026#34;; String s; switch((s = str).hashCode()) { default: break; case 99162322: if(s.equals(\u0026#34;hello\u0026#34;)) System.out.println(\u0026#34;hello\u0026#34;); break; case 113318802: if(s.equals(\u0026#34;world\u0026#34;)) System.out.println(\u0026#34;world\u0026#34;); break; } } } 即switch判断是通过**equals()和hashCode()**方法来实现的\nequals()检查是必要的，因为有可能发生碰撞，所以性能没有直接使用枚举进行switch或纯整数常量性能高\n泛型 # 编译器处理泛型有两种方式：Code specialization和Code sharing。C++和 C#是使用Code specialization的处理机制，而 Java 使用的是Code sharing的机制\nCode sharing 方式为每个泛型类型创建唯一的字节码表示，并且将该泛型类型的实例都映射到这个唯一的字节码表示上。将多种泛型类形实例映射到唯一的字节码表示是通过**类型擦除（type erasue）**实现的。\n对于 Java 虚拟机来说，他根本不认识Map\u0026lt;String, String\u0026gt; map 这样的语法。需要在编译阶段通过类型擦除的方式进行解语法糖 类型擦除的主要过程如下： 1.将所有的泛型参数用其最左边界（最顶级的父类型）类型替换。 2.移除所有的类型参数。 两个例子\nMap擦除\nMap\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;String, String\u0026gt;(); map.put(\u0026#34;name\u0026#34;, \u0026#34;hollis\u0026#34;); map.put(\u0026#34;wechat\u0026#34;, \u0026#34;Hollis\u0026#34;); map.put(\u0026#34;blog\u0026#34;, \u0026#34;www.hollischuang.com\u0026#34;); //解语法糖之后 Map map = new HashMap(); map.put(\u0026#34;name\u0026#34;, \u0026#34;hollis\u0026#34;); map.put(\u0026#34;wechat\u0026#34;, \u0026#34;Hollis\u0026#34;); map.put(\u0026#34;blog\u0026#34;, \u0026#34;www.hollischuang.com\u0026#34;); 其他擦除\npublic static \u0026lt;A extends Comparable\u0026lt;A\u0026gt;\u0026gt; A max(Collection\u0026lt;A\u0026gt; xs) { Iterator\u0026lt;A\u0026gt; xi = xs.iterator(); A w = xi.next(); while (xi.hasNext()) { A x = xi.next(); if (w.compareTo(x) \u0026lt; 0) w = x; } return w; } //擦除后变成 public static Comparable max(Collection xs){ Iterator xi = xs.iterator(); Comparable w = (Comparable)xi.next(); while(xi.hasNext()) { Comparable x = (Comparable)xi.next(); if(w.compareTo(x) \u0026lt; 0) w = x; } return w; } 小结\n虚拟机中并不存在泛型，泛型类没有自己独有的Class类对象，即不存在List\u0026lt;String\u0026gt;.class 或是 List\u0026lt;Integer\u0026gt;.class ，而只有List.class 虚拟机中，只有普通类和普通方法，所有泛型类的类型参数，在编译时都会被擦除 自动装箱与拆箱 # 装箱过程，通过调用包装器的valueOf方法实现的，而拆箱过程，则是通过调用包装器的xxxValue方法实现的\n自动装箱\npublic static void main(String[] args) { int i = 10; Integer n = i; } //反编译后的代码 public static void main(String args[]) { int i = 10; Integer n = Integer.valueOf(i); } 自动拆箱\npublic static void main(String[] args) { Integer i = 10; int n = i; } //反编译后的代码 public static void main(String args[]) { Integer i = Integer.valueOf(10); int n = i.intValue(); //注意，是intValue，不是initValue } 可变长参数 # variable arguments，是在Java 1.5中引入的一个特性，允许一个方法把任意数量的值作为参数，代码：\npublic static void main(String[] args) { print(\u0026#34;Holis\u0026#34;, \u0026#34;公众号:Hollis\u0026#34;, \u0026#34;博客：www.hollischuang.com\u0026#34;, \u0026#34;QQ：907607222\u0026#34;); } public static void print(String... strs) { for (int i = 0; i \u0026lt; strs.length; i++) { System.out.println(strs[i]); } } //反编译后代码 public static void main(String args[]) { print(new String[] { \u0026#34;Holis\u0026#34;, \u0026#34;\\u516C\\u4F17\\u53F7:Hollis\u0026#34;, \u0026#34;\\u535A\\u5BA2\\uFF1Awww.hollischuang.com\u0026#34;, \u0026#34;QQ\\uFF1A907607222\u0026#34; }); } public static transient void print(String strs[]) { for(int i = 0; i \u0026lt; strs.length; i++) System.out.println(strs[i]); } 如上，可变参数在被使用的时候，会创建一个数组，数组的长度，就是调用该方法的传递的实参的个数，然后再把参数值全部放到这个数组当中，最后把这个数组作为参数传递到被调用的方法中\n枚举 # 关键字enum可以将一组具名的值的有限集合创建为一种新的类型，而这些具名的值可以作为常规的程序组件使用，这是一种非常有用的功能\n写一个enum类进行测试\npublic enum T { SPRING,SUMMER; } //反编译之后 // Decompiled by Jad v1.5.8g. Copyright 2001 Pavel Kouznetsov. // Jad home page: http://www.kpdus.com/jad.html // Decompiler options: packimports(3) // Source File Name: T.java package com.ly.review.base; public final class T extends Enum { /** 下面这个和博客不太一样,博客里面是这样的 // ENUM$VALUES是博客编译后的数组名 public static T[] values() { T at[]; int i; T at1[]; System.arraycopy(at = ENUM$VALUES, 0, at1 = new T[i = at.length], 0, i); return at1; } */ public static T[] values() { return (T[])$VALUES.clone(); } public static T valueOf(String s) { return (T)Enum.valueOf(com/ly/review/base/T, s); } private T(String s, int i) { super(s, i); } public static final T Spring; public static final T SUMMER; private static final T $VALUES[]; static { Spring = new T(\u0026#34;Spring\u0026#34;, 0); SUMMER = new T(\u0026#34;SUMMER\u0026#34;, 1); $VALUES = (new T[] { Spring, SUMMER }); } } 重要代码：\npublic final class T extends Enum 说明该类不可继承\npublic static final T Spring; public static final T SUMMER; 说明枚举类型不可修改\n内部类 # 内部类又称为嵌套类，可以把内部类理解成外部类的一个普通成员 内部类之所以也是语法糖，是因为它仅仅是一个编译时的概念，outer.java里面定义了一个内部类inner，一旦编译成功，就会生成两个完全不同的.class文件了，分别是outer.class和outer$inner.class。所以内部类的名字完全可以和它的外部类名字相同。\n代码如下：\npublic class OutterClass { private String userName; public String getUserName() { return userName; } public void setUserName(String userName) { this.userName = userName; } public static void main(String[] args) { } class InnerClass{ private String name; public String getName() { return name; } public void setName(String name) { this.name = name; } } } 编译之后，会生成两个class文件OutterClass.class和OutterClass$InnerClass.class。所以内部类是可以跟外部类完全一样的名字的 如果要对OutterClass.class进行反编译，那么他会把OutterClass$InnerClass.class也一起进行反编译\npublic class OutterClass { class InnerClass { public String getName() { return name; } public void setName(String name) { this.name = name; } private String name; final OutterClass this$0; InnerClass() { this.this$0 = OutterClass.this; super(); } } public OutterClass() { } public String getUserName() { return userName; } public void setUserName(String userName){ this.userName = userName; } public static void main(String args1[]) { } private String userName; } 条件编译 # —般情况下，程序中的每一行代码都要参加编译。但有时候出于对程序代码优化的考虑，希望只对其中一部分内容进行编译，此时就需要在程序中加上条件，让编译器只对满足条件的代码进行编译，将不满足条件的代码舍弃，这就是条件编译。\npublic class ConditionalCompilation { public static void main(String[] args) { final boolean DEBUG = true; if(DEBUG) { System.out.println(\u0026#34;Hello, DEBUG!\u0026#34;); } final boolean ONLINE = false; if(ONLINE){ System.out.println(\u0026#34;Hello, ONLINE!\u0026#34;); } } } //反编译之后如下 public class ConditionalCompilation { public ConditionalCompilation() { } public static void main(String args[]) { boolean DEBUG = true; System.out.println(\u0026#34;Hello, DEBUG!\u0026#34;); boolean ONLINE = false; } } Java 语法的条件编译，是通过判断条件为常量的 if 语句实现的。其原理也是 Java 语言的语法糖。根据 if 判断条件的真假，编译器直接把分支为 false 的代码块消除。通过该方式实现的条件编译，必须在方法体内实现，而无法在正整个 Java 类的结构或者类的属性上进行条件编译\n断言 # Java 在执行的时候默认是不启动断言检查的（这个时候，所有的断言语句都将忽略！），如果要开启断言检查，则需要用开关-enableassertions或-ea来开启\n代码如下：\npublic class AssertTest { public static void main(String args[]) { int a = 1; int b = 1; assert a == b; System.out.println(\u0026#34;公众号：Hollis\u0026#34;); assert a != b : \u0026#34;Hollis\u0026#34;; System.out.println(\u0026#34;博客：www.hollischuang.com\u0026#34;); } } //反编译之后代码如下 public class AssertTest { public AssertTest() { } public static void main(String args[]) { int a = 1; int b = 1; if(!$assertionsDisabled \u0026amp;\u0026amp; a != b) throw new AssertionError(); System.out.println(\u0026#34;\\u516C\\u4F17\\u53F7\\uFF1AHollis\u0026#34;); if(!$assertionsDisabled \u0026amp;\u0026amp; a == b) { throw new AssertionError(\u0026#34;Hollis\u0026#34;); } else { System.out.println(\u0026#34;\\u535A\\u5BA2\\uFF1Awww.hollischuang.com\u0026#34;); return; } } static final boolean $assertionsDisabled = !com/hollis/suguar/AssertTest.desiredAssertionStatus(); } 断言的底层是if语言，如果断言为true，则什么都不做；如果断言为false，则程序抛出AssertError来打断程序执行 -enableassertions会设置$assertionsDisabled字段的值 数值字面量 # java7中，字面量允许在数字之间插入任意多个下划线，不会对字面值产生影响，可以方便阅读\n源代码：\npublic class Test { public static void main(String... args) { int i = 10_000; System.out.println(i); } } //反编译后 public class Test { public static void main(String[] args) { int i = 10000; System.out.println(i); } } for-each # 源代码：\npublic static void main(String... args) { String[] strs = {\u0026#34;Hollis\u0026#34;, \u0026#34;公众号：Hollis\u0026#34;, \u0026#34;博客：www.hollischuang.com\u0026#34;}; for (String s : strs) { System.out.println(s); } List\u0026lt;String\u0026gt; strList = ImmutableList.of(\u0026#34;Hollis\u0026#34;, \u0026#34;公众号：Hollis\u0026#34;, \u0026#34;博客：www.hollischuang.com\u0026#34;); for (String s : strList) { System.out.println(s); } } //反编译之后 public static transient void main(String args[]) { String strs[] = { \u0026#34;Hollis\u0026#34;, \u0026#34;\\u516C\\u4F17\\u53F7\\uFF1AHollis\u0026#34;, \u0026#34;\\u535A\\u5BA2\\uFF1Awww.hollischuang.com\u0026#34; }; String args1[] = strs; int i = args1.length; for(int j = 0; j \u0026lt; i; j++) { String s = args1[j]; System.out.println(s); } List strList = ImmutableList.of(\u0026#34;Hollis\u0026#34;, \u0026#34;\\u516C\\u4F17\\u53F7\\uFF1AHollis\u0026#34;, \u0026#34;\\u535A\\u5BA2\\uFF1Awww.hollischuang.com\u0026#34;); String s; for(Iterator iterator = strList.iterator(); iterator.hasNext(); System.out.println(s)) s = (String)iterator.next(); } 会改成普通的for语句循环，或者使用迭代器\ntry-with-resource # 关闭资源的方式，就是再finally块里释放，即调用close方法\n//正常使用 public static void main(String[] args) { BufferedReader br = null; try { String line; br = new BufferedReader(new FileReader(\u0026#34;d:\\\\hollischuang.xml\u0026#34;)); while ((line = br.readLine()) != null) { System.out.println(line); } } catch (IOException e) { // handle exception } finally { try { if (br != null) { br.close(); } } catch (IOException ex) { // handle exception } } } JDK7之后提供的关闭资源的方式：\npublic static void main(String... args) { try (BufferedReader br = new BufferedReader(new FileReader(\u0026#34;d:\\\\ hollischuang.xml\u0026#34;))) { String line; while ((line = br.readLine()) != null) { System.out.println(line); } } catch (IOException e) { // handle exception } } 编译后：\npublic static transient void main(String args[]) { BufferedReader br; Throwable throwable; br = new BufferedReader(new FileReader(\u0026#34;d:\\\\ hollischuang.xml\u0026#34;)); throwable = null; String line; try { while((line = br.readLine()) != null) System.out.println(line); } catch(Throwable throwable2) { throwable = throwable2; throw throwable2; } if(br != null) if(throwable != null) try { br.close(); } catch(Throwable throwable1) { throwable.addSuppressed(throwable1); } else br.close(); break MISSING_BLOCK_LABEL_113; Exception exception; exception; if(br != null) if(throwable != null) try { br.close(); } catch(Throwable throwable3) { throwable.addSuppressed(throwable3); } else br.close(); throw exception; IOException ioexception; ioexception; } } 也就是我们没有做关闭的操作，编译器都帮我们做了\nLambda表达 # 使用lambda表达式便利list\npublic static void main(String... args) { List\u0026lt;String\u0026gt; strList = ImmutableList.of(\u0026#34;Hollis\u0026#34;, \u0026#34;公众号：Hollis\u0026#34;, \u0026#34;博客：www.hollischuang.com\u0026#34;); strList.forEach( s -\u0026gt; { System.out.println(s); } ); } 反编译之后\npublic static /* varargs */ void main(String ... args) { ImmutableList strList = ImmutableList.of((Object)\u0026#34;Hollis\u0026#34;, (Object)\u0026#34;\\u516c\\u4f17\\u53f7\\uff1aHollis\u0026#34;, (Object)\u0026#34;\\u535a\\u5ba2\\uff1awww.hollischuang.com\u0026#34;); strList.forEach((Consumer\u0026lt;String\u0026gt;)LambdaMetafactory.metafactory(null, null, null, (Ljava/lang/Object;)V, lambda$main$0(java.lang.String ), (Ljava/lang/String;)V)()); } private static /* synthetic */ void lambda$main$0(String s) { System.out.println(s); } lambda表达式的实现其实是依赖了一些底层的api，在编译阶段，会把lambda表达式进行解糖，转换成调用内部api的方式\n可能遇到的坑 # 泛型 # 泛型遇到重载\npublic class GenericTypes { public static void method(List\u0026lt;String\u0026gt; list) { System.out.println(\u0026#34;invoke method(List\u0026lt;String\u0026gt; list)\u0026#34;); } public static void method(List\u0026lt;Integer\u0026gt; list) { System.out.println(\u0026#34;invoke method(List\u0026lt;Integer\u0026gt; list)\u0026#34;); } } 这种方法是编译不过去的，因为参数List\u0026lt;Integer\u0026gt; 和List\u0026lt;String\u0026gt;编译之后都被擦出了，变成了一样的原生类型List，擦除动作导致这两个方法的特征签名变得一模一样。\n泛型的类型参数不能用在 Java 异常处理的 catch 语句中。因为异常处理是由 JVM 在运行时刻来进行的。由于类型信息被擦除，JVM 是无法区分两个异常类型MyException\u0026lt;String\u0026gt;和MyException\u0026lt;Integer\u0026gt;的\n泛型类的所有静态变量是共享的\npublic class StaticTest{ public static void main(String[] args){ GT\u0026lt;Integer\u0026gt; gti = new GT\u0026lt;Integer\u0026gt;(); gti.var=1; GT\u0026lt;String\u0026gt; gts = new GT\u0026lt;String\u0026gt;(); gts.var=2; System.out.println(gti.var); } } class GT\u0026lt;T\u0026gt;{ public static int var=0; public void nothing(T x){} } 以上代码输出结果为：2！\n由于经过类型擦除，所有的泛型类实例都关联到同一份字节码上，泛型类的所有静态变量是共享的。\n自动装箱与拆箱 # 对于自动装箱，整形对象通过使用相同的缓存和重用，适用于整数值区间 [ -128，+127 ]\npublic static void main(String[] args) { Integer a = 1000; Integer b = 1000; Integer c = 100; Integer d = 100; System.out.println(\u0026#34;a == b is \u0026#34; + (a == b)); System.out.println((\u0026#34;c == d is \u0026#34; + (c == d))); } //结果 a == b is false c == d is true 增强for循环 # 遍历时不要使用list的remove方法：\nfor (Student stu : students) { if (stu.getId() == 2) students.remove(stu); } //会报ConcurrentModificationException异常，Iterator在工作的时候不允许被迭代的对象被改变，但可以使用Iterator本身的remove()来删除对象，会在删除当前对象的同时，维护索引的一致性 "},{"id":162,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0010lyjava_spi/","title":"java_spi","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n简介 # 为了实现在模块装配的时候不用再程序里面动态指明，这就需要一种服务发现机制。JavaSPI就是提供了这样的一个机制：为某个接口寻找服务实现的机制。有点类似IoC的思想，将装配的控制权交到了程序之外\nSPI介绍 # SPI，ServiceProviderInterface 使用SPI：Spring框架、数据库加载驱动、日志接口、以及Dubbo的扩展实现\n感觉下面这个图不太对，被调用方应该 一般模块之间都是通过接口进行通讯，\n当实现方提供了接口和实现，我们可以通过调用实现方的接口从而拥有实现方给我们提供的能力，这就是 API ，这种接口和实现都是放在实现方的。\n当接口存在于调用方这边时，就是 SPI ，由接口调用方确定接口规则，然后由不同的厂商去根据这个规则对这个接口进行实现，从而提供服务。[可以理解成业务方，或者说使用方。它使用了这个接口，而且制定了接口规范，但是具体实现，由被调用方实现]\n我的理解：被调用方（提供接口的人），调用方（使用接口的人），但是其实这里只把调用方\u0026ndash;\u0026gt;使用接口的人 这个关系是对的。\n也就是说，正常情况下由被调用方自己提供接口和实现，即API。而现在，由调用方（这里的调用方其实可以理解成上面的被调用方），提供了接口还使用了接口，而由被调用方进行接口实现\n实战演示 # SLF4J只是一个日志门面（接口），但是SLF4J的具体实现可以有多种，如：Logback/Log4j/Log4j2等等\n简易版本 # ServiceProviderInterface\n目录结构\n│ service-provider-interface.iml │ ├─.idea │ │ .gitignore │ │ misc.xml │ │ modules.xml │ └─ workspace.xml │ └─src └─edu └─jiangxuan └─up └─spi Logger.java LoggerService.java Main.class Logger接口，即SPI 服务提供者接口，后面的服务提供者要针对这个接口进行实现\npackage edu.jiangxuan.up.spi; public interface Logger { void info(String msg); void debug(String msg); } LoggerService类，主要是为服务使用者（调用方）提供特定功能，这个类是实现JavaSPI机制的关键所在\npackage edu.jiangxuan.up.spi; import java.util.ArrayList; import java.util.List; import java.util.ServiceLoader; public class LoggerService { private static final LoggerService SERVICE = new LoggerService(); private final Logger logger; private final List\u0026lt;Logger\u0026gt; loggerList; private LoggerService() { ServiceLoader\u0026lt;Logger\u0026gt; loader = ServiceLoader.load(Logger.class); List\u0026lt;Logger\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); for (Logger log : loader) { list.add(log); } // LoggerList 是所有 ServiceProvider loggerList = list; if (!list.isEmpty()) { // Logger 只取一个 logger = list.get(0); } else { logger = null; } } //简单单例 public static LoggerService getService() { return SERVICE; } public void info(String msg) { if (logger == null) { System.out.println(\u0026#34;info 中没有发现 Logger 服务提供者\u0026#34;); } else { logger.info(msg); } } public void debug(String msg) { if (loggerList.isEmpty()) { System.out.println(\u0026#34;debug 中没有发现 Logger 服务提供者\u0026#34;); } loggerList.forEach(log -\u0026gt; log.debug(msg)); } } Main类（服务使用者，调用方）\npackage org.spi.service; public class Main { public static void main(String[] args) { LoggerService service = LoggerService.getService(); service.info(\u0026#34;Hello SPI\u0026#34;); service.debug(\u0026#34;Hello SPI\u0026#34;); } } /** 结果 info 中没有发现 Logger 服务提供者 debug 中没有发现 Logger 服务提供者 */ 新的项目，来实现Logger接口\n项目结构\n│ service-provider.iml │ ├─.idea │ │ .gitignore │ │ misc.xml │ │ modules.xml │ └─ workspace.xml │ ├─lib │ service-provider-interface.jar | └─src ├─edu │ └─jiangxuan │ └─up │ └─spi │ └─service │ Logback.java │ └─META-INF └─services edu.jiangxuan.up.spi.Logger 首先需要有一个实现类\npackage edu.jiangxuan.up.spi.service; import edu.jiangxuan.up.spi.Logger; public class Logback implements Logger { @Override public void info(String s) { System.out.println(\u0026#34;Logback info 打印日志：\u0026#34; + s); } @Override public void debug(String s) { System.out.println(\u0026#34;Logback debug 打印日志：\u0026#34; + s); } } 将之前项目打包的jar导入项目中\n之后要src 目录下新建 META-INF/services 文件夹，然后新建文件 edu.jiangxuan.up.spi.Logger （SPI 的全类名，接口名），文件里面的内容是：edu.jiangxuan.up.spi.service.Logback （Logback 的全类名，即 SPI 的实现类的包名 + 类名）\n这是 JDK SPI 机制 ServiceLoader 约定好的标准。\nJava 中的 SPI 机制就是在每次类加载的时候会先去找到 class 相对目录下的 META-INF 文件夹下的 services 文件夹下的文件，将这个文件夹下面的所有文件先加载到内存中，然后根据这些文件的文件名和里面的文件内容找到相应接口的具体实现类，找到实现类后就可以通过反射去生成对应的对象，保存在一个 list 列表里面，所以可以通过迭代或者遍历的方式拿到对应的实例对象，生成不同的实现。\n即：文件名一定要是接口的全类名，然后里面的内容一定要是实现类的全类名，实现类可以有多个，直接换行就好了，多个实现类的时候，会一个一个的迭代加载。\n接下来同样将 service-provider 项目打包成 jar 包，这个 jar 包就是服务提供方的实现。通常我们导入 maven 的 pom 依赖就有点类似这种，只不过我们现在没有将这个 jar 包发布到 maven 公共仓库中，所以在需要使用的地方只能手动的添加到项目中 效果展示 package edu.jiangxuan.up.service; import edu.jiangxuan.up.spi.LoggerService; public class TestJavaSPI { public static void main(String[] args) { LoggerService loggerService = LoggerService.getService(); loggerService.info(\u0026#34;你好\u0026#34;); loggerService.debug(\u0026#34;测试Java SPI 机制\u0026#34;); } } 通过使用 SPI 机制，可以看出服务（LoggerService）和 服务提供者两者之间的耦合度非常低，如果说我们想要换一种实现，那么其实只需要修改 service-provider 项目中针对 Logger 接口的具体实现就可以了，只需要换一个 jar 包即可，也可以有在一个项目里面有多个实现，这不就是 SLF4J 原理吗？\nServiceLoader # JDK 官方给的注释：一种加载服务实现的工具。\n具体实现 # 自己实现 # //个人简易版\npackage edu.jiangxuan.up.service; import java.io.BufferedReader; import java.io.InputStream; import java.io.InputStreamReader; import java.lang.reflect.Constructor; import java.net.URL; import java.net.URLConnection; import java.util.ArrayList; import java.util.Enumeration; import java.util.List; public class MyServiceLoader\u0026lt;S\u0026gt; { // 对应的接口 Class 模板 private final Class\u0026lt;S\u0026gt; service; // 对应实现类的 可以有多个，用 List 进行封装 private final List\u0026lt;S\u0026gt; providers = new ArrayList\u0026lt;\u0026gt;(); // 类加载器 private final ClassLoader classLoader; // 暴露给外部使用的方法，通过调用这个方法可以开始加载自己定制的实现流程。 public static \u0026lt;S\u0026gt; MyServiceLoader\u0026lt;S\u0026gt; load(Class\u0026lt;S\u0026gt; service) { return new MyServiceLoader\u0026lt;\u0026gt;(service); } // 构造方法私有化 private MyServiceLoader(Class\u0026lt;S\u0026gt; service) { this.service = service; this.classLoader = Thread.currentThread().getContextClassLoader(); doLoad(); } // 关键方法，加载具体实现类的逻辑 private void doLoad() { try { // 读取所有 jar 包里面 META-INF/services 包下面的文件，这个文件名就是接口名，然后文件里面的内容就是具体的实现类的路径加全类名 Enumeration\u0026lt;URL\u0026gt; urls = classLoader.getResources(\u0026#34;META-INF/services/\u0026#34; + service.getName()); // 挨个遍历取到的文件 while (urls.hasMoreElements()) { // 取出当前的文件 URL url = urls.nextElement(); System.out.println(\u0026#34;File = \u0026#34; + url.getPath()); // 建立链接 URLConnection urlConnection = url.openConnection(); urlConnection.setUseCaches(false); // 获取文件输入流 InputStream inputStream = urlConnection.getInputStream(); // 从文件输入流获取缓存 BufferedReader bufferedReader = new BufferedReader(new InputStreamReader(inputStream)); // 从文件内容里面得到实现类的全类名 String className = bufferedReader.readLine(); while (className != null) { // ★★【重点】 通过反射拿到实现类的实例 Class\u0026lt;?\u0026gt; clazz = Class.forName(className, false, classLoader); // 如果声明的接口跟这个具体的实现类是属于同一类型，（可以理解为Java的一种多态，接口跟实现类、父类和子类等等这种关系。）则构造实例 if (service.isAssignableFrom(clazz)) { Constructor\u0026lt;? extends S\u0026gt; constructor = (Constructor\u0026lt;? extends S\u0026gt;) clazz.getConstructor(); S instance = constructor.newInstance(); // 把当前构造的实例对象添加到 Provider的列表里面 providers.add(instance); } // 继续读取下一行的实现类，可以有多个实现类，只需要换行就可以了。 className = bufferedReader.readLine(); } } } catch (Exception e) { System.out.println(\u0026#34;读取文件异常。。。\u0026#34;); } } // 返回spi接口对应的具体实现类列表 public List\u0026lt;S\u0026gt; getProviders() { return providers; } } 基本流程：\n通过 URL 工具类从 jar 包的 /META-INF/services 目录下面找到对应的文件， 读取这个文件的名称找到对应的 spi 接口， 通过 InputStream 流将文件里面的具体实现类的全类名读取出来， 根据获取到的全类名，先判断跟 spi 接口是否为同一类型，如果是的，那么就通过反射的机制构造对应的实例对象， 将构造出来的实例对象添加到 Providers 的列表中。 "},{"id":163,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0009lyunsafe_class/","title":"unsafe类","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\nsun.misc.Unsafe\n提供执行低级别、不安全操作的方法，如直接访问系统内存资源、自主管理内存资源等，效率快，但由于有了操作内存空间的能力，会增加指针问题风险。且这些功能的实现依赖于本地方法，Java代码中只是声明方法头，具体实现规则交给本地代码 为什么要使用本地方法 # 需要用到Java中不具备的依赖于操作系统的特性，跨平台的同时要实现对底层控制 对于其他语言已经完成的现成功能，可以使用Java调用 对时间敏感/性能要求非常高，有必要使用更为底层的语言 对于同一本地方法，不同的操作系统可能通过不同的方式来实现的\nUnsafe创建 # sun.misc.Unsafe部分源码\npublic final class Unsafe { // 单例对象 private static final Unsafe theUnsafe; ...... private Unsafe() { } //Sensitive : 敏感的 英[ˈsensətɪv] @CallerSensitive public static Unsafe getUnsafe() { Class var0 = Reflection.getCallerClass(); // 仅在引导类加载器`BootstrapClassLoader`加载时才合法 if(!VM.isSystemDomainLoader(var0.getClassLoader())) { throw new SecurityException(\u0026#34;Unsafe\u0026#34;); } else { return theUnsafe; } } } 会先判断当前类是否由Bootstrap classloader加载。即只有启动类加载器加载的类才能够调用Unsafe类中的方法\n如何使用Unsafe这个类\n利用反射获得Unsafe类中已经实例化完成的单例对象theUnsafe\nprivate static Unsafe reflectGetUnsafe() { try { Field field = Unsafe.class.getDeclaredField(\u0026#34;theUnsafe\u0026#34;); field.setAccessible(true); return (Unsafe) field.get(null); } catch (Exception e) { log.error(e.getMessage(), e); return null; } } 通过Java命令行命令-Xbootclasspath/a把调用Unsafe相关方法的类A所在jar包路径追加到默认的bootstrap路径中，使得A被引导类加载器加载\njava -Xbootclasspath/a:${path} // 其中path为调用Unsafe相关方法的类所在jar包路径 Unsafe功能 # 内存操作、内存屏障、对象操作、数据操作、CAS操作、线程调度、Class操作、系统信息\n内存操作 # 相关方法:\n//分配新的本地空间 public native long allocateMemory(long bytes); //重新调整内存空间的大小 public native long reallocateMemory(long address, long bytes); //将内存设置为指定值 public native void setMemory(Object o, long offset, long bytes, byte value); //内存拷贝 public native void copyMemory(Object srcBase, long srcOffset,Object destBase, long destOffset,long bytes); //清除内存 public native void freeMemory(long address); 测试：\npackage com.unsafe; import lombok.extern.slf4j.Slf4j; import sun.misc.Unsafe; import java.lang.reflect.Field; import java.util.concurrent.TimeUnit; @Slf4j public class UnsafeGet { private Unsafe unsafe; public UnsafeGet() { this.unsafe = UnsafeGet.reflectGetUnsafe(); ; } private static Unsafe reflectGetUnsafe() { try { Field field = Unsafe.class.getDeclaredField(\u0026#34;theUnsafe\u0026#34;); field.setAccessible(true); return (Unsafe) field.get(null); } catch (Exception e) { log.error(e.getMessage(), e); return null; } } public void example() throws InterruptedException { int size = 4; //使用allocateMemory方法申请 4 字节长度的内存空间 long addr = unsafe.allocateMemory(size); //setMemory(Object var1, long var2, long var4, byte var6) //从var1的偏移量var2处开始，每个字节都设置为var6，设置var4个字节 unsafe.setMemory(null, addr, size, (byte) 1); //找到一个新的size*2大小的内存块，并且拷贝原来addr的值过来 long addr3 = unsafe.reallocateMemory(addr, size * 2); //实际操作中这个地址可能等于addr（有概率，没找到原因，这里先假设重新分配了一块） System.out.println(\u0026#34;addr: \u0026#34; + addr); System.out.println(\u0026#34;addr3: \u0026#34; + addr3); System.out.println(\u0026#34;addr值: \u0026#34; + unsafe.getInt(addr)); System.out.println(\u0026#34;addr3值: \u0026#34; + unsafe.getLong(addr3)); try { for (int i = 0; i \u0026lt; 2; i++) { // copyMemory(Object var1, long var2, Object var4, long var5, long var7); // 从var1的偏移量var2处开始，拷贝数据到var4的偏移量var5上，每次拷贝var7个字节 //所以i = 0时，拷贝到了addr3的前4个字节；i = 1 时，拷贝到了addr3的后4个字节 unsafe.copyMemory(null, addr, null, addr3 + size * i, 4); } System.out.println(unsafe.getInt(addr)); System.out.println(unsafe.getLong(addr3)); } finally { log.info(\u0026#34;start-------\u0026#34;); unsafe.freeMemory(addr); log.info(\u0026#34;end-------\u0026#34;); unsafe.freeMemory(addr3); //实际操作中这句话没执行，不知道原因 } } public static void main(String[] args) throws InterruptedException { long l = Long.parseLong(\u0026#34;0000000100000001000000010000000100000001000000010000000100000001\u0026#34;, 2); System.out.println(l); new UnsafeGet().example(); /** 输出 72340172838076673 addr: 46927104 addr3: 680731776 addr值: 16843009 addr3值: 16843009 16843009 72340172838076673 2023-01-31 14:19:28 下午 [Thread: main] INFO:start------- */ } } 对于setMemory的解释 来源\n/** 将给定内存块中的所有字节设置为固定值(通常是0)。 内存块的地址由对象引用o和偏移地址共同决定，如果对象引用o为null，offset就是绝对地址。第三个参数就是内存块的大小，如果使用allocateMemory进行内存开辟的话，这里的值应该和allocateMemory的参数一致。 value就是设置的固定值，一般为0(这里可以参考netty的DirectByteBuffer)。一般而言，o为null 所有有个重载方法是public native void setMemory(long offset, long bytes, byte value); 等效于setMemory(null, long offset, long bytes, byte value);。 */ public native void setMemory(Object o, long offset, long bytes, byte value); 分析：\n分析一下运行结果，首先使用allocateMemory方法申请 4 字节长度的内存空间，在循环中调用setMemory方法向每个字节写入内容为byte类型的 1，当使用 Unsafe 调用getInt方法时，因为一个int型变量占 4 个字节，会一次性读取 4 个字节，组成一个int的值，对应的十进制结果为 16843009。\n对于reallocateMemory方法：\n在代码中调用reallocateMemory方法重新分配了一块 8 字节长度的内存空间，通过比较addr和addr3可以看到和之前申请的内存地址是不同的。在代码中的第二个 for 循环里，调用copyMemory方法进行了两次内存的拷贝，每次拷贝内存地址addr开始的 4 个字节，分别拷贝到以addr3和addr3+4开始的内存空间上：\n拷贝完成后，使用getLong方法一次性读取8个字节，得到long类型的值\n这种分配属于堆外内存，无法进行垃圾回收，需要我们把这些内存当作资源去手动调用freeMemory方法进行释放，否则会产生内存泄漏。通常是try-finally进行内存释放\n为什么使用堆外内存\n对垃圾回收停顿的改善，堆外内存直接受操作系统管理而不是JVM 提升程序I/O操作的性能。通常I/O通信过程中，存在堆内内存到堆外内存的数据拷贝操作，对于需要频繁进行内存间的数据拷贝且生命周期较短的暂存数据，建议都存储到堆外内存 典型应用 DirectByteBuffer，Java用于实现堆外内存的重要类，对于堆外内存的创建、使用、销毁等逻辑均由Unsafe提供的堆外内存API来实现\n//DirectByteBuffer类源 DirectByteBuffer(int cap) { // package-private super(-1, 0, cap, cap); boolean pa = VM.isDirectMemoryPageAligned(); int ps = Bits.pageSize(); long size = Math.max(1L, (long)cap + (pa ? ps : 0)); Bits.reserveMemory(size, cap); long base = 0; try { // 分配内存并返回基地址 base = unsafe.allocateMemory(size); } catch (OutOfMemoryError x) { Bits.unreserveMemory(size, cap); throw x; } // 内存初始化 unsafe.setMemory(base, size, (byte) 0); if (pa \u0026amp;\u0026amp; (base % ps != 0)) { // Round up to page boundary address = base + ps - (base \u0026amp; (ps - 1)); } else { address = base; } // 跟踪 DirectByteBuffer 对象的垃圾回收，以实现堆外内存释放 cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null; } 内存屏障 # 介绍\n编译器和 CPU 会在保证程序输出结果一致的情况下，会对代码进行重排序，从指令优化角度提升性能 后果是，导致 CPU 的高速缓存和内存中数据的不一致 内存屏障（Memory Barrier）就是通过阻止屏障两边的指令重排序从而避免编译器和硬件的不正确优化情况 Unsafe提供了三个内存屏障相关方法\n//内存屏障，禁止load操作重排序。屏障前的load操作不能被重排序到屏障后，屏障后的load操作不能被重排序到屏障前 public native void loadFence(); //内存屏障，禁止store操作重排序。屏障前的store操作不能被重排序到屏障后，屏障后的store操作不能被重排序到屏障前 public native void storeFence(); //内存屏障，禁止load、store操作重排序 public native void fullFence(); 以loadFence方法为例，会禁止读操作重排序，保证在这个屏障之前的所有读操作都已经完成，并且将缓存数据设为无效，重新从主存中进行加载 在某个线程修改Runnable中的flag\n@Getter class ChangeThread implements Runnable{ /**volatile**/ boolean flag=false; @Override public void run() { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\u0026#34;subThread change flag to:\u0026#34; + flag); flag = true; } } 在主线程的while循环中，加入内存屏障，测试是否能感知到flag的修改变化\npublic static void main(String[] args){ ChangeThread changeThread = new ChangeThread(); new Thread(changeThread).start(); while (true) { boolean flag = changeThread.isFlag(); unsafe.loadFence(); //加入读内存屏障 //假设上面这句unsafe.loadFence()去掉，那么 /* 流程：1. 这里的flag为主线程读取到的flag，且此时子线程还没有修改 2. 3秒后子线程进行了修改，由于没有内存屏障，主线程（注意，不是主 存储，是主线程）还是原来的值，值没有刷新，导致不一致 */ if (flag){ System.out.println(\u0026#34;detected flag changed\u0026#34;); break; } //这里不能有System.out.println语句，不然会导致同步 /* synchronized的规定 线程解锁前,必须把共享变量刷新到主内存 线程加锁前将清空工作内存共享变量的值,需要从主存中获取共享变量的值。 */ /** public void println(String x) { synchronized (this) { print(x); newLine(); } } */ } System.out.println(\u0026#34;main thread end\u0026#34;); } //运行结果 subThread change flag to:false detected flag changed main thread end 如果删除上面的loadFence()方法，就会出现下面的情况，主线程无法感知flag发生的变化，会一直在while中循环 典型应用 Java8新引入的锁\u0026mdash;StampedLock，乐观锁，类似于无锁的操作，完全不会阻塞写线程获取写锁，从而缓解读多写少的”饥饿“现象。由于StampedLock提供的乐观读锁不阻塞写线程获取读锁，当线程共享变量从主内存load到线程工作内存时，存在数据不一致的问题\n/** StampedLock 的 validate 方法会通过 Unsafe 的 loadFence 方法加入一个 load 内存屏障 */ public boolean validate(long stamp) { U.loadFence(); return (stamp \u0026amp; SBITS) == (state \u0026amp; SBITS); } 对象操作 # 对象属性\n//在对象的指定偏移地址获取一个对象引用 public native Object getObject(Object o, long offset); //在对象指定偏移地址写入一个对象引用 public native void putObject(Object o, long offset, Object x); 对象实例化 类：\n@Data public class A { private int b; public A(){ this.b =1; } } 对象实例化\n允许我们使用非常规的方式进行对象的实例化\npublic void objTest() throws Exception{ A a1=new A(); System.out.println(a1.getB()); A a2 = A.class.newInstance(); System.out.println(a2.getB()); A a3= (A) unsafe.allocateInstance(A.class); System.out.println(a3.getB()); } //结果 1 1 0\n\u0026gt; 打印结果分别为 1、1、0，说明通过**`allocateInstance`方法创建对象**过程中，**不会调用类的构造方法**。使用这种方式创建对象时，只用到了`Class`对象，所以说如果想要**跳过对象的初始化阶段**或者**跳过构造器的安全检查**，就可以使用这种方法。在上面的例子中，如果将 A 类的**构造函数改为`private`**类型，将无法通过构造函数和反射创建对象，但**`allocateInstance`方法仍然有效**。 - 典型应用 - 常规对象实例化方式，从本质上来说，都是通过new机制来实现对象的创建 - 非常规的实例化方式：Unsafe中提供allocateInstance方法，**仅通过Class对象**就可以创建此类的实例对象 #### 数组操作 - 介绍 ```java //下面两个方法配置使用，即可定位数组中每个元素在内存中的位置 //返回数组中第一个元素的偏移地址 public native int arrayBaseOffset(Class\u0026lt;?\u0026gt; arrayClass); //返回数组中一个元素占用的大小 public native int arrayIndexScale(Class\u0026lt;?\u0026gt; arrayClass); 典型应用\n这两个与数据操作相关的方法，在 java.util.concurrent.atomic 包下的 AtomicIntegerArray（可以实现对 Integer 数组中每个元素的原子性操作）中有典型的应用，如下图 AtomicIntegerArray 源码所示，通过 Unsafe 的 arrayBaseOffset 、arrayIndexScale 分别获取数组首元素的偏移地址 base 及单个元素大小因子 scale 。后续相关原子性操作，均依赖于这两个值进行数组中元素的定位，如下图二所示的 getAndAdd 方法即通过 checkedByteOffset 方法获取某数组元素的偏移地址，而后通过 CAS 实现原子性操作。\nCAS操作 # 相关操作\n/** * CAS * @param o 包含要修改field的对象 * @param offset 对象中某field的偏移量 * @param expected 期望值 * @param update 更新值 * @return true | false */ public final native boolean compareAndSwapObject(Object o, long offset, Object expected, Object update); public final native boolean compareAndSwapInt(Object o, long offset, int expected,int update); public final native boolean compareAndSwapLong(Object o, long offset, long expected, long update); CAS，AS 即比较并替换（Compare And Swap)，是实现并发算法时常用到的一种技术。CAS 操作包含三个操作数——内存位置、预期原值及新值。执行 CAS 操作的时候，将内存位置的值与预期原值比较，如果相匹配，那么处理器会自动将该位置值更新为新值，否则，处理器不做任何操作。我们都知道，CAS 是一条 CPU 的原子指令（cmpxchg 指令），不会造成所谓的数据不一致问题，Unsafe 提供的 CAS 方法（如 compareAndSwapXXX）底层实现即为 CPU 指令 cmpxchg\n输出\nprivate volatile int a; public static void main(String[] args){ CasTest casTest=new CasTest(); new Thread(()-\u0026gt;{ /* 一开始a=0的时候，i=1，所以a + 1；之后 a = 1的时候，i = 2 ，所以a 又加1 ；而如果是不等于的话，就会一直原子获取a的值，直到等于 i -1 */ for (int i = 1; i \u0026lt; 5; i++) { casTest.increment(i); System.out.print(casTest.a+\u0026#34; \u0026#34;); } }).start(); new Thread(()-\u0026gt;{ for (int i = 5 ; i \u0026lt;10 ; i++) { casTest.increment(i); System.out.print(casTest.a+\u0026#34; \u0026#34;); } }).start(); } private void increment(int x){ while (true){ try { long fieldOffset = unsafe.objectFieldOffset(CasTest.class.getDeclaredField(\u0026#34;a\u0026#34;)); if (unsafe.compareAndSwapInt(this,fieldOffset,x-1,x)) break; } catch (NoSuchFieldException e) { e.printStackTrace(); } } } //结果 1 2 3 4 5 6 7 8 9 使用两个线程去修改int型属性a的值，并且只有在a的值等于传入的参数x减一时，才会将a的值变为x，也就是实现对a的加一的操作 线程调度(多线程问题) # //Unsafe类提供的相关方法 //取消阻塞线程 public native void unpark(Object thread); //阻塞线程 public native void park(boolean isAbsolute, long time); //获得对象锁（可重入锁） @Deprecated public native void monitorEnter(Object o); //释放对象锁 @Deprecated public native void monitorExit(Object o); //尝试获取对象锁 @Deprecated public native boolean tryMonitorEnter(Object o); 方法 park、unpark 即可实现线程的挂起与恢复，将一个线程进行挂起是通过 park 方法实现的，调用 park 方法后，线程将一直阻塞直到超时或者中断等条件出现；unpark 可以终止一个挂起的线程，使其恢复正常。\n此外，Unsafe 源码中monitor相关的三个方法已经被标记为deprecated，不建议被使用：\n//获得对象锁 @Deprecated public native void monitorEnter(Object var1); //释放对象锁 @Deprecated public native void monitorExit(Object var1); //尝试获得对象锁 @Deprecated public native boolean tryMonitorEnter(Object var1); monitorEnter方法用于获得对象锁，monitorExit用于释放对象锁，如果对一个没有被monitorEnter加锁的对象执行此方法，会抛出IllegalMonitorStateException异常。tryMonitorEnter方法尝试获取对象锁，如果成功则返回true，反之返回false。\n典型操作\nJava 锁和同步器框架的核心类 AbstractQueuedSynchronizer (AQS)，就是通过调用**LockSupport.park()和LockSupport.unpark()实现线程的阻塞和唤醒**的，而 LockSupport 的 park 、unpark 方法实际是调用 Unsafe 的 park 、unpark 方式实现的。\npublic static void park(Object blocker) { Thread t = Thread.currentThread(); setBlocker(t, blocker); UNSAFE.park(false, 0L); setBlocker(t, null); } public static void unpark(Thread thread) { if (thread != null) UNSAFE.unpark(thread); } LockSupport 的park方法调用了 Unsafe 的park方法来阻塞当前线程，此方法将线程阻塞后就不会继续往后执行，直到有其他线程调用unpark方法唤醒当前线程。下面的例子对 Unsafe 的这两个方法进行测试：\npublic static void main(String[] args) { Thread mainThread = Thread.currentThread(); new Thread(()-\u0026gt;{ try { TimeUnit.SECONDS.sleep(5); //5s后唤醒main线程 System.out.println(\u0026#34;subThread try to unpark mainThread\u0026#34;); unsafe.unpark(mainThread); } catch (InterruptedException e) { e.printStackTrace(); } }).start(); System.out.println(\u0026#34;park main mainThread\u0026#34;); unsafe.park(false,0L); System.out.println(\u0026#34;unpark mainThread success\u0026#34;); } //输出 park main mainThread subThread try to unpark mainThread unpark mainThread success 流程图如下：\nClass操作 # Unsafe对class的相关操作主要包括类加载和静态变量的操作方法\n静态属性读取相关的方法\n//获取静态属性的偏移量 public native long staticFieldOffset(Field f); //获取静态属性的对象指针---另一说,获取静态变量所属的类在方法区的首地址 public native Object staticFieldBase(Field f); //判断类是否需要实例化（用于获取类的静态属性前进行检测） public native boolean shouldBeInitialized(Class\u0026lt;?\u0026gt; c); 测试\n@Data public class User { public static String name=\u0026#34;Hydra\u0026#34;; int age; } private void staticTest() throws Exception { User user=new User(); System.out.println(unsafe.shouldBeInitialized(User.class)); Field sexField = User.class.getDeclaredField(\u0026#34;name\u0026#34;);//获取到静态属性 long fieldOffset = unsafe.staticFieldOffset(sexField);//获取静态属性的偏移量 Object fieldBase = unsafe.staticFieldBase(sexField); //获取静态属性对应的是哪个类 Object object = unsafe.getObject(fieldBase, fieldOffset);//获取到静态属性 对象 System.out.println(object); } /** 运行结果:falseHydra */ 在 Unsafe 的对象操作中，我们学习了通过objectFieldOffset方法获取对象属性偏移量并基于它对变量的值进行存取，但是它不适用于类中的静态属性，这时候就需要使用staticFieldOffset方法。在上面的代码中，只有在获取Field对象的过程中依赖到了Class，而获取静态变量的属性时不再依赖于Class。\n在上面的代码中首先创建一个User对象，这是因为如果一个类没有被实例化，那么它的静态属性也不会被初始化，最后获取的字段属性将是null(如果直接使用User.name ，那么是会导致类被初始化的）。所以在获取静态属性前，需要调用shouldBeInitialized方法，判断在获取前是否需要初始化这个类。如果删除创建 User 对象的语句，运行结果会变为：truenull\ndefineClass方法允许程序在运行时动态创建一个类\npublic native Class\u0026lt;?\u0026gt; defineClass(String name, byte[] b, int off, int len, ClassLoader loader,ProtectionDomain protectionDomain); 利用class类字节码文件，动态创建一个类\nprivate static void defineTest() { String fileName=\u0026#34;F:\\\\workspace\\\\unsafe-test\\\\target\\\\classes\\\\com\\\\cn\\\\model\\\\User.class\u0026#34;; File file = new File(fileName); try(FileInputStream fis = new FileInputStream(file)) { byte[] content=new byte[(int)file.length()]; fis.read(content); Class clazz = unsafe.defineClass(null, content, 0, content.length, null, null); Object o = clazz.newInstance(); Object age = clazz.getMethod(\u0026#34;getAge\u0026#34;).invoke(o, null); System.out.println(age); } catch (Exception e) { e.printStackTrace(); } } 系统信息 # //获取系统相关信息 //返回系统指针的大小。返回值为4（32位系统）或 8（64位系统）。 【应该是字节数】 public native int addressSize(); //内存页的大小，此值为2的幂次方。 public native int pageSize(); "},{"id":164,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0008lybig_decimal/","title":"big_decimal","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n精度的丢失 # float a = 2.0f - 1.9f; float b = 1.8f - 1.7f; System.out.println(a);// 0.100000024 System.out.println(b);// 0.099999905 System.out.println(a == b);// false 为什么会有精度丢失的风险\n这个和计算机保存浮点数的机制有很大关系。我们知道计算机是二进制的，而且计算机在表示一个数字时，宽度是有限的，无限循环的小数存储在计算机时，只能被截断，所以就会导致小数精度发生损失的情况。这也就是解释了为什么浮点数没有办法用二进制精确表示\n使用BigDecimal来定义浮点数的值，然后再进行浮点数的运算操作即可\nBigDecimal常见方法 # 我们在使用 BigDecimal 时，为了防止精度丢失，推荐使用它的BigDecimal(String val)构造方法或者 BigDecimal.valueOf(double val) 静态方法来创建对象\n加减乘除\nBigDecimal a = new BigDecimal(\u0026#34;1.0\u0026#34;); BigDecimal b = new BigDecimal(\u0026#34;0.9\u0026#34;); System.out.println(a.add(b));// 1.9 System.out.println(a.subtract(b));// 0.1 System.out.println(a.multiply(b));// 0.90 System.out.println(a.divide(b));// 无法除尽，抛出 ArithmeticException 异常 System.out.println(a.divide(b, 2, RoundingMode.HALF_UP));// 1.11 使用divide方法的时候，尽量使用3个参数版本（roundingMode.oldMode)\n保留规则\npublic enum RoundingMode { // 2.5 -\u0026gt; 3 , 1.6 -\u0026gt; 2 // -1.6 -\u0026gt; -2 , -2.5 -\u0026gt; -3 UP(BigDecimal.ROUND_UP), //数轴上靠近哪个取哪个 // 2.5 -\u0026gt; 2 , 1.6 -\u0026gt; 1 // -1.6 -\u0026gt; -1 , -2.5 -\u0026gt; -2 DOWN(BigDecimal.ROUND_DOWN), //数轴上离哪个远取哪个 // 2.5 -\u0026gt; 3 , 1.6 -\u0026gt; 2 // -1.6 -\u0026gt; -1 , -2.5 -\u0026gt; -2 CEILING(BigDecimal.ROUND_CEILING), // 2.5 -\u0026gt; 2 , 1.6 -\u0026gt; 1 // -1.6 -\u0026gt; -2 , -2.5 -\u0026gt; -3 FLOOR(BigDecimal.ROUND_FLOOR), ////数轴上 正数：远离哪个取哪个 负数：靠近哪个取哪个 // 2.5 -\u0026gt; 3 , 1.6 -\u0026gt; 2 // -1.6 -\u0026gt; -2 , -2.5 -\u0026gt; -3 HALF_UP(BigDecimal.ROUND_HALF_UP),// 数轴上 正数：靠近哪个取哪个 负数：远离哪个取哪个 //...... } 大小比较\n使用compareTo\nBigDecimal a = new BigDecimal(\u0026#34;1.0\u0026#34;); BigDecimal b = new BigDecimal(\u0026#34;0.9\u0026#34;); System.out.println(a.compareTo(b));// 1 保留几位小数\nBigDecimal m = new BigDecimal(\u0026#34;1.255433\u0026#34;); BigDecimal n = m.setScale(3,RoundingMode.HALF_DOWN); System.out.println(n);// 1.255 使用compareTo替换equals方法，equals不止会比较直，还会比较精度 BigDecimal工具类分享 (用来操作double算术)\nimport java.math.BigDecimal; import java.math.RoundingMode; /** * 简化BigDecimal计算的小工具类 */ public class BigDecimalUtil { /** * 默认除法运算精度 */ private static final int DEF_DIV_SCALE = 10; private BigDecimalUtil() { } /** * 提供精确的加法运算。 * * @param v1 被加数 * @param v2 加数 * @return 两个参数的和 */ public static double add(double v1, double v2) { BigDecimal b1 = BigDecimal.valueOf(v1); BigDecimal b2 = BigDecimal.valueOf(v2); return b1.add(b2).doubleValue(); } /** * 提供精确的减法运算。 * * @param v1 被减数 * @param v2 减数 * @return 两个参数的差 */ public static double subtract(double v1, double v2) { BigDecimal b1 = BigDecimal.valueOf(v1); BigDecimal b2 = BigDecimal.valueOf(v2); return b1.subtract(b2).doubleValue(); } /** * 提供精确的乘法运算。 * * @param v1 被乘数 * @param v2 乘数 * @return 两个参数的积 */ public static double multiply(double v1, double v2) { BigDecimal b1 = BigDecimal.valueOf(v1); BigDecimal b2 = BigDecimal.valueOf(v2); return b1.multiply(b2).doubleValue(); } /** * 提供（相对）精确的除法运算，当发生除不尽的情况时，精确到 * 小数点以后10位，以后的数字四舍五入。 * * @param v1 被除数 * @param v2 除数 * @return 两个参数的商 */ public static double divide(double v1, double v2) { return divide(v1, v2, DEF_DIV_SCALE); } /** * 提供（相对）精确的除法运算。当发生除不尽的情况时，由scale参数指 * 定精度，以后的数字四舍五入。 * * @param v1 被除数 * @param v2 除数 * @param scale 表示表示需要精确到小数点以后几位。 * @return 两个参数的商 */ public static double divide(double v1, double v2, int scale) { if (scale \u0026lt; 0) { throw new IllegalArgumentException( \u0026#34;The scale must be a positive integer or zero\u0026#34;); } BigDecimal b1 = BigDecimal.valueOf(v1); BigDecimal b2 = BigDecimal.valueOf(v2); return b1.divide(b2, scale, RoundingMode.HALF_UP).doubleValue(); } /** * 提供精确的小数位四舍五入处理。 * * @param v 需要四舍五入的数字 * @param scale 小数点后保留几位 * @return 四舍五入后的结果 */ public static double round(double v, int scale) { if (scale \u0026lt; 0) { throw new IllegalArgumentException( \u0026#34;The scale must be a positive integer or zero\u0026#34;); } BigDecimal b = BigDecimal.valueOf(v); BigDecimal one = new BigDecimal(\u0026#34;1\u0026#34;); return b.divide(one, scale, RoundingMode.HALF_UP).doubleValue(); } /** * 提供精确的类型转换(Float) * * @param v 需要被转换的数字 * @return 返回转换结果 */ public static float convertToFloat(double v) { BigDecimal b = new BigDecimal(v); return b.floatValue(); } /** * 提供精确的类型转换(Int)不进行四舍五入 * * @param v 需要被转换的数字 * @return 返回转换结果 */ public static int convertsToInt(double v) { BigDecimal b = new BigDecimal(v); return b.intValue(); } /** * 提供精确的类型转换(Long) * * @param v 需要被转换的数字 * @return 返回转换结果 */ public static long convertsToLong(double v) { BigDecimal b = new BigDecimal(v); return b.longValue(); } /** * 返回两个数中大的一个值 * * @param v1 需要被对比的第一个数 * @param v2 需要被对比的第二个数 * @return 返回两个数中大的一个值 */ public static double returnMax(double v1, double v2) { BigDecimal b1 = new BigDecimal(v1); BigDecimal b2 = new BigDecimal(v2); return b1.max(b2).doubleValue(); } /** * 返回两个数中小的一个值 * * @param v1 需要被对比的第一个数 * @param v2 需要被对比的第二个数 * @return 返回两个数中小的一个值 */ public static double returnMin(double v1, double v2) { BigDecimal b1 = new BigDecimal(v1); BigDecimal b2 = new BigDecimal(v2); return b1.min(b2).doubleValue(); } /** * 精确对比两个数字 * * @param v1 需要被对比的第一个数 * @param v2 需要被对比的第二个数 * @return 如果两个数一样则返回0，如果第一个数比第二个数大则返回1，反之返回-1 */ public static int compareTo(double v1, double v2) { BigDecimal b1 = BigDecimal.valueOf(v1); BigDecimal b2 = BigDecimal.valueOf(v2); return b1.compareTo(b2); } } "},{"id":165,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0007lyproxy_pattern/","title":"Java代理模式","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n代理模式 # 使用代理对象来代替对真实对象的访问，就可以在不修改原目标对象的前提下提供额外的功能操作，扩展目标对象的功能，即在目标对象的某个方法执行前后可以增加一些自定义的操作\n静态代理 # 静态代理中，我们对目标对象的每个方法的增强都是手动完成的（*后面会具体演示代码*），非常不灵活（*比如接口一旦新增加方法，目标对象和代理对象都要进行修改*）且麻烦(*需要对每个目标类都单独写一个代理类*）。 实际应用场景非常非常少，日常开发几乎看不到使用静态代理的场景。\n上面我们是从实现和应用角度来说的静态代理，从 JVM 层面来说， 静态代理在编译时就将接口、实现类、代理类这些都变成了一个个实际的 class 文件。\n定义一个接口及其实现类； 创建一个代理类同样实现这个接口 将目标对象注入进代理类，然后在代理类的对应方法调用目标类中的对应方法。这样的话，我们就可以通过代理类屏蔽对目标对象的访问，并且可以在目标方法执行前后做一些自己想做的事情。 代码:\n//定义发送短信的接口 public interface SmsService { String send(String message); } //实现发送短信的接口 public class SmsServiceImpl implements SmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } //创建代理类并同样实现发送短信的接口 public class SmsProxy implements SmsService { private final SmsService smsService; public SmsProxy(SmsService smsService) { this.smsService = smsService; } @Override public String send(String message) { //调用方法之前，我们可以添加自己的操作 System.out.println(\u0026#34;before method send()\u0026#34;); smsService.send(message); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\u0026#34;after method send()\u0026#34;); return null; } } //实际使用 public class Main { public static void main(String[] args) { SmsService smsService = new SmsServiceImpl(); SmsProxy smsProxy = new SmsProxy(smsService); smsProxy.send(\u0026#34;java\u0026#34;); } } //打印结果 before method send() send message:java after method send() 动态代理 # 从JVM角度来说，动态代理是在运行时动态生成类字节码，并加载到JVM中的。 SpringAOP和RPC等框架都实现了动态代理\nJDK动态代理 # //定义并发送短信的接口 public interface SmsService { String send(String message); } public class SmsServiceImpl implements SmsService { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } //JDK动态代理类 import java.lang.reflect.InvocationHandler; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; /** * @author shuang.kou * @createTime 2020年05月11日 11:23:00 */ public class DebugInvocationHandler implements InvocationHandler { /** * 代理类中的真实对象 */ private final Object target; public DebugInvocationHandler(Object target) { this.target = target; }、 public Object invoke(Object proxy, Method method, Object[] args) throws InvocationTargetException, IllegalAccessException { //调用方法之前，我们可以添加自己的操作 System.out.println(\u0026#34;before method \u0026#34; + method.getName()); Object result = method.invoke(target, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return result; } } 当我们的动态代理对象调用原方法时，实际上调用的invoke()，然后invoke代替我们调用了被代理对象的原生方法\n//工厂类及实际使用 public class JdkProxyFactory { public static Object getProxy(Object target) { return Proxy.newProxyInstance( target.getClass().getClassLoader(), // 目标类的类加载器 target.getClass().getInterfaces(), // 代理需要实现的接口，可指定多个 new DebugInvocationHandler(target) // 代理对象对应的自定义 InvocationHandler ); } } //实际使用 SmsService smsService = (SmsService) JdkProxyFactory.getProxy(new SmsServiceImpl()); smsService.send(\u0026#34;java\u0026#34;); //输出 before method send send message:java after method send CGLIB动态代理机制 # JDK动态代理问题：只能代理实现了接口的类Spring 的AOP中，如果使用了接口，则使用JDK动态代理；否则采用CGLB\n继承\n核心是Enhancer类及MethodInterceptor接口\npublic interface MethodInterceptor extends Callback{ // 拦截被代理类中的方法 public Object intercept(Object obj, java.lang.reflect.Method method, Object[] args,MethodProxy proxy) throws Throwable; } 对象，被拦截方法，参数，调用原始方法\n实例\n//定义一个类，及方法拦截器 package github.javaguide.dynamicProxy.cglibDynamicProxy; public class AliSmsSer pvice { public String send(String message) { System.out.println(\u0026#34;send message:\u0026#34; + message); return message; } } //MethodInterceptor （方法拦截器） import net.sf.cglib.proxy.MethodInterceptor; import net.sf.cglib.proxy.MethodProxy; import java.lang.reflect.Method; /** * 自定义MethodInterceptor */ public class DebugMethodInterceptor implements MethodInterceptor { /** * @param o 代理对象（增强的对象） * @param method 被拦截的方法（需要增强的方法） * @param args 方法入参 * @param methodProxy 用于调用原始方法 */ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { //调用方法之前，我们可以添加自己的操作 System.out.println(\u0026#34;before method \u0026#34; + method.getName()); Object object = methodProxy.invokeSuper(o, args); //调用方法之后，我们同样可以添加自己的操作 System.out.println(\u0026#34;after method \u0026#34; + method.getName()); return object; } } // 获取代理类 import net.sf.cglib.proxy.Enhancer; public class CglibProxyFactory { public static Object getProxy(Class\u0026lt;?\u0026gt; clazz) { // 创建动态代理增强类 Enhancer enhancer = new Enhancer(); // 设置类加载器 enhancer.setClassLoader(clazz.getClassLoader()); // 设置被代理类 enhancer.setSuperclass(clazz); // 设置方法拦截器 enhancer.setCallback(new DebugMethodInterceptor()); // 创建代理类 return enhancer.create(); } } //实际使用 AliSmsService aliSmsService = (AliSmsService) CglibProxyFactory.getProxy(AliSmsService.class); aliSmsService.send(\u0026#34;java\u0026#34;); 对比 # 灵活性：动态代理更为灵活，且不需要实现接口，可以直接代理实现类，并且不需要针对每个对象都创建代理类；一旦添加方法，动态代理类不需要修改； JVM层面：静态代理在编译时就将接口、实现类变成实际的class文件，而动态代理是在运行时生成动态类字节码，并加载到JVM中 "},{"id":166,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0006lyreflex/","title":"java-reflex","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n何为反射 # 赋予了我们在运行时分析类以及执行类中方法的能力；运行中获取任意一个类的所有属性和方法，以及调用这些方法和属性\n应用场景 # Spring/Spring Boot 、MyBatis等框架都用了大量反射机制，以下为\nJDK动态代理\n接口及实现类\npackage proxy; public interface Car { public void run(); } //实现类 package proxy; public class CarImpl implements Car{ public void run() { System.out.println(\u0026#34;car running\u0026#34;); } } 代理类 及main方法使用 [ˌɪnvəˈkeɪʃn] 祈祷\npackage proxy; import java.lang.reflect.InvocationHandler; import java.lang.reflect.Method; //JDK动态代理代理类 public class CarHandler implements InvocationHandler{ //真实类的对象 private Object car; //构造方法赋值给真实的类 public CarHandler(Object obj){ this.car = obj; } //代理类执行方法时，调用的是这个方法 public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\u0026#34;before\u0026#34;); Object res = method.invoke(car, args); System.out.println(\u0026#34;after\u0026#34;); return res; } } //main方法使用 package proxy; import java.lang.reflect.Proxy; public class main { public static void main(String[] args) { CarImpl carImpl = new CarImpl(); CarHandler carHandler = new CarHandler(carImpl); Car proxy = (Car)Proxy.newProxyInstance( main.class.getClassLoader(), //第一个参数，获取ClassLoader carImpl.getClass().getInterfaces(), //第二个参数，获取被代理类的接口 carHandler);//第三个参数，一个InvocationHandler对象，表示的是当我这个动态代理对象在调用方法的时候，会关联到哪一个InvocationHandler对象上 proxy.run(); } } //输出 before car running after Cglib动态代理（没有实现接口的Car\n类\npackage proxy; public class CarNoInterface { public void run() { System.out.println(\u0026#34;car running\u0026#34;); } } cglib代理类 [ˌɪntəˈseptə(r)] interceptor 拦截\npackage proxy; import java.lang.reflect.Method; import org.springframework.cglib.proxy.Enhancer; import org.springframework.cglib.proxy.MethodInterceptor; import org.springframework.cglib.proxy.MethodProxy; public class CglibProxy implements MethodInterceptor{ private Object car; /** * 创建代理对象 * * @param target * @return */ public Object getInstance(Object object) { this.car = object; Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(this.car.getClass()); // 回调方法 enhancer.setCallback(this); // 创建代理对象 return enhancer.create(); } @Override public Object intercept(Object obj, Method method, Object[] args,MethodProxy proxy) throws Throwable { System.out.println(\u0026#34;事物开始\u0026#34;); proxy.invokeSuper(obj, args); System.out.println(\u0026#34;事物结束\u0026#34;); return null; } } 使用\npackage proxy; import java.lang.reflect.Proxy; public class main { public static void main(String[] args) { CglibProxy cglibProxy = new CglibProxy(); CarNoInterface carNoInterface = (CarNoInterface)cglibProxy.getInstance(new CarNoInterface()); carNoInterface.run(); } } //输出 事物开始 car running 事物结束 我们可以基于反射分析类，然后获取到类/属性/方法/方法参数上的注解，之后做进一步的处理\n反射机制的优缺点\n优点\n让代码更加灵活 确定，增加安全问题，可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译时，且性能较差） 反射实战\n获取Class对象的几种方式\nClass alunbarClass = TargetObject.class;//第一种 Class alunbarClass1 = Class.forName(\u0026#34;cn.javaguide.TargetObject\u0026#34;);//第二种 TargetObject o = new TargetObject(); Class alunbarClass2 = o.getClass(); //第三种 ClassLoader.getSystemClassLoader().loadClass(\u0026#34;cn.javaguide.TargetObject\u0026#34;); //第4种，通过类加载器获取Class对象不会进行初始化，意味着不进行包括初始化等一系列操作，静态代码块和静态对象不会得到执行 反射的基本操作 例子：\npackage cn.javaguide; public class TargetObject { private String value; public TargetObject() { value = \u0026#34;JavaGuide\u0026#34;; } public void publicMethod(String s) { System.out.println(\u0026#34;I love \u0026#34; + s); } private void privateMethod() { System.out.println(\u0026#34;value is \u0026#34; + value); } } 通过反射操作这个类的方法以及参数\npackage cn.javaguide; import java.lang.reflect.Field; import java.lang.reflect.InvocationTargetException; import java.lang.reflect.Method; public class Main { public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InstantiationException, InvocationTargetException, NoSuchFieldException { /** * 获取 TargetObject 类的 Class 对象并且创建 TargetObject 类实例 */ Class\u0026lt;?\u0026gt; targetClass = Class.forName(\u0026#34;cn.javaguide.TargetObject\u0026#34;); TargetObject targetObject = (TargetObject) targetClass.newInstance(); /** (Car)Proxy.newProxyInstance( main.class.getClassLoader(), //第一个参数，获取ClassLoader carImpl.getClass().getInterfaces(), //第二个参数，获取被代理类的接口 carHandler); **/ /** * 获取 TargetObject 类中定义的所有方法 */ Method[] methods = targetClass.getDeclaredMethods(); for (Method method : methods) { System.out.println(method.getName()); } /** * 获取指定方法并调用 */ Method publicMethod = targetClass.getDeclaredMethod(\u0026#34;publicMethod\u0026#34;, String.class); publicMethod.invoke(targetObject, \u0026#34;JavaGuide\u0026#34;); /** * 获取指定参数并对参数进行修改 */ Field field = targetClass.getDeclaredField(\u0026#34;value\u0026#34;); //为了对类中的参数进行修改我们取消安全检查 field.setAccessible(true); field.set(targetObject, \u0026#34;JavaGuide\u0026#34;); /** * 调用 private 方法 */ Method privateMethod = targetClass.getDeclaredMethod(\u0026#34;privateMethod\u0026#34;); //为了调用private方法我们取消安全检查 privateMethod.setAccessible(true); privateMethod.invoke(targetObject); } } //输出 publicMethod privateMethod I love JavaGuide value is JavaGuide\n- "},{"id":167,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0005lyserialize/","title":"Java序列化详解","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n什么是序列化？什么是反序列化 # 当需要持久化Java对象，比如将Java对象保存在文件中、或者在网络中传输Java对象，这些场景都需要用到序列化\n即：\n序列化：将数据结构/对象，转换成二进制字节流 反序列化：将在序列化过程中所生成的二进制字节流，转换成数据结构或者对象的过程 对于Java，序列化的是对象(Object)，也就是实例化后的类(Class)\n序列化的目的，是通过网络传输对象，或者说是将对象存储到文件系统、数据库、内存中，如图： 实际场景 # 对象在进行网络传输（比如远程方法调用 RPC 的时候）之前需要先被序列化，接收到序列化的对象之后需要再进行反序列化； 将对象存储到文件中的时候需要进行序列化，将对象从文件中读取出来需要进行反序列化。 将对象存储到缓存数据库（如 Redis）时需要用到序列化，将对象从缓存数据库中读取出来需要反序列化 序列化协议对于TCP/IP 4层模型的哪一层 # 4层包括，网络接口层，网络层，传输层，应用层 如下图所示：\nOSI七层协议模型中，表示层就是对应用层的用户数据，进行处理转换成二进制流；反过来的话，就是将二进制流转换成应用层的用户数据，即序列化和反序列化，\n因为，OSI 七层协议模型中的应用层、表示层和会话层对应的都是 TCP/IP 四层模型中的应用层，所以序列化协议属于 TCP/IP 协议应用层的一部分\n常见序列化协议对比 # kryo 英音 [k\u0026rsquo;rɪəʊ] ，除了JDK自带的序列化，还有hessian、kryo、protostuff\nJDK自带的序列化，只需要实现java.io.Serializable接口即可\n@AllArgsConstructor @NoArgsConstructor @Getter @Builder @ToString public class RpcRequest implements Serializable { private static final long serialVersionUID = 1905122041950251207L; private String requestId; private String interfaceName; private String methodName; private Object[] parameters; private Class\u0026lt;?\u0026gt;[] paramTypes; private RpcMessageTypeEnum rpcMessageTypeEnum; } serialVersionUID用于版本控制，会被写入二进制序列，反序列化如果发现和当前类不一致则会抛出InvalidClassException异常。一般不使用JDK自带序列化，1 不支持跨语言调用 2 性能差，序列化之后字节数组体积过大\nKryo 由于变长存储特性并使用了字节码生成机制，拥有较高的运行速度和较小字节码体积，代码：\n/** * Kryo serialization class, Kryo serialization efficiency is very high, but only compatible with Java language * * @author shuang.kou * @createTime 2020年05月13日 19:29:00 */ @Slf4j public class KryoSerializer implements Serializer { /** * Because Kryo is not thread safe. So, use ThreadLocal to store Kryo objects */ private final ThreadLocal\u0026lt;Kryo\u0026gt; kryoThreadLocal = ThreadLocal.withInitial(() -\u0026gt; { Kryo kryo = new Kryo(); kryo.register(RpcResponse.class); kryo.register(RpcRequest.class); return kryo; }); @Override public byte[] serialize(Object obj) { try (ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); Output output = new Output(byteArrayOutputStream)) { Kryo kryo = kryoThreadLocal.get(); // Object-\u0026gt;byte:将对象序列化为byte数组 kryo.writeObject(output, obj); kryoThreadLocal.remove(); return output.toBytes(); } catch (Exception e) { throw new SerializeException(\u0026#34;Serialization failed\u0026#34;); } } @Override public \u0026lt;T\u0026gt; T deserialize(byte[] bytes, Class\u0026lt;T\u0026gt; clazz) { try (ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes); Input input = new Input(byteArrayInputStream)) { Kryo kryo = kryoThreadLocal.get(); // byte-\u0026gt;Object:从byte数组中反序列化出对象 Object o = kryo.readObject(input, clazz); kryoThreadLocal.remove(); return clazz.cast(o); } catch (Exception e) { throw new SerializeException(\u0026#34;Deserialization failed\u0026#34;); } } } Protobuf 出自google\nProtoStuff，更为易用\nhessian，轻量级的自定义描述的二进制RPC协议，跨语言，hessian2，为阿里修改过的hessian lite，是dubbo RPC默认启用的序列化方式\n总结\n如果不需要跨语言可以考虑Kryo Protobuf，ProtoStuff，hessian支持跨语言 "},{"id":168,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0004lypassbyvalue/","title":"为什么Java中只有值传递","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n形参\u0026amp;\u0026amp;实参\n实参(实际参数，Arguments)，用于传递给函数/方法的参数，必须有确定的值\n形参(形式参数，Parameters)，用于定义函数/方法，接收实参，不需要有确定的值\nString hello = \u0026#34;Hello!\u0026#34;; // hello 为实参 sayHello(hello); // str 为形参 void sayHello(String str) { System.out.println(str); } 值传递\u0026amp;\u0026amp;引用传递\n程序设计将实参传递给方法的方式分为两种，值传递：方法接收实参值的拷贝，会创建副本；引用传递：方法接受的是实参所引用的对象在堆中的地址，不会创建副本，对形参的修改将影响到实参 Java中只有值传递，原因：\n传递基本类型参数\npublic static void main(String[] args) { int num1 = 10; int num2 = 20; swap(num1, num2); System.out.println(\u0026#34;num1 = \u0026#34; + num1); System.out.println(\u0026#34;num2 = \u0026#34; + num2); } public static void swap(int a, int b) { int temp = a; a = b; b = temp; System.out.println(\u0026#34;a = \u0026#34; + a); System.out.println(\u0026#34;b = \u0026#34; + b); } //输出 a = 20 b = 10 num1 = 10 num2 = 20 传递引用类型参数 1\npublic static void main(String[] args) { int[] arr = { 1, 2, 3, 4, 5 }; System.out.println(arr[0]); //1 change(arr); System.out.println(arr[0]);//0 } public static void change(int[] array) { // 将数组的第一个元素变为0 array[0] = 0; } change方法的参数，拷贝的是arr(实参)的地址，所以array和arr指向的是同一个数组对象 传递引用类型参数2\npublic class Person { private String name; // 省略构造函数、Getter\u0026amp;Setter方法 } public static void main(String[] args) { Person xiaoZhang = new Person(\u0026#34;小张\u0026#34;); Person xiaoLi = new Person(\u0026#34;小李\u0026#34;); swap(xiaoZhang, xiaoLi); System.out.println(\u0026#34;xiaoZhang:\u0026#34; + xiaoZhang.getName()); System.out.println(\u0026#34;xiaoLi:\u0026#34; + xiaoLi.getName()); } public static void swap(Person person1, Person person2) { Person temp = person1; person1 = person2; person2 = temp; System.out.println(\u0026#34;person1:\u0026#34; + person1.getName()); System.out.println(\u0026#34;person2:\u0026#34; + person2.getName()); } //结果 person1:小李 person2:小张 xiaoZhang:小张 xiaoLi:小李 这里并不会交换xiaoZhang和xiaoLi，只会交换swap方法栈里的person1和person2\n小结 Java 中将实参传递给方法（或函数）的方式是 值传递 ：\n如果参数是基本类型的话，很简单，传递的就是基本类型的字面量值的拷贝，会创建副本。 如果参数是引用类型，传递的就是实参所引用的对象在堆中地址值的拷贝，同样也会创建副本。 "},{"id":169,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0003lyjava_guide_basic_3/","title":"javaGuide基础3","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n异常 # unchecked exceptions (运行时异常)\nchecked exceptions (非运行时异常，编译异常）\nJava异常类层次结构图 Exception和Error有什么区别\n除了RuntimeException及其子类以外，其他的Exception类及其子类都属于受检查异常\nException : 程序本身可以处理的异常（可通过catch捕获）\nChecked Exception ，受检查异常，必须处理(catch 或者 throws ，否则编译器通过不了) IOException，ClassNotFoundException，SQLException，FileNotFoundException\nUnchecked Exception ， 不受检查异常 ， 可以不处理\n（算数异常，类型转换异常，不合法的线程状态异常，下标超出异常，空指针异常，参数类型异常，数字格式异常，不支持操作异常） ArithmeticException，ClassCastException，IllegalThreadStateException，IndexOutOfBoundsException\nNullPointerException，IllegalArgumentException，NumberFormatException，SecurityException，UnsupportedOperationException ```illegal 英[ɪˈliːɡl] 非法的``` ```Arithmetic 英[əˈrɪθmətɪk] 算术``` Error： 程序无法处理的错误 ，不建议通过catch 捕获，已办错误发生时JVM会选择线程终止\nOutOfMemoryError （堆，Java heap space），VirtualMachineError，StackOverFlowError，AssertionError （断言），IOError\nThrowable类常用方法\nString getMessage() //简要描述 String toString() //详细 String getLocalizedMessage() //本地化信息，如果子类(Throwable的子类)没有覆盖该方法，则与gtMessage() 结果一样 void printStackTrace() //打印Throwable对象封装的异常信息 try-catch-finally如何使用 try后面必须要有catch或者finally；无论是否捕获异常，finally都会执行；当在 try 块或 catch 块中遇到 return 语句时，finally 语句块将在方法返回之前被执行。\n不要在 finally 语句块中使用 return! 当 try 语句和 finally 语句中都有 return 语句时，try 语句块中的 return 语句会被忽略。这是因为 try 语句中的 return 返回值会先被暂存在一个本地变量中，当执行到 finally 语句中的 return 之后，这个本地变量的值就变为了 finally 语句中的 return 返回值。\npublic static void main(String[] args) { System.out.println(f(2)); }\npublic static int f(int value) { try { return value * value; } finally { if (value == 2) { return 0; } } } /*\n0 */\nfinally中的代码不一定执行（如果finally之前虚拟机就已经被终止了）\n另外两种情况，程序所在的线程死亡；关闭CPU；都会导致代码不执行 使用try-with-resources代替try-catch-finally\n适用范围：任何实现java.lang.AutoCloseable或者java.io.Closeable的对象【比如InputStream、OutputStream、Scanner、PrintWriter等需要调用close()方法的资源】\n在try-with-resources中，任何catch或finally块在声明的资源关闭后运行\n例子\n//读取文本文件的内容 Scanner scanner = null; try { scanner = new Scanner(new File(\u0026#34;D://read.txt\u0026#34;)); while (scanner.hasNext()) { System.out.println(scanner.nextLine()); } } catch (FileNotFoundException e) { e.printStackTrace(); } finally { if (scanner != null) { scanner.close(); } } 改造后：\ntry (Scanner scanner = new Scanner(new File(\u0026#34;test.txt\u0026#34;))) { while (scanner.hasNext()) { System.out.println(scanner.nextLine()); } } catch (FileNotFoundException fnfe) { fnfe.printStackTrace(); } 可以使用分隔符来分割\ntry (BufferedInputStream bin = new BufferedInputStream(new FileInputStream(new File(\u0026#34;test.txt\u0026#34;))); BufferedOutputStream bout = new BufferedOutputStream(new FileOutputStream(new File(\u0026#34;out.txt\u0026#34;)))) { int b; while ((b = bin.read()) != -1) { bout.write(b); } } catch (IOException e) { e.printStackTrace(); } 需要注意的地方\n不要把异常定义为静态变量，因为这样会导致异常栈信息错乱。每次手动抛出异常，我们都需要手动 new 一个异常对象抛出。 抛出的异常信息一定要有意义。 建议抛出更加具体的异常比如字符串转换为数字格式错误的时候应该抛出NumberFormatException而不是其父类IllegalArgumentException。 使用日志打印异常之后就不要再抛出异常了（两者不要同时存在一段代码逻辑中）。 泛型 # 什么是泛型？有什么作用 Java泛型（Generics）JDK5中引入的一个新特性，使用泛型参数，可以增强代码的可读性以及稳定性\n编译器可以对泛型参数进行检测，并通过泛型参数可以指定传入的对象类型，比如ArrayList\u0026lt;Person\u0026gt; persons=new ArrayList\u0026lt;Person\u0026gt;()这行代码指明该ArrayList对象只能传入Person对象，若传入其他类型的对象则会报错\n原生List返回类型为Object，需要手动转换类型才能使用，使用泛型后编译器自动转换 泛型使用方式\n泛型类\n//此处T可以随便写为任意标识，常见的如T、E、K、V等形式的参数常用于表示泛型 //在实例化泛型类时，必须指定T的具体类型 public class Generic\u0026lt;T\u0026gt;{ private T key; public Generic(T key) { this.key = key; } public T getKey(){ return key; } } // 使用 Generic\u0026lt;Integer\u0026gt; genericInteger = new Generic\u0026lt;Integer\u0026gt;(123456); 泛型接口\npublic interface Generator\u0026lt;T\u0026gt; { public T method(); } 不指定类型使用\nclass GeneratorImpl\u0026lt;T\u0026gt; implements Generator\u0026lt;T\u0026gt;{ @Override public T method() { return null; } } 指定类型使用\nclass GeneratorImpl\u0026lt;T\u0026gt; implements Generator\u0026lt;String\u0026gt;{ @Override public String method() { return \u0026#34;hello\u0026#34;; } } 泛型方法\npublic static \u0026lt; E \u0026gt; void printArray( E[] inputArray ) { for ( E element : inputArray ){ System.out.printf( \u0026#34;%s \u0026#34;, element ); } System.out.println(); } //使用 // 创建不同类型数组： Integer, Double 和 Character Integer[] intArray = { 1, 2, 3 }; String[] stringArray = { \u0026#34;Hello\u0026#34;, \u0026#34;World\u0026#34; }; printArray( intArray ); printArray( stringArray ); 上面称为静态方法，Java中泛型只是一个占位符，必须在传递类型后才能使用，类在实例化时才能传递类型参数，而类型方法的加载优先于类的实例化，静态泛型方法是**没有办法使用类上声明的泛型（即上面的第二点中类名旁边的T）**的，只能使用自己声明的\u0026lt;E\u0026gt;\n也可以是非静态的\nclass A{ private String name; private int age; public \u0026lt;E\u0026gt; int geA(E e){ System.out.println(e.toString()); return 1; } } //使用,其中 \u0026lt;Object\u0026gt; 可以省略 a.\u0026lt;Object\u0026gt;geA(new Object()); 反射 # 反射赋予了我们在运行时分析类以及执行类中方法的能力，通过反射可以获取任意一个类的所有属性和方法\n反射增加（导致）了安全问题，可以无视泛型参数的安全检查（泛型参数的安全检查发生在编译期），不过其对于框架来说实际是影响不大的\n应用场景\n一般用于框架中，框架中大量使用了动态代理，而动态代理的实现也依赖于反射\n//JDK动态代理 interface ILy { String say(String word); } class LyImpl implements ILy{ @Override public String say(String word) { return \u0026#34;hello ,\u0026#34;+word; } } @Slf4j class MyInvocationHandler implements InvocationHandler { private final Object target; public MyInvocationHandler(Object target) { this.target = target; } @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { log.info(\u0026#34;调用前\u0026#34;); Object result = method.invoke(target, args); log.info(\u0026#34;结果是:\u0026#34;+result); log.info(\u0026#34;调用后\u0026#34;); return result; } } public class Test { String a; public static void main(String[] args) { LyImpl target = new LyImpl(); ILy targetProxy = (ILy)Proxy.newProxyInstance(Test.class.getClassLoader(), target.getClass().getInterfaces(), new MyInvocationHandler(target)); targetProxy.say(\u0026#34;dxs\u0026#34;); } } //cglib动态代理 @Slf4j class MyCglibProxyInterceptor implements MethodInterceptor{ @Override public Object intercept(Object o, Method method, Object[] args, MethodProxy methodProxy) throws Throwable { log.info(\u0026#34;调用前\u0026#34;); //注意，这里是invokeSuper，如果是invoke就会调用自己，导致死循环(递归) Object result = methodProxy.invokeSuper(o, args); //上面这个写法有问题，应该是 //Object result = method.invoke(o, args); log.info(\u0026#34;调用结果\u0026#34;+result); log.info(\u0026#34;调用后\u0026#34;); return result; } } public class Test { String a; public static void main(String[] args) { Enhancer enhancer=new Enhancer(); enhancer.setClassLoader(Test.class.getClassLoader()); enhancer.setSuperclass(LyImpl.class); enhancer.setCallback(new MyCglibProxyInterceptor()); //方法一(通过) ILy o = (ILy)enhancer.create(); //方法二(通过) //LyImpl o = (LyImpl)enhancer.create(); o.say(\u0026#34;lyly\u0026#34;); } } 注解也使用到了反射，比如Spring上的@Component注解。 可以基于反射分析类，然后获取到类/属性/方法/方法的参数上的注解，获取注解后，做进一步的处理\n注解 # 注解，Java5引入，用于修饰类、方法或者变量，提供某些信息供程序在编译或者运行时使用\n@Target(ElementType.METHOD) @Retention(RetentionPolicy.SOURCE) public @interface Override { } //注解本质上是一个继承了Annotation的特殊接口 public interface Override extends Annotation{ } 注解只有被解析后才会生效\n编译期直接扫描 ：编译器在编译 Java 代码的时候扫描对应的注解并处理，比如某个方法使用@Override 注解，编译器在编译的时候就会检测当前的方法是否重写了父类对应的方法。 运行期通过反射处理 ：像框架中自带的注解(比如 Spring 框架的 @Value 、@Component)都是通过反射来进行处理的。(创建类的时候使用反射分析类，获取注解，对创建的对象进一步处理) SPI # 介绍 Service Provider Interface ，服务提供者的接口 ， 专门提供给服务提供者或者扩展框架功能的开发者去使用的一个接口 SPI 将服务接口和具体的服务实现分离开来，将服务调用方和服务实现者解耦，能够提升程序的扩展性、可维护性。修改或者替换服务实现并不需要修改调用方。 很多框架都使用了 Java 的 SPI 机制，比如：Spring 框架、数据库加载驱动、日志接口、以及 Dubbo 的扩展实现等等。 SPI扩展实现 API和SPI区别 模块之间通过接口进行通讯，在服务调用方和服务实现方(服务提供者)之间引入一个“接口” 当接口和实现，都是放在实现方的时候，这就是API\n当接口存在于调用方，由接口调用方确定接口规则，然后由不同的厂商去根据这个规则对这个接口进行实现，从而提供服务，即SPI\n举个通俗易懂的例子：公司 H 是一家科技公司，新设计了一款芯片，然后现在需要量产了，而市面上有好几家芯片制造业公司，这个时候，只要 H 公司指定好了这芯片生产的标准（定义好了接口标准），那么这些合作的芯片公司（服务提供者）就按照标准交付自家特色的芯片（提供不同方案的实现，但是给出来的结果是一样的）\n通过 SPI 机制提供了接口设计的灵活性，缺点： 需要遍历加载所有的实现类，不能做到按需加载，效率较低 当多个ServiceLoader同时load时，会有并发问题 I/O # 序列化和反序列化\n序列化：将数据结构或对象换成二级制字节流的过程 反序列化：将在序列化过程中所生成的二进制字节流转换成数据结构或者对象的过程 对于Java，序列化的都是对象（Object），即实例化后的类（Class） 维基\n序列化（serialization）在计算机科学的数据处理中，是指将数据结构或对象状态转换成可取用格式（例如存成文件，存于缓冲，或经由网络中发送），以留待后续在相同或另一台计算机环境中，能恢复原先状态的过程。依照序列化格式重新获取字节的结果时，可以利用它来产生与原始对象相同语义的副本。对于许多对象，像是使用大量引用的复杂对象，这种序列化重建的过程并不容易。面向对象中的对象序列化，并不概括之前原始对象所关系的函数。这种过程也称为对象编组（marshalling）。从一系列字节提取数据结构的反向操作，是反序列化（也称为解编组、deserialization、unmarshalling）。\n序列化的目的，通过网络传输对象，或者说是将对象存储到文件系统、数据库、内存中 被transient修饰的变量，不进行序列化：即当对象被反序列化时，被transient修饰的变量值不会被持久化和恢复 transient 英[ˈtrænziənt]\ntransient 只能修饰变量，不能修饰类和方法。 transient 修饰的变量，在反序列化后变量值将会被置成类型的默认值。例如，如果是修饰 int 类型，那么反序列后结果就是 0。 static 变量因为不属于任何对象(Object)，所以无论有没有 transient 关键字修饰，均不会被序列化。 Java IO流\nIO 即 Input/Output，输入和输出。数据输入到计算机内存的过程即输入，反之输出到外部存储（比如数据库，文件，远程主机）的过程即输出。数据传输过程类似于水流，因此称为 IO 流。IO 流在 Java 中分为输入流和输出流，而根据数据的处理方式又分为字节流和字符流。\nJavaIO流的类都是从如下4个抽象类基类中派生出来的\nInputStream/Reader: 所有的输入流的基类，前者是字节输入流，后者是字符输入流。 OutputStream/Writer: 所有输出流的基类，前者是字节输出流，后者是字符输出流。 不管是文件读写还是网络发送接收，信息的最小存储单元都是字节，那为什么I/O流操作要分为字节流操作和字符流操作\n字符流由Java虚拟机将字节转换得到，过程较为耗时 如果不知道编码类型的过，使用字节流的过程中很容易出现乱码 语法糖 # syntactic 英[sɪnˈtæktɪk] 句法的\n指的是为了方便程序员开发程序而设计的一种特殊语法，对编程语言的功能并没有影响，语法糖写出来的代码往往更简单简洁且容易阅读，比如for-each，原理：基于普通的for循环和迭代器\nString[] strs = {\u0026#34;JavaGuide\u0026#34;, \u0026#34;公众号：JavaGuide\u0026#34;, \u0026#34;博客：https://javaguide.cn/\u0026#34;}; for (String s : strs) { System.out.println(s); } JVM 其实并不能识别语法糖，Java 语法糖要想被正确执行，需要先通过编译器进行解糖，也就是在程序编译阶段将其转换成 JVM 认识的基本语法。这也侧面说明，Java 中真正支持语法糖的是 Java 编译器而不是 JVM。如果你去看com.sun.tools.javac.main.JavaCompiler的源码，你会发现在compile()中有一个步骤就是调用desugar()，这个方法就是负责解语法糖的实现的。\nJava中常见的语法糖：\n泛型、自动拆装箱、变长参数、枚举、内部类、增强 for 循环、try-with-resources 语法、lambda 表达式等\n"},{"id":170,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0002lyjava_guide_basic_2/","title":"javaGuide基础2","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n面向对象基础 # 区别\n面向过程把解决问题的过程拆成一个个方法，通过一个个方法的执行解决问题。 面向对象会先抽象出对象，然后用对象执行方法的方式解决问题。 面向对象编程 易维护、易复用、易扩展 对象实例与对象引用的不同\nnew 运算符，new 创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。\n一个对象引用可以指向 0 个或 1 个对象（一根绳子可以不系气球，也可以系一个气球）;一个对象可以有 n 个引用指向它（可以用 n 条绳子系住一个气球）。\n对象的相等一般比较的是内存中存放的内容是否相等；引用相等一般比较的是他们指向的内存地址是否相等\n如果一个类没有声明构造方法，该程序能正确执行吗? 如果我们自己添加了类的构造方法（无论是否有参），Java 就不会再添加默认的无参数的构造方法了\n构造方法特点：名字与类名相同；没有返回值但不能用void声明构造函数；生成类的对象时自动执行 构造方法不能重写(override)，但能重载 (overload) 面向对象三大特征\n封装\n把一个对象的状态信息(属性)隐藏在对象内部，不允许直接访问，但提供可以被外界访问的方法来操作属性\npublic class Student { private int id;//id属性私有化 private String name;//name属性私有化 //获取id的方法 public int getId() { return id; } //设置id的方法 public void setId(int id) { this.id = id; } //获取name的方法 public String getName() { return name; } //设置name的方法 public void setName(String name) { this.name = name; } } 继承\n不通类型的对象，相互之间有一定数量的共同点，同时每个对象定义了额外的特性使得他们与众不同。继承是使用已存在的类的定义作为基础建立新类的技术\n父类中的私有属性和方法子类无法访问，只是拥有 子类可以拥有自己的属性、方法，即对父类进行拓展 子类可以用自己的方式实现父类的方法（重写） 多态\n对象类型和引用类型之间具有继承(类)/实现(接口)的关系 引用类型变量发出的方法具体调用哪个类的方法，只有程序运行期间才能确定 多态不能调用“只在子类存在而父类不存在”的方法 如果子类重写了父类的方法，真正执行的是子类覆盖的方法，如果子类没有覆盖父类的方法，执行的是父类的方法 接口和抽象类有什么共同点和区别\n共同：都不能被实例化；都可以包含抽象方法；都可以有默认实现的方法。 区别 接口主要用于对类的行为进行约束；抽象类主要用于代码复用（强调所属） 类只能继承一个类，但能实现多个接口 接口中的成员只能是public static final不能被修改且具有初始值；而抽象类中的成员变量默认为default,也可以被public,protected,private修饰,可以不用赋初值 关于访问权限控制\npublic：Java 访问限制最宽的修饰符，一般称之为“公共的”。被其修饰的类、属性以及方法不仅可以跨类访问，而且可以跨包访问。 protected：介于 public 和 private 之间的一种访问修饰符，一般称之为“保护访问权限”。被其修饰的属性以及方法只能被类本身及其子类(即使子类在不同的包中)，以及同包的其他类访问。外包的非子类不可以访问。 default：“默认访问权限“或“包访问权限”，即不加任何访问修饰符。只允许在同包访问，外包的所有类都不能访问。接口例外 private：Java 访问限制最窄的修饰符，一般称之为“私有的”。被其修饰的属性以及方法只能被该类的对象访问，其子类不能访问，更不允许跨包访问。 深拷贝和浅拷贝的区别？什么是引用拷贝\n浅拷贝：浅拷贝会在堆上创建新对象，但是如果原对象内部的属性是引用类型的话，浅拷贝会复制内部对象的引用地址，即拷贝对象和原对象共用一个内部对象\n深拷贝，会完全复制整个对象，包括对象内包含的内部对象\n例子\n浅拷贝\npublic class Address implements Cloneable{ private String name; // 省略构造函数、Getter\u0026amp;Setter方法 @Override public Address clone() { try { return (Address) super.clone(); } catch (CloneNotSupportedException e) { throw new AssertionError(); } } } public class Person implements Cloneable { private Address address; // 省略构造函数、Getter\u0026amp;Setter方法 @Override public Person clone() { try { Person person = (Person) super.clone(); return person; } catch (CloneNotSupportedException e) { throw new AssertionError(); } } } //------------------测试-------------------- Person person1 = new Person(new Address(\u0026#34;武汉\u0026#34;)); Person person1Copy = person1.clone(); // true System.out.println(person1.getAddress() == person1Copy.getAddress()); 深拷贝\n//修改了Person类的clone()方法进行修改 @Override public Person clone() { try { Person person = (Person) super.clone(); person.setAddress(person.getAddress().clone()); return person; } catch (CloneNotSupportedException e) { throw new AssertionError(); } } //--------------测试------- Person person1 = new Person(new Address(\u0026#34;武汉\u0026#34;)); Person person1Copy = person1.clone(); // false System.out.println(person1.getAddress() == person1Copy.getAddress()); 引用拷贝，即两个不同的引用指向同一个对象\n如图\nJava常见类 # Object # 常见方法\n/** * native 方法，用于返回当前运行时对象的 Class 对象，使用了 final 关键字修饰，故不允许子类重写。 */ public final native Class\u0026lt;?\u0026gt; getClass() /** * native 方法，用于返回对象的哈希码，主要使用在哈希表中，比如 JDK 中的HashMap。 */ public native int hashCode() /** * 用于比较 2 个对象的内存地址是否相等，String 类对该方法进行了重写以用于比较字符串的值是否相等。 */ public boolean equals(Object obj) /** * naitive 方法，用于创建并返回当前对象的一份拷贝。 */ protected native Object clone() throws CloneNotSupportedException /** * 返回类的名字实例的哈希码的 16 进制的字符串。建议 Object 所有的子类都重写这个方法。 */ public String toString() /** * native 方法，并且不能重写。唤醒一个在此对象监视器上等待的线程(监视器相当于就是锁的概念)。如果有多个线程在等待只会任意唤醒一个。 */ public final native void notify() /** * native 方法，并且不能重写。跟 notify 一样，唯一的区别就是会唤醒在此对象监视器上等待的所有线程，而不是一个线程。 */ public final native void notifyAll() /** * native方法，并且不能重写。暂停线程的执行。注意：sleep 方法没有释放锁，而 wait 方法释放了锁 ，timeout 是等待时间。 */ public final native void wait(long timeout) throws InterruptedException /** * 多了 nanos 参数，这个参数表示额外时间（以毫微秒为单位，范围是 0-999999）。 所以超时的时间还需要加上 nanos 毫秒。。 */ public final void wait(long timeout, int nanos) throws InterruptedException /** * 跟之前的2个wait方法一样，只不过该方法一直等待，没有超时时间这个概念 */ public final void wait() throws InterruptedException /** * 实例被垃圾回收器回收的时候触发的操作 */ protected void finalize() throws Throwable { } == 和 equals() 区别\n对于基本类型来说，== 比较的是值 对于引用类型，== 比较的是对象的内存地址 Java是值传递，所以本质上比较的都是值，只是引用类型变量存的值是对象地址 equals不能用于判断基本数据类型的变量，且只存在于Object类中，而Object类是所有类的直接或间接父类\nequals默认实现：\npublic boolean equals(Object obj) { return (this == obj); } 如果类没有重写该方法，则如上\n如果重写了，则一般都是重写equals方法来比较对象中的属性是否相等\n关于String 和 new String 的区别： String a = \u0026ldquo;xxx\u0026rdquo; 始终返回的是常量池中的引用；而new String 始终返回的是堆中的引用\n对于String a = \u0026ldquo;xxx\u0026rdquo; ，先到常量池中查找是否存在值为\u0026quot;xxx\u0026quot;的字符串，如果存在，直接将常量池中该值对应的引用返回，如果不存在，则在常量池中创建该对象，并返回引用。\n对于new String(\u0026ldquo;xxx\u0026rdquo;)，先到常量池中查找是否存在值为\u0026quot;xxx\u0026quot;的字符串，如果存在，则直接在堆中创建对象，并返回堆中的索引；如果不存在，则先在常量池中创建对象(值为xxx)，然后再在堆中创建对象，并返回堆中该对象的引用地址\n来自 https://blog.csdn.net/weixin_44844089/article/details/103648448\n例子：\nString a = new String(\u0026#34;ab\u0026#34;); // a 为一个引用 String b = new String(\u0026#34;ab\u0026#34;); // b为另一个引用,对象的内容一样 String aa = \u0026#34;ab\u0026#34;; // 放在常量池中 String bb = \u0026#34;ab\u0026#34;; // 从常量池中查找 System.out.println(aa == bb);// true System.out.println(a == b);// false System.out.println(a.equals(b));// true System.out.println(42 == 42.0);// true String 类重写了equals()方法\npublic boolean equals(Object anObject) { if (this == anObject) { return true; } if (anObject instanceof String) { String anotherString = (String)anObject; int n = value.length; if (n == anotherString.value.length) { char v1[] = value; char v2[] = anotherString.value; int i = 0; while (n-- != 0) { if (v1[i] != v2[i]) return false; i++; } return true; } } return false; } hashCode()有什么用\nhashCode()的作用是获取哈希码(int整数)，也称为散列码，作用是确定该对象在哈希表中的索引位置。函数定义在Object类中，且为本地方法，通常用来将对象的内存地址转换为整数之后返回；散列表存储的是键值对(key-value)，根据“键”快速检索出“值”，其中利用了散列码\n为什么需要hashCode\n当你把对象加入 HashSet 时，HashSet 会先计算对象的 hashCode 值来判断对象加入的位置，同时也会与其他已经加入的对象的 hashCode 值作比较，如果没有相符的 hashCode，HashSet 会假设对象没有重复出现。但是如果发现有相同 hashCode 值的对象，这时会调用 equals() 方法来检查 hashCode 相等的对象是否真的相同。如果两者相同，HashSet 就不会让其加入操作成功。如果不同的话，就会重新散列到其他位置【注意，我觉得这里应该是使用拉链法，说成散列到其他位置貌似有点不对】。这样我们就大大减少了 equals 的次数，相应就大大提高了执行速度。\nhashCode()和equals()都用于比较两个对象是否相等，为什么要同时提供两个方法（因为在一些容器中，如HashMap、HashSet中，判断元素是否在容器中效率更高)\n两个对象的hashCode值相等并不代表两个对象就相等 因为hashCode所使用的哈希算法也许会让多个对象传回相同哈希值，取决于哈希算法 总结\n如果两个对象的hashCode 值相等，那这两个对象不一定相等（哈希碰撞）。 如果两个对象的hashCode 值相等并且equals()方法也返回 true，我们才认为这两个对象相等。 如果两个对象的**hashCode 值不相等**，我们就可以直接认为这两个对象不相等。 String # String、StringBuffer，StringBuilder区别 String是不可变的，StringBuffer和StringBuilder都继承自AbstractStringBuilder类，是可变的（提供了修改字符串的方法）\nString中的变量不可变，所以是线程安全的，而StringBuffer对方法加了同步锁，所以是线程安全的；而StringBuilder是线程不安全的\n三者使用建议\n操作少量的数据: 适用 String 单线程操作字符串缓冲区下操作大量数据: 适用 StringBuilder 多线程操作字符串缓冲区下操作大量数据: 适用 StringBuffer String 为什么是不可变的\n代码\npublic final class String implements java.io.Serializable, Comparable\u0026lt;String\u0026gt;, CharSequence { private final char value[]; //... } 如上，保存字符串的数组被final修饰且为私有，并且String类没有提供暴露修改该字符串的方法 String类被修饰为final修饰导致不能被继承，避免子类破坏 Java9\npublic final class String implements java.io.Serializable,Comparable\u0026lt;String\u0026gt;, CharSequence { // @Stable 注解表示变量最多被修改一次，称为“稳定的”。 @Stable private final byte[] value; } abstract class AbstractStringBuilder implements Appendable, CharSequence { byte[] value; } Java9为何String底层实现由char[] 改成了 byte[] 新版的 String 其实支持两个编码方案： Latin-1 和 UTF-16。如果字符串中包含的汉字没有超过 Latin-1 可表示范围内的字符，那就会使用 Latin-1 作为编码方案。Latin-1 编码方案下，byte 占一个字节(8 位)，char 占用 2 个字节（16），byte 相较 char 节省一半的内存空间。\nJDK 官方就说了绝大部分字符串对象只包含 Latin-1 可表示的字符。 [ˈlætɪn] 字符串使用“+” 还是 Stringbuilder Java本身不支持运算符重载，但 “ + ” 和 “+=” 是专门为String重载过的运算符，Java中仅有的两个\nString str1 = \u0026#34;he\u0026#34;; String str2 = \u0026#34;llo\u0026#34;; String str3 = \u0026#34;world\u0026#34;; String str4 = str1 + str2 + str3; 对应的字节码：\n字符串对象通过“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象。因此这里就会产生问题，如下代码，会产生过多的StringBuilder对象\nString[] arr = {\u0026#34;he\u0026#34;, \u0026#34;llo\u0026#34;, \u0026#34;world\u0026#34;}; String s = \u0026#34;\u0026#34;; for (int i = 0; i \u0026lt; arr.length; i++) { s += arr[i]; } System.out.println(s); 会循环创建StringBuilder对象，建议自己创建一个新的StringBuilder并使用：\nString[] arr = {\u0026#34;he\u0026#34;, \u0026#34;llo\u0026#34;, \u0026#34;world\u0026#34;}; StringBuilder s = new StringBuilder(); for (String value : arr) { s.append(value); } System.out.println(s); String#equals()和Object#equals()有何区别 String的equals被重写过，比较的是字符串的值是否相等，而Object的equals比较的是对象的内存地址\n字符串常量池\n是JVM为了提升性能和减少内存消耗针对字符串（String类）专门开辟的一块区域，主要目的是为了避免字符串的重复创建\n// 在堆中创建字符串对象”ab“ (这里也可以说是在常量池中创建对象) // 将字符串对象”ab“的引用(常量池中的饮用)保存在字符串常量池中 String aa = \u0026#34;ab\u0026#34;; // 直接返回字符串常量池中字符串对象”ab“的引用 String bb = \u0026#34;ab\u0026#34;; System.out.println(aa==bb);// true String s1 = new String(\u0026ldquo;abc\u0026rdquo;);这句话创建了几个字符串对象？ # 会创建 1 或 2 个字符串对象。 如果常量池中存在值为\u0026quot;abc\u0026quot;的对象，则直接在堆中创建一个对象，并且返回该对象的引用；如果不存在，则先在常量池中创建该对象，然后再在堆中创建该对象，并且返回该对象（堆中）的引用\n下面这个解释，说明常量池存储的是引用（堆中某一块区域的）\n// 字符串常量池中已存在字符串对象“abc”的引用 String s1 = \u0026#34;abc\u0026#34;; // 下面这段代码只会在堆中创建 1 个字符串对象“abc” String s2 = new String(\u0026#34;abc\u0026#34;); intern方法的作用，是一个native方法，作用是将指定的字符串对象的引用保存在字符串常量池中\n// 在堆中创建字符串对象”Java“ // 将字符串对象”Java“的引用保存在字符串常量池中 String s1 = \u0026#34;Java\u0026#34;; // 直接返回字符串常量池中字符串对象”Java“对应的引用 String s2 = s1.intern(); // 会在堆中在单独创建一个字符串对象 String s3 = new String(\u0026#34;Java\u0026#34;); // 直接返回字符串常量池中字符串对象”Java“对应的引用 String s4 = s3.intern(); // s1 和 s2 指向的是堆中的同一个对象 System.out.println(s1 == s2); // true // s3 和 s4 指向的是堆中不同的对象 System.out.println(s3 == s4); // false // s1 和 s4 指向的是堆中的同一个对象 System.out.println(s1 == s4); //true 问题：String 类型的变量和常量做“+”运算时发生了什么\nString str1 = \u0026#34;str\u0026#34;; String str2 = \u0026#34;ing\u0026#34;; String str3 = \u0026#34;str\u0026#34; + \u0026#34;ing\u0026#34;; String str4 = str1 + str2; String str5 = \u0026#34;string\u0026#34;; System.out.println(str3 == str4);//false System.out.println(str3 == str5);//true System.out.println(str4 == str5);//false 常量折叠\n对于 String str3 = \u0026quot;str\u0026quot; + \u0026quot;ing\u0026quot;; 编译器会给你优化成 String str3 = \u0026quot;string\u0026quot;; 。\n并不是所有的常量都会进行折叠，只有编译器在程序编译期就可以确定值的常量才可以：\n基本数据类型( byte、boolean、short、char、int、float、long、double)以及字符串常量。 final 修饰的基本数据类型和字符串变量 字符串通过 “+”拼接得到的字符串、基本数据类型之间算数运算（加减乘除）、基本数据类型的位运算（\u0026laquo;、\u0026raquo;、\u0026raquo;\u0026gt; ） 引用的值在程序编译期间是无法确认的，无法对其优化\n对象引用和“+”的字符串拼接方式，实际上是通过 StringBuilder 调用 append() 方法实现的，拼接完成之后调用 toString() 得到一个 String 对象 。 如上面代码String str4 = str1 + str2; 但是如果使用了final关键字声明之后，就可以让编译器当作常量来处理\nfinal String str1 = \u0026#34;str\u0026#34;; final String str2 = \u0026#34;ing\u0026#34;; // 下面两个表达式其实是等价的 String c = \u0026#34;str\u0026#34; + \u0026#34;ing\u0026#34;;// 常量池中的对象 String d = str1 + str2; // 常量池中的对象 System.out.println(c == d);// true 但是如果编译器在运行时才能知道其确切值的话，就无法对其优化\nfinal String str1 = \u0026#34;str\u0026#34;; final String str2 = getStr(); //str2只有在运行时才能确定其值 String c = \u0026#34;str\u0026#34; + \u0026#34;ing\u0026#34;;// 常量池中的对象 String d = str1 + str2; // 在堆上创建的新的对象 System.out.println(c == d);// false public static String getStr() { return \u0026#34;ing\u0026#34;; } "},{"id":171,"href":"/zh/docs/technology/Review/java_guide/java/Basic/ly0001lyjava_guide_basic_1/","title":"javaGuide基础1","section":"基础","content":" 转载自https://github.com/Snailclimb/JavaGuide （添加小部分笔记）感谢作者!\n基础概念及常识 # Java语言特点\n面向对象（封装、继承、多态） 平台无关性（Java虚拟机） 等等 JVM并非只有一种，只要满足JVM规范，可以开发自己专属JVM\nJDK与JRE\nJDK，JavaDevelopmentKit，包含JRE，还有编译器（javac）和工具（如javadoc、jdb）。能够创建和编译程序 JRE，Java运行时环境，包括Java虚拟机、Java类库，及Java命令等。但是不能创建新程序 字节码，采用字节码的好处\nJava中，JVM可以理解的代码称为字节码（.class文件)，不面向任何处理器，只面向虚拟机 Java程序从源代码到运行的过程 java代码必须先编译为字节码，之后呢，.class\u0026ndash;\u0026gt;机器码，这里JVM类加载器先加载字节码文件，然后通过解释器进行解释执行（也就是字节码需要由Java解释器来解释执行） Java解释器是JVM的一部分 编译与解释并存\n编译型：通过编译器将源代码一次性翻译成可被该平台执行的机器码，执行快、开发效率低 解释型：通过解释器一句一句的将代码解释成机器代码后执行，执行慢，开发效率高 如图 为什么说 Java 语言“编译与解释并存”？\n这是因为 Java 语言既具有编译型语言的特征，也具有解释型语言的特征。因为 Java 程序要经过先编译，后解释两个步骤，由 Java 编写的程序需要先经过编译步骤，生成字节码（.class 文件），这种字节码必须由 Java 解释器来解释执行。\nJava与C++区别\n没学过C++，Java不提供指针直接访问内存 Java为单继承；但是Java支持继承多接口 Java有自动内存管理垃圾回收机制（GC），不需要程序员手动释放无用内存 注释分为 单行注释、多行注释、文档注释 标识符与关键字 标识符即名字，关键字则是被赋予特殊含义的标识符\n自增自减运算符 当 b = ++a 时，先自增（自己增加 1），再赋值（赋值给 b）；当 b = a++ 时，先赋值(赋值给 b)，再自增（自己增加 1）\ncontinue/break/return\ncontinue ：指跳出当前的这一次循环，继续下一次循环。 break ：指跳出整个循环体，继续执行循环下面的语句。 return 用于跳出所在方法，结束该方法的运行。 变量\n成员变量和局部变量 成员变量可以被 public,private,static 等修饰符所修饰，而局部变量不能被访问控制修饰符及 static 所修饰；但是，成员变量和局部变量都能被 final 所修饰 从变量在内存中的存储方式来看,如果成员变量是使用 static 修饰的，那么这个成员变量是属于类的，如果没有使用 static 修饰，这个成员变量是属于实例的。而对象存在于堆内存，局部变量则存在于栈内存。 从变量在内存中的生存时间上看，成员变量是对象的一部分，它随着对象的创建而存在，而局部变量随着方法的调用而自动生成，随着方法的调用结束而消亡（即方法栈弹出后消亡）。 final必须显示赋初始值，其他都自动以类型默认值赋值 静态变量：被类所有实例共享 字符型常量与字符串常量区别\n形式 : 字符常量是单引号引起的一个字符，字符串常量是双引号引起的 0 个或若干个字符。 含义 : 字符常量相当于一个整型值( ASCII 值),可以参加表达式运算; 字符串常量代表一个地址值(该字符串在内存中存放位置)。 占内存大小 ： 字符常量只占 2 个字节; 字符串常量占若干个字节。 静态方法为什么不能调用非静态成员?\n静态方法是属于类的，在类加载的时候就会分配内存，可以通过类名直接访问。而非静态成员属于实例对象，只有在对象实例化之后才存在，需要通过类的实例对象去访问。 在类的非静态成员不存在的时候静态成员就已经存在了，此时调用在内存中还不存在的非静态成员，属于非法操作。 调用方式\n使用类名.方法名 调用静态方法，或者对象.方法名 （不建议） 调用静态方法可以无需创建对象 重载\n发生在同一个类中（或者父类与子类之间），方法名必须相同，参数类型不同、个数不同、顺序不同、方法返回值和访问修饰符可以不同\n不允许存在（只有返回值不同的两个方法(方法名和参数个数及类型相同)) 重载就是同一个类中多个同名方法根据不同的传参来执行不同的逻辑处理。 重写\n发生在运行期，子类对父类的允许访问的方法实现过程进行重新编写\n方法名、参数列表必须相同，子类方法返回值类型应比父类方法返回值类型更小或相等（也就是更具体），抛出的异常范围小于等于父类，访问修饰符范围大于等于父类（不能说父类可以访问而子类不能访问）。【注意，这里只针对方法，类属性则没有这个限制】\npackage com.javaguide; import java.io.IOException; public class TestParent { private String a; protected AParent x() { return new AParent(); } protected void b() throws Exception { } } class TestChild extends TestParent { public String a; /** * 返回类型有误，没有比父类更具体 * * @return */ /* protected AParentParent x() { return new AChild(); }*/ protected AChild x() { return new AChild(); } /** * 抛异常类型有误 没有比父类更具体 * * @throws Throwable */ /*protected void b() throws Throwable { }*/ protected void b() throws IOException { } } 如果父类方法访问修饰符为 private/final/static 则子类就不能重写该方法，但是被 static 修饰的方法能够被再次声明\nclass TestChild extends TestParent { public static void ab(){} } //父类 public class TestParent { protected static void ab(){} } 构造方法无法被重写\n可变长参数\n代码 可变参数只能作为函数的最后一个参数\npublic static void method2(String arg1, String... args) { //...... } 遇到方法重载的情况怎么办呢？会优先匹配固定参数还是可变参数的方法呢？\n答案是会优先匹配固定参数的方法\nJava 的可变参数编译后实际会被转换成一个数组，我们看编译后生成的 class文件就可以看出来了。\n基本数据类型，8种\n6种数字类型，1种字符类型，1种布尔值 byte,short,int,long ; float,double ; char boolean 1个字节8位，其中 byte 1字节，short 2字节，int 4字节 ，long 8字节 float 4字节，double 8 字节 char 2字节，boolean 1位 基本数据类型和包装类型的区别\n包装类型可用于泛型，而基本类型不可以 对于基本数据类型，局部变量会存放在Java虚拟机栈中的局部变量表中，成员变量（未被static修饰）存放在Java虚拟机堆中。\n包装类型属于对象类型，几乎所有对象实例都存在于堆中 相比对象类型，基本数据类型占用空间非常小 \u0026ldquo;基本数据类型存放在栈中\u0026rdquo; 这句话是错的，基本数据类型的成员变量如果没有被static修饰的话（不建议这么用，应该使用基本数据类型对应的包装类型），就存放在堆中。\n（如果被static修饰了，如果1.7则在方法区，1.7及以上移到了 Java堆中） 包装类型的缓存机制 Byte，Short，Integer，Long这4中包装类默认创建了数值[-128,127]的相应类型的缓存数据，Character创建了数值在[0,127]范围的缓存数据，Boolean直接返回True or False\nInteger缓存代码\npublic static Integer valueOf(int i) { if (i \u0026gt;= IntegerCache.low \u0026amp;\u0026amp; i \u0026lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i); } private static class IntegerCache { static final int low = -128; static final int high; static { // high value may be configured by property int h = 127; } } Character缓存代码\npublic static Character valueOf(char c) { if (c \u0026lt;= 127) { // must cache return CharacterCache.cache[(int)c]; } return new Character(c); } private static class CharacterCache { private CharacterCache(){} static final Character cache[] = new Character[127 + 1]; static { for (int i = 0; i \u0026lt; cache.length; i++) cache[i] = new Character((char)i); } } Boolean缓存代码\npublic static Boolean valueOf(boolean b) { return (b ? TRUE : FALSE); } 注意Float和Double没有使用缓存机制，且 只有调用valueOf（或者自动装箱）才会使用缓存，当使用new的时候是直接创建新对象\npublic Integer(int value) { this.value = value; } 举例\nBoolean t=new Boolean(true); Boolean f=new Boolean(true); System.out.println(t==f); //false System.out.println(t.equals(f)); //true Boolean t1=Boolean.valueOf(true); Boolean f1=Boolean.valueOf(true); System.out.println(t1==f1); //true System.out.println(Boolean.TRUE==Boolean.TRUE); //true //============================================// Integer i1 = 33; //这里发生了自动装箱，相当于Integer.valueOf(30) Integer i2 = 33; System.out.println(i1 == i2);// 输出 true Float i11 = 333f; Float i22 = 333f; System.out.println(i11 == i22);// 输出 false Double i3 = 1.2; Double i4 = 1.2; System.out.println(i3 == i4);// 输出 false //===========================================// Integer i1 = 40; Integer i2 = new Integer(40); System.out.println(i1==i2); 如上，所有整型包装类对象之间值的比较，应该全部使用equals方法比较 什么是自动装箱和拆箱\n装箱：将基本类型用它们对应的引用类型包装起来； 拆箱：将包装类型转换为基本数据类型； 举例说明\nInteger i = 10 ;//装箱 相当于Integer.valueOf(10) int n = i ;//拆箱 对应的字节码\nL1 LINENUMBER 8 L1 ALOAD 0 BIPUSH 10 INVOKESTATIC java/lang/Integer.valueOf (I)Ljava/lang/Integer; PUTFIELD AutoBoxTest.i : Ljava/lang/Integer; L2 LINENUMBER 9 L2 ALOAD 0 ALOAD 0 GETFIELD AutoBoxTest.i : Ljava/lang/Integer; INVOKEVIRTUAL java/lang/Integer.intValue ()I PUTFIELD AutoBoxTest.n : I RETURN 如图，Integer i = 10 等价于Integer i = Integer.valueOf(10)\nint n= i 等价于 int n= i.intValue();\n频繁拆装箱会严重影响系统性能\n浮点数运算的时候会有精度丢失的风险\n这个和计算机保存浮点数的机制有很大关系。我们知道计算机是二进制的，而且计算机在表示一个数字时，宽度是有限的，无限循环的小数存储在计算机时，只能被截断，所以就会导致小数精度发生损失的情况。这也就是解释了为什么浮点数没有办法用二进制精确表示。\n十进制下的0.2无法精确转换成二进制小数\n// 0.2 转换为二进制数的过程为，不断乘以 2，直到不存在小数为止， // 在这个计算过程中，得到的整数部分从上到下排列就是二进制的结果。 0.2 * 2 = 0.4 -\u0026gt; 0 0.4 * 2 = 0.8 -\u0026gt; 0 0.8 * 2 = 1.6 -\u0026gt; 1 0.6 * 2 = 1.2 -\u0026gt; 1 0.2 * 2 = 0.4 -\u0026gt; 0（发生循环） 使用BigDecimal解决上面的问题\nBigDecimal a = new BigDecimal(\u0026#34;1.0\u0026#34;); BigDecimal b = new BigDecimal(\u0026#34;0.9\u0026#34;); BigDecimal c = new BigDecimal(\u0026#34;0.8\u0026#34;); BigDecimal x = a.subtract(b); BigDecimal y = b.subtract(c); System.out.println(x); /* 0.1 */ System.out.println(y); /* 0.1 */ System.out.println(Objects.equals(x, y)); /* true */ 超过long整形的数据，使用BigInteger\nJava中，64位long整型是最大的整数类型\nlong l = Long.MAX_VALUE; System.out.println(l + 1); // -9223372036854775808 System.out.println(l + 1 == Long.MIN_VALUE); // true //BigInteger内部使用int[] 数组来存储任意大小的整型数据 //对于常规整数类型，使用BigInteger运算的效率会降低 "},{"id":172,"href":"/zh/docs/technology/Review/ssm/scope_transaction/","title":"作用域及事务","section":"Ssm","content":" 四种作用域 # singleton：默认值，当IOC容器一创建就会创建bean实例，而且是单例的，每次得到的是同一个 prototype：原型的，IOC容器创建时不再创建bean实例。每次调用getBean方法时再实例化该bean（每次都会进行实例化） request：每次请求会实例化一个bean session：在一次会话中共享一个bean 事务 # 事务是什么 # 逻辑上的一组操作，要么都执行，要么都不执行\n事务的特性 # ACID\nAtomicity /ˌætəˈmɪsəti/原子性 , 要么全部成功，要么全部失败 Consistency /kənˈsɪstənsi/ 一致性 , 数据库的完整性 Isolation /ˌaɪsəˈleɪʃn/ 隔离性 , 数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致 , 这里涉及到事务隔离级别 Durability /ˌdjʊərəˈbɪləti/ 持久性 , 事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失 Spring支持两种方式的事务管理 # 编程式事务管理 /ˈeksɪkjuːt/ execute\n使用transactionTemplate\n@Autowired private TransactionTemplate transactionTemplate; public void testTransaction() { transactionTemplate.execute(new TransactionCallbackWithoutResult() { @Override protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) { try { // .... 业务代码 } catch (Exception e){ //回滚 transactionStatus.setRollbackOnly(); } } }); } 使用transactionManager\n@Autowired private PlatformTransactionManager transactionManager; public void testTransaction() { TransactionStatus status = transactionManager.getTransaction(new DefaultTransactionDefinition()); try { // .... 业务代码 transactionManager.commit(status); } catch (Exception e) { transactionManager.rollback(status); } } 声明式事务管理\n@Transactional(propagation = Propagation.REQUIRED) public void aMethod { //do something B b = new B(); C c = new C(); b.bMethod(); c.cMethod(); } 事务传播行为 # Definition /ˌdefɪˈnɪʃ(ə)n/ 定义\nPropagation /ˌprɒpəˈɡeɪʃn/ 传播\n假设有代码如下：\n@Service Class A { @Autowired B b; @Transactional(propagation = Propagation.REQUIRED) public void aMethod { //do something b.bMethod(); } } @Service Class B { @Transactional(propagation = Propagation.XXXXXX) public void bMethod { //do something } } 共7种，其中主要有4种如下\nTransactionDefinition.PROPAGATION_REQUIRED 如果外部方法没有开启事务，则内部方法创建一个新的事务，即内外两个方法的事务互相独立；如果外部方法存在事务，则内部方法加入该事务，即内外两个方法使用同一个事务\nTransactionDefinition.PROPAGATION_REQUIRES_NEW 如果外部方法存在事务，则会挂起当前的事务，并且开启一个新事务，当外部方法抛出异常时，内部方法不会回滚；而当内部方法抛出异常时，外部方法会检测到并进行回滚。 如果外部方法不存在事务，则也会开启一个新事务\nTransactionDefinition.PROPAGATION_NESTED: 如果外部方法开启事务，则在内部再开启一个事务，作为嵌套事务存在；如果外部方法无事务，则单独开启一个事务\n在外围方法开启事务的情况下Propagation.NESTED修饰的内部方法属于外部事务的子事务，外围主事务回滚，子事务一定回滚，而内部子事务可以单独回滚而不影响外围主事务和其他子事务，也就是和上面的PROPAGATION_REQUIRES_NEW相反\nTransactionDefinition.PROPAGATION_MANDATORY 如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常 mandatory /ˈmændətəri/ 强制的\n下面三个比较不常用\nTransactionDefinition.PROPAGATION_SUPPORTS: 如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。 TransactionDefinition.PROPAGATION_NOT_SUPPORTED: 以非事务方式运行，如果当前存在事务，则把当前事务挂起。 TransactionDefinition.PROPAGATION_NEVER: 以非事务方式运行，如果当前存在事务，则抛出异常。 事务隔离级别 # TransactionDefinition.ISOLATION_DEFAULT TransactionDefinition.ISOLATION_READ_UNCOMMITTED 读未提交，级别最低，允许读取尚未提交的数据，可能会导致脏读、幻读或不可重复读 TransactionDefinition.ISOLATION_READ_COMMITTED 读已提交，对同一字段的多次读取结果都是一致的。可以阻止脏读，但幻读或不可重复读仍会发生 TransactionDefinition.ISOLATION_SERIALIZABLE 串行化，可以防止脏读、幻读及不可重复读，所有事务依次逐个执行，完全服从ACID，但严重影响性能 "},{"id":173,"href":"/zh/docs/technology/Review/basics/member_variables_and_local_variables/","title":"成员变量与局部变量","section":"Java基础(尚硅谷)_","content":" 代码 # static int s; int i; int j; { int i = 1; i++; j++; s++; } public void test(int j) { j++; i++; s++; } public static void main(String[] args) { Exam5 obj1 = new Exam5(); Exam5 obj2 = new Exam5(); obj1.test(10); obj1.test(20); obj2.test(30); System.out.println(obj1.i + \u0026#34;,\u0026#34; + obj1.j + \u0026#34;,\u0026#34; + obj1.s); System.out.println(obj2.i + \u0026#34;,\u0026#34; + obj2.j + \u0026#34;,\u0026#34; + obj2.s); } 运行结果 # 2,1,5 1,1,5 分析 # 就近原则 # 代码中有很多修改变量的语句，下面是用就近原则+作用域分析的图 局部变量和类变量 # 局部变量包括方法体{}，形参，以及代码块\n带static为类变量，不带的为实例变量\n代码中的变量分类 修饰符 \u0026ndash;局部变量只有final \u0026ndash; 实例变量 public , protected , private , final , static , volatile transient\n存储位置\n局部变量：栈\n实例变量：堆\n类变量：方法区（类信息、常量、静态变量）\n作用域 局部变量：从声明处开始，到所属的 } 结束 this 题中的s既可以用成员变量访问，也可以用类名访问\n生命周期\n局部变量：每一个线程，每一次调用执行都是新的生命周期 实例变量：随着对象的创建而初始化，随着对象被回收而消亡（垃圾回收器），每一个对象的实例变量是独立的 类变量：随着类的初始化而初始化，随着类的卸载而消亡，该类的所有对象的类变量是共享的 代码的执行，jvm中 # Exam5 obj1=new Exam5();\nobj1.test(10)\n非静态代码块或者进入方法，都会在栈中开辟空间存储局部变量 注意：静态代码块定义的变量，只会存在于静态代码块中。不是类变量，也不属于成员变量\n"},{"id":174,"href":"/zh/docs/technology/Review/basics/recursion_and_iteration/","title":"递归与迭代","section":"Java基础(尚硅谷)_","content":" 编程题 # 有n步台阶，一次只能上1步或2步，共有多少种走法\n分析 # 分析\nn = 1，1步 f(1) = 1\nn = 2, 两个1步,2步 f(2) = 2\nn = 3, 分两种情况： 最后1步是2级台阶/最后1步是1级台阶， 即 f(3) = f(1)+f(2) n = 4, 分两种情况： 最后1步是2级台阶/最后1步是1级台阶， 即f(4) = f(2)+f(3)\n也就是说，不管有几(n)个台阶，总要分成两种情况：最后1步是2级台阶/最后1步是1级台阶，即 f(n)= f(n-2) + f(n-1)\n递归 # public static int f(int n){ if(n==1 || n==2){ return n; } return f(n-2)+f(n-1); } public static void main(String[] args) { System.out.println(f(1)); //1 System.out.println(f(2)); //2 System.out.println(f(3)); //3 System.out.println(f(4)); //5 System.out.println(f(5)); //8 } debug调试 方法栈 f(4)\u0026mdash;-\u0026gt;分解成f(2)+f(3) f(2)\u0026mdash;返回- f(3)\u0026mdash;f(2)返回\u0026mdash;f(1)返回 【f(3)分解成f(2)和f(1)】 方法栈的个数： 使用循环 # public static int loop(int n){ if (n \u0026lt; 1) { throw new IllegalArgumentException(n + \u0026#34;不能小于1\u0026#34;); } if (n == 1 || n == 2) { return n; } int one=2;//最后只走1步，会有2种走法 int two=1;//最后走2步，会有1种走法 int sum=0; for(int i=3;i\u0026lt;=n;i++){ //最后跨两级台阶+最后跨一级台阶的走法 sum=two+one; two=one; one=sum; } return sum; } 小结 # 方法调用自身称为递归，利用变量的原值推出新值称为迭代(while循环) 递归\n优点：大问题转换为小问题，代码精简\n缺点：浪费空间（栈空间），可能会照成栈的溢出 迭代\n优点：效率高，时间只受循环次数限制，不受出栈入栈时间\n缺点：不如递归精简，可读性稍差 "},{"id":175,"href":"/zh/docs/technology/Review/basics/method_parameter_passing_mechanism/","title":"方法的参数传递机制","section":"Java基础(尚硅谷)_","content":" 代码 # public class Exam4 { public static void main(String[] args) { int i = 1; String str = \u0026#34;hello\u0026#34;; Integer num = 2; int[] arr = {1, 2, 3, 4, 5}; MyData my = new MyData(); change(i, str, num, arr, my); System.out.println(\u0026#34;i = \u0026#34; + i); System.out.println(\u0026#34;str = \u0026#34; + str); System.out.println(\u0026#34;num = \u0026#34; + num); System.out.println(\u0026#34;arr = \u0026#34; + Arrays.toString(arr)); System.out.println(\u0026#34;my.a = \u0026#34; + my.a); } public static void change(int j, String s, Integer n, int[] a, MyData m) { j+=1; s+=\u0026#34;world\u0026#34;; n+=1; a[0]+=1; m.a+=1; } } 结果\ni = 1 str = hello num = 2 arr = [2, 2, 3, 4, 5] my.a = 11 知识点 # 方法的参数传递机制 String、包装类等对象的不可变性 分析 # 对于包装类，如果是使用new，那么一定是开辟新的空间；如果是直接赋值，那么-128-127之间会有缓存池(堆中)\n//当使用new的时候，一定在堆中新开辟的空间 Integer a1= new Integer(12); Integer b1= new Integer(12); System.out.println(a1 == b1);//false Integer a2= -128; Integer b2= -128; System.out.println(a2 == b2);//true Integer a21= -129; Integer b21= -129; System.out.println(a21 == b21);//false Integer a3= 127; Integer b3= 127; System.out.println(a3 == b3);//true Integer a4= 22; Integer b4= 22; System.out.println(a4 == b4);//true Integer a31= 128; Integer b31= 128; System.out.println(a31 == b31);//false 对于String类\n//先查找常量池中是否有\u0026#34;abc\u0026#34;，如果有直接返回在常量池中的引用, //如果没有，则在常量池中创建\u0026#34;abc\u0026#34;,然后返回该引用 String a=\u0026#34;abc\u0026#34;; //先查找常量池中是否有\u0026#34;abc\u0026#34;，如果有则在堆内存中创建对象，然后返回堆内存中的地址 //如果没有，则先在常量池中创建字符串对象，然后再在堆内存中创建对象，最后返回堆内存中的地址 String ab=new String(\u0026#34;abc\u0026#34;); System.out.println(a==ab);//true //intern() //判断常量池中是否有ab对象的字符串，如果存在\u0026#34;abc\u0026#34;则返回\u0026#34;abc\u0026#34;在 //常量池中的引用，如果不存在则在常量池中创建, //并返回\u0026#34;abc\u0026#34;在常量池中的引用 System.out.println(a==ab.intern());//true change方法调用之前，jvm中的结构 方法栈帧中的数据 执行change方法后，实参给形参赋值： 基本数据类型：数据值 引用数据类型：地址值\n当实参是特殊的类型时：比如String、包装类等对象，不可变，即 s+=\u0026quot;world\u0026quot;; 会导致创建两个对象，如图（ Integer也是） 数组和对象，则是找到堆内存中的地址，直接更改\n"},{"id":176,"href":"/zh/docs/technology/Review/basics/class_and_instance_initialization/","title":"类、实例初始化","section":"Java基础(尚硅谷)_","content":" 代码 # public class Son extends Father{ private int i=test(); private static int j=method(); static { System.out.print(\u0026#34;(6)\u0026#34;); } Son(){ System.out.print(\u0026#34;(7)\u0026#34;); } { System.out.print(\u0026#34;(8)\u0026#34;); } public int test(){ System.out.print(\u0026#34;(9)\u0026#34;); return 1; } public static int method(){ System.out.print(\u0026#34;(10)\u0026#34;); return 1; } public static void main(String[] args) { Son s1=new Son(); System.out.println(); Son s2=new Son(); } } public class Father { private int i=test(); private static int j=method(); static { System.out.print(\u0026#34;(1)\u0026#34;); } Father(){ System.out.print(\u0026#34;(2)\u0026#34;); } { System.out.print(\u0026#34;(3)\u0026#34;); } public int test() { System.out.print(\u0026#34;(4)\u0026#34;); return 1; } public static int method() { System.out.print(\u0026#34;(5)\u0026#34;); return 1; } } 输出：\n(5)(1)(10)(6)(9)(3)(2)(9)(8)(7) (9)(3)(2)(9)(8)(7) 分析 # 类初始化过程\n当实例化了一个对象/或main所在类会导致类初始化 子类初始化前会先初始化父类 类初始化执行的是clinit 方法，编译查看字节码可得知 clinit 由静态类变量显示赋值语句 以及 静态代码块组成(由上到下顺序)，且只执行一次\n如下\n实例初始化过程\n执行的是init方法 由非静态实例变量显示赋值语句 以及 非静态代码块 [从上到下顺序] 以及对应构造器代码[最后执行] 组成 其中，子类构造器一定会调用super() [最前面] 1） super() 【最前】 2）i = test() 3）子类的非静态代码块 【2，3按顺序】 4) 子类的无参构造(最后)\n重写的问题 如上所示，初始化Son对象的时候，会先调用super()方法，即初始化父类，然后会先调用父类的 非静态变量赋值以及非静态代码块，最后才是父类的构造器代码块\n调用父类非静态变量赋值的时候，如果调用了非静态方法，就会涉及到重写问题，比如这里的\npublic class Father{ private int i= test(); } 这里会调用子类(当前正在初始化的对象)的test()方法，而不是父类的test()\n哪些方法不可被重写 final方法、静态方法、父类中的private等修饰使得子类不可见的方法 "},{"id":177,"href":"/zh/docs/technology/Review/basics/singleton_design_pattern/","title":"单例设计模式","section":"Java基础(尚硅谷)_","content":" 特点 # 该类只有一个实例 构造器私有化 该类内部自行创建该实例 使用静态变量保存 能向外部提供这个实例 直接暴露 使用静态变量的get方法获取 几大方法 # 饿汉式 # 随着类的加载进行初始化，不管是否需要都会直接创建实例对象\npublic class Singleton1 { public static final Singleton1 INSTANCE=new Singleton1(); private Singleton1() { } } 枚举 # 枚举类表示该类型的对象是有限的几个\npublic enum Singleton2 { INSTANCE } 使用静态代码块 # 随着类的加载进行初始化\npublic class Singleton2 { public static final Singleton2 INSTANCE; static { INSTANCE = new Singleton2(); } private Singleton2() { } } 如图，当初始化实例时需要进行复杂取值操作时，可以取代第一种方法 懒汉式 # 延迟创建对象\npublic class Singleton4 { //为了防止重排序，需要添加volatile关键字 private static volatile Singleton4 INSTANCE; private Singleton4() { } /** * double check * @return */ public static Singleton4 getInstance() { //2 先判断一次,对于后面的操作(此时已经创建了对象)能减少加锁次数 if (INSTANCE == null) { //如果这里不加锁会导致线程安全问题，可能刚进了判断语句之后，执行权被剥夺了又创建好了对象， //所以判断及创建对象必须是原子操作 synchronized (Singleton4.class) { if (INSTANCE == null) { //用来模拟多线程被剥夺执行权 try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } //如果这个地方不加volatile,会出现的问题是,指令重排 1,2,3是正常的, //会重排成1,3,2 然后别的线程去拿的时候，判断为非空，但是实际上运行的时候，发现里面的数据是空的 //1 memory = allocate();//分配对象空间 //2 instance(memory); //初始化对象 //3 instance = memory; //设置instance指向刚刚分配的位置 INSTANCE = new Singleton4(); } } } return INSTANCE; } } 使用静态内部类 # public class Singleton6 { private Singleton6(){ } private static class Inner{ private static final Singleton6 INSTANCE=new Singleton6(); } public static Singleton6 getInstance(){ return Inner.INSTANCE; } } 只有当内部类被加载和初始化的时候，才会创建INSTANCE实例对象 静态内部类不会自动随着外部类的加载和初始化而初始化，他需要单独去加载和初始化 又由于他是在内部类加载和初始化时，创建的，属于类加载器处理的，所以是线程安全的 "},{"id":178,"href":"/zh/docs/technology/Review/basics/self_incrementing_variable/","title":"自增变量","section":"Java基础(尚硅谷)_","content":" 题目 # int i=1; i=i++; int j=i++; int k = i+ ++i * i++; System.out.println(\u0026#34;i=\u0026#34;+i); System.out.println(\u0026#34;j=\u0026#34;+j); System.out.println(\u0026#34;k=\u0026#34;+k); 讲解 # 对于操作数栈和局部变量表的理解 # 对于下面的代码\nint i=10; int j=9; j=i; 反编译之后，查看字节码\n0 bipush 10 2 istore_1 3 bipush 9 5 istore_2 6 iload_1 7 istore_2 8 return 如下图，这三行代码，是依次把10，9先放到局部变量表的1，2位置。\n之后呢，再把局部变量表中1位置的值，放入操作数栈中\n最后，将操作数栈弹出一个数(10)，将数值赋给局部变量表中的位置2\n如上图，当方法为静态方法时，局部变量表0位置存储的是实参第1个数\n(当方法为非静态方法时，局部变量表0位置存储的是this引用)\n对于下面这段代码\nint i=10; int j=20; i=i++; j=++j; System.out.println(i); System.out.println(j); 编译后的字节码\n0 bipush 10 2 istore_1 3 bipush 20 5 istore_2 6 iload_1 7 iinc 1 by 1 10 istore_1 11 iinc 2 by 1 14 iload_2 15 istore_2 16 getstatic #5 \u0026lt;java/lang/System.out : Ljava/io/PrintStream;\u0026gt; 19 iload_1 20 invokevirtual #6 \u0026lt;java/io/PrintStream.println : (I)V\u0026gt; 23 getstatic #5 \u0026lt;java/lang/System.out : Ljava/io/PrintStream;\u0026gt; 26 iload_2 27 invokevirtual #6 \u0026lt;java/io/PrintStream.println : (I)V\u0026gt; 30 return 如上对于j = ++j ;是\n11 iinc 2 by 1 14 iload_2 15 istore_2 先对局部变量表2中的 值 加1，然后将结果 放入操作数栈中，之后再将操作数栈弹出一个数并赋值给 位置2\n对于题目的解释 # int i=1; i=i++; int j=i++; int k = i+ ++i * i++; System.out.println(\u0026#34;i=\u0026#34;+i); System.out.println(\u0026#34;j=\u0026#34;+j); System.out.println(\u0026#34;k=\u0026#34;+k); 编译后的字节码\n0 iconst_1 1 istore_1 2 iload_1 3 iinc 1 by 1 6 istore_1 7 iload_1 8 iinc 1 by 1 11 istore_2 12 iload_1 13 iinc 1 by 1 16 iload_1 17 iload_1 18 iinc 1 by 1 21 imul 22 iadd 23 istore_3 对于 int j = i++\n7 iload_1 8 iinc 1 by 1 11 istore_2 先将i的值放进栈中，然后将局部变量表中的i + 1，之后将栈中的值赋值给j 到这步骤的时候，i = 2 ，j = 1\n最后一步 int k = i+ ++i * i++\n12 iload_1 13 iinc 1 by 1 16 iload_1 17 iload_1 18 iinc 1 by 1 21 imul 22 iadd 23 istore_3 如字节码所示，先将i load进操作数栈中(2)，然后将局部变量表中的i 自增 (3)，之后将自增后的结果(3)放入操作数栈中，第二次将局部变量表中的i放入操作数栈中。然后此时操作数栈中存在 3 3 2 (由栈顶到栈底) ，依次进行乘法加法 （3*3+2） =11 ，放入局部变量表3 中。 所以结果为 2， 1，11\n小结 # ​\n"},{"id":179,"href":"/zh/docs/technology/Git/git_sgg/19-26/","title":"19-26_git_尚硅谷","section":"基础(尚硅谷视频)_","content":" 介绍 # 使用代码托管中心（远程服务器） 团队内写作 push\u0026ndash;clone\u0026ndash;push\u0026mdash; \u0026ndash;pull 跨团队写作 fork（到自己的远程库）\u0026mdash;clone 创建远程库\u0026amp;创建别名 # 官网：https://github.com 现在yuebuqun注册一个账号 创建一个远程库git-demo，创建成功 创建远程库别名 git remote -v （查看别名） 为远程库创建别名 git remote add git-demo https://github.com/lwmfjc/git-demo.git 别名创建成功 fetch和push都可以使用别名 推送本地库到远程库 # 推送master分支 切换git checkout master 推送 git push git-demo master 拉取远程库到本地库 # git pull git-demo master 结果 克隆远程库到本地 # git clone xxxxxxx/git-demo.git clone之后有默认的别名，且已经初始化了本地库 团队内写作 # lhc修改了git-demo下的hello.txt 之后进行git add hello.txt git commit -m \u0026ldquo;lhc-commit \u0026quot; hello.txt 现在进行push git push origin master 出错了 使用ybq，对库进行设置，管理成员 添加成员即可 输入账号名 将邀请函 发送给lhc 现在再次推送，则推送成功 团队外合作 # 先把别人的项目fork下来 之后进行修改并且commit pull request (拉取请求) 请求 东方不败：\n岳不群：看到别人发过来的请求 可以同意 合并申请 SSH免密登录 # ssh免密公钥添加\n添加之前,\ngit config --global user.name \u0026#34;username\u0026#34; git config --global user.email useremail@qq.com 删除~/.ssh 使用\nssh-keygen -t rsa -C xxxx@xx.com # 再次到~/.ssh 查看 cat id_rsa 私钥 把私钥复制到 账号\u0026ndash;设置\u0026ndash;ssh and gpgkeys 测试是否成功 "},{"id":180,"href":"/zh/docs/life/archive/20220724/","title":"人为什么要结婚(找对象)","section":"往日归档","content":"其实这是我在六七年前思考的一个问题，我觉得结婚，并不能单纯的作为一个世俗任务。很多人，是因为年纪到了结婚，因为父母催结婚，因为看到别人结婚而结婚，总之，是为别人而活。但我觉得，结婚的本质，应该是两个人生活的结合，包括了很多，比如生活中的喜怒哀乐互享，这是最基础的，开心了有人替你高兴，生气难过了有人安慰你、心疼你。如果连这个都做不到而各活各的，那我实在想不明白这种婚姻的意义在哪，而现在很多情况正是这样，有为了家庭而工作辛苦而没有交集的，也有单纯的相处腻了、懒了。\n而说到腻，这就在于一点，就是有些婚姻是很仓促的，压根就没看清楚对方的样子(性格、三观)，或者是不清楚自己喜欢的是什么样的人，就已经在一起了，之后才发现对方很多问题不是自己能接受的，但是这个时候已经晚了。所以“内在”，才能持久吸引一个人，因为这是不轻易随时光变迁而改变的。\n分享也并非简单的分享，如果分享的东西对方没有啥感觉，那这种关系也是很难持久的。因此，最佳的婚姻，应该是异性知己，你的一些心理，不用向对方解释太多，当然 这里并不是说一开始就是这种状态，更多是通过后面不断了解、不断磨合而达成这种状态，当你被别人误会了有人理解，这是世间最好的良药。理解一个人，就是拯救一个世界，一花一世界，一树一菩提。\n婚姻，就是找个互相理解的爱人，共享世间冷暖，白首不相离。\n"},{"id":181,"href":"/zh/docs/technology/Git/git_sgg/09-18/","title":"09-18_git_尚硅谷","section":"基础(尚硅谷视频)_","content":" 命令 # 命令-设置用户签名\n查看 git config user.name git config user.email 设置 git config --global user.name ly001 git config --global user.email xxx@xx.com git的配置文件查看 作用：区分不同操作者身份，跟后面登陆的账号没有关系 初始化本地库\ngit init 多出一个文件夹 查看本地库状态\ngit status 默认在master分支 新增一个文件 vim hello.txt 此时查看本地库的状态 untracketd files 未被追踪的文件，也就是这个文件还在工作区 添加暂存区\ngit add hello.txt LF 将会被替换成 CRLF，windows里面是CRLF，也就是说\n这个换行符自动转换会把自动把你代码里 与你当前操作系统不相同的换行的方式 转换成当前系统的换行方式（即LF和CRLF 之间的转换）\n这是因为这个hello.txt是使用vm hello.txt在git bash里面添加的，如果直接在windows文件管理器添加一个文件（hello2.txt)，就会发现没有这个警告，因为他已经是CRLF了 （为了和视频保持一致，git rm \u0026ndash;cached hello2.txt 后删除这个文件） 查看当前状态，绿色表示git已经追踪到了这个文件\n文件已经存在于暂存区 使用git rm --cached hello.txt可以将文件从暂存区删除 使用后，文件又出现在工作区了（未添加） 提交本地库\ngit commit -m \u0026quot;first commit\u0026quot; hello.txt 会出现一些警告，以及此时提交的修改和生成的版本号（前七位） git status 使用git reflog查看引用日志信息 git log 查看详细日志信息 修改命令\n前提，修改了文件 git status\n红色表示git还没有追踪到这个修改，如果此时commit ，会提示没有需要commit的 使用git add hello.txt 将文件修改添加到暂存区 之后git status 注意，这里如果提交到暂存区之后，使用git restore是无法恢复文件的\ngit restore --staged \u0026lt;file\u0026gt;...\u0026quot; to unstage 使用这个命令丢弃这个文件的commit操作\n几个命令的区别：\ngit restore file 的命令是丢弃你在工作区修改的内容,(修改的内容会丢失) git restore \u0026ndash;staged file 丢弃你在工作区的修改不被commit 。但是你的修改依然在工作区。 git rm \u0026ndash;cached file和git restore \u0026ndash;staged file 效果好像一样，这里不做更进一步的分析 回到最初，这里主要是为了看修改，如最上面，将第一行后面添加了22222\ncommit 之后的提示，删除了一行，添加了一行（修改的另一种说法） 如果，HEAD -\u0026gt; master ，指针指向了第二个版本 这里再做第三次修改，并add 及commit 查看工作区，永远只有最后那次修改的文件 版本穿梭\ngit reflog和git log 回顾：hello.txt先是5行，然后第一行加了2，之后第二行加了3\n使用git reset \u0026ndash;hard 版本号进行穿梭，这里多了一行，是因为我复制的时候复制粗了版本号\n使用cat 查看，发现文件已经在另一个版本 查看.git的一些文件 说明目前是在master这个版本上 下面这个文件 .git/refs/heads/master 记录了指向master分支的哪个版本号 这里将文件指向最初的版本 此时查看刚才说的那个记录某个分支当前指向版本的文件，已经做了更新 再穿梭为后面的版本 git reset \u0026ndash;hard file 图片解释 master指针指向first,second,third head永远都是指向master（当前分支，目前只有master，所以不变）\n分支 # 概述和优点 查看\u0026amp;创建\u0026amp;切换\ngit branch 分支名 #创建分支 git branch -v #查看分支 git checkout 分支名 #切换分支 git merge 分支名 #把指定的分支合并到当前分支上 查看分支并显示当前分支指向的版本 git branch -v 创建分支 git branch hot-fix git branch #再次查看 切换分支\ngit branch hot-fix 此时修改一个文件并提交 查看.git/head文件，会发现现在它指向hot-fix分支 合并分支（正常合并）\n切换分支 将某分支xx合并到当前分支 git merge 分支名\n如图，合并成功 以后面那个分支的修改为主\n合并分支（冲突合并）\n前提，现在master分支倒数第二行修改并添加和提交 此时切换到hot-fix分支 修改倒数第一行 将文件从工作区添加到暂存区并提交到本地库 此时再切回master\ngit checkout master git merge hot-fix 提示出错了，而且所有有异常的文件，都以下面的形式标注 按dd进行删除某一行 改完了之后，保存并提交即可 切回之后查看hot-fix分支，发现这里的文件是没有变化的 原理 "},{"id":182,"href":"/zh/docs/technology/Git/git_sgg/01-08/","title":"01-08_git_尚硅谷","section":"基础(尚硅谷视频)_","content":" 概述 # 课程介绍 # Git - git介绍\u0026ndash;分布式版本控制+集中式版本控制 - git安装\u0026ndash;基于官网，2.31.1 windows - 基于开发案例 详细讲解常用命令 - git分支\u0026mdash;特性、创建、转换、合并、代码合并冲突解决 - idea集成git Github 如何创建远程库 推送 push 拉取 pull 克隆 clone ssh免密登录 idea github集成 Gitee码云 码云创建远程库 Idea集成Gitee Gitlab gitlab服务器的搭建和部署 idea集成gitlab 课程目标：五个小时，熟练掌握git、github、gitee 官网介绍 # git是免费的开源的分布式版本控制系统 廉价的本地库 分支功能 Everything is local 版本控制介绍 # 记录文件内容变化，以便将来查阅特定版本修订记录的系统 如果没有git 为什么需要版本控制（从个人开发过渡到团队合作） 分布式版本控制VS集中式版本控制 # SVN，单一的集中管理的服务器，保存所有文件的修订版本。其他人都先连到这个中央服务器上获取最新处理是否冲突 缺点，单点故障，如果某段时间内故障了，那么就没法提交 Git，每台电脑都是代码库 如果远程库挂了，本地还是可以做版本控制的，只不过不能做代码推送而已 每个客户端保存的都是完整的项目（包括历史记录） 发展历史 # linux系统版本控制历史 1991-2002 手动合并 2002 BitKeeper授权Linux社区免费使用（版本控制系统） 社区将其破解 2005 用C语言开发了一个分布式版本控制系统：Git 两周开发时间 2008年 GitHub上线 工作机制和代码托管中心 # 工作机制\n如果git commit ，会生成对应的历史版本，那么这里的历史版本是删不掉的 如果只是在工作区，或者添加到了暂存区，那么是可以恢复(删掉（操作记录）)的 git add (让git知道有这个文件) 如果只有v1,v2,v3，V3版本是删不掉的，如果要恢复成v2，只能再提交一次版本 远程库\u0026ndash; 代码托管中心是基于网络服务器的远程代码仓库，简称为远程库 局域网 GitLab\n互联网 GitHub Gitee 码云\n安装 # git安装、客户端使用(windows)\ngit安装位置 任意 非中文、无空格\n选项配置 编辑器选择 是否修改初始化分支的名字\u0026ndash;默认master 默认第二个，这里选择第一个，只能在git bash里面使用 后台客户端协议 配置行末换行符 windows\u0026ndash;CRLF linux\u0026ndash;LF\n默认，让git根据系统自动转换\n从远程拉取代码时，模式\u0026ndash;用默认 凭据管理器 记录登陆行为，不用每次登录 其他配置 软链接文件 缓存 再git bash里运行第三方程序\n安装成功\u0026mdash;视频里面是2.31 "},{"id":183,"href":"/zh/docs/technology/Maven/advance_dljd/01-21/","title":"01-21 maven多模块管理_动力节点","section":"进阶(动力节点)_","content":" 场景介绍 # 业务依赖 多模块管理 版本管理 第1种方式 # 创建父工程 # 先创建一个空项目 在这个空项目下，创建一个module当作maven父工程 结构 pom文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;001-maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;!-- packaging 标签指定打包方式，默认为jar --\u0026gt; \u0026lt;!-- maven父工程必须遵守以下两点要求 1、packaging标签的文本内容必须设置为pom 2、把src删除 --\u0026gt; \u0026lt;/project\u0026gt; 介绍pom文件 # pom 项目对象模型，project object model，该文件可以子工程被继承 maven多模块管理，其实就是让它的子模块的pom文件来继承父工程的pom\n创建maven java子工程 # 新建一个module\n注意路径，002在IDEA-maven的目录下 查看pom文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;!--指向父工程的gav坐标--\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;001-maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;!--相对路径--\u0026gt; \u0026lt;relativePath\u0026gt;../001-maven-parent/pom.xml\u0026lt;/relativePath\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;002-maven-java\u0026lt;/artifactId\u0026gt; \u0026lt;/project\u0026gt; 创建maven web子工程 # 创建新模块 查看pom\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;001-maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;relativePath\u0026gt;../001-maven-parent/pom.xml\u0026lt;/relativePath\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;003-maven-web\u0026lt;/artifactId\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; \u0026lt;/project\u0026gt; 修改子工程为父工程 # ​\t1 父工程的pom.xml种的packaging标签的文本内容必须设置pom\n​\t2 删除src目录\n如图，比如这里修改002-maven-java为父工程 添加004为002的子工程 查看pom文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;002-maven-java\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;relativePath\u0026gt;../002-maven-java/pom.xml\u0026lt;/relativePath\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;004-maven-java-1\u0026lt;/artifactId\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/project\u0026gt; 手动修改Maven工程为子工程(非idea中) # 这里说的是，创建子工程的时候，没有选择父工程 创建完之后的pom.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;005-maven-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;/project\u0026gt; 修改（添加parent标签即可）\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;001-maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;relativePath\u0026gt;../001-maven-parent/pom.xml\u0026lt;/relativePath\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;005-maven-java\u0026lt;/artifactId\u0026gt; \u0026lt;/project\u0026gt; 注意 子模块继承父工程所有依赖 # 比如在父工程添加这块依赖\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.46\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 如下 父工程添加的依赖，所有子模块会无条件继承\n父工程管理依赖 # 依赖冗余的问题 加强管理\n\u0026lt;!--加强管理--\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.46\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 结果，依赖都没有了 子工程声明式继承父工程依赖 # 比如002-maven-java（子模块，但又是004的父工程）需要mysql\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 效果 子模块依赖的版本号继承父工程依赖的版本号 如果子模块指定以来的版本号，那就不会继承父工程依赖的版本号 父工程管理依赖版本号 # 使用properties变量\n\u0026lt;properties\u0026gt; \u0026lt;!--自定义标签名称--\u0026gt; \u0026lt;!--约定：通常管理依赖版本号的标签名：项目名称-字段version， 项目名称.字段version--\u0026gt; \u0026lt;junit-version\u0026gt;4.12\u0026lt;/junit-version\u0026gt; \u0026lt;mysql-connector-java-version\u0026gt;5.1.46\u0026lt;/mysql-connector-java-version\u0026gt; \u0026lt;dubbo-version\u0026gt;2.5.3\u0026lt;/dubbo-version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;!--加强管理--\u0026gt; \u0026lt;dependencyManagement\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${junit-version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${mysql-connector-java-version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dubbo\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${dubbo-version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 回顾第1种实现方式 # 父工程的要求 子工程的添加 子工程改为父工程 子工程和父工程是平级的 父工程加强管理 \u0026lt;dependencyManagement\u0026gt;\u0026lt;/\u0026lt;dependencyManagement\u0026gt; 注意，第一种方法父工程的pom.xml中，这个也应该是必须的 第2种方式 # 创建父工程 # 最顶层创建一个工程（父工程） pom文件（未处理）\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; \u0026lt;/project\u0026gt; 目录结构(未处理) 处理后 pom.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; \u0026lt;!-- 1.packaging标签文本内容必须设置为pom 2.删除src目录 --\u0026gt; \u0026lt;/project\u0026gt; 结构 创建子工程 # 子工程的pom.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;!--指向父工程--\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;!--注意，这里不需要找pom.xml，因为该子工程和父工程的pom.xml同级--\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;maven-java-001\u0026lt;/artifactId\u0026gt; \u0026lt;/project\u0026gt; 父工程的pom.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;!--父工程包含的所有子模块--\u0026gt; \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;maven-java-001\u0026lt;/module\u0026gt; \u0026lt;/modules\u0026gt; \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; \u0026lt;!-- 1.packaging标签文本内容必须设置为pom 2.删除src目录 --\u0026gt; \u0026lt;/project\u0026gt; 第二个子模块\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;maven-web-001\u0026lt;/artifactId\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; \u0026lt;/project\u0026gt; 父工程pom.xml的变化 创建子工程的子工程 # 父工程必须遵循\npackaging标签文本内容设置为pom 删除src目录 创建子工程 maven-java-001的pom.xml查看\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;!--指向父工程--\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;maven-parent\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;!--注意，这里不需要找pom.xml，因为该子工程和父工程的pom.xml同级--\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;maven-java-001\u0026lt;/artifactId\u0026gt; \u0026lt;packaging\u0026gt;pom\u0026lt;/packaging\u0026gt; \u0026lt;modules\u0026gt; \u0026lt;module\u0026gt;maven-java-0101\u0026lt;/module\u0026gt; \u0026lt;/modules\u0026gt; \u0026lt;/project\u0026gt; 子模块的pom.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;parent\u0026gt; \u0026lt;artifactId\u0026gt;maven-java-001\u0026lt;/artifactId\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode.maven\u0026lt;/groupId\u0026gt; \u0026lt;version\u0026gt;1.0.0\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;artifactId\u0026gt;maven-java-0101\u0026lt;/artifactId\u0026gt; \u0026lt;/project\u0026gt; 父工程管理依赖 # 父工程的pom文件 子模块也一起继承了 父工程管理所有依赖 如果子工程需要，则使用声明式依赖 也可以自己指定版本号 父工程管理依赖的版本号 # 使用properties管理版本号，和第一种方式一样 子工程继承父工程编译插件 # 修改之后，这里为了看效果，改成1.6\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.6\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.6\u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 第3种方式 # 前面两种混合使用\n先创建一个空项目 然后假设有三个父工程 然后每个父工程又都有子模块 "},{"id":184,"href":"/zh/docs/technology/Maven/base_dljd/31-43/","title":"31-43 maven基础_动力节点","section":"基础(动力节点)_","content":" idea中设置maven # 和idea集成maven 创建普通的j2se项目 # 使用idea创建空白项目 新建一个module 使用模板创建普通java项目 输入gav 设置maven信息 标准的maven工程 与创建网站有关，删掉即可\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven-j2se\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;!--设置网站，注释掉即可--\u0026gt; \u0026lt;!-- \u0026lt;name\u0026gt;ch01-maven-j2se\u0026lt;/name\u0026gt; \u0026lt;!– FIXME change it to the project\u0026#39;s website –\u0026gt; \u0026lt;url\u0026gt;http://www.example.com\u0026lt;/url\u0026gt;--\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;!--maven常用设置--\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt;\u0026lt;!--单元测试--\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.11\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;!--插件版本的配置，无特殊指定则删除--\u0026gt; \u0026lt;pluginManagement\u0026gt;\u0026lt;!-- lock down plugins versions to avoid using Maven defaults (may be moved to parent pom) --\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!-- clean lifecycle, see https://maven.apache.org/ref/current/maven-core/lifecycles.html#clean_Lifecycle --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-clean-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.1.0\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- default lifecycle, jar packaging: see https://maven.apache.org/ref/current/maven-core/default-bindings.html#Plugin_bindings_for_jar_packaging --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-resources-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.2\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.0\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.22.1\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-jar-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.2\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-install-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.5.2\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-deploy-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.8.2\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- site lifecycle, see https://maven.apache.org/ref/current/maven-core/lifecycles.html#site_Lifecycle --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-site-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.7.1\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-project-info-reports-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/pluginManagement\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt; 单元测试 # 关于idea颜色 编写java程序\npackage com.bjpowernode; public class HelloMaven { public int addNumber(int n1,int n2){ return n1+n2; } public static void main(String[] args) { HelloMaven helloMaven=new HelloMaven(); int res=helloMaven.addNumber(10,20); System.out.println(\u0026#34;res = \u0026#34;+res); } } 测试使用 idea中maven工具窗口 # Maven生成的目录 使用mvn clean进行清理\nλ mvn clean [INFO] Scanning for projects... [INFO] [INFO] ------------------\u0026lt; com.bjpowernode:ch01-maven-j2se \u0026gt;------------------- [INFO] Building ch01-maven-j2se 1.0 [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ ch01-maven-j2se --- [INFO] Deleting D:\\Users\\ly\\Documents\\git\\mavenwork\\04-project\\ch01-maven-j2se\\target [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 0.438 s [INFO] Finished at: 2022-07-13T23:39:03+08:00 [INFO] ------------------------------------------------------------------------ 窗口 单元测试 打包 install安装 其他 重新更新依赖项 创建web项目加入servlet依赖 # 结构 创建java文件夹和资源文件夹 pom文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch02-maven-web\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;war\u0026lt;/packaging\u0026gt; \u0026lt;name\u0026gt;ch02-maven-web Maven Webapp\u0026lt;/name\u0026gt; \u0026lt;!-- FIXME change it to the project\u0026#39;s website --\u0026gt; \u0026lt;url\u0026gt;http://www.example.com\u0026lt;/url\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;project.build.sourceEncoding\u0026gt;UTF-8\u0026lt;/project.build.sourceEncoding\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!--servlet依赖--\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/javax.servlet/javax.servlet-api --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javax.servlet-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.0.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--jsp依赖--\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/javax.servlet.jsp/javax.servlet.jsp-api --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;javax.servlet.jsp\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;javax.servlet.jsp-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3.3\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--junit--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.11\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; 和上面进行对比 创建servlet # 创建完之后\n代码\npackage com.bjpowernode.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; public class HelloServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } } web.xml\n\u0026lt;!DOCTYPE web-app PUBLIC \u0026#34;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\u0026#34; \u0026#34;http://java.sun.com/dtd/web-app_2_3.dtd\u0026#34; \u0026gt; \u0026lt;web-app\u0026gt; \u0026lt;display-name\u0026gt;Archetype Created Web Application\u0026lt;/display-name\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;HelloServlet\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;com.bjpowernode.controller.HelloServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;/web-app\u0026gt; 添加mapping\n\u0026lt;!DOCTYPE web-app PUBLIC \u0026#34;-//Sun Microsystems, Inc.//DTD Web Application 2.3//EN\u0026#34; \u0026#34;http://java.sun.com/dtd/web-app_2_3.dtd\u0026#34; \u0026gt; \u0026lt;web-app\u0026gt; \u0026lt;display-name\u0026gt;Archetype Created Web Application\u0026lt;/display-name\u0026gt; \u0026lt;servlet\u0026gt; \u0026lt;servlet-name\u0026gt;HelloServlet\u0026lt;/servlet-name\u0026gt; \u0026lt;servlet-class\u0026gt;com.bjpowernode.controller.HelloServlet\u0026lt;/servlet-class\u0026gt; \u0026lt;/servlet\u0026gt; \u0026lt;servlet-mapping\u0026gt; \u0026lt;servlet-name\u0026gt;HelloServlet\u0026lt;/servlet-name\u0026gt; \u0026lt;url-pattern\u0026gt;/hello\u0026lt;/url-pattern\u0026gt; \u0026lt;/servlet-mapping\u0026gt; \u0026lt;/web-app\u0026gt; 添加jsp\n\u0026lt;%-- Created by IntelliJ IDEA. User: ly Date: 2022/7/16 Time: 18:10 To change this template use File | Settings | File Templates. --%\u0026gt; \u0026lt;%@ page contentType=\u0026#34;text/html;charset=UTF-8\u0026#34; language=\u0026#34;java\u0026#34; %\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;index\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;a href=\u0026#34;hello\u0026#34; \u0026gt;访问\u0026lt;/a\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 设置转发\npackage com.bjpowernode.controller; import javax.servlet.*; import javax.servlet.http.*; import java.io.IOException; public class HelloServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { System.out.println(\u0026#34;收到请求了\u0026#34;); //转发到show request.getRequestDispatcher(\u0026#34;/show.jsp\u0026#34;) .forward(request,response); } @Override protected void doPost(HttpServletRequest request, HttpServletResponse response) throws ServletException, IOException { } } 设置tomcat并发布\nidea出现not found for the web module. 复习核心的概念 # 约定的目录结构 pom 项目对象模型，groupId,artifactId,version gav 仓库 本地仓库 \u0026hellip;../.m2/repository 远程仓库 生命周期，clean，compile，test-compile，test，package，install maven和idea集成 设置maven安装目录和配置文件 设置Runner，创建maven时速度快 使用模板创建 se和web 导入模块到idea # 导入02这个项目 结果 当磁盘中文件夹名字和项目名不一样时 如果导入后颜色不对，则需要右键 mark as scope依赖范围 # scope标签\n依赖范围：scope标签，这个依赖在项目构建的哪个阶段起作用\n值：compile，默认，参与构建项目的所有阶段； test：测试，在测试阶段使用，比如执行mvn test 会使用junit provided：提供者，项目在部署到服务器时，不需要提供这个依赖的jar，而是由服务器提供这个以来的jar包 打包时只有mysql war文件 给服务器，即放到tomcat的webapps中 启动tomcat之后，会自动解压 访问 自定义变量 # properties标签，常用设置 test报告 这种需要将文件夹删除，然后reimport 全局变量，比如依赖版本号 重复的问题 在properties里面定义即可 使用全局变量 ${变量名} 处理文件的默认规则 # 使用资源插件 例子 放置三个文件 进行四个操作，会生成资源文件（src/resources）拷贝到target/classes目录下 如果在java下的包中放资源文件 没有拷贝 即maven只处理src/main/java目录下的.java文件，把这些编译成class，拷贝到target/classes目录中，不处理其他文件 资源插件 # build下\n\u0026lt;build\u0026gt; \u0026lt;!--资源插件--\u0026gt; \u0026lt;resources\u0026gt; \u0026lt;resource\u0026gt; \u0026lt;!--所在的目录--\u0026gt; \u0026lt;directory\u0026gt;src/main/java\u0026lt;/directory\u0026gt; \u0026lt;!--包括properties及xml后缀文件--\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*.properties\u0026lt;/include\u0026gt; \u0026lt;include\u0026gt;**/*.xml\u0026lt;/include\u0026gt; \u0026lt;include\u0026gt;**/*.txt\u0026lt;/include\u0026gt; \u0026lt;include\u0026gt;**/*\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;**/*.java\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;!--不使用过滤器，*.xml已经起到过滤作用了--\u0026gt; \u0026lt;filtering\u0026gt;false\u0026lt;/filtering\u0026gt; \u0026lt;/resource\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/resources\u0026gt; \u0026lt;/build\u0026gt; 结果 "},{"id":185,"href":"/zh/docs/technology/Maven/base_dljd/17-30/","title":"17-30 maven基础_动力节点","section":"基础(动力节点)_","content":" 本地仓库的设置 # 远程仓库\u0026ndash;\u0026gt;本地仓库\nmaven仓库\n存放maven工具自己的jar包 第三方jar，比如mysql驱动 自己写的程序，可以打包为jar，存放到仓库 分类\n本地仓库（本机）：位于自己计算机中，磁盘中某个目录\n默认位置 登录操作系统的账号目录/.m2/repository C:\\Users\\ly.m2\\repository\n可修改 比如放在d盘中\n英[rɪˈpɒzətri] D:\\software\\apache-maven-3.8.6\\repository 备份并编辑 改成左斜杠的方式\n\u0026lt;settings xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.2.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/SETTINGS/1.2.0 https://maven.apache.org/xsd/settings-1.2.0.xsd\u0026#34;\u0026gt; \u0026lt;!-- localRepository | The path to the local repository maven will use to store artifacts. | | Default: ${user.home}/.m2/repository \u0026lt;localRepository\u0026gt;/path/to/local/repo\u0026lt;/localRepository\u0026gt; --\u0026gt; \u0026lt;localRepository\u0026gt;D:/software/apache-maven-3.8.6/repository\u0026lt;/localRepository\u0026gt; 把之前user下的repository的文件都拷贝到 D:/software/apache-maven-3.8.6/repository 下 然后再对Hello项目进行编译 mvn compile 发现不会下载任何文件，且user下的repository也不会再进行下载\n下面的资源是从maven中下载，或者用maven打包的 pom.xml来说明某个项目需要怎么处理代码、项目结构\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.1.9\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/project\u0026gt; mvn命令需要在pom.xml所在的目录下执行 仓库的工作方式 # 生命周期插件命令 # 包括 清理（删除target文件，但是不处理已经install的jar）、编译（当前目录生成target目录，放置编译主程序之后生成的字节码）、测试（生成surefire-reports，保存测试结果）、报告、打包（打包主程序[编译、编译测试、测试，并按照pom.xml配置把主程序打包成jar包或war包]）、安装（把本工程打包，并按照工程坐标保存到本地仓库中）、部署(打包，保存到本地仓库，并保存到私服中，且自动把项目部署到web容器中) 插件：要完成构建项目的各个阶段，要使用maven的命令，执行命令的功能，是通过插件完成的 插件就是jar，一些类 命令：执行maven功能，通过命令发出，比如mvn compile（编译时由相关的类来操作） junit使用 # 单元测试 junit：单元测试的工具，java中经常使用 单元，java中指的是方法，方法就是一个单元，方法是测试的最小单位\n作用，使用junit去测试方法是否完成了要求，开发人员自测\n使用单元测试\n加入junit的依赖（需要用他的类和方法）\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.12\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 在src/test/java目录中创建测试类文件，写测试代码\n测试类的定义，名称一般是Test+要测试的类名称 测试它的包名和要测试的类包名一样 在类中定义方法，要测试的代码 方法定义：public方法，没有返回值，名称自定义（建议Test+测试的方法名称） 方法没有参数 测试类中的方法，可以单独执行，测试类也可以单独执行 在该方法上面加入注解@Test 注意：mvn compile的时候，会下载3.8.2的jar包 创建测试类和测试方法 # package com.bjpowernode; //导入包 import org.junit.Assert; import org.junit.Test; public class TestHelloMaven{ //定义多个独立的测试方法，每个方法都是独立的 public void testAddNumber(){ System.out.println(\u0026#34;执行了测试方法testAddNumber\u0026#34;); HelloMaven hello=new HelloMaven(); int res=hello.addNumber(10,20); //把计算结果res交给junit判断 //期望值，实际值 Assert.assertEquals(30,res); } } 相关命令 # mvn clean ，清理，删除以前生成的数据（删除target目录） 插件及版本 maven-clean-plugin:2.5\nd:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\u0026gt;mvn clean [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ ch01-maven --- [INFO] Deleting d:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 0.354 s [INFO] Finished at: 2022-07-09T17:03:46+08:00 [INFO] ------------------------------------------------------------------------ 代码的编译 mvn compile：编译命令，把src/main/java 目录中的java代码编译为class文件 同时把class文件拷贝到target/classes目录，这个目录classes是存放类文件的根目录（也叫做类路径，classpath）\n编译后放到target\\classes中 插件：maven-compiler-plugin:3.1 编译代码 maven-resources-plugin:2.6:resources 资源插件，作用是把src/main/resources目录中的文件拷贝到target/classes 目录中\nλ mvn compile [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 1 resource [INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ch01-maven --- [INFO] Changes detected - recompiling the module! [WARNING] File encoding has not been set, using platform encoding GBK, i.e. build is platform dependent! [INFO] Compiling 1 source file to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\classes [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 3.164 s [INFO] Finished at: 2022-07-09T17:20:30+08:00 [INFO] ------------------------------------------------------------------------ 测试resources插件 mvn test-compile:编译命令，编译src/test/java 目录中的源文件，把生成的class拷贝到target/test-classes目录中，同时把src/test/resources目录中的文件拷贝到test-classes目录 命令执行前 执行后 插件 maven-resources-plugin:2.6:resources maven-compiler-plugin:3.1:compile maven-resources-plugin:2.6:testResources maven-compiler-plugin:3.1:testCompile\nλ mvn test-compile [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- Downloading from central: https://repo.maven.apache.org/maven2/junit/junit/4.12/junit-4.12.jar Downloading from central: https://repo.maven.apache.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar Downloaded from central: https://repo.maven.apache.org/maven2/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar (45 kB at 24 kB/s) Downloaded from central: https://repo.maven.apache.org/maven2/junit/junit/4.12/junit-4.12.jar (315 kB at 118 kB/s) [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 2 resources [INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ch01-maven --- [INFO] Nothing to compile - all classes are up to date [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 1 resource [INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ ch01-maven --- [INFO] Changes detected - recompiling the module! [WARNING] File encoding has not been set, using platform encoding GBK, i.e. build is platform dependent! [INFO] Compiling 1 source file to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\test-classes [WARNING] /D:/Users/ly/Documents/git/mavenwork/Hello/src/test/java/com/bjpowernode/TestHelloMaven.java:[2,7] 编码GBK的不可映射字符 [WARNING] /D:/Users/ly/Documents/git/mavenwork/Hello/src/test/java/com/bjpowernode/TestHelloMaven.java:[8,42] 编码GBK的不可映射字符 [WARNING] /D:/Users/ly/Documents/git/mavenwork/Hello/src/test/java/com/bjpowernode/TestHelloMaven.java:[14,29] 编码GBK的不可映射字符 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 8.085 s [INFO] Finished at: 2022-07-09T17:28:14+08:00 [INFO] ------------------------------------------------------------------------ mvn test 测试命令，执行test-classes目录的程序，测试src/main/java目录中的主程序是否符合要求 注意，这里还是会用到编译插件和资源插件，从 T E S T S 开始测试 结果Results :\nTests run: 1, Failures: 0, Errors: 0, Skipped: 0 测试插件 maven-surefire-plugin:2.12.4\nλ mvn test [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 2 resources [INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ch01-maven --- [INFO] Nothing to compile - all classes are up to date [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 1 resource [INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ ch01-maven --- [INFO] Changes detected - recompiling the module! [WARNING] File encoding has not been set, using platform encoding GBK, i.e. build is platform dependent! [INFO] Compiling 1 source file to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\test-classes [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ ch01-maven --- [INFO] Surefire report directory: D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\surefire-reports ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.bjpowernode.TestHelloMaven 执行了测试方法testAddNumber hello maven -addNumber Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.131 sec Results : Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 4.630 s [INFO] Finished at: 2022-07-09T17:32:49+08:00 [INFO] ------------------------------------------------------------------------ 测试报告 测试失败的情况 结果\nλ mvn test [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 2 resources [INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ch01-maven --- [INFO] Nothing to compile - all classes are up to date [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent![INFO] Copying 1 resource [INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ ch01-maven --- [INFO] Changes detected - recompiling the module! [WARNING] File encoding has not been set, using platform encoding GBK, i.e. build is platform dependent! [INFO] Compiling 1 source file to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\test-classes [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ ch01-maven --- [INFO] Surefire report directory: D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\surefire-reports ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.bjpowernode.TestHelloMaven 执行了测试方法testAddNumber hello maven -addNumber Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.217 sec \u0026lt;\u0026lt;\u0026lt; FAILURE! testAddNumber(com.bjpowernode.TestHelloMaven) Time elapsed: 0.043 sec \u0026lt;\u0026lt;\u0026lt; FAILURE! java.lang.AssertionError: expected:\u0026lt;60\u0026gt; but was:\u0026lt;30\u0026gt; at org.junit.Assert.fail(Assert.java:88) at org.junit.Assert.failNotEquals(Assert.java:834) at org.junit.Assert.assertEquals(Assert.java:645) at org.junit.Assert.assertEquals(Assert.java:631) at com.bjpowernode.TestHelloMaven.testAddNumber(TestHelloMaven.java:15) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:252) at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:141) at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:112) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:189) at org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:165) at org.apache.maven.surefire.booter.ProviderFactory.invokeProvider(ProviderFactory.java:85) at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:115) at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:75) Results : Failed tests: testAddNumber(com.bjpowernode.TestHelloMaven): expected:\u0026lt;60\u0026gt; but was:\u0026lt;30\u0026gt; Tests run: 1, Failures: 1, Errors: 0, Skipped: 0 [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time: 8.018 s [INFO] Finished at: 2022-07-09T17:35:38+08:00 [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal org.apache.maven.plugins:maven-surefire-plugin:2.12.4:test (default-test) on project ch01-maven: There are test failures. [ERROR] [ERROR] Please refer to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\surefire-reports for the individual test results. [ERROR] -\u0026gt; [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureException mvn package 打包，作用是把项目中的资源class文件和配置文件，都放到一个压缩包中，默认压缩文件是jar类型，web应用是war类型，扩展名jar/war 这里进行了编译、测试、打包 [INFO] Building jar: D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\ch01-maven-1.0-SNAPSHOT.jar 打包插件 maven-jar-plugin:2.4:jar (default-jar) @ ch01-maven maven-jar-plugin:2.4用来执行打包，会生成jar扩展名文件\nλ mvn package [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent! [INFO] Copying 2 resources [INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ch01-maven --- [INFO] Changes detected - recompiling the module! [WARNING] File encoding has not been set, using platform encoding GBK, i.e. build is platform dependent! [INFO] Compiling 1 source file to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\classes [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent! [INFO] Copying 1 resource [INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ ch01-maven --- [INFO] Changes detected - recompiling the module! [WARNING] File encoding has not been set, using platform encoding GBK, i.e. build is platform dependent! [INFO] Compiling 1 source file to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\test-classes [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ ch01-maven --- [INFO] Surefire report directory: D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\surefire-reports ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.bjpowernode.TestHelloMaven 执行了测试方法testAddNumber hello maven -addNumber Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.135 sec Results : Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ ch01-maven --- [INFO] Building jar: D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\ch01-maven-1.0-SNAPSHOT.jar [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 5.624 s [INFO] Finished at: 2022-07-09T17:40:44+08:00 [INFO] ------------------------------------------------------------------------ 生成ch01-maven-1.0-SNAPSHOT.jar 坐标\n\u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; 打包的文件名 artifactId-version.packaging 查看jar 打包的文件中，包括src/main目录中所有的生成的class文件和配置文件（resources下），和测试test无关\nmvn install 把生成的打包文件（jar）安装到maven仓库中 插件：maven-install-plugin-2.4\nλ mvn install [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent! [INFO] Copying 2 resources [INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ch01-maven --- [INFO] Nothing to compile - all classes are up to date [INFO] [INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent! [INFO] Copying 1 resource [INFO] [INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ ch01-maven --- [INFO] Nothing to compile - all classes are up to date [INFO] [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ ch01-maven --- [INFO] Surefire report directory: D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\surefire-reports ------------------------------------------------------- T E S T S ------------------------------------------------------- Running com.bjpowernode.TestHelloMaven 执行了测试方法testAddNumber hello maven -addNumber Tests run: 1, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.162 sec Results : Tests run: 1, Failures: 0, Errors: 0, Skipped: 0 [INFO] [INFO] --- maven-jar-plugin:2.4:jar (default-jar) @ ch01-maven --- [INFO] [INFO] --- maven-install-plugin:2.4:install (default-install) @ ch01-maven --- [INFO] Installing D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\ch01-maven-1.0-SNAPSHOT.jar to D:\\software\\apache-maven-3.8.6\\repository\\com\\bjpowernode\\ch01-maven\\1.0-SNAPSHOT\\ch01-maven-1.0-SNAPSHOT.jar [INFO] Installing D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\pom.xml to D:\\software\\apache-maven-3.8.6\\repository\\com\\bjpowernode\\ch01-maven\\1.0-SNAPSHOT\\ch01-maven-1.0-SNAPSHOT.pom [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 5.063 s [INFO] Finished at: 2022-07-09T17:48:43+08:00 [INFO] ------------------------------------------------------------------------ 如上，\nInstalling D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\ch01-maven-1.0-SNAPSHOT.jar to D:\\software\\apache-maven-3.8.6\\repository\\com\\bjpowernode\\ch01-maven\\1.0-SNAPSHOT\\ch01-maven-1.0-SNAPSHOT.jar 路径 com\\bjpowernode\\ch01-maven\\1.0-SNAPSHOT ，如下，跟坐标有关\n\u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;!--groupId出现点，则使用\\（文件夹）分割 artifactId 独立文件夹 version 独立文件夹 --\u0026gt; 结果 部署 mvn deploy 部署主程序（把本工程打包，按照本工程的坐标保存到本地仓库中，并且保存到私服仓库中，还会自动把项目部署到web容器中\n以上命令是可以组合着用的\nλ mvn clean compile [INFO] Scanning for projects... [INFO] [INFO] ---------------------\u0026lt; com.bjpowernode:ch01-maven \u0026gt;--------------------- [INFO] Building ch01-maven 1.0-SNAPSHOT [INFO] --------------------------------[ jar ]--------------------------------- [INFO] [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ ch01-maven --- [INFO] Deleting D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target [INFO] [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ ch01-maven --- [WARNING] Using platform encoding (GBK actually) to copy filtered resources, i.e. build is platform dependent! [INFO] Copying 2 resources [INFO] [INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ ch01-maven --- [INFO] Changes detected - recompiling the module! [WARNING] File encoding has not been set, using platform encoding GBK, i.e. build is platform dependent! [INFO] Compiling 1 source file to D:\\Users\\ly\\Documents\\git\\mavenwork\\Hello\\target\\classes [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 2.725 s [INFO] Finished at: 2022-07-09T17:53:36+08:00 [INFO] ------------------------------------------------------------------------ 配置插件 # 常用插件设置\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;source\u0026gt;1.8\u0026lt;/source\u0026gt; \u0026lt;target\u0026gt;1.8\u0026lt;/target\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; 先看一下，目前的版本 maven-compiler-plugin:3.1:compile "},{"id":186,"href":"/zh/docs/technology/Maven/base_dljd/01-16/","title":"01-16 maven基础_动力节点","section":"基础(动力节点)_","content":" 课程介绍 # maven 自动化构建\u0026ndash;\u0026gt;开发\u0026ndash;编译\u0026ndash;运行-测试\u0026ndash;打包\u0026ndash;部署 （m ei \u0026rsquo; ven） maven的作用 # 软件是一个工程 软件中重复的操作（开发阶段） 需求分析 设计阶段 开发阶段（编码），编译，测试 测试阶段（专业测试），测试报告 项目打包，发布，给客户安装项目 maven 项目自动构建，清理、编译、测试、打包、安装、部署 管理依赖：项目中需要使用的其他资源 Maven中的概念 # 没有使用maven，管理jar，手动处理jar，以及jar之间的依赖 maven是apache 【əˈpætʃi】基金会的开源项目，使用java语法开发 maven是项目的自动化构建工具，管理项目依赖 maven中的概念 POM 约定的目录 坐标 依赖管理 仓库管理 生命周期 插件和目标 继承 （高级内容） 聚合 （高级内容） Maven资源的获取与安装，测试 # https://maven.apache.org/index.html\n各种内容 要求 视频用的3.6.3 ，这里下载3.8.6 （最新的，不要和电脑原配置冲突，方便学习，后续改回3.8.4）\n检查java home 如果没有需要进行配置 将maven的bin目录配置到path环境变量下（这里使用的是下一节的方法，视频中没有用MAVEN_HOME，而是直接将maven的bin目录路径加到path中） maven解压后的目录结构 另一种安装方式 # 确定JAVA_HOME是否有效 创建M2_HOME(MAVEN_HOME)，值为maven的安装目录 在path环境中，加入%M2_HOME%\\bin 测试maven安装 mvn -v 约定的目录结构 # 大多数人遵守的目录结构\n一个maven项目对应一个文件夹，比如Hello\nHello \\src \\main\t叫做主程序目录（完成项目功能的代码和配置文件） \\java\t源代码（包和相关的类定义） \\resources 配置文件 \\test\t放置测试程序代码（开发人员自己写的测试代码） \\java\t测试代码（junit） \\resources 测试程序的配置文件 \\pom.xml\tmaven的配置文件 Hello的Maven项目 # maven可以独立使用：创建项目、编译代码、测试程序、打包、部署等\n和idea一起使用，实现编码、测试、打包\npom.xml基本模板\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/project\u0026gt; 目录创建 在main下创建一个com.bjpowernode的包，以及一个java文件\npackage com.bjpowernode; public class HelloMaven{ public int addNumber(int n1,int n2){ System.out.println(\u0026#34;hello maven -addNumber\u0026#34;); return n1+n2; } public static void main(String args[]){ HelloMaven hello=new HelloMaven(); int res=hello.addNumber(10,20); System.out.println(\u0026#34;在main方法中，执行hello的方法=\u0026#34;+res); } } 在Hello目录下，进行编译 使用mvn compile进行编译 第一次会下载一些东西 查看target文件 进入classes执行java程序\njava com.bjpowernode.HelloMaven pom-modelVersion # pom\u0026ndash;Project Object Model 项目对象模型\nMaven把一个项目的结构和内容抽象成一个模型，在xml文件中进行声明，以方便进行构建和描述\npom文件解释\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!--project是根标签，后面的是约束文件 (maven-v4_0_0.xsd)--\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;!--pom模型版本,4.0.0--\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;!--坐标--\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;/project\u0026gt; pom-groupId，artifactId，version # 坐标组成，groupid,artifactId,version 作用：资源的唯一标识，maven中每个资源都是坐标，简称gav groupId：组织名称，代码。公司或单位标识，常使用公司域名的倒写 如果规模大，可以是 域名倒写+大项目名称 例如百度无人车项目 ： com.baidu.appollo artifactId：项目名称，如果groupId中有项目，此时当前的值就是子项目名，项目名称是唯一的 versionId：项目版本号，使用数字，推荐三位 例如 主版本号.次版本号.小版本号 例如 5.2.5 带快照的版本，以-SNAPSHOT结尾，即非稳定版本 pom-gav作用 # 每个maven项目都有自己的gav 管理依赖，使用其他jar包，也用gav标识 坐标 坐标值的获取 https://mvnrepository.com/ 例如mysql pom-依赖的使用 # 依赖dependency 项目中使用的其他资源（jar） 需要使用maven来表示依赖、管理依赖，通过使用dependencies、dependency和gav完成依赖的使用\n\u0026lt;dependencies\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.29\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!--maven使用gav标识，从互联网下载依赖的jar，下载到本机中，由maven管理项目使用的这些jar--\u0026gt; 完整\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!--project是根标签，后面的是约束文件 (maven-v4_0_0.xsd)--\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;!--pom模型版本,4.0.0--\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;!--坐标--\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.29\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;!--maven使用gav标识，从互联网下载依赖的jar，下载到本机中，由maven管理项目使用的这些jar--\u0026gt; \u0026lt;!--packaging 项目打包类型---\u0026gt; \u0026lt;/project\u0026gt; pom-打包类型 # \u0026lt;packaging\u0026gt; 项目打包类型\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!--project是根标签，后面的是约束文件 (maven-v4_0_0.xsd)--\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\u0026#34;\u0026gt; \u0026lt;!--pom模型版本,4.0.0--\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;!--坐标--\u0026gt; \u0026lt;groupId\u0026gt;com.bjpowernode\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;ch01-maven\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;/project\u0026gt; 其他 pom-继承和聚合 # 继承 parent 聚合 modules "},{"id":187,"href":"/zh/docs/technology/Linux/hanshunping/52-x/","title":"52-X","section":"韩顺平老师_","content":" crond快速入门 # 使用命令 crontab -e 创建一个定时任务\n*/1 * * * * ls -l /etc/ \u0026gt; /tmp/to.txt 特殊符号 ，代表不连续 -破折号 表示连续 其他 定时调用脚本\n编辑脚本 my.sh\ndate \u0026gt;\u0026gt; /home/mycal date \u0026gt;\u0026gt; /home/mycal 给脚本赋予x权限\nchmod u+x my.sh crontab -e\n*/1 * * * * my.sh 数据库备份 crontab -r 删除\ncrontab -l 列出\ncrontab -e 编辑任务\natd 是否在运行 yum install -y atd systemctl start atd\njob队列 at选项 at指定时间 添加任务 at 5pm tomorrow 明天下午5点\nat now + 2 minutes 2分钟后\natrm 5 删除5号\n两分钟后执行某个脚本 磁盘分区 # 分区跟文件系统的关系 (挂载) 将一个分区挂载到某个目录，用户进入到某个目录，就相当于访问到某个分区了 lsblk linux分IDE硬盘和SCSI硬盘 目前基本是SCSI硬盘 sdx~ x代表abcd，~表示数字 lsblk -f 文件类型，唯一标识符 现在挂载一个分区 如图 给虚拟机，添加一个硬盘 重启后，使用lsblk 进行分区 fdisk /dev/sdb 之后输入p， 输入分区数（这里是1） 最后一步，输入w ，写入分区并退出 查看 将分区格式化 mkfs -t ext4 /dev/sdb1 查看 进行挂载 mount /dev/sdb1 /newdisk/ umount /dev/sdb1 卸载 用命令行挂载的指令，重启后挂载关系会消失 永久挂载：修改/etc/fstab # df -h 查看磁盘使用情况 du -h \u0026ndash;max-depth=1 /opt ls -l /opt | grep \u0026ldquo;^-\u0026rdquo; | wc -l 使用正则，并统计数量 ls -lR /opt 注意，这里加了R，将递归显示 使用yum install -y tree 网络配置 # ifconfig 查看ip\n网络的互通 虚拟网络编辑器 使用ping判断主机间是否互通\nvi /etc/sysconfig/network-scripts/ifcfg-ens33 编辑ip\nTYPE=\u0026#34;Ethernet\u0026#34; PROXY_METHOD=\u0026#34;none\u0026#34; BROWSER_ONLY=\u0026#34;no\u0026#34; DEFROUTE=\u0026#34;yes\u0026#34; IPV4_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6INIT=\u0026#34;yes\u0026#34; IPV6_AUTOCONF=\u0026#34;yes\u0026#34; IPV6_DEFROUTE=\u0026#34;yes\u0026#34; IPV6_FAILURE_FATAL=\u0026#34;no\u0026#34; IPV6_ADDR_GEN_MODE=\u0026#34;stable-privacy\u0026#34; NAME=\u0026#34;ens33\u0026#34; UUID=\u0026#34;8c2741af-382a-44a6-b161-aed16a29875d\u0026#34; DEVICE=\u0026#34;ens33\u0026#34; BOOTPROTO=\u0026#34;static\u0026#34; ONBOOT=\u0026#34;yes\u0026#34; IPADDR=192.168.200.160 GATEWAY=192.168.200.2 DNS1=192.168.200.2 注意最后五行 修改hostname vim /etc/hostname\n进程 # 每一个执行的程序被称为一个进程，每一个进程都分配一个ID号- 每个进程都可以以前台/后台方式运行 一半系统服务以后台进程方式存在的 使用ps显示进程 ps -aux 一些参数解释 使用grep过滤 进程的父进程 ps -ef 由systemd生成启动其他进程 子进程之间关系 进程的终止 kill / killall killall 将子进程一起杀死 kill -9 强制终止 如果把sshd杀死，那就再也连不上了 重新启动sshd /bin/systemctl start sshd.service yum -y install psmisc pstree -u 带上用户 pstree -p 带上进程号 服务管理 # 服务，本质上就是进程 service 服务名 start|stop|restart|reload|status centos7.0之后，主要用systemctl 还使用service的命令 网络连接查看 服务的运行级别 systemctl set-default graphical.target //默认进入图形化界面 rpm管理 # 软件包管理 # "},{"id":188,"href":"/zh/docs/technology/Linux/hanshunping/40-51/","title":"linux_韩老师_40-51","section":"韩顺平老师_","content":" 组介绍 # 每个用户必定属于某个组 每个文件有几个概念：所有者、所在组、其他组 tom创建了hello.txt，则所有者为tom，默认所在组为tom组 除了所在组，就是其他组 ls -ahl （h更友好，a隐藏，l列表） 所有者 # 使用chown root helo.java 修改，效果如下 所在组修改 # 组的创建 groupadd monster 创建一个用户并让他属于该组 useradd -g monster fox 注意逻辑，此时使用fox创建文件 passwd fox 给fox创建密码 如图，创建一个文件 使用chgrp fruit orange.txt 修改文件的所在组 改变某个用户所在组 usermod -g fruit fox 使用 cat /etc/group 查看所有的组 当一个用户属于多个组的时候，groups会出现多个组名 rwx权限 # rwxrwxrwx 第一列有十位，第0位确认文件类型 -普通文件，l是链接；d是目录；c是字符设备文件、鼠标、键盘；b块设备 1-3表示文件所有者拥有的权限；4-6是文件所在组所拥有的权限，7-9 其他组所拥有的权限\nrwx作用到文件，r代表可读可查看，w代表可修改（如果是删除权限，则必须在该文件所在的目录有写权限，才能删除），x代表可执行 rwx作用到目录，r表示可以读取(ls可查看目录内容)，w表示可写（可以在目录内创建、删除、重命名目录），x表示可以进入该目录 rwx分别用数字表示，4，2，1。当拥有所有权限，则为7 最后面的数字，代表连接数（或者子目录数） 1213 文件大小（字节），如果是文件夹则显示4096 最后abc表示文件名，蓝色表示是目录 修改权限 # chmod 修改权限，u：所有者，g：所有组，o：其他人，a 所有（ugo总和） chmod u=rwx,g=rw,o=x 文件/目录名 这里等号表示直接给权限 chmod o+w 文件/目录名 这里加表示+权限 chmod a-x 文件/目录名 chmod u=rwx,g=rx,o=rx abc 给文件添加执行权限（会变成绿色的） 使用数字 将abc.txt文件权限修改成rwxr-xr-x使用数字实现 chmod 755 abc 修改所有者和所在组 # chown tom abc #修改文件所有者为tom chown -R tom abc #修改文件夹及其所有子目录所有者为tom chgrp -R fruit kkk #修改文件夹所在组为fruit 权限管理应用实例 # 警察和土匪的游戏\n前提，有police和bandit两个组，\njack，jerry属于警察组\nxh，xq属于土匪组\ngroupadd police groupadd bandit useradd -g police jack useradd -g police jerry useradd -g bandit xh useradd -g bandit xq chmod 640 jack.txt\nchmod o=r,g=rw jack.txt\n如果要对目录内操作，那么先有改目录相应权限\nchmod 770 jack 放开jack目录权限 题目 对一个目录不能ls(没有读权限)，但是是可以直接读写目录中的文件的（有权限的情况下）\n# "},{"id":189,"href":"/zh/docs/technology/MySQL/bl_sgg/96-00/","title":"mysql高阶_sgg 96-00","section":"进阶(尚硅谷)_","content":" 章节概述 # 架构篇\n1-3 4 5 6 索引及调优篇\n01 02-03\n04-05\n06 事务篇\n01-02 03 04 日志与备份篇\n01 02 03 CentOS环境准备 # 这里主要是做了克隆，并没有讲到CentOS的安装，所以笔记不记录了 MySQL的卸载 # 查找当前系统已经装了哪些 rpm -qa |grep mysql\n查找mysql服务运行状态 systemctl status mysql\n停止mysql服务 systemctl stop mysql\n删除\nyum remove mysql-community-client-plugins-8.0.29-1.el7.x86_64 yum remove mysql-community-common-8.0.29-1.el7.x86_64 查找带mysql名字的文件夹 find / -name mysql\n进行删除\nrm -rf /usr/lib64/mysql rm -rf /usr/share/mysql rm -rf /etc/selinux/targeted/active/modules/100/mysql rm -rf /etc/my.cnf Linux下安装MySQL8.0与5.7版本 # 版本介绍 下载地址 : https://www.mysql.com/downloads/ 进入 即 https://dev.mysql.com/downloads/mysql/ 版本选择 下载最大的那个，离线版 下载后解压，并将下面六个放进linux中\n如果是5.7，则需要进入 https://downloads.mysql.com/archives/community/\n下载后解压 拷贝进linux 安装前，给/tmp临时目录权限\nchmod -R 777 /tmp\n检查依赖\nrpm -qa |grep libaio ##libaio-0.3.109-13.el7.x86_64 rpm -qa |grep net-tools ##net-tools-2.0-0.24.20131004git.el7.x86_64 确保目录下已经存在5（4）个文件并严格按顺序执行\nrpm -ivh mysql-community-common-8.0.29-1.el7.x86_64.rpm rpm -ivh mysql-community-client-plugins-8.0.29-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-8.0.29-1.el7.x86_64.rpm rpm -ivh mysql-community-client-8.0.29-1.el7.x86_64.rpm rpm -ivh mysql-community-icu-data-files-8.0.29-1.el7.x86_64.rpm rpm -ivh mysql-community-server-8.0.29-1.el7.x86_64.rpm 安装libs的时候，会报错\nerror: Failed dependencies: mariadb-libs is obsoleted by mysql-community-libs-8.0.29-1.el7.x86_64 使用下面命令，视频的方法\nyum remove mysql-libs 使用下面命令，卸载mariadb (这是我自己的方法)\nrpm -qa | grep mariadb\n查找到对应的版本 mariadb-libs-5.5.60-1.el7_5.x86_64 # 下面卸载查找出来的版本 # yum remove mariadb-libs-5.5.60-1.el7_5.x86_64 # 再次执行后安装成功\n服务初始化 mysqld --initialize --user=mysql\n查看默认生成的密码 cat /var/log/mysqld.log 判断mysql是否启动 systemctl status mysqld\n启动服务systemctl start mysqld 再次判断，发现已经启动 设置为自动启动\n查看当前是否开机自启动 systemctl list-unit-files|grep mysqld.service 如果是disable，则可以使用下面命令开机自启动 systemctl enable mysqld.service 进行登录\nmysql -u root -p 用刚才的密码\n使用查询，提示需要重置密码 密码更新\nalter user \u0026#39;root\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;123456\u0026#39;; quit # 退出重新登录 5.7的安装 赋予权限并检查包，这里发现缺少了libaio，所以yum install libaio\nSQLyog实现MySQL8.0和5.7的远程连接 # sqlyog下载 https://github.com/webyog/sqlyog-community/wiki/Downloads\n默认情况下会有连接出错 先测试ip及端口号 此时linux端口号并没有开放 使用systemctl status firewalld发现防火墙开启 （active) 使用systemctl stop firewalld将防火墙关闭 开机时关闭防火墙systemctl disable firewalld 此时还是报错 这是由于root不允许被远程连接\n查看user表，发现只允许本地登录 修改并更新权限\nupdate user set host = \u0026#39;192.168.1.%\u0026#39; where user= \u0026#39;root\u0026#39;; #或者 update user set host = \u0026#39;%\u0026#39; where user= \u0026#39;root\u0026#39;; #更新权限 flush privileges; 之后如果出现下面的问题（视频中有，我没遇到） ALTER USER \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED WITH mysql_native_password BY \u0026#39;123456\u0026#39;; 然后就可以连接了 命令行进行远程连接 mysql -u root -h 192.168.200.150 -P3306 -p\n字符集的修改与底层原理说明 # 比较规则_请求到响应过程中的编码与解码过程 # SQL大小写规范与sql_model的设置 # "},{"id":190,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/3.2.1/","title":"算法红皮书 3.2.1","section":"_算法(第四版)_","content":" 二叉查找树 # 使用每个结点含有两个链接（链表中每个结点只含有一个链接）的二叉查找树来高效地实现符号表\n该数据结构由结点组成，结点包含的链接可以为空(null)或者指向其他结点\n一棵二叉查找树（BST）是一棵二叉树，其中每个结点都含有一个Comparable 的键（以 及相关联的值）且每个结点的键都大于其左子树中的任意结点的键而小于右子树的任意结点的键。\n基本实现 # 数据表示\n每个结点都含有一个键、一个值、一条左链接、一条右链接和一个结点计数器 左链接指向一棵由小于该结点的所有键组成的二叉查找树，右链接指向一棵由大于该节点的所有键组成的二叉查找树，变量N给出了以该结点为根的子树的结点总数 对于任意节点总是成立 size(x)=size(x.left)+size(x.right)+1 多棵二叉查找树表示同一组有序的键来实现构建和使用二叉查找树的高校算法 查找\n在符号表中查找一个键可能得到两种结果：如果含有该键的结点存在表中，我们的查找就命中了，然后返回值；否则查找未命中（返回null) 递归：如果树是空的，则查找未命中；如果被查找的键和根节点的键相等，查找命中，否则在适当的子树中查找：如果被查找的键较小就选择左子树，否则选择右子树 下面的get()方法，第一个参数是一个结点（子树根节点），第二个参数是被查找的键，代码会保证只有该结点所表示的子树才会含有和被查找的键相等的结点 从根结点开始，在每个结点中查找的进程都会递归地在它的一个子结点上展开，因此一次查找也就定义了树的一条路径。对于命中的查找，路径在含有被查找的键的结点处结束。对于未命中的查找，路径的终点是一个空链接 基于二叉查找树的符号表\npublic class BST\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;, Value\u0026gt; { private Node root; // 二叉查找树的根结点 private class Node { private Key key; // 键 private Value val; // 值 private Node left, right; // 指向子树的链接 private int N; // 以该结点为根的子树中的结点总数 public Node(Key key, Value val, int N) { this.key = key; this.val = val; this.N = N; } } public int size() { return size(root); } private int size(Node x) { if (x == null) return 0; else return x.N; } public Value get(Key key) // 请见算法3.3（续1） public void put(Key key, Value val) // 请见算法3.3（续1） // max()、min()、floor()、ceiling()方法请见算法3.3（续2） // select()、rank()方法请见算法3.3（续3） // delete()、deleteMin()、deleteMax()方法请见算法3.3（续4） // keys()方法请见算法3.3（续5） } 每个Node 对象都是一棵含有N 个结点的子树的根结点，它的左链接指向一棵由小于该结点的所有键组成的二叉查找树，右链接指向一棵由大于该结点的所有键组成的二叉查找 树。root 变量指向二叉查找树的根结点Node 对象（这棵树包含了符号表中的所有键值对） 二叉查找树的查找和排序方法的实现\npublic Value get(Key key) { return get(root, key); } private Value get(Node x, Key key) { // 在以x为根结点的子树中查找并返回key所对应的值； // 如果找不到则返回null if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp \u0026lt; 0) return get(x.left, key); else if (cmp \u0026gt; 0) return get(x.right, key); else return x.val; } public void put(Key key, Value val) { // 查找key，找到则更新它的值，否则为它创建一个新的结点 root = put(root, key, val); } private Node put(Node x, Key key, Value val) { // 如果key存在于以x为根结点的子树中则更新它的值； // 否则将以key和val为键值对的新结点插入到该子树中 if (x == null) return new Node(key, val, 1); int cmp = key.compareTo(x.key); //注意，这里进行比较后，确认新节点应该放在当前节点的左边还是右边 if (cmp \u0026lt; 0) x.left = put(x.left, key, val); else if (cmp \u0026gt; 0) x.right = put(x.right, key, val); else x.val = val; x.N = size(x.left) + size(x.right) + 1; return x; } 插入 put()方法的实现逻辑和递归查找很相似：如果树是空的，就返回一个含有该键值对的新节点；如果被查找的键小于根节点的键，我们就会继续在左子树中插入该键，否则在右子树中插入该键\n递归\n可以将递归调用前的代码想象成沿着树向下走：它会将给定的键和每个结点的键相比较并根据结果向左或者向右移动到下一个结点。然后可以将递归调用后的代码想象成沿着树向上爬 在一棵简单的二叉查找树中，唯一的新链接就是在最底层指向新结点的链接，重置更上层的链接可以通过比较语句来避免。同样，我们只需要将路径上每个结点中的计数器的值加1，但我们使用了更加通用的代码，使之等于结点的所有子结点的计数器之和加1 使用二叉查找树的标准索引用例的轨迹 分析 # 在由N 个随机键构造的二叉查找树中，查找命中平均所需的比较次数为∼ 2lnN\n在由N 个随机键构造的二叉查找树中插入操作和查找未命中平均所需的比较次数为∼ 2lnN（约1.39lgN）\n有序性相关的方法与删除操作 # 最大键和最小键 # 如果根结点的左链接为空，那么一棵二叉查找树中最小的键就是根结点；如果左链接非空，那么 树中的最小键就是左子树中的最小键\n向上取整和向下取整 # 如果给定的键key 小于二叉查找树的根结点的键，那么小于等于key 的最大键floor(key) 一定 在根结点的左子树中；如果给定的键key 大于二叉查找树的根结点，那么只有当根结点右子树中存在小于等于key 的结点时，小于等于key 的最大键才会出现在右子树中，否则根结点就是小于等于key的最大键\n选择操作 # public Key min() { return min(root).key; } private Node min(Node x) { if (x.left == null) return x; return min(x.left); } public Key floor(Key key) { Node x = floor(root, key); if (x == null) return null; return x.key; } private Node floor(Node x, Key key) { if (x == null) return null; int cmp = key.compareTo(x.key); if (cmp == 0) return x; if (cmp \u0026lt; 0) return floor(x.left, key); Node t = floor(x.right, key); if (t != null) return t; else return x; } 排名 # 删除最大键和删除最小键 # 删除操作 # 范围查找 # 性能分析 # "},{"id":191,"href":"/zh/docs/technology/MyBatis-Plus/bl_sgg/40-57/","title":"mybatis-plus-sgg-40-57","section":"基础(尚硅谷)_","content":" LambdaXxxWrapper # LambdaQueryWrapper主要是为了防止字段名写错\n@Test public void test11(){ String username=\u0026#34;abc\u0026#34;; Integer ageBegin=null; Integer ageEnd=30; LambdaQueryWrapper\u0026lt;User\u0026gt; queryWrapper=new LambdaQueryWrapper\u0026lt;\u0026gt;(); queryWrapper.like(StringUtils.isNotBlank(username),User::getUserName,username) .ge(ageBegin!=null,User::getAge,ageBegin); userMapper.selectList(queryWrapper); } sql日志打印\n==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE is_deleted_ly=0 AND (name LIKE ?) ==\u0026gt; Parameters: %abc%(String) \u0026lt;== Total: 0 LambdaUpdateWrapper\n@Test public void test12() { //(age\u0026gt;23且用户名包含a) 或 (邮箱为null) LambdaUpdateWrapper\u0026lt;User\u0026gt; updateWrapper = new LambdaUpdateWrapper\u0026lt;\u0026gt;(); updateWrapper.like(User::getUserName, \u0026#34;a\u0026#34;) .and(userUpdateWrapper -\u0026gt; userUpdateWrapper.gt(User::getAge, 23).or().isNotNull(User::getEmail)); updateWrapper.set(User::getUserName, \u0026#34;小黑\u0026#34;).set(User::getEmail, \u0026#34;abc@ly.com\u0026#34;); userMapper.update(null, updateWrapper); } sql日志打印\n==\u0026gt; Preparing: UPDATE t_user SET name=?,email=? WHERE is_deleted_ly=0 AND (name LIKE ? AND (age \u0026gt; ? OR email IS NOT NULL)) ==\u0026gt; Parameters: 小黑(String), abc@ly.com(String), %a%(String), 23(Integer) \u0026lt;== Updates: 0 MyBatis分页 # 先使用配置类\n@Configuration @MapperScan(\u0026#34;com.ly.mybatisplus.mapper\u0026#34;) public class MyBatisConfig { @Bean public MybatisPlusInterceptor mybatisPlusInterceptor(){ MybatisPlusInterceptor mybatisPlusInterceptor=new MybatisPlusInterceptor(); mybatisPlusInterceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL)); return mybatisPlusInterceptor; } } 使用\n@Test public void testPage() { Page\u0026lt;User\u0026gt; page = new Page\u0026lt;\u0026gt;(); page.setCurrent(2);//当前页页码 page.setSize(3);//每页条数 Page\u0026lt;User\u0026gt; userPage = userMapper.selectPage(page, null); System.out.println(userPage.getRecords() + \u0026#34;----\\n\u0026#34; + userPage.getPages() + \u0026#34;----\\n\u0026#34; + userPage.getTotal() + \u0026#34;---\\n\u0026#34;) ; } sql日志打印\n==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE is_deleted_ly=0 LIMIT ?,? ==\u0026gt; Parameters: 3(Long), 3(Long) \u0026lt;== Columns: id, userName, age, email, is_deleted_ly \u0026lt;== Row: 4, 被修改了, 21, test4@baomidou.com, 0 \u0026lt;== Row: 5, 被修改了, 24, email被修改了, 0 \u0026lt;== Row: 6, 张三5, 18, test5@baomidou.com, 0 \u0026lt;== Total: 3 结果Page对象的数据\n[User(id=4, userName=被修改了, age=21, email=test4@baomidou.com, isDeletedLy=0), User(id=5, userName=被修改了, age=24, email=email被修改了, isDeletedLy=0), User(id=6, userName=张三5, age=18, email=test5@baomidou.com, isDeletedLy=0)]---- 3---- 8--- 自定义分页功能\n首先，设置类型别名所在的包\nmybatis-plus: type-aliases-package: com.ly.mybatisplus.pojo 在Mapper类中编写接口方法\n@Repository public interface UserMapper extends BaseMapper\u0026lt;User\u0026gt; { /** * 通过年龄查询并分页 * @param page mybatis-plus提供的，必须存在且在第一个位置 * @param age * @return */ Page\u0026lt;User\u0026gt; selectPageVO(Page\u0026lt;User\u0026gt; page,Integer age); } 注意第一个参数\n在Mapper.xml中编写语句\n\u0026lt;select id=\u0026#34;selectPageVO\u0026#34; resultType=\u0026#34;User\u0026#34;\u0026gt; select uid,name,email from t_user where age \u0026gt; #{age} \u0026lt;/select\u0026gt; 测试方法\n@Test public void testPageCustom() { Page\u0026lt;User\u0026gt; page = new Page\u0026lt;\u0026gt;(); page.setCurrent(3);//当前页页码 page.setSize(5);//每页条数 Page\u0026lt;User\u0026gt; userPage = userMapper.selectPageVO(page, 12); System.out.println(userPage.getRecords() + \u0026#34;----\\n\u0026#34; + userPage.getPages() + \u0026#34;----\\n\u0026#34; + userPage.getTotal() + \u0026#34;---\\n\u0026#34;) ; } sql日志输出\n==\u0026gt; Preparing: SELECT COUNT(*) AS total FROM t_user WHERE age \u0026gt; ? ==\u0026gt; Parameters: 12(Integer) \u0026lt;== Columns: total \u0026lt;== Row: 20 \u0026lt;== Total: 1 //从第10行开始（不包括第10行），取5条记录 ==\u0026gt; Preparing: select uid,name,email from t_user where age \u0026gt; ? LIMIT ?,? ==\u0026gt; Parameters: 12(Integer), 10(Long), 5(Long) \u0026lt;== Columns: uid, name, email \u0026lt;== Row: 11, a, null \u0026lt;== Row: 12, a, null \u0026lt;== Row: 13, a, null \u0026lt;== Row: 14, a, null \u0026lt;== Row: 15, a, null \u0026lt;== Total: 5 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@706fe5c6] [null, null, null, null, null]---- 4---- 20--- 注意上面那个sql，他会先查询条数，如果条数\u0026lt;=0，那么就不会执行下面的数据搜索了\n悲观锁和乐观锁 # 场景 乐观锁根据版本号使用 version\n乐观锁实现流程 模拟冲突 # 表创建\nCREATE TABLE t_product ( id BIGINT ( 20 ) NOT NULL COMMENT \u0026#39;主键id\u0026#39;, NAME VARCHAR ( 30 ) null DEFAULT NULL COMMENT \u0026#39;商品名称\u0026#39;, price INT ( 11 ) DEFAULT 0 COMMENT \u0026#39;价格\u0026#39;, version INT ( 11 ) DEFAULT 0 COMMENT \u0026#39;乐观锁版本号\u0026#39;, PRIMARY KEY ( id ) ) 创建ProductMapper\n@Repository public interface ProductMapper extends BaseMapper\u0026lt;Product\u0026gt; { } 数据库数据 代码\n@Test public void testModel() { //小李查询商品 Product productLi = productMapper.selectById(1L); //小王查询商品 Product productWang = productMapper.selectById(1L); //小李将商品加50 productLi.setPrice(productLi.getPrice()+50); productMapper.updateById(productLi); //小王将价格降低30 productWang.setPrice(productWang.getPrice()-30); productMapper.updateById(productWang); } sql日志\n==\u0026gt; Preparing: UPDATE t_product SET name=?, price=?, version=? WHERE id=? ==\u0026gt; Parameters: 外星人(String), 150(Integer), 0(Integer), 1(Long) \u0026lt;== Updates: 1 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@6325f352] Creating a new SqlSession SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@70730db] was not registered for synchronization because synchronization is not active JDBC Connection [HikariProxyConnection@91831175 wrapping com.mysql.cj.jdbc.ConnectionImpl@74ea46e2] will not be managed by Spring ==\u0026gt; Preparing: UPDATE t_product SET name=?, price=?, version=? WHERE id=? ==\u0026gt; Parameters: 外星人(String), 70(Integer), 0(Integer), 1(Long) \u0026lt;== Updates: 1 //最终结果为70\n乐观锁插件 # 在实体类中使用@Version注解表示乐观锁版本号\n@Version private Integer version; 配置类\n@Bean public MybatisPlusInterceptor mybatisPlusInterceptor(){ MybatisPlusInterceptor mybatisPlusInterceptor=new MybatisPlusInterceptor(); mybatisPlusInterceptor.addInnerInterceptor(new PaginationInnerInterceptor(DbType.MYSQL)); //添加乐观锁插件 mybatisPlusInterceptor.addInnerInterceptor(new OptimisticLockerInnerInterceptor()); return mybatisPlusInterceptor; } 再次运行代码\n@Test public void testModel() { //小李查询商品 Product productLi = productMapper.selectById(1L); //小王查询商品 Product productWang = productMapper.selectById(1L); //小李将商品加50 productLi.setPrice(productLi.getPrice()+50); productMapper.updateById(productLi); //小王将价格降低30 productWang.setPrice(productWang.getPrice()-30); productMapper.updateById(productWang); } sql日志查看\n==\u0026gt; Preparing: UPDATE t_product SET name=?, price=?, version=? WHERE id=? AND version=? ==\u0026gt; Parameters: 外星人(String), 120(Integer), 1(Integer), 1(Long), 0(Integer) \u0026lt;== Updates: 1 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@2d64160c] Creating a new SqlSession SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@33063f5b] was not registered for synchronization because synchronization is not active JDBC Connection [HikariProxyConnection@356539350 wrapping com.mysql.cj.jdbc.ConnectionImpl@127a7272] will not be managed by Spring ==\u0026gt; Preparing: UPDATE t_product SET name=?, price=?, version=? WHERE id=? AND version=? ==\u0026gt; Parameters: 外星人(String), 40(Integer), 1(Integer), 1(Long), 0(Integer) \u0026lt;== Updates: 0 优化修改流程 # @Test public void testModel() { //小李查询商品 Product productLi = productMapper.selectById(1L); //小王查询商品 Product productWang = productMapper.selectById(1L); //小李将商品加50 productLi.setPrice(productLi.getPrice() + 50); productMapper.updateById(productLi); //小王将价格降低30 productWang.setPrice(productWang.getPrice() - 30); int i = productMapper.updateById(productWang); //如果小王操作失败,再获取一次 if (i == 0) { Product product = productMapper.selectById(1L); product.setPrice(product.getPrice() - 30); productMapper.updateById(product); } } sql日志打印\n==\u0026gt; Preparing: UPDATE t_product SET name=?, price=?, version=? WHERE id=? AND version=? ==\u0026gt; Parameters: 外星人(String), 150(Integer), 6(Integer), 1(Long), 5(Integer) \u0026lt;== Updates: 1 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@544e8149] Creating a new SqlSession SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@48a0c8aa] was not registered for synchronization because synchronization is not active JDBC Connection [HikariProxyConnection@1637000661 wrapping com.mysql.cj.jdbc.ConnectionImpl@5f481b73] will not be managed by Spring ==\u0026gt; Preparing: UPDATE t_product SET name=?, price=?, version=? WHERE id=? AND version=? ==\u0026gt; Parameters: 外星人(String), 70(Integer), 6(Integer), 1(Long), 5(Integer) \u0026lt;== Updates: 0 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@48a0c8aa] Creating a new SqlSession SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@4cbc2e3b] was not registered for synchronization because synchronization is not active JDBC Connection [HikariProxyConnection@43473566 wrapping com.mysql.cj.jdbc.ConnectionImpl@5f481b73] will not be managed by Spring ==\u0026gt; Preparing: SELECT id,name,price,version FROM t_product WHERE id=? ==\u0026gt; Parameters: 1(Long) \u0026lt;== Columns: id, name, price, version \u0026lt;== Row: 1, 外星人, 150, 6 \u0026lt;== Total: 1 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@4cbc2e3b] Creating a new SqlSession SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@57562473] was not registered for synchronization because synchronization is not active JDBC Connection [HikariProxyConnection@2050360660 wrapping com.mysql.cj.jdbc.ConnectionImpl@5f481b73] will not be managed by Spring ==\u0026gt; Preparing: UPDATE t_product SET name=?, price=?, version=? WHERE id=? AND version=? ==\u0026gt; Parameters: 外星人(String), 120(Integer), 7(Integer), 1(Long), 6(Integer) \u0026lt;== Updates: 1 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@57562473] 通用枚举 # 添加一个enum类\n@Getter public enum SexEnum { MALE(1, \u0026#34;男\u0026#34;), FEMALE(2, \u0026#34;女\u0026#34;); private Integer sex; private String sexName; SexEnum(Integer sex, String sexName) { this.sex = sex; this.sexName = sexName; } } 数据库增加一个sex 字段，实体类增加一个sex属性 实体类\nprivate SexEnum sex; 进行添加\n@Test public void testEnum(){ User user=new User(); user.setUserName(\u0026#34;enum - 测试名字\u0026#34;); user.setSexEnum(SexEnum.MALE); int insert = userMapper.insert(user); System.out.println(insert); } 注意看sql日志，有报错信息\n==\u0026gt; Preparing: INSERT INTO t_user ( name, sex ) VALUES ( ?, ? ) ==\u0026gt; Parameters: enum - 测试名字(String), MALE(String) ### SQL: INSERT INTO t_user ( name, sex ) VALUES ( ?, ? ) ### Cause: java.sql.SQLException: Incorrect integer value: \u0026#39;MALE\u0026#39; for column \u0026#39;sex\u0026#39; at row 1 插入了非数字\n修正，enum类添加注解\n@EnumValue //将注解所标识的属性的值设置到数据库 private Integer sex; 扫描通用枚举的包 application.yml中\nmybatis-plus: type-enums-package: com.ly.mybatisplus.enums 运行测试类并查看日志\n==\u0026gt; Preparing: INSERT INTO t_user ( name, sex ) VALUES ( ?, ? ) ==\u0026gt; Parameters: enum - 测试名字(String), 1(Integer) \u0026lt;== Updates: 1 代码生成器 # {% post_link study/mybatis_plus/official/hello 在28%进度的地方 %}\nmybatis-plus 代码自动生成\nmaven 依赖\n\u0026lt;!-- https://mvnrepository.com/artifact/com.baomidou/mybatis-plus-generator --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-generator\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.apache.velocity/velocity-engine-core --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.velocity\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;velocity-engine-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在测试类中编写程序让其自动生成\nimport com.baomidou.mybatisplus.generator.FastAutoGenerator; import com.baomidou.mybatisplus.generator.config.DataSourceConfig; import org.apache.ibatis.jdbc.ScriptRunner; import java.io.InputStream; import java.io.InputStreamReader; import java.sql.Connection; import java.sql.SQLException; /** * \u0026lt;p\u0026gt; * 快速生成 * \u0026lt;/p\u0026gt; * * @author lanjerry * @since 2021-09-16 */ public class FastAutoGeneratorTest { /** * 执行初始化数据库脚本 */ public static void before() throws SQLException { Connection conn = DATA_SOURCE_CONFIG.build().getConn(); InputStream inputStream = FastAutoGeneratorTest.class.getResourceAsStream(\u0026#34;/db/schema-mysql.sql\u0026#34;); ScriptRunner scriptRunner = new ScriptRunner(conn); scriptRunner.setAutoCommit(true); scriptRunner.runScript(new InputStreamReader(inputStream)); conn.close(); } /** * 数据源配置 */ private static final DataSourceConfig.Builder DATA_SOURCE_CONFIG = new DataSourceConfig .Builder(\u0026#34;jdbc:mysql://localhost:3306/mybatis_plus_demo?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;allowMultiQueries=true\u0026amp;nullCatalogMeansCurrent=true\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;123456\u0026#34;); /** * 执行 run */ public static void main(String[] args) throws SQLException { before(); FastAutoGenerator.create(DATA_SOURCE_CONFIG) // 全局配置 .globalConfig((scanner, builder) -\u0026gt; builder.author(scanner.apply(\u0026#34;请输入作者名称\u0026#34;))) // 包配置 .packageConfig((scanner, builder) -\u0026gt; builder.parent(scanner.apply(\u0026#34;请输入包名\u0026#34;))) // 策略配置 .strategyConfig((scanner, builder) -\u0026gt; builder.addInclude(scanner.apply(\u0026#34;请输入表名，多个表名用,隔开\u0026#34;))) /* 模板引擎配置，默认 Velocity 可选模板引擎 Beetl 或 Freemarker .templateEngine(new BeetlTemplateEngine()) .templateEngine(new FreemarkerTemplateEngine()) */ .execute(); } } shang gui gu 配置 模拟多数据源环境 # 新建一个mybatis-plus数据库和表 maven依赖添加\n\u0026lt;!-- https://mvnrepository.com/artifact/com.baomidou/dynamic-datasource-spring-boot-starter --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;dynamic-datasource-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 前提 使用mybatis_plus中的t_product表 及mybatis_plus1中的t_product1表\nyml配置\nspring: datasource: dynamic: primary: master #设置默认的数据源或者数据源组,默认值即为master strict: false #严格匹配数据源,默认false. true未匹配到指定数据源时抛异常,false使用默认数据源 datasource: master: url: jdbc:mysql://localhost:3306/mybatis_plus?characterEncoding=utf-8\u0026amp;\u0026amp;useSSL=false\u0026amp;\u0026amp;allowPublicKeyRetrieval=true username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver # 3.2.0开始支持SPI可省略此配置 slave_1: url: jdbc:mysql://localhost:3306/mybatis_plus_1?characterEncoding=utf-8\u0026amp;\u0026amp;useSSL=false\u0026amp;\u0026amp;allowPublicKeyRetrieval=true username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver #slave_2: # url: ENC(xxxxx) # 内置加密,使用请查看详细文档 # username: ENC(xxxxx) # password: ENC(xxxxx) # driver-class-name: com.mysql.jdbc.Driver #......省略 #以上会配置一个默认库master，一个组slave下有两个子库slave_1,slave_2 代码\n结构 安装MyBatisX插件 # 插件市场 自动定位 MyBatis代码快速生成 # 配置 url及密码配置 使用 自动生成 "},{"id":192,"href":"/zh/docs/technology/MyBatis-Plus/bl_sgg/19-39/","title":"mybatis-plus-sgg-19-39","section":"基础(尚硅谷)_","content":" 通用Service应用 # 这里会出现 publicKey is now allowed ，在数据库连接语句后面加上这句话即可 allowPublicKeyRetrieval=true\nspring: #配置数据源 datasource: #配置数据源类型 type: com.zaxxer.hikari.HikariDataSource #配置数据源各个信息 driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/mybatis_plus?characterEncoding=utf-8\u0026amp;\u0026amp;useSSL=false\u0026amp;\u0026amp;allowPublicKeyRetrieval=true username: root password: 123456 查询\n@Test public void testList(){ //List\u0026lt;User\u0026gt; list = userService.list(); long count = userService.count(); System.out.println(\u0026#34;总条数：\u0026#34;+count); } SQL执行语句\n==\u0026gt; Preparing: SELECT COUNT( * ) FROM user ==\u0026gt; Parameters: \u0026lt;== Columns: COUNT( * ) \u0026lt;== Row: 5 \u0026lt;== Total: 1 批量添加\n@Test public void batchInsert(){ List\u0026lt;User\u0026gt; users=new ArrayList\u0026lt;\u0026gt;(); for(int i=0;i\u0026lt;10;i++){ User user=new User(); user.setName(\u0026#34;name\u0026#34;+i); user.setEmail(\u0026#34;email\u0026#34;+i); users.add(user); } boolean b = userService.saveBatch(users); System.out.println(\u0026#34;result:\u0026#34;+b); } sql日志输出\n==\u0026gt; Preparing: INSERT INTO user ( id, name, email ) VALUES ( ?, ?, ? ) ==\u0026gt; Parameters: 1532579686881243138(Long), name0(String), email0(String) ==\u0026gt; Parameters: 1532579687124512770(Long), name1(String), email1(String) ==\u0026gt; Parameters: 1532579687128707074(Long), name2(String), email2(String) ==\u0026gt; Parameters: 1532579687128707075(Long), name3(String), email3(String) ==\u0026gt; Parameters: 1532579687132901377(Long), name4(String), email4(String) ==\u0026gt; Parameters: 1532579687137095681(Long), name5(String), email5(String) ==\u0026gt; Parameters: 1532579687137095682(Long), name6(String), email6(String) ==\u0026gt; Parameters: 1532579687141289985(Long), name7(String), email7(String) ==\u0026gt; Parameters: 1532579687145484289(Long), name8(String), email8(String) ==\u0026gt; Parameters: 1532579687145484290(Long), name9(String), email9(String) result:true 注意，这里是一个个的insert into ，而不是一条(单个的sql语句进行循环添加)\nMyBatis-Plus常用注解1 # 现在将mysql数据库表user名改为t_user 会提示下面的报错\nCause: java.sql.BatchUpdateException: Table \u0026#39;mybatis_plus.user\u0026#39; doesn\u0026#39;t exist 说明mybatis plus查询的时候会去找实体类名一样的表\n使用@TableName(\u0026ldquo;t_user\u0026rdquo;) 设置实体类对应的表名\n@Data @TableName(\u0026#34;t_user\u0026#34;) public class User { private Long id; private String name; private Integer age; private String email; } 修改后执行成功 统一添加\nmybatis-plus: configuration: global-config: db-config: table-prefix: t_ 指定主键名 假设现在把数据库列名和bean的属性名id改为uid,此时新增一条记录\nField \u0026#39;uid\u0026#39; doesn\u0026#39;t have a default value ; Field \u0026#39;uid\u0026#39; doesn\u0026#39;t have a default value; nested exception is java.sql.SQLException: Field \u0026#39;uid\u0026#39; doesn\u0026#39;t have a default value 说明此时没有为uid赋值 使用@TableId告诉mybatis-plus那个字段为主键，让mybatis-plus为他赋默认值\n@Data public class User { @TableId private Long uid; private String name; private Integer age; private String email; } sql打印\n==\u0026gt; Preparing: INSERT INTO t_user ( uid, name, age ) VALUES ( ?, ?, ? ) ==\u0026gt; Parameters: 1532582462671618050(Long), 张三(String), 18(Integer) \u0026lt;== Updates: 1 @TableId的value属性 # 用于指定绑定的主键的字段 假设此时将bean的主键属性名为id，数据库主键名是uid\n此时运行，会提示\n### SQL: INSERT INTO t_user ( id, name, age ) VALUES ( ?, ?, ? ) ### Cause: java.sql.SQLSyntaxErrorException: Unknown column \u0026#39;id\u0026#39; in \u0026#39;field list\u0026#39; 他会拿bean的属性来生成sql语句\n加上@TableId(value=\u0026ldquo;uid\u0026rdquo;)后运行正常\n@TableId的value属性 # /** * 生成ID类型枚举类 * * @author hubin * @since 2015-11-10 */ @Getter public enum IdType { /** * 数据库ID自增 * \u0026lt;p\u0026gt;该类型请确保数据库设置了 ID自增 否则无效\u0026lt;/p\u0026gt; */ AUTO(0), /** * 该类型为未设置主键类型(注解里等于跟随全局,全局里约等于 INPUT) */ NONE(1), /** * 用户输入ID * \u0026lt;p\u0026gt;该类型可以通过自己注册自动填充插件进行填充\u0026lt;/p\u0026gt; */ INPUT(2), /* 以下3种类型、只有当插入对象ID 为空，才自动填充。 */ /** * 分配ID (主键类型为number或string）, * 默认实现类 {@link com.baomidou.mybatisplus.core.incrementer.DefaultIdentifierGenerator}(雪花算法) * * @since 3.3.0 */ ASSIGN_ID(3), /** * 分配UUID (主键类型为 string) * 默认实现类 {@link com.baomidou.mybatisplus.core.incrementer.DefaultIdentifierGenerator}(UUID.replace(\u0026#34;-\u0026#34;,\u0026#34;\u0026#34;)) */ ASSIGN_UUID(4); private final int key; IdType(int key) { this.key = key; } } //使用自增 @TableId(value=\u0026#34;uid\u0026#34;,type = IdType.AUTO ) private Long id; 然后将数据库主键设置为自动递增\n新增后id为6\n通过全局属性设置主键生成策略 # 全局配置设置\nmybatis-plus: global-config: db-config: id-type: auto 雪花算法 # 数据库扩展方式：主从复制、业务分库、数据库分表 数据库拆分：水平拆分、垂直拆分 水平分表相对垂直分表，会引入更多的复杂性，比如要求唯一的数据id该怎么处理 可以给每个分表都给定一个范围大小，但是这样分段大小不好取 可以取模，但是如果增加了机器，原来的值主键（怎么处理是个问题 雪花算法，由Twitter公布的分布式主键生成算法 能够保证不同表的主键的不重复性，以及相同表的主键的有序性 核心思想 MyBatis-Plus常用注解2 # 此时数据库字段名为name，如果现在实体类的名字改为userName，那么会报错\nINSERT INTO t_user ( user_name, age ) VALUES ( ?, ? ) 又一次证明了MyBatis-plus通过实体类属性猜测数据库表的相关字段\n使用@TableFiled来指定对应的字段名\n@TableField(value = \u0026#34;name\u0026#34;) private String userName; 查询\n代码\n@Test public void selectTest() { User user = userService.getById(5L); System.out.println(\u0026#34;结果:\u0026#34; + user); } sql执行语句\n==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE uid=? AND is_deleted_ly=0 ==\u0026gt; Parameters: 5(Long) \u0026lt;== Columns: id, userName, age, email, is_deleted_ly \u0026lt;== Row: 5, Billie, 24, email被修改了, 0 \u0026lt;== Total: 1 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@5e048149] 结果:User(id=5, userName=Billie, age=24, email=email被修改了, isDeletedLy=0) 逻辑删除(主要是允许数据的恢复) 这里增加一个isDeletedLy字段（这里为了测试，一般是isDeleted）\n在User类添加下面的字段\n@TableLogic private Integer isDeletedLy; 逻辑删除\n代码\n@Test public void deleteLogic() { boolean save = userService.removeBatchByIds(Arrays.asList(1L,2L,3L)); System.out.println(\u0026#34;结果:\u0026#34; + save); } sql执行语句 注意，这里使用了is_deleted_ly=0是因为在下面的步骤加入了逻辑删除注解\n==\u0026gt; Preparing: UPDATE t_user SET is_deleted_ly=1 WHERE uid=? AND is_deleted_ly=0 ==\u0026gt; Parameters: 1(Long) ==\u0026gt; Parameters: 2(Long) ==\u0026gt; Parameters: 3(Long) 结果 条件构造器 # 结构 解释 查看BaseWrapper源码\n/** * Mapper 继承该接口后，无需编写 mapper.xml 文件，即可获得CRUD功能 * \u0026lt;p\u0026gt;这个 Mapper 支持 id 泛型\u0026lt;/p\u0026gt; * * @author hubin * @since 2016-01-23 */ public interface BaseMapper\u0026lt;T\u0026gt; extends Mapper\u0026lt;T\u0026gt; { /** * 插入一条记录 * * @param entity 实体对象 */ int insert(T entity); /** * 根据 ID 删除 * * @param id 主键ID */ int deleteById(Serializable id); /** * 根据实体(ID)删除 * * @param entity 实体对象 * @since 3.4.4 */ int deleteById(T entity); /** * 根据 columnMap 条件，删除记录 * * @param columnMap 表字段 map 对象 */ int deleteByMap(@Param(Constants.COLUMN_MAP) Map\u0026lt;String, Object\u0026gt; columnMap); /** * 根据 entity 条件，删除记录 * * @param queryWrapper 实体对象封装操作类（可以为 null,里面的 entity 用于生成 where 语句） */ int delete(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 删除（根据ID或实体 批量删除） * * @param idList 主键ID列表或实体列表(不能为 null 以及 empty) */ int deleteBatchIds(@Param(Constants.COLLECTION) Collection\u0026lt;?\u0026gt; idList); /** * 根据 ID 修改 * * @param entity 实体对象 */ int updateById(@Param(Constants.ENTITY) T entity); /** * 根据 whereEntity 条件，更新记录 * * @param entity 实体对象 (set 条件值,可以为 null) * @param updateWrapper 实体对象封装操作类（可以为 null,里面的 entity 用于生成 where 语句） */ int update(@Param(Constants.ENTITY) T entity, @Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; updateWrapper); /** * 根据 ID 查询 * * @param id 主键ID */ T selectById(Serializable id); /** * 查询（根据ID 批量查询） * * @param idList 主键ID列表(不能为 null 以及 empty) */ List\u0026lt;T\u0026gt; selectBatchIds(@Param(Constants.COLLECTION) Collection\u0026lt;? extends Serializable\u0026gt; idList); /** * 查询（根据 columnMap 条件） * * @param columnMap 表字段 map 对象 */ List\u0026lt;T\u0026gt; selectByMap(@Param(Constants.COLUMN_MAP) Map\u0026lt;String, Object\u0026gt; columnMap); /** * 根据 entity 条件，查询一条记录 * \u0026lt;p\u0026gt;查询一条记录，例如 qw.last(\u0026#34;limit 1\u0026#34;) 限制取一条记录, 注意：多条数据会报异常\u0026lt;/p\u0026gt; * * @param queryWrapper 实体对象封装操作类（可以为 null） */ default T selectOne(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper) { List\u0026lt;T\u0026gt; ts = this.selectList(queryWrapper); if (CollectionUtils.isNotEmpty(ts)) { if (ts.size() != 1) { throw ExceptionUtils.mpe(\u0026#34;One record is expected, but the query result is multiple records\u0026#34;); } return ts.get(0); } return null; } /** * 根据 Wrapper 条件，判断是否存在记录 * * @param queryWrapper 实体对象封装操作类 * @return */ default boolean exists(Wrapper\u0026lt;T\u0026gt; queryWrapper) { Long count = this.selectCount(queryWrapper); return null != count \u0026amp;\u0026amp; count \u0026gt; 0; } /** * 根据 Wrapper 条件，查询总记录数 * * @param queryWrapper 实体对象封装操作类（可以为 null） */ Long selectCount(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 entity 条件，查询全部记录 * * @param queryWrapper 实体对象封装操作类（可以为 null） */ List\u0026lt;T\u0026gt; selectList(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 Wrapper 条件，查询全部记录 * * @param queryWrapper 实体对象封装操作类（可以为 null） */ List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; selectMaps(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 Wrapper 条件，查询全部记录 * \u0026lt;p\u0026gt;注意： 只返回第一个字段的值\u0026lt;/p\u0026gt; * * @param queryWrapper 实体对象封装操作类（可以为 null） */ List\u0026lt;Object\u0026gt; selectObjs(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 entity 条件，查询全部记录（并翻页） * * @param page 分页查询条件（可以为 RowBounds.DEFAULT） * @param queryWrapper 实体对象封装操作类（可以为 null） */ \u0026lt;P extends IPage\u0026lt;T\u0026gt;\u0026gt; P selectPage(P page, @Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 Wrapper 条件，查询全部记录（并翻页） * * @param page 分页查询条件 * @param queryWrapper 实体对象封装操作类 */ \u0026lt;P extends IPage\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;\u0026gt; P selectMapsPage(P page, @Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); } Wrapper条件组装 queryWrapper测试\n@Test public void test01() { QueryWrapper\u0026lt;User\u0026gt; userQueryWrapper = new QueryWrapper\u0026lt;\u0026gt;(); //链式结构调用 userQueryWrapper.like(\u0026#34;name\u0026#34;, \u0026#34;a\u0026#34;) .between(\u0026#34;age\u0026#34;, 10, 30) .isNotNull(\u0026#34;email\u0026#34;); List\u0026lt;User\u0026gt; users = userMapper.selectList(userQueryWrapper); users.forEach(System.out::println); } sql日志打印\n//注意，这里出现了逻辑删除条件 ==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE is_deleted_ly=0 AND (name LIKE ? AND age BETWEEN ? AND ? AND email IS NOT NULL) ==\u0026gt; Parameters: %a%(String), 10(Integer), 30(Integer) \u0026lt;== Columns: id, userName, age, email, is_deleted_ly \u0026lt;== Row: 4, Sandy, 21, test4@baomidou.com, 0 \u0026lt;== Row: 5, Billiea, 24, email被修改了, 0 \u0026lt;== Total: 2 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@19650aa6] User(id=4, userName=Sandy, age=21, email=test4@baomidou.com, isDeletedLy=0) User(id=5, userName=Billiea, age=24, email=email被修改了, isDeletedLy=0) 使用排序\n@Test public void test02() { QueryWrapper\u0026lt;User\u0026gt; userQueryWrapper = new QueryWrapper\u0026lt;\u0026gt;(); userQueryWrapper.orderByDesc(\u0026#34;age\u0026#34;) .orderByAsc(\u0026#34;uid\u0026#34;); List\u0026lt;User\u0026gt; users = userMapper.selectList(userQueryWrapper); users.forEach(System.out::println); } sql日志打印\n==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE is_deleted_ly=0 ORDER BY age DESC,uid ASC ==\u0026gt; Parameters: \u0026lt;== Columns: id, userName, age, email, is_deleted_ly \u0026lt;== Row: 7, 张三6, 38, test6@baomidou.com, 0 \u0026lt;== Row: 5, Billiea, 24, email被修改了, 0 \u0026lt;== Row: 4, Sandy, 21, test4@baomidou.com, 0 \u0026lt;== Row: 6, 张三5, 18, test5@baomidou.com, 0 \u0026lt;== Row: 8, 张三a, 18, null, 0 \u0026lt;== Total: 5 Closing non transactional SqlSession [org.apache.ibatis.session.defaults.DefaultSqlSession@7158daf2] User(id=7, userName=张三6, age=38, email=test6@baomidou.com, isDeletedLy=0) User(id=5, userName=Billiea, age=24, email=email被修改了, isDeletedLy=0) User(id=4, userName=Sandy, age=21, email=test4@baomidou.com, isDeletedLy=0) User(id=6, userName=张三5, age=18, email=test5@baomidou.com, isDeletedLy=0) User(id=8, userName=张三a, age=18, email=null, isDeletedLy=0) 条件逻辑删除\n代码\n@Test public void test03() { QueryWrapper\u0026lt;User\u0026gt; userQueryWrapper=new QueryWrapper\u0026lt;\u0026gt;(); userQueryWrapper.isNull(\u0026#34;email\u0026#34;); int deleted = userMapper.delete(userQueryWrapper); System.out.println(deleted); } sql日志输出\n==\u0026gt; Preparing: UPDATE t_user SET is_deleted_ly=1 WHERE is_deleted_ly=0 AND (email IS NULL) ==\u0026gt; Parameters: \u0026lt;== Updates: 1 修改\n@Test public void test04() { QueryWrapper\u0026lt;User\u0026gt; userQueryWrapper=new QueryWrapper\u0026lt;\u0026gt;(); //(age\u0026gt;23且用户名包含a) 或 (邮箱为null) userQueryWrapper.gt(\u0026#34;age\u0026#34;,23) .like(\u0026#34;name\u0026#34;,\u0026#34;a\u0026#34;) .or() .isNull(\u0026#34;email\u0026#34;); User user=new User(); user.setUserName(\u0026#34;被修改了\u0026#34;); int deleted = userMapper.update(user,userQueryWrapper); System.out.println(deleted); } sql日志打印\n==\u0026gt; Preparing: UPDATE t_user SET name=? WHERE is_deleted_ly=0 AND (age \u0026gt; ? AND name LIKE ? OR email IS NULL) ==\u0026gt; Parameters: 被修改了(String), 23(Integer), %a%(String) \u0026lt;== Updates: 1 条件优先级\n@Test public void test05() { QueryWrapper\u0026lt;User\u0026gt; userQueryWrapper = new QueryWrapper\u0026lt;\u0026gt;(); //(age\u0026gt;23且用户名包含a) 或 (邮箱为null) userQueryWrapper .like(\u0026#34;name\u0026#34;, \u0026#34;a\u0026#34;) //and里面是一个条件构造器 .and( userQueryWrapper1 -\u0026gt; userQueryWrapper1.gt(\u0026#34;age\u0026#34;, 20) .or() .isNull(\u0026#34;email\u0026#34;) ); User user = new User(); user.setUserName(\u0026#34;被修改了\u0026#34;); int deleted = userMapper.update(user, userQueryWrapper); System.out.println(deleted); } sql日志输出\n==\u0026gt; Preparing: UPDATE t_user SET name=? WHERE is_deleted_ly=0 AND (name LIKE ? AND (age \u0026gt; ? OR email IS NULL)) ==\u0026gt; Parameters: 被修改了(String), %a%(String), 20(Integer) \u0026lt;== Updates: 1 注意 or也有优先级的参数 只查询某些字段\n@Test public void test06() { QueryWrapper\u0026lt;User\u0026gt; userQueryWrapper =new QueryWrapper\u0026lt;\u0026gt;(); userQueryWrapper.select(\u0026#34;uid\u0026#34;,\u0026#34;name\u0026#34;); List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; maps = userMapper.selectMaps(userQueryWrapper); System.out.println(maps); } sql输出\n==\u0026gt; Preparing: SELECT uid,name FROM t_user WHERE is_deleted_ly=0 ==\u0026gt; Parameters: \u0026lt;== Columns: uid, name \u0026lt;== Row: 4, 被修改了 \u0026lt;== Row: 5, 被修改了 \u0026lt;== Row: 6, 张三5 \u0026lt;== Row: 7, 张三6 \u0026lt;== Total: 4 子查询 假设需要完整下面的sql查询 代码\n@Test public void test7(){ //查询id小于等于100 QueryWrapper\u0026lt;User\u0026gt; userQueryWrapper=new QueryWrapper\u0026lt;\u0026gt;(); userQueryWrapper.inSql(\u0026#34;uid\u0026#34;, \u0026#34;select uid from t_user where uid \u0026lt;= 100\u0026#34;); List\u0026lt;User\u0026gt; users = userMapper.selectList(userQueryWrapper); users.forEach(System.out::println); } sql输出\n==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE is_deleted_ly=0 AND (uid IN (select uid from t_user where uid \u0026lt;= 100)) ==\u0026gt; Parameters: \u0026lt;== Columns: id, userName, age, email, is_deleted_ly \u0026lt;== Row: 4, 被修改了, 21, test4@baomidou.com, 0 \u0026lt;== Row: 5, 被修改了, 24, email被修改了, 0 \u0026lt;== Row: 6, 张三5, 18, test5@baomidou.com, 0 \u0026lt;== Row: 7, 张三6, 38, test6@baomidou.com, 0 \u0026lt;== Total: 4 UpdateWrapper\n@Test public void test8(){ //(age\u0026gt;23且用户名包含a) 或 (邮箱为null) UpdateWrapper\u0026lt;User\u0026gt; updateWrapper=new UpdateWrapper\u0026lt;\u0026gt;(); updateWrapper.like(\u0026#34;name\u0026#34;,\u0026#34;a\u0026#34;) .and(userUpdateWrapper -\u0026gt; userUpdateWrapper.gt(\u0026#34;age\u0026#34;,23).or().isNotNull(\u0026#34;email\u0026#34;)); updateWrapper.set(\u0026#34;name\u0026#34;,\u0026#34;小黑\u0026#34;).set(\u0026#34;email\u0026#34;,\u0026#34;abc@ly.com\u0026#34;); userMapper.update(null,updateWrapper); } sql日志输出\n==\u0026gt; Preparing: UPDATE t_user SET name=?,email=? WHERE is_deleted_ly=0 AND (name LIKE ? AND (age \u0026gt; ? OR email IS NOT NULL)) ==\u0026gt; Parameters: 小黑(String), abc@ly.com(String), %a%(String), 23(Integer) \u0026lt;== Updates: 0 模拟用户操作组装条件\n@Test public void test9(){ String username=\u0026#34;\u0026#34;; Integer ageBegin=null; Integer ageEnd=30; QueryWrapper\u0026lt;User\u0026gt; queryWrapper=new QueryWrapper\u0026lt;\u0026gt;(); if(StringUtils.isNotBlank(username)){ queryWrapper.like(\u0026#34;user_name\u0026#34;,username); } if( ageBegin!=null){ queryWrapper.gt(\u0026#34;age\u0026#34;,ageBegin); } if( ageEnd!=null){ queryWrapper.le(\u0026#34;age\u0026#34;,ageEnd); } userMapper.selectList(queryWrapper); } sql日志打印\n==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE is_deleted_ly=0 AND (age \u0026lt;= ?) ==\u0026gt; Parameters: 30(Integer) \u0026lt;== Columns: id, userName, age, email, is_deleted_ly \u0026lt;== Row: 4, 被修改了, 21, test4@baomidou.com, 0 \u0026lt;== Row: 5, 被修改了, 24, email被修改了, 0 \u0026lt;== Row: 6, 张三5, 18, test5@baomidou.com, 0 \u0026lt;== Total: 3 使用condition处理条件\n@Test public void test10(){ String username=\u0026#34;abc\u0026#34;; Integer ageBegin=null; Integer ageEnd=30; QueryWrapper\u0026lt;User\u0026gt; queryWrapper=new QueryWrapper\u0026lt;\u0026gt;(); queryWrapper.like(StringUtils.isNotBlank(username),\u0026#34;name\u0026#34;,username) .ge(ageBegin!=null,\u0026#34;age\u0026#34;,ageBegin); userMapper.selectList(queryWrapper); } sql日志输出\n==\u0026gt; Preparing: SELECT uid AS id,name AS userName,age,email,is_deleted_ly FROM t_user WHERE is_deleted_ly=0 AND (name LIKE ?) ==\u0026gt; Parameters: %abc%(String) \u0026lt;== Total: 0 "},{"id":193,"href":"/zh/docs/technology/MyBatis-Plus/bl_sgg/12-18/","title":"mybatis-plus-sgg-12-18","section":"基础(尚硅谷)_","content":" BaseMapper # 注：使用 mvn dependency:resolve -Dclassifier=sources 来获得mapper源码\n一些接口介绍\n/** * 插入一条记录 * * @param entity 实体对象 */ int insert(T entity); /** * 根据 ID 删除 * * @param id 主键ID */ int deleteById(Serializable id); /** * 根据实体(ID)删除 * * @param entity 实体对象 * @since 3.4.4 */ int deleteById(T entity); /** * 根据 columnMap 条件，删除记录 * * @param columnMap 表字段 map 对象 */ int deleteByMap(@Param(Constants.COLUMN_MAP) Map\u0026lt;String, Object\u0026gt; columnMap); /** * 根据 entity 条件，删除记录 * * @param queryWrapper 实体对象封装操作类（可以为 null,里面的 entity 用于生成 where 语句） */ int delete(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 删除（根据ID或实体 批量删除） * * @param idList 主键ID列表或实体列表(不能为 null 以及 empty) */ int deleteBatchIds(@Param(Constants.COLLECTION) Collection\u0026lt;?\u0026gt; idList); /** * 根据 ID 修改 * * @param entity 实体对象 */ int updateById(@Param(Constants.ENTITY) T entity); /** * 根据 whereEntity 条件，更新记录 * * @param entity 实体对象 (set 条件值,可以为 null) * @param updateWrapper 实体对象封装操作类（可以为 null,里面的 entity 用于生成 where 语句） */ int update(@Param(Constants.ENTITY) T entity, @Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; updateWrapper); /** * 根据 ID 查询 * * @param id 主键ID */ T selectById(Serializable id); /** * 查询（根据ID 批量查询） * * @param idList 主键ID列表(不能为 null 以及 empty) */ List\u0026lt;T\u0026gt; selectBatchIds(@Param(Constants.COLLECTION) Collection\u0026lt;? extends Serializable\u0026gt; idList); /** * 查询（根据 columnMap 条件） * * @param columnMap 表字段 map 对象 */ List\u0026lt;T\u0026gt; selectByMap(@Param(Constants.COLUMN_MAP) Map\u0026lt;String, Object\u0026gt; columnMap); /** * 根据 entity 条件，查询一条记录 * \u0026lt;p\u0026gt;查询一条记录，例如 qw.last(\u0026#34;limit 1\u0026#34;) 限制取一条记录, 注意：多条数据会报异常\u0026lt;/p\u0026gt; * * @param queryWrapper 实体对象封装操作类（可以为 null） */ default T selectOne(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper) { List\u0026lt;T\u0026gt; ts = this.selectList(queryWrapper); if (CollectionUtils.isNotEmpty(ts)) { if (ts.size() != 1) { throw ExceptionUtils.mpe(\u0026#34;One record is expected, but the query result is multiple records\u0026#34;); } return ts.get(0); } return null; } /** * 根据 Wrapper 条件，判断是否存在记录 * * @param queryWrapper 实体对象封装操作类 * @return */ default boolean exists(Wrapper\u0026lt;T\u0026gt; queryWrapper) { Long count = this.selectCount(queryWrapper); return null != count \u0026amp;\u0026amp; count \u0026gt; 0; } /** * 根据 Wrapper 条件，查询总记录数 * * @param queryWrapper 实体对象封装操作类（可以为 null） */ Long selectCount(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 entity 条件，查询全部记录 * * @param queryWrapper 实体对象封装操作类（可以为 null） */ List\u0026lt;T\u0026gt; selectList(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 Wrapper 条件，查询全部记录 * * @param queryWrapper 实体对象封装操作类（可以为 null） */ List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; selectMaps(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 Wrapper 条件，查询全部记录 * \u0026lt;p\u0026gt;注意： 只返回第一个字段的值\u0026lt;/p\u0026gt; * * @param queryWrapper 实体对象封装操作类（可以为 null） */ List\u0026lt;Object\u0026gt; selectObjs(@Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 entity 条件，查询全部记录（并翻页） * * @param page 分页查询条件（可以为 RowBounds.DEFAULT） * @param queryWrapper 实体对象封装操作类（可以为 null） */ \u0026lt;P extends IPage\u0026lt;T\u0026gt;\u0026gt; P selectPage(P page, @Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 Wrapper 条件，查询全部记录（并翻页） * * @param page 分页查询条件 * @param queryWrapper 实体对象封装操作类 */ \u0026lt;P extends IPage\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;\u0026gt; P selectMapsPage(P page, @Param(Constants.WRAPPER) Wrapper\u0026lt;T\u0026gt; queryWrapper); BaseMapper测试\n新增\n@Test public void testInsert(){ User user=new User(); user.setName(\u0026#34;小明\u0026#34;); user.setAge(11); user.setEmail(\u0026#34;xx@163.com\u0026#34;); int insertNum = userMapper.insert(user); System.out.println(\u0026#34;result:\u0026#34;+insertNum); System.out.println(\u0026#34;result:\u0026#34;+user); } sql日志输出\n==\u0026gt; Preparing: INSERT INTO user ( id, name, age, email ) VALUES ( ?, ?, ?, ? ) ==\u0026gt; Parameters: 1532542803866394625(Long), 小明(String), 11(Integer), xx@163.com(String) \u0026lt;== Updates: 1 删除\nid删除\n@Test public void testDelete(){ int result = userMapper.deleteById(1532542803866394625L); System.out.println(result); } sql日志输出\n==\u0026gt; Preparing: DELETE FROM user WHERE id=? ==\u0026gt; Parameters: 1532542803866394625(Long) \u0026lt;== Updates: 1 Map删除\n@Test public void testDeleteByMap(){ Map\u0026lt;String,Object\u0026gt; hash=new HashMap\u0026lt;\u0026gt;(); hash.put(\u0026#34;name\u0026#34;,\u0026#34;Sandy\u0026#34;); hash.put(\u0026#34;age\u0026#34;,\u0026#34;1234\u0026#34;); int result = userMapper.deleteByMap(hash); System.out.println(result); } sql日志输出\n==\u0026gt; Preparing: DELETE FROM user WHERE name = ? AND age = ? ==\u0026gt; Parameters: Sandy(String), 1234(String) \u0026lt;== Updates: 0 批量删除\n@Test public void testDeleteByIds(){ List\u0026lt;Long\u0026gt; ids = Arrays.asList(1L, 2L, 5L); int result = userMapper.deleteBatchIds(ids); System.out.println(result); } sql日志输出\n==\u0026gt; Preparing: DELETE FROM user WHERE id IN ( ? , ? , ? ) ==\u0026gt; Parameters: 1(Long), 2(Long), 5(Long) \u0026lt;== Updates: 3 修改\n根据id修改\n@Test public void testUpdateById (){ User user=new User(); user.setId(5L); user.setEmail(\u0026#34;email被修改了\u0026#34; ); int result = userMapper.updateById(user); System.out.println(result); } sql日志输出\n==\u0026gt; Preparing: UPDATE user SET email=? WHERE id=? ==\u0026gt; Parameters: email被修改了(String), 5(Long) \u0026lt;== Updates: 1 注意，这里不会修改另一个字段name的值\n查询\n通过id查询用户信息\n@Test public void testSelectById (){ User user = userMapper.selectById(3); System.out.println(user); } sql日志输出\n==\u0026gt; Preparing: SELECT id,name,age,email FROM user WHERE id=? ==\u0026gt; Parameters: 3(Integer) \u0026lt;== Columns: id, name, age, email \u0026lt;== Row: 3, Tom, 28, test3@baomidou.com \u0026lt;== Total: 1 通过id集合查询\n@Test public void testSelectByIds() { List\u0026lt;User\u0026gt; users = userMapper.selectBatchIds(Arrays.asList(1L, 2L, 5L)); users.forEach(System.out::println); } sql日志输出\n==\u0026gt; Preparing: SELECT id,name,age,email FROM user WHERE id IN ( ? , ? , ? ) ==\u0026gt; Parameters: 1(Long), 2(Long), 5(Long) \u0026lt;== Columns: id, name, age, email \u0026lt;== Row: 1, Jone, 18, test1@baomidou.com \u0026lt;== Row: 2, Jack, 20, test2@baomidou.com \u0026lt;== Row: 5, Billie, 24, email被修改了 \u0026lt;== Total: 3 通过map查询\n@Test public void testSelectMap() { Map\u0026lt;String, Object\u0026gt; hashMap = new HashMap\u0026lt;\u0026gt;(); hashMap.put(\u0026#34;name\u0026#34;,\u0026#34;Jon\u0026#34;); hashMap.put(\u0026#34;age\u0026#34;,18); List\u0026lt;User\u0026gt; users = userMapper.selectByMap(hashMap); users.forEach(System.out::println); } sql日志输出\n==\u0026gt; Preparing: SELECT id,name,age,email FROM user WHERE name = ? AND age = ? ==\u0026gt; Parameters: Tom(String), 18(Integer) \u0026lt;== Columns: id, name, age, email \u0026lt;== Row: 3, Tom, 18, test3@baomidou.com \u0026lt;== Total: 1 查询所有数据\n@Test public void testSelectAll() { List\u0026lt;User\u0026gt; users = userMapper.selectList(null); users.forEach(System.out::println); } sql日志输出\n==\u0026gt; Preparing: SELECT id,name,age,email FROM user ==\u0026gt; Parameters: \u0026lt;== Columns: id, name, age, email \u0026lt;== Row: 1, Jone, 18, test1@baomidou.com \u0026lt;== Row: 2, Jack, 20, test2@baomidou.com \u0026lt;== Row: 3, Tom, 18, test3@baomidou.com \u0026lt;== Row: 4, Sandy, 21, test4@baomidou.com \u0026lt;== Row: 5, Billie, 24, email被修改了 \u0026lt;== Total: 5 自定义功能 # mapper映射文件默认位置\nmybatis-plus: configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl mapper-locations: - classpath:/mapper/**/*.xml #默认位置 映射文件配置 /mapper/UserMapper.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34; \u0026gt; \u0026lt;mapper namespace=\u0026#34;com.ly.mybatisplus.mapper.UserMapper\u0026#34;\u0026gt; \u0026lt;select id=\u0026#34;selectMapById\u0026#34; resultType=\u0026#34;map\u0026#34;\u0026gt; select id,name,age,email from user where id = #{id} and 1=1 \u0026lt;/select\u0026gt; \u0026lt;/mapper\u0026gt; 代码执行\n@Test public void testSelectCustom() { Map\u0026lt;String, Object\u0026gt; map = userMapper.selectMapById(2L); System.out.println(map); } sql日志执行\n==\u0026gt; Preparing: select id,name,age,email from user where id = ? and 1=1 ==\u0026gt; Parameters: 2(Long) \u0026lt;== Columns: id, name, age, email \u0026lt;== Row: 2, Jack, 20, test2@baomidou.com \u0026lt;== Total: 1 通用Service接口 # 和通用Mapper的方法名有区分 Service CRUD中\n使用get查询【mapper-select】 remove删除 【mapper-delete】 list查询集合 page分页 IService源码\n/** * 顶级 Service * * @author hubin * @since 2018-06-23 */ public interface IService\u0026lt;T\u0026gt; { /** * 默认批次提交数量 */ int DEFAULT_BATCH_SIZE = 1000; /** * 插入一条记录（选择字段，策略插入） * * @param entity 实体对象 */ default boolean save(T entity) { return SqlHelper.retBool(getBaseMapper().insert(entity)); } /** * 插入（批量） * * @param entityList 实体对象集合 */ @Transactional(rollbackFor = Exception.class) default boolean saveBatch(Collection\u0026lt;T\u0026gt; entityList) { return saveBatch(entityList, DEFAULT_BATCH_SIZE); } /** * 插入（批量） * * @param entityList 实体对象集合 * @param batchSize 插入批次数量 */ boolean saveBatch(Collection\u0026lt;T\u0026gt; entityList, int batchSize); /** * 批量修改插入 * * @param entityList 实体对象集合 */ @Transactional(rollbackFor = Exception.class) default boolean saveOrUpdateBatch(Collection\u0026lt;T\u0026gt; entityList) { return saveOrUpdateBatch(entityList, DEFAULT_BATCH_SIZE); } /** * 批量修改插入 * * @param entityList 实体对象集合 * @param batchSize 每次的数量 */ boolean saveOrUpdateBatch(Collection\u0026lt;T\u0026gt; entityList, int batchSize); /** * 根据 ID 删除 * * @param id 主键ID */ default boolean removeById(Serializable id) { return SqlHelper.retBool(getBaseMapper().deleteById(id)); } /** * 根据 ID 删除 * * @param id 主键(类型必须与实体类型字段保持一致) * @param useFill 是否启用填充(为true的情况,会将入参转换实体进行delete删除) * @return 删除结果 * @since 3.5.0 */ default boolean removeById(Serializable id, boolean useFill) { throw new UnsupportedOperationException(\u0026#34;不支持的方法!\u0026#34;); } /** * 根据实体(ID)删除 * * @param entity 实体 * @since 3.4.4 */ default boolean removeById(T entity) { return SqlHelper.retBool(getBaseMapper().deleteById(entity)); } /** * 根据 columnMap 条件，删除记录 * * @param columnMap 表字段 map 对象 */ default boolean removeByMap(Map\u0026lt;String, Object\u0026gt; columnMap) { Assert.notEmpty(columnMap, \u0026#34;error: columnMap must not be empty\u0026#34;); return SqlHelper.retBool(getBaseMapper().deleteByMap(columnMap)); } /** * 根据 entity 条件，删除记录 * * @param queryWrapper 实体包装类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default boolean remove(Wrapper\u0026lt;T\u0026gt; queryWrapper) { return SqlHelper.retBool(getBaseMapper().delete(queryWrapper)); } /** * 删除（根据ID 批量删除） * * @param list 主键ID或实体列表 */ default boolean removeByIds(Collection\u0026lt;?\u0026gt; list) { if (CollectionUtils.isEmpty(list)) { return false; } return SqlHelper.retBool(getBaseMapper().deleteBatchIds(list)); } /** * 批量删除 * * @param list 主键ID或实体列表 * @param useFill 是否填充(为true的情况,会将入参转换实体进行delete删除) * @return 删除结果 * @since 3.5.0 */ @Transactional(rollbackFor = Exception.class) default boolean removeByIds(Collection\u0026lt;?\u0026gt; list, boolean useFill) { if (CollectionUtils.isEmpty(list)) { return false; } if (useFill) { return removeBatchByIds(list, true); } return SqlHelper.retBool(getBaseMapper().deleteBatchIds(list)); } /** * 批量删除(jdbc批量提交) * * @param list 主键ID或实体列表(主键ID类型必须与实体类型字段保持一致) * @return 删除结果 * @since 3.5.0 */ @Transactional(rollbackFor = Exception.class) default boolean removeBatchByIds(Collection\u0026lt;?\u0026gt; list) { return removeBatchByIds(list, DEFAULT_BATCH_SIZE); } /** * 批量删除(jdbc批量提交) * * @param list 主键ID或实体列表(主键ID类型必须与实体类型字段保持一致) * @param useFill 是否启用填充(为true的情况,会将入参转换实体进行delete删除) * @return 删除结果 * @since 3.5.0 */ @Transactional(rollbackFor = Exception.class) default boolean removeBatchByIds(Collection\u0026lt;?\u0026gt; list, boolean useFill) { return removeBatchByIds(list, DEFAULT_BATCH_SIZE, useFill); } /** * 批量删除(jdbc批量提交) * * @param list 主键ID或实体列表 * @param batchSize 批次大小 * @return 删除结果 * @since 3.5.0 */ default boolean removeBatchByIds(Collection\u0026lt;?\u0026gt; list, int batchSize) { throw new UnsupportedOperationException(\u0026#34;不支持的方法!\u0026#34;); } /** * 批量删除(jdbc批量提交) * * @param list 主键ID或实体列表 * @param batchSize 批次大小 * @param useFill 是否启用填充(为true的情况,会将入参转换实体进行delete删除) * @return 删除结果 * @since 3.5.0 */ default boolean removeBatchByIds(Collection\u0026lt;?\u0026gt; list, int batchSize, boolean useFill) { throw new UnsupportedOperationException(\u0026#34;不支持的方法!\u0026#34;); } /** * 根据 ID 选择修改 * * @param entity 实体对象 */ default boolean updateById(T entity) { return SqlHelper.retBool(getBaseMapper().updateById(entity)); } /** * 根据 UpdateWrapper 条件，更新记录 需要设置sqlset * * @param updateWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.update.UpdateWrapper} */ default boolean update(Wrapper\u0026lt;T\u0026gt; updateWrapper) { return update(null, updateWrapper); } /** * 根据 whereEntity 条件，更新记录 * * @param entity 实体对象 * @param updateWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.update.UpdateWrapper} */ default boolean update(T entity, Wrapper\u0026lt;T\u0026gt; updateWrapper) { return SqlHelper.retBool(getBaseMapper().update(entity, updateWrapper)); } /** * 根据ID 批量更新 * * @param entityList 实体对象集合 */ @Transactional(rollbackFor = Exception.class) default boolean updateBatchById(Collection\u0026lt;T\u0026gt; entityList) { return updateBatchById(entityList, DEFAULT_BATCH_SIZE); } /** * 根据ID 批量更新 * * @param entityList 实体对象集合 * @param batchSize 更新批次数量 */ boolean updateBatchById(Collection\u0026lt;T\u0026gt; entityList, int batchSize); /** * TableId 注解存在更新记录，否插入一条记录 * * @param entity 实体对象 */ boolean saveOrUpdate(T entity); /** * 根据 ID 查询 * * @param id 主键ID */ default T getById(Serializable id) { return getBaseMapper().selectById(id); } /** * 查询（根据ID 批量查询） * * @param idList 主键ID列表 */ default List\u0026lt;T\u0026gt; listByIds(Collection\u0026lt;? extends Serializable\u0026gt; idList) { return getBaseMapper().selectBatchIds(idList); } /** * 查询（根据 columnMap 条件） * * @param columnMap 表字段 map 对象 */ default List\u0026lt;T\u0026gt; listByMap(Map\u0026lt;String, Object\u0026gt; columnMap) { return getBaseMapper().selectByMap(columnMap); } /** * 根据 Wrapper，查询一条记录 \u0026lt;br/\u0026gt; * \u0026lt;p\u0026gt;结果集，如果是多个会抛出异常，随机取一条加上限制条件 wrapper.last(\u0026#34;LIMIT 1\u0026#34;)\u0026lt;/p\u0026gt; * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default T getOne(Wrapper\u0026lt;T\u0026gt; queryWrapper) { return getOne(queryWrapper, true); } /** * 根据 Wrapper，查询一条记录 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} * @param throwEx 有多个 result 是否抛出异常 */ T getOne(Wrapper\u0026lt;T\u0026gt; queryWrapper, boolean throwEx); /** * 根据 Wrapper，查询一条记录 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ Map\u0026lt;String, Object\u0026gt; getMap(Wrapper\u0026lt;T\u0026gt; queryWrapper); /** * 根据 Wrapper，查询一条记录 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} * @param mapper 转换函数 */ \u0026lt;V\u0026gt; V getObj(Wrapper\u0026lt;T\u0026gt; queryWrapper, Function\u0026lt;? super Object, V\u0026gt; mapper); /** * 查询总记录数 * * @see Wrappers#emptyWrapper() */ default long count() { return count(Wrappers.emptyWrapper()); } /** * 根据 Wrapper 条件，查询总记录数 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default long count(Wrapper\u0026lt;T\u0026gt; queryWrapper) { return SqlHelper.retCount(getBaseMapper().selectCount(queryWrapper)); } /** * 查询列表 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default List\u0026lt;T\u0026gt; list(Wrapper\u0026lt;T\u0026gt; queryWrapper) { return getBaseMapper().selectList(queryWrapper); } /** * 查询所有 * * @see Wrappers#emptyWrapper() */ default List\u0026lt;T\u0026gt; list() { return list(Wrappers.emptyWrapper()); } /** * 翻页查询 * * @param page 翻页对象 * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default \u0026lt;E extends IPage\u0026lt;T\u0026gt;\u0026gt; E page(E page, Wrapper\u0026lt;T\u0026gt; queryWrapper) { return getBaseMapper().selectPage(page, queryWrapper); } /** * 无条件翻页查询 * * @param page 翻页对象 * @see Wrappers#emptyWrapper() */ default \u0026lt;E extends IPage\u0026lt;T\u0026gt;\u0026gt; E page(E page) { return page(page, Wrappers.emptyWrapper()); } /** * 查询列表 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; listMaps(Wrapper\u0026lt;T\u0026gt; queryWrapper) { return getBaseMapper().selectMaps(queryWrapper); } /** * 查询所有列表 * * @see Wrappers#emptyWrapper() */ default List\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt; listMaps() { return listMaps(Wrappers.emptyWrapper()); } /** * 查询全部记录 */ default List\u0026lt;Object\u0026gt; listObjs() { return listObjs(Function.identity()); } /** * 查询全部记录 * * @param mapper 转换函数 */ default \u0026lt;V\u0026gt; List\u0026lt;V\u0026gt; listObjs(Function\u0026lt;? super Object, V\u0026gt; mapper) { return listObjs(Wrappers.emptyWrapper(), mapper); } /** * 根据 Wrapper 条件，查询全部记录 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default List\u0026lt;Object\u0026gt; listObjs(Wrapper\u0026lt;T\u0026gt; queryWrapper) { return listObjs(queryWrapper, Function.identity()); } /** * 根据 Wrapper 条件，查询全部记录 * * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} * @param mapper 转换函数 */ default \u0026lt;V\u0026gt; List\u0026lt;V\u0026gt; listObjs(Wrapper\u0026lt;T\u0026gt; queryWrapper, Function\u0026lt;? super Object, V\u0026gt; mapper) { return getBaseMapper().selectObjs(queryWrapper).stream().filter(Objects::nonNull).map(mapper).collect(Collectors.toList()); } /** * 翻页查询 * * @param page 翻页对象 * @param queryWrapper 实体对象封装操作类 {@link com.baomidou.mybatisplus.core.conditions.query.QueryWrapper} */ default \u0026lt;E extends IPage\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;\u0026gt; E pageMaps(E page, Wrapper\u0026lt;T\u0026gt; queryWrapper) { return getBaseMapper().selectMapsPage(page, queryWrapper); } /** * 无条件翻页查询 * * @param page 翻页对象 * @see Wrappers#emptyWrapper() */ default \u0026lt;E extends IPage\u0026lt;Map\u0026lt;String, Object\u0026gt;\u0026gt;\u0026gt; E pageMaps(E page) { return pageMaps(page, Wrappers.emptyWrapper()); } /** * 获取对应 entity 的 BaseMapper * * @return BaseMapper */ BaseMapper\u0026lt;T\u0026gt; getBaseMapper(); /** * 获取 entity 的 class * * @return {@link Class\u0026lt;T\u0026gt;} */ Class\u0026lt;T\u0026gt; getEntityClass(); /** * 以下的方法使用介绍: * * 一. 名称介绍 * 1. 方法名带有 query 的为对数据的查询操作, 方法名带有 update 的为对数据的修改操作 * 2. 方法名带有 lambda 的为内部方法入参 column 支持函数式的 * 二. 支持介绍 * * 1. 方法名带有 query 的支持以 {@link ChainQuery} 内部的方法名结尾进行数据查询操作 * 2. 方法名带有 update 的支持以 {@link ChainUpdate} 内部的方法名为结尾进行数据修改操作 * * 三. 使用示例,只用不带 lambda 的方法各展示一个例子,其他类推 * 1. 根据条件获取一条数据: `query().eq(\u0026#34;column\u0026#34;, value).one()` * 2. 根据条件删除一条数据: `update().eq(\u0026#34;column\u0026#34;, value).remove()` * */ /** * 链式查询 普通 * * @return QueryWrapper 的包装类 */ default QueryChainWrapper\u0026lt;T\u0026gt; query() { return ChainWrappers.queryChain(getBaseMapper()); } /** * 链式查询 lambda 式 * \u0026lt;p\u0026gt;注意：不支持 Kotlin \u0026lt;/p\u0026gt; * * @return LambdaQueryWrapper 的包装类 */ default LambdaQueryChainWrapper\u0026lt;T\u0026gt; lambdaQuery() { return ChainWrappers.lambdaQueryChain(getBaseMapper()); } /** * 链式查询 lambda 式 * kotlin 使用 * * @return KtQueryWrapper 的包装类 */ default KtQueryChainWrapper\u0026lt;T\u0026gt; ktQuery() { return ChainWrappers.ktQueryChain(getBaseMapper(), getEntityClass()); } /** * 链式查询 lambda 式 * kotlin 使用 * * @return KtQueryWrapper 的包装类 */ default KtUpdateChainWrapper\u0026lt;T\u0026gt; ktUpdate() { return ChainWrappers.ktUpdateChain(getBaseMapper(), getEntityClass()); } /** * 链式更改 普通 * * @return UpdateWrapper 的包装类 */ default UpdateChainWrapper\u0026lt;T\u0026gt; update() { return ChainWrappers.updateChain(getBaseMapper()); } /** * 链式更改 lambda 式 * \u0026lt;p\u0026gt;注意：不支持 Kotlin \u0026lt;/p\u0026gt; * * @return LambdaUpdateWrapper 的包装类 */ default LambdaUpdateChainWrapper\u0026lt;T\u0026gt; lambdaUpdate() { return ChainWrappers.lambdaUpdateChain(getBaseMapper()); } /** * \u0026lt;p\u0026gt; * 根据updateWrapper尝试更新，否继续执行saveOrUpdate(T)方法 * 此次修改主要是减少了此项业务代码的代码量（存在性验证之后的saveOrUpdate操作） * \u0026lt;/p\u0026gt; * * @param entity 实体对象 */ default boolean saveOrUpdate(T entity, Wrapper\u0026lt;T\u0026gt; updateWrapper) { return update(entity, updateWrapper) || saveOrUpdate(entity); } } IService有一个实现类：ServiceImpl\n自定义一个业务Service接口，继承IService\npublic interface UserService extends IService\u0026lt;User\u0026gt;{ } 编写一个实现类，实现UserService接口，并继承ServiceImpl\npublic class UserServiceImpl extends ServiceImpl\u0026lt;UserMapper, User\u0026gt; implements UserService { } 这样既可以使用自定义的功能，也可以使用MybatisPlus提供的功能\n# "},{"id":194,"href":"/zh/docs/technology/MyBatis-Plus/bl_sgg/01-11/","title":"mybatis-plus-sgg-01-11","section":"基础(尚硅谷)_","content":" 简介 # MyBatis-Plus是一个MyBatis的增强工具，在MyBatis的基础上只做增强不做改变，为简化开发、提高效率而生 这里以MySQL数据库为案例，以Idea作为IDE，使用Maven作为构建工具，使用SpringBoot完成各种功能 课程主要内容 特性 润物无声、效率至上、丰富功能 支持的数据库 框架结构 左边：扫描实体，从实体抽取属性猜测数据库字段 通过默认提供的方法使用sql语句，然后注入mybatis容器 开发环境 # 测试数据库和表 # 这里创建数据库mybatis_plus\n然后创建表user\nDROP TABLE IF EXISTS user; CREATE TABLE user ( id BIGINT(20) NOT NULL COMMENT \u0026#39;主键ID\u0026#39;, name VARCHAR(30) NULL DEFAULT NULL COMMENT \u0026#39;姓名\u0026#39;, age INT(11) NULL DEFAULT NULL COMMENT \u0026#39;年龄\u0026#39;, email VARCHAR(50) NULL DEFAULT NULL COMMENT \u0026#39;邮箱\u0026#39;, PRIMARY KEY (id) ); 插入默认数据\nDELETE FROM user; INSERT INTO user (id, name, age, email) VALUES (1, \u0026#39;Jone\u0026#39;, 18, \u0026#39;test1@baomidou.com\u0026#39;), (2, \u0026#39;Jack\u0026#39;, 20, \u0026#39;test2@baomidou.com\u0026#39;), (3, \u0026#39;Tom\u0026#39;, 28, \u0026#39;test3@baomidou.com\u0026#39;), (4, \u0026#39;Sandy\u0026#39;, 21, \u0026#39;test4@baomidou.com\u0026#39;), (5, \u0026#39;Billie\u0026#39;, 24, \u0026#39;test5@baomidou.com\u0026#39;); Spring Boot工程 # 添加依赖，并install Lombok 插件\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.0\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.29\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.24\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/com.baomidou/mybatis-plus-generator --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-generator\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 基础配置 # 创建spring boot启动类\n@SpringBootApplication public class MybatisPlusApplication { public static void main(String[] args) { SpringApplication.run(MybatisPlusApplication.class, args); } } 配置resources/application.yml文件\nspring: #配置数据源 datasource: #配置数据源类型 type: com.zaxxer.hikari.HikariDataSource #配置数据源各个信息 driver-class-name: com.mysql.cj.jdbc.Driver url: jdbc:mysql://localhost:3306/mybatis_plus?characterEncoding=utf-8\u0026amp;\u0026amp;useSSL=false username: root password: 123456 这个时候启动会直接结束，因为我们没有使用springboot-web 包 实体类的创建\npackage com.ly.mybatisplus.pojo; import lombok.Data; //相当于get set 无参构造器 hashCode()和equals()、toString()方法重写 @Data public class User { private Long id; private String name; private Integer age; private String email; } mapper的创建 mapper/UserMapper\npackage com.ly.mybatisplus.mapper; import com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.ly.mybatisplus.pojo.User; import org.springframework.stereotype.Repository; //将这个类标记成持久层组件 处理测试类中红色下划线的问题 @Repository public interface UserMapper extends BaseMapper\u0026lt;User\u0026gt; { } 设置mapper接口所在的包\npackage com.ly.mybatisplus; import org.mybatis.spring.annotation.MapperScan; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; @SpringBootApplication //扫描指定包下的mapper接口 @MapperScan(\u0026#34;com.ly.mybatisplus.mapper\u0026#34;) public class MybatisPlusApplication { public static void main(String[] args) { SpringApplication.run(MybatisPlusApplication.class, args); } } 测试 # 测试类的创建\nimport com.ly.mybatisplus.MybatisPlusApplication; import com.ly.mybatisplus.mapper.UserMapper; import com.ly.mybatisplus.pojo.User; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import java.util.List; //可能是由于没有使用web包依赖，这里要加入classes指定启动类 @SpringBootTest(classes = MybatisPlusApplication.class) public class MybatisPlusTest { @Autowired private UserMapper userMapper; @Test public void testSelect(){ //通过条件构造器查询list集合 null表示没有条件 List\u0026lt;User\u0026gt; users = userMapper.selectList(null); users.forEach(System.out::println); } } 加入日志功能 # 配置application.yml加入日志\n#日志 mybatis-plus: configuration: log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 效果 如上图，查询的字段名来自于实体类属性 "},{"id":195,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/3.1.1-3.1.7/","title":"算法红皮书 3.1.1-3.1.7","section":"_算法(第四版)_","content":" 查找 # 经典查找算法\n用符号表这个词来描述抽象的表格，将信息（值）存储在其中，然后按照指定的键来获取这些信息\n符号表也被称为字典\n在英语字典里，键就是单词，值就是单词对应的定义、发音和词源 符号表有时又叫索引 在一本书的索引中，键就是术语，而值就是书中该术语出现的所有页码 下面学习三种经典的数据类型：二叉查找树、红黑树和散列表\n符号表 # 符号表最主要的目的是将键和值联系起来\n用例能够将一个键值对插入符号表并希望在之后能够从符号表的所有键值对中按照键直接找到相对应的值\n符号表是一种存储键值对的数据结构，支持两种操作：插入(put)，即将一组新的键值对存入表中；查找(get)，即根据给定的键得到相应的值\n典型的符号表应用 API # 符号表是一种典型的数据类型 ：代表着一组定义清晰的值及相应的操作。使用应用程序编程接口（API）来精确地定义这些操作 一种简单的泛型符号表API ST(Symbol Table) 泛型 对于符号表，我们通过明确地指定查找时键和值的类型来区分它们的不同角色【key和value】\n重复的键\n这里假设每个键只对应着一个值（表中不允许重复值） 当用例代码向表中存入的键值对和表中已有的键（及关联的值）冲突时，新的值会替代旧的值 上述定义了关联数组的抽象形式，可以将符号表想象成数组，键即索引，值即数组中的值 在一个关联数组中，键可以是任意类型，但我们仍然可以用它来快速访问数组的值 非Java使用st[key]来替代st.get(key)，用st[key]=val来替代st.put(key,val) 键不能为空\n值不能为空（因为规定当键不存在时get()返回空） 当值为空表示删除\n删除操作\n延时删除，先将键对应的值置空，之后在某个时刻删除所有值为空的键\n即时删除，立即从表中删除指定的键 put实现的开头：\nif(val == null){ delete(key); return; } 便捷方法 迭代 在API第一行加上implements Iterable\u0026lt;Key\u0026gt; ，所有实现都包含iterator()方法来实现hasNext()和next()方法的迭代器；这里采用另一种方式：定义keys返回一个Iterable\u0026lt;Key\u0026gt;对象以方便便利所有的键，且允许遍历一部分\n键的等价性 自定义的键需要重写equals()方法；且最好使用不可变数据类型作为键\n有序符号表 # 一种有序的泛型符号表的API 最大值和最小值、向下取整和向上取整、排名和选择 对于0到size()-1的所有i都有i==rank(select(i))，且所有的键都满足key == select(rank(key)) 范围查找 例外情况 当一个方法需要返回一个键但表中没有合适的键可以返回时，我们约定抛出一个异常 有序符号表中冗余有序性方法的默认实现 所有Comparable类型中compareTo()方法和equals()方法的一致性 ★★成本模型 在学习符号表的实现时，我们会统计比较的次数（等价性测试或是键的相互比较），在内循环**不进行比较（极少）**的情况下，我们会统计数组的访问次数 用例举例 # 如何使用\n行为测试用例 简单的符号表测试用例 测试用例的键、值和输出 性能测试用例 查找频率最高的单词\npublic class FrequencyCounter { public static void main(String[] args) { int minlen = Integer.parseint(args[0]); // 最小键长 ST\u0026lt;String, Integer\u0026gt; st = new ST\u0026lt;String, Integer\u0026gt;(); while (!StdIn.isEmpty()) { // 构造符号表并统计频率 String word = StdIn.readString(); if (word.length() \u0026lt; minlen) continue; // 忽略较短的单词 if (!st.contains(word)) st.put(word, 1); else st.put(word, st.get(word) + 1); } // 找出出现频率最高的单词 String max = \u0026#34; \u0026#34;; st.put(max, 0); for (String word : st.keys()) if (st.get(word) \u0026gt; st.get(max)) max = word; StdOut.println(max + \u0026#34; \u0026#34; + st.get(max)); } } 每个单词都会被作为键进行搜索，因此处理性能和输入文本的单词总量必然有关；其次，输入的每个单词都会被存入符号表（输入中不重复单词的总数也就是所有键都被插入以后符号表的大小），因此输入流中不同的单词的总数也是相关的\n无序链表中的顺序查找 # 顺序查找的定义：使用链表，每个结点存储一个键值对，get()实现即为遍历链表，用equals()方法比较需被查找的键和每个节点中的键。如果匹配成功我们就返回相应的值，否则返回null。put()实现也是遍历链表，用equals()方法比较需被查找的键和每个节点中的键。如果匹配成功我们就用第二个参数指定更新和该键相关联的值，否则我们就用给定的键值对创建一个新的结点并将其插入到链表的开头。这种方法称为顺序查找\n命中表示一次成功的查找，未命中表示一次失败的查找\n使用基于链表的符号表的索引用例的轨迹 顺序查找（基于无序链表）\npublic class SequentialSearchST\u0026lt;Key,Value\u0026gt; { private Node first; //链表首结点 private class Node{ //链表结点的定义 Key key; Value val; Node next; public Node(Key key, Value val, Node next) { this.key = key; this.val = val; this.next = next; } } public Value get(Key key) { // 查找给定的键，返回相关联的值 for (Node x = first; x != null; x = x.next) if (key.equals(x.key)) return x.val; // 命中 return null; // 未名中 } public void put(Key key, Value val) { // 查找给定的键，找到则更新其值，否则在表中新建结点 for (Node x = first; x != null; x = x.next) if (key.equals(x.key)) { x.val = val; return; } // 命中，更新 first = new Node(key, val, first); // 未命中，新建结点 } } 在含有N 对键值的基于（无序）链表的符号表中，未命中的查找和插入操作都需要N 次比较。命中的查找在最坏情况下需要N 次比较。特别地，向一个空表中插入N 个不同的键需要∼ N2/2 次比较\n查找一个已经存在的键并不需要线性级别的时间。一种度量方法是查找表中的每个键，并将总 时间除以N\n有序数组中的二分查找 # 有序符号表API：它使用的数据结构是一对平行的数组，一个存储键一个存储值\n//rank()：小于k的键的数量\npublic class BinarySearchST\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;, Value\u0026gt; { private Key[] keys; private Value[] vals; private int N; public BinarySearchST(int capacity) { // 调整数组大小的标准代码请见算法1.1 keys = (Key[]) new Comparable[capacity]; vals = (Value[]) new Object[capacity]; } public int size() { return N; } public Value get(Key key) { if (isEmpty()) return null; int i = rank(key); //注意，这里i不一定就是刚好是key所在的索引，他表示比key的值小的个数 if (i \u0026lt; N \u0026amp;\u0026amp; keys[i].compareTo(key) == 0) return vals[i]; else return null; } public int rank(Key key) // 请见算法3.2（续1） public void put(Key key, Value val) { // 查找键，找到则更新值，否则创建新的元素 int i = rank(key); if (i \u0026lt; N \u0026amp;\u0026amp; keys[i].compareTo(key) == 0) { vals[i] = val; return; } //根据成本模型，这里不统计 for (int j = N; j \u0026gt; i; j--) { keys[j] = keys[j-1]; vals[j] = vals[j-1]; } keys[i] = key; vals[i] = val; N++; } public void delete(Key key) // 该方法的实现请见练习3.1.16 } 二分查找 我们使用有序数组存储键的原因是，经典二分查找法能够根据数组的索引大大减少每次查找所需的比较次数\n递归的二分查找\npublic int rank(Key key, int lo, int hi) { if (hi \u0026lt; lo) return lo; int mid = lo + (hi - lo) / 2; int cmp = key.compareTo(keys[mid]); if (cmp \u0026lt; 0) return rank(key, lo, mid-1); else if (cmp \u0026gt; 0) return rank(key, mid+1, hi); else return mid; //如果存在，返回key所在位置的索引（也就是key之前的元素的个数 ） } rank()的性质：如果表中存在该键，rank()应该返回该键的位置，也就是表中小于它的键的数量；如果表中不存在该键，ran()还是应该返回表中小于它的键的数量\n好好想想算法3.2（续1）中非递归的rank() 为什么能够做到这些（你可以证明两个版本的等价性，或者直接证明非递归版本中的循环在结束时lo 的值正好等于表中小于被查找的键的键的数量），所有程序员都能从这些思考中有所收获。（提示：lo 的初始值为0，且永远不会变小） 假设有下面这么一组数(key value)\n0 1 2 3 4 1 2 3 5 9 我要查找6，那么轨迹为： low=0，high=4，mid=2 low=2+1=3，high=4，mid=3 low=3+1=4，high=4，mid=4 low=4，high=4-1，此时high\u0026lt;low，返回low【也就是说找到了最接近于要查找的数的下标】\n带图轨迹 基于二分查找的有序符号表的其他操作\npublic Key min() { return keys[0]; } public Key max() { return keys[N-1]; } public Key select(int k) { return keys[k]; } //大于等于key的最小整数 public Key ceiling(Key key) { int i = rank(key); return keys[i]; } //小于等于key的最大整数 public Key floor(Key key) // 请见练习3.1.17 public Key delete(Key key) // 请见练习3.1.16 public Iterable\u0026lt;Key\u0026gt; keys(Key lo, Key hi) { Queue\u0026lt;Key\u0026gt; q = new Queue\u0026lt;Key\u0026gt;(); for (int i = rank(lo); i \u0026lt; rank(hi); i++) q.enqueue(keys[i]); if (contains(hi)) q.enqueue(keys[rank(hi)]); return q; } 对二分查找的分析 # 在N 个键的有序数组中进行二分查找最多需要（lgN+1）次比较（无论是否成功）\n向大小为N 的有序数组中插入一个新的元素在最坏情况下需要访问∼ 2N 次数组，因此向一个空符号表中插入N 个元素在最坏情况下需要访问∼ N2 次数组\n预览 # 简单的符号表实现的成本总结 符号表的各种实现的优缺点 我们有若干种高效的符号表实现，它们能够并且已经被应用于无数程序之中了 "},{"id":196,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/2.5/","title":"算法红皮书 2.5","section":"_算法(第四版)_","content":" 排序如此有用的原因是，在有序的数组中查找一个元素，要比在一个无序的数组中查找简单得多 通用排序算法是最重要的 算法思想虽然简单，但是适用领域广泛 将各种数据排序 # Java的约定使得我们能够利用Java的回调机制将任意实现Comparable接口的数据类型排序\n我们的代码直接能够将String、Integer、Double 和一些其他例如File 和URL 类型的数组排序，因为它们都实现了Comparable 接口 交易事务 商业数据的处理，设想一家互联网商业公司为每笔交易记录都保存了所有的相关信息\npublic int compareTo(Transaction that) { return this.when.compareTo(that.when); } 指针排序 我们使用的方法在经典教材中被称为指针排序，因为我们只处理元素的引用而不移动数据本身\n不可变的键 用不可变的数据类型作为键，比如String、Integer、Double和File等\n廉价的交换\n使用引用的另一个好处是不必移动整个元素对于几乎任意大小的元素，使用引用使得在一般情况下交换的成本和比较的成本几乎相同（代价是需要额外的空间存储这些引用）\n研究将数字排序的算法性能的一种方法就是观察其所需的比较和交换总数，因为这里隐式地假设了比较和交换的成本是相同的\n多种排序方法\n根据情况将一组对象按照不同的方式排序。Java 的Comparator 接口允许我们在一个类之中实现多种排序方法 多键数组\n一个元素的多种属性都可能被用作排序的键\n我们可以定义多种比较器，要将Transaction 对象的数组按照时间排序可以调用： Insertion.sort(a, new Transaction.WhenOrder()) 或者这样来按照金额排序： Insertion.sort(a, new Transaction.HowMuchOrder()) 使用Comparator的插入排序\npublic static void sort(Object[] a, Comparator c) { int N = a.length; for (int i = 1; i \u0026lt; N; i++) for (int j = i; j \u0026gt; 0 \u0026amp;\u0026amp; less(Comparator, a[j], a[j-1]); j--) exch(a, j, j-1); } private static Boolean less(Comparator c, Object v, Object w) { return c.compare(v, w) \u0026lt; 0; } private static void exch(Object[] a, int i, int j) { Object t = a[i]; a[i] = a[j]; a[j] = t; } 使用比较器实现优先队列\n扩展优先队列 导入 java.util.Comparator； 为 MaxPQ 添加一个实例变量 comparator 以及一个构造函数，该构造函数接受一个比较器 作为参数并用它将comparator 初始化； 在 less()中检查 comparator属性是否为 null（如果不是的话就用它进行比较）。 //使用了Comparator的插入排序 import java.util.Comparator; public class Transaction { ... private final String who; private final Date when; private final double amount; ... public static class WhoOrder implements Comparator\u0026lt;Transaction\u0026gt; { public int compare(Transaction v, Transaction w) { return v.who.compareTo(w.who); } } public static class WhenOrder implements Comparator\u0026lt;Transaction\u0026gt; { public int compare(Transaction v, Transaction w) { return v.when.compareTo(w.when); } } public static class HowMuchOrder implements Comparator\u0026lt;Transaction\u0026gt; { public int compare(Transaction v, Transaction w) { if (v.amount \u0026lt; w.amount) return -1; if (v.amount \u0026gt; w.amount) return +1; return 0; } } } 稳定性\n如果一个排序算法能够保留数组中重复元素的相对位置则可以被称为是稳定的 例如，考虑一个需要处理大量含有地理位置和时间戳的事件的互联网商业应用程 序。首先，我们在事件发生时将它们挨个存储在一个数组中，这样在数组中它们已经是按照时间顺序排好了的。现在假设在进一步处理前将按照地理位置切分。一种简单的方法是将数组按照位置排序。如果排序算法不是稳定的，排序后的每个城市的交易可能不会再是按照时间顺序排列的了 我们学习过的一部分算法是稳定的（插入排序和归并排序），但很多不是（选择排序、希尔排序、快速排序和堆排序） 有很多办法能够将任意排序算法变成稳定的（请见练习2.5.18），但一般只有在稳定性是必要的情况下稳定的排序算法才有优势 图示 我应该使用哪种排序算法 # 各种排序算法的性能特点 快速排序是最快的通用排序算法 将原始类型数据排序 一些性能优先的应用的重点可能是将数字排序，因此更合理的做法是跳过引用直接将原始数据 类型的数据排序 Java系统库的排序算法 java.util.Arrays.sort() Java 的系统程序员选择对原始数据类型使用（三向切分的）快速排序，对引用类型使用归并排 序。这些选择实际上也暗示着用速度和空间（对于原始数据类型）来换取稳定性（对于引用类型）， 如果考虑稳定性，则选择Merge.sort() 归并排序 问题的归约 # 归约指的是为解决某个问题而发明的算法正好可以用来解决另一种问题\n使用解决问题B 的方法来解决问题A 时，你都是在将A 归约为B。\n如果先将数据排序，那么解决剩下的问题就剩下线性级别的时间，归约后的运行时间的增长数量级由平方级别降低到了线性级别\n找出重复元素的个数（先排序，后遍历）\nQuick.sort(a); int count = 1; // 假设a.length \u0026gt; 0. for (int i = 1; i \u0026lt; a.length; i++) if (a[i].compareTo(a[i-1]) != 0) count++; Kendall tau距离\n优先队列\n在2.4 节中我们已经见过两个被归约为优先队列操作的问题的例子。一个是2.4.2.1 节中的TopM，它能够找到输入流中M 个最大的元素；另一个是2.4.4.7 节中的Multiway，它能够将M 个输入流归并为一个有序的输出流。这两个问题都可以轻易用长度为M 的优先队列解决\n中位数与顺序统计 (与快速排序有关)\n排序应用一览 # 商业计算：按照名字或者数字排序的账号、按照日期或者金额排序的交易、按照 邮编或者地址排序的邮件、按照名称或者日期排序的文件等， 处理这些数据必然需要排序算 信息搜索：有序的顺序可以使用经典的二分查找法 运筹学指的是研究数学模型并将其应用于问题解决和决策的领域 事件驱动模拟、数值计算、组合搜索 基于排序算法的算法 Prim算法和Dijkstra算法 Kruskal算法 霍夫曼压缩 字符串处理 "},{"id":197,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/2.4/","title":"算法红皮书 2.4","section":"_算法(第四版)_","content":" 优先队列 # 有些情况下，不需要要求处理的元素全部有序，只要求每次都处理键值最大的元素，然后再收集更多的元素，然后再处理键值最大的元素 需要一种数据结构，支持操作：删除最大元素和插入元素，这种数据类型叫做优先队列 优先队列的基本表现形式：其一或两种操作都能在线性时间内完成 基于二叉堆数据结构的优先队列，用数组保存元素并按照一定条件排序，以实现高效的删除最大元素和插入元素 API # 抽象数据类型，最重要的操作是删除最大元素和插入元素 delMax()和insert()\n用“最大元素”代替“最大键值”或是“键值最大的元素”\n泛型优先队列的API 优先队列的调用示例 从N各输入中找到最大的M各元素所需成本 优先队列的用例 pq里面最多放5个，当大于5个的时候，就从中剔除1个\npublic class TopM { public static void main(String[] args) { // 打印输入流中最大的M行 int M = Integer.parseint(args[0]); MinPQ\u0026lt;Transaction\u0026gt; pq = new MinPQ\u0026lt;Transaction\u0026gt;(M+1); while (StdIn.hasNextLine()) { // 为下一行输入创建一个元素并放入优先队列中 pq.insert(new Transaction(StdIn.readLine())); if (pq.size() \u0026gt; M) pq.delMin(); // 如果优先队列中存在M+1个元素则删除其中最小的元素 } // 最大的M个元素都在优先队列中 Stack\u0026lt;Transaction\u0026gt; stack = new Stack\u0026lt;Transaction\u0026gt;(); while (!pq.isEmpty()) stack.push(pq.delMin()); for (Transaction t : stack) StdOut.println(t); } } 应用 初级实现 # 数组实现（无序） insert元素和栈的push()方法完全一样；要删除最大元素，可以添加一段类似选择排序的内循环的代码，将最大元素的边界元素交换，然后删除 数组实现（有序） insert()方法时，始终将较大的元素，向右边移动一格以使数组有序；删除最大元素就是pop() 链表表示法 可以用基于链表的下压栈的代码作为基础，而后可以选择修改pop() 来找到并返回最大元素，或是修改push() 来保证所有元素为逆序并用pop() 来删除并返回链表的首元素(也就是最大的元素) 优先队列的各种实现在最坏情况下运行时间的增长数量级 在一个优先队列上执行的一系列操作如表2.4.4所示 堆的定义 # 当一棵二叉树的每个节点都大于等于他的两个子结点时，它被称为堆有序\n重要性质1\n在堆有序的二叉树中，每个结点都小于等于它的父结点（如果有的话）。从任意结点向上，我们都能得到一列非递减的元素；从任意结点向下，我们都能得到一列非递增的元素\n重要命题 根结点是堆有序的二叉树中的最大结点\n二叉堆表示法\n如果使用指针来表示堆有序的二叉树，需要三个指针来找到它的上下结点 使用数组来表示(前提是使用完全二叉树来表示)，那么只要一层一层由上向下从左至右，在每个结点的下方连接两个更小的结点，直至将N个结点全部连接完毕 即将二叉树的结点按照层级顺序放入数组中 二叉堆是一组能够用堆有序的完全二叉树排序的元素，并在数组中按照层级储存（不使 用数组的第一个位置）\n图解 下面将二叉树 简称为堆\n在一个堆中，位置k 的结点的父结点的位置为k/2，而它的两个子结点的位置则分别为2k 和2k+1。这样在不使用指针的情况下（我们在第3 章中讨论二叉树时会用到它们）我们也可以通过计算数组的索引在树中上下移动：从a[k] 向上一层就令k 等于k/2，向下一层则令k 等于2k 或2k+1\n一棵大小为N的完全二叉树的高度为[lgN]\n当N达到2的幂时树的高度为加1 数组不使用位置[0]\n堆的算法 # 堆实现的比较和交换方法\nprivate Boolean less(int i, int j) { return pq[i].compareTo(pq[j]) \u0026lt; 0; } private void exch(int i, int j) { Key t = pq[i]; pq[i] = pq[j]; pq[j] = t; } 堆的操作首先进行一些简单的改动，打破堆的状态，再遍历堆并按照要求将堆的状态回复，这个过程称为堆的有序化\n当某个结点的优先级上升（或是在堆底加入一个新的元素）时，我们需要由下至上恢复堆的顺序。当某个结点的优先级下降（例如，将根结点替换为一个较小的元素）时，我们需要由上至下恢复堆的顺序\n由下至上的堆有序化（上浮）【在最后位置插入一个元素】\n说明 如果堆的有序状态因为某个结点变得比它的父结点更大而被打破，那么我们就需要通过交换它和它的父结点来修复堆。交换后，这个结点比它的两个子结点都大（一个是曾经的父结点，另一个比它更小，因为它是曾经父结点的子结点），但这个结点仍然可能比它现在的父结点更大。我们可以一遍遍地用同样的办法恢复秩序，将这个结点不断向上移动直到我们遇 到了一个更大的父结点。\n代码\nprivate void swim(int k) { while (k \u0026gt; 1 \u0026amp;\u0026amp; less(k/2, k)) { exch(k/2, k); k = k/2; } } 由上至下的堆有序化（下沉）【在根节点插入一个元素】\n如果堆的有序状态因为某个结点变得比它的两个子结点或是其中之一更小了而被打破了，那么我们可以通过将它和它的两个子结点中的较大者交换来恢复堆。交换可能会在子结点处继续打破堆的有序状态，因此我们需要不断地用相同的方式将其修复，将结点向下移动直到它的子结点都比它更小或是到达了堆的底部\n代码\nprivate void sink(int k) { while (2*k \u0026lt;= N) { int j = 2*k; //j\u0026lt;N用来判断j是否存在右兄弟结点，当j==N（即j为树的[从左到右]最末一个结点，那么它没有右兄弟结点） if (j \u0026lt; N \u0026amp;\u0026amp; less(j, j+1)) j++; //当根节点没有小于子节点时，跳出循环 if (!less(k, j)) break; exch(k, j); k = j; } } 对于上面的说明\n插入元素。我们将新元素加到数组末尾，增加堆的大小并让这个新元素上浮到合适的位 置（如图2.4.5 左半部分所示）。 删除最大元素。我们从数组顶端删去最大的元素并将数组的最后一个元素放到顶端，减 小堆的大小并让这个元素下沉到合适的位置（如图2.4.5 右半部分所示） 上面对优先队列API的实现，能够保证插入元素和删除元素这两个操作的用时，和队列的大小仅成对数关系 图解堆的操作 基于堆的优先队列\npublic class MaxPQ\u0026lt;Key extends Comparable\u0026lt;Key\u0026gt;\u0026gt; { private Key[] pq; // 基于堆的完全二叉树 private int N = 0; // 存储于pq[1..N]中，pq[0]没有使用 public MaxPQ(int maxN) { pq = (Key[]) new Comparable[maxN+1]; } public Boolean isEmpty() { return N == 0; } public int size() { return N; } public void insert(Key v) { pq[++N] = v; swim(N); } public Key delMax() { Key max = pq[1]; // 从根结点得到最大元素 exch(1, N--); // 将其和最后一个结点交换 pq[N+1] = null; // 防止对象游离 sink(1); // 恢复堆的有序性 return max; } // 辅助方法的实现请见本节前面的代码框 private Boolean less(int i, int j) private void exch(int i, int j) private void swim(int k) private void sink(int k) } 说明\n优先队列由一个基于堆的完全二叉树表示， 存储于数组pq[1..N] 中，pq[0] 没有使用。在insert() 中，我们将N 加一并把新元素添加在数组最后，然后用swim() 恢复堆的秩序。在delMax() 中，我们从pq[1] 中得到需要返回的元素，然后将pq[N] 移动到pq[1]，将N 减一并用sink() 恢复堆的秩序。同时我们还将不再使用的pq[N+1] 设为null，以便系统回收它所占用的空间。和以前一样（请见1.3 节），这里省略了动态调整数组大小的代码\n对于一个含有N个元素的基于堆的优先队列，插入元素操作只需不超过（lgN+1）次比较，删除最大元素的操作需要不超过2lgN 次比较。\n在堆上进行操作 多叉堆 基于用数组表示的完全三叉树构造堆并修改相应的代码并不困难。对于数组中1 至N 的N 个元素，位置k的结点大于等于位于3k-1、3k 和3k+1 的结点，小于等于位于(k+1)/3 的结点\n调整数组大小 添加一个没有参数的构造函数， 在insert() 中添加将数组长度加倍的代码，在delMax()中添加将数组长度减半的代码，就像在1.3 节中的栈那样\n元素的不可变性 优先队列存储了用例创建的对象，但同时假设用例代码不会改变它们\n索引优先队列 注意minIndex()，最小元素的索引不一定是0，这里说的索引不是IndexMinPQ数据结构中的数组的索引。这两个不是一个意思 表2.4.6 含有N 个元素的基于堆的索引优先队列所有操作在最坏情况下的成本 索引优先队列用例 将多个有序的输入流归并成一个有序的输出流 ★注意，这多个输入流本身是有序的\npublic class Multiway { public static void merge(In[] streams) { int N = streams.length; IndexMinPQ\u0026lt;String\u0026gt; pq = new IndexMinPQ\u0026lt;String\u0026gt;(N); for (int i = 0; i \u0026lt; N; i++){ if (!streams[i].isEmpty()){ //初始化，从文件流中读取一个数，放到优先队列中 pq.insert(i, streams[i].readString()); } } while (!pq.isEmpty()) { StdOut.println(pq.min()); //从优先队列中取最小的数出来 int i = pq.delMin(); if (!streams[i].isEmpty()) //取出数的那个位置，再从文件流读一个值放进去 pq.insert(i, streams[i].readString()); } } public static void main(String[] args) { int N = args.length; In[] streams = new In[N]; for (int i = 0; i \u0026lt; N; i++) streams[i] = new In(args[i]); //三个文件地址 merge(streams); } } 堆排序 # 我们可以把任意优先队列变成一种排序方法，将所有元素插入一个查找最小元素的优先队列，然后再重复调用删除最小元素的操作来将他们按顺序删去\n用堆来实现经典而优雅的排序算法\u0026ndash;堆排序 为了与前面代码保持一致，使用面向最大元素的优先队列并重复删除最大元素；为了排序需要，直接使用swim()和sink()，且将需要排序的数组本身作为堆，省去额外空间 堆的构造\n可以从左到右，就像连续向优先队列中插入元素一样\n从右到左，用sink()函数构造子堆\n★ 重要前提：每个子堆都符合优先序列的根节点大于其他两个子节点（也就是我们可以跳过大小为1的子堆） 所以只要对每个子堆的根节点，进行sink()函数操作就可以构造出优先队列结构的数组了\n进行排序 主要是将数组的位置1和N-1进行交换，然后在1位置进行sink()操作 不断循环，即可让整个数组有序\nsort(Comparable[] a) { int N = a.length; for (int k = N/2; k \u0026gt;= 1; k--) sink(a, k, N); while (N \u0026gt; 1) { exch(a, 1, N--); sink(a, 1, N); } } 注意，这里的sink()函数被修改过，主要是指定了要sink的最后一个位置【sink() 被修改过，以a[] 和N 作为参数】 堆排序的轨迹（每次下沉后的数组内容） 堆排序：堆的构造（左）和下沉排序（右） 堆排序的主要工作都是在第二阶段完成的。这里我们将堆中的最大元素删除，然后放入堆缩小 后数组中空出的位置\n将N个元素排序，堆排序只需少于（2N x lgN+2N ）次比较（以及一般次数的交换）\n。2N 项来自于堆的构造（ 见命题R）。2NlgN 项来自于每次下沉操作最大可能需要2lgN次比较（见命题P 与命题Q）\n我们将该实现和优先队列的API 独立开来是为了突出这个排序算法的简洁性（sort() 方法只需8 行代码，sink() 函数8 行），并使其可以嵌入其他代码之中。\n小结\n在最坏的情况下它也能保证使用～ 2NlgN 次比较和恒定的额外空间。当空间十分紧张的时候（例如在嵌入式系统或低成本的移动设备中）它很流行，因为它只用几行就能实现（甚至机器码也是）较好的性能。但现代系统的许多应用很少使用它，因为它无法利用缓存。数组元素很少和相邻的其他元素进行比较，因此缓存未命中的次数要远远高于大多数比较都在相邻元素间进行的算法，如快速排序、归并排序，甚至是希尔排序 用堆实现的优先队列在现代应用程序中越来越重要，因为它能在插入操作和删除最大元素操作混合的动态场景中保证对数级别的运行时间 "},{"id":198,"href":"/zh/docs/technology/Flowable/zsx_design/01/","title":"zsx_flowable_design01","section":"Flowable","content":" 模型设计完后，下面三个表有变化\nact_cio_model act_cio_model_module_rel act_ge_bytearray 部署之后，四个表有变化 act_cio_deployment 多了39条记录 act_ge_bytearray 多了两条记录 act_re_deployment 多了一条记录 act_re_procdef 多了一条记录 流程开始运行\n下面只写上主要的几个表 送审时这个结点只能选一个 流程运行时变量表 "},{"id":199,"href":"/zh/docs/technology/Linux/hanshunping/28-39/","title":"linux_韩老师_28-39","section":"韩顺平老师_","content":" 文件目录 # 用来定位绝对路径或相对路径 cd ~ 用来定位家目录 cd .. 返回上一级 cd - 返回上一次目录\nmkdir 用于创建目录 mkdir -p hello/l1/l2 多级目录创建\nrecursion 递归 rm -rf 要删除的目录 #递归删除\n使用cp进行复制，加上 -r 进行递归复制\nrm 删除某个文件（带提示）\nrm -f 删除文件（不带提示） rm -rf 强制删除递归文件（夹） mv 用来重命名（移动到同一目录下）、（或者移动文件）\n注意，下面的命令，是将hello移动到hello2下，并改名为a（而不是hello2下的a目录） mv Hello.java hello2/a\nmv Hello.java hello2/a/ 移动到hello2下的a目录下(最后有一个斜杠) 移动目录\nmv hello2 hello1/AB 或者 mv hello2/ hello1/AB\n或者 mv hello2/ hello1/AB/\n会把整个hello2文件夹（包括hello2）移动到AB下\n同样是上面的指令，如果AB不存在，那么就会将hello2移动到hello1下，并将hello2文件夹，改名为AB\ncat 指令\ncat -p /etc/profile 浏览并显示文件 管道命令 cat -p /etc/profile | more 把前面的结果再交给more处理 （输入enter查看下一行，空格查看下一页） less指令\nless /etc/profile less指令显示的时候，是按需加载内容，效率较高, q退出 echo 输出到控制台\necho $HOSTNAME 输出环境变量 head 文件前几行\nhead -3 /etc/profile #查看文件前三行 tail 文件后几行\n实时监控 tail -f mydate.txt 覆盖 echo \u0026ldquo;hello\u0026rdquo; \u0026gt; mydate.txt 追加 echo \u0026ldquo;hi\u0026rdquo; \u0026raquo; mydate.txt cal \u0026gt; mydate.txt 将日志添加到文件后 ln指令 ln -s /root/ /home/myroot 在home下创建一个软链接，名为myroot，连接到root 此时cd myroot，就会进入root文件夹 使用rm -f 删除软连接 动态链接库 history 查看曾经执行过的命令 ! + 数字，执行曾经执行过的指令 时间日期 # date指令\u0026ndash; 显示当前日期 date date +%Y 年份 date +%m 月份 date +%d 哪一天 date \u0026ldquo;+%Y-%m-%d %H:%M:%S\u0026rdquo; 年月日时分秒 cal 2020 #2020年所有日历 查找指令 # find /home -name hello.txt 在/home目录下，按名字查找hello.txt find /home -user tom 按拥有者查找\nfind / -size -10M | more 查找小于10M的文件 ls -lh (h,以更符合人类查看的的方式显示) locate 搜索文件 （locate之前要使用updatedb指令创建） (先使用yum install -y mlocate 进行安装)\n进行查找 which ls 查看ls在哪个目录下 grep 过滤查找，管道符，\u0026quot;|\u0026quot; 表示将前一个命令的处理结果输出传递给后面的命令处理\ncat /etc/profile | grep 22 -n -i 压缩和解压 # 使用gzip 和 gunzip tar 用来压缩或者解压 压缩后的格式 .tar.gz 选项说明 -c 产生.tar打包文件 -v 显示详情信息 -f 指定压缩后的文件名 -z 打包同时压缩 -x 解包.tar文件 使用 tar -zcvf pc.tar.gz /home/pig.txt /home/cat.txt 解压 tar -zxvf pc.tar.gz 解压到指定的目录 tar -zxvf pc.tar.gz -C tom/ # "},{"id":200,"href":"/zh/docs/technology/Linux/hanshunping/21-27/","title":"linux_韩老师_21-33","section":"韩顺平老师_","content":" 用户管理 # 使用ssh root@192.168.200.201进行服务器连接 xshell中 ctr+shift+r 用来重新连接\n用户解释图 添加一个用户milan，会自动创建该用户的家目录milan\n当登录该用户时，会自动切换到家目录下 指定家目录 指定密码 用milan登录，自动切换到/home/milan pwd:显示当前用户所在的目录\n用户删除\n删除用户但保留家目录 需要用超级管理员才能删除 使用su -u root切换到超级管理员 先logout然后再删除 删除用户及家目录 userdel -r milan 建议保留家目录 查询root用户信息\n使用id xx 查询 切换用户 su - xx\n从权限高切换到权限低的用户不需要密码；反之需要 使用logout（exit也行），从root用户回到jack 查看当前用户 who am i 即使切换了用户，返回的还是root（第一次登录时的用户) 用户组（角色）\n增加、删除组\ngroupadd wudang groupdel wudang 如果添加用户的时候没有指定组，那么会创建一个跟用户名一样的名字的组 id是1002，组为king\n添加用户zwj，添加组wudang，并将zwj添加到wudang组里面\ngroupadd wudang useradd -g wudang zwj 修改用户所在组\ngroupadd mojiao usermod -g mojiao zwj 关于用户和组相关的文件\n/etc/passwd 每行的含义 shell 解释和翻译指令 一般用bash，还有其他，很多\n/etc/shadow 口令配置文件\n每行的含义 /etc/group 记录组的信息 组名：口令：组标识号：组内用户列表\n运行级别 # 基本介绍\n0 关机 1 单用户（找回密码） 2 多用户状态没有网络服务 3 多用户状态有网络服务 4系统未使用保留给用户 5 图形界面 6 系统重启 在图形界面输入init 3 会直接进入终端界面\n之后输入init 5 会重新进入图形界面 init 0 会直接关机\n指定默认级别 centosOS7之前，在/etc/inittab文件中 之后进行了简化，如下 查看默认级别\nsystemctl get-default # multi-user.target 设置默认级别\nsystemctl set-default multi-user.target 找回root密码 # 这里讲的是centos os7之后\n重启后，立马按e\n然后光标往下滑 在utf-8后面，加入 init=/bin/sh （进入单用户实例，注意 这里不要加入空格）\n然后ctrl+x 表示启动\n然后输入\nmount -o remount,rw / passwd 修改成功 然后再输入\ntouch /.autorelabel exec /sbin/init exec /sbin/init 之后时间比较长，等待一会，密码则生效\n(卡住两三分钟)\nssh root@192.168.200.201 登录成功\n帮助指令 # man ls linux中，隐藏文件以 . 开头（以点开头） 输入q退出man ls选项可以组合使用 ls -l 单列输出(use a long listing format)，信息最全 ls -la 单列输出，包括隐藏文件 ls -al /root 显示/root目录下的内容 help 内置命令的帮助信息\n该命令在zsh下不能用，所以使用下面指令切换 chsh -s /bin/bash #zsh切换到bash，重启后生效 chsh -s /bin/zsh #bash切换到zsh，重启后生效 help cd End "},{"id":201,"href":"/zh/docs/technology/MyBatis-Plus/official/hello/","title":"官方的hello-world","section":"My Batis Plus","content":" 简介 # MyBatis-Plus (opens new window)（简称 MP）是一个 MyBatis (opens new window)的增强工具，在 MyBatis 的基础上只做增强不做改变，为简化开发、提高效率而生。 快速开始 # 数据库的Schema脚本 resources/db/schema-mysql.sql\nDROP TABLE IF EXISTS user; CREATE TABLE user ( id BIGINT(20) NOT NULL COMMENT \u0026#39;主键ID\u0026#39;, name VARCHAR(30) NULL DEFAULT NULL COMMENT \u0026#39;姓名\u0026#39;, age INT(11) NULL DEFAULT NULL COMMENT \u0026#39;年龄\u0026#39;, email VARCHAR(50) NULL DEFAULT NULL COMMENT \u0026#39;邮箱\u0026#39;, PRIMARY KEY (id) ); 数据库Data脚本 resources/db/data-mysql.sql\nDELETE FROM user; INSERT INTO user (id, name, age, email) VALUES (1, \u0026#39;Jone\u0026#39;, 18, \u0026#39;test1@baomidou.com\u0026#39;), (2, \u0026#39;Jack\u0026#39;, 20, \u0026#39;test2@baomidou.com\u0026#39;), (3, \u0026#39;Tom\u0026#39;, 28, \u0026#39;test3@baomidou.com\u0026#39;), (4, \u0026#39;Sandy\u0026#39;, 21, \u0026#39;test4@baomidou.com\u0026#39;), (5, \u0026#39;Billie\u0026#39;, 24, \u0026#39;test5@baomidou.com\u0026#39;); 创建一个spring boot工程（使用maven）\n父工程\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.7.0\u0026lt;/version\u0026gt; \u0026lt;relativePath/\u0026gt; \u0026lt;/parent\u0026gt; springboot 相关仓库及mybatis-plus、mysql、Lombok相关仓库引入\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.h2database\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;h2\u0026lt;/artifactId\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/mysql/mysql-connector-java --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.29\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.projectlombok/lombok --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.projectlombok\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;lombok\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.18.24\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;provided\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 配置resources/application.yml文件\nspring: datasource: url: jdbc:mysql://localhost:3306/mybatis_plus_demo?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;allowMultiQueries=true\u0026amp;nullCatalogMeansCurrent=true username: root password: 123456 driver-class-name: com.mysql.cj.jdbc.Driver sql: init: schema-locations: classpath:db/schema-mysql.sql data-locations: classpath:db/data-mysql.sql mode: always entity类和mapper类的处理\nentity\n@Data public class User { private Long id; private String name; private Integer age; private String email; } mapper\nimport com.baomidou.mybatisplus.core.mapper.BaseMapper; import com.baomidou.mybatisplus.samples.quickstart.entity.User; public interface UserMapper extends BaseMapper\u0026lt;User\u0026gt; { } 测试类\nimport com.baomidou.mybatisplus.samples.quickstart.Application; import com.baomidou.mybatisplus.samples.quickstart.entity.User; import com.baomidou.mybatisplus.samples.quickstart.mapper.UserMapper; import org.junit.jupiter.api.Assertions; import org.junit.jupiter.api.Test; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import java.util.List; @SpringBootTest(classes = {Application.class}) public class SampleTest { @Autowired private UserMapper userMapper; @Test public void testSelect() { System.out.println((\u0026#34;----- selectAll method test ------\u0026#34;)); List\u0026lt;User\u0026gt; userList = userMapper.selectList(null); Assertions.assertEquals(5, userList.size()); userList.forEach(System.out::println); } } mybatis-plus 代码自动生成 # maven 依赖\n\u0026lt;!-- https://mvnrepository.com/artifact/com.baomidou/mybatis-plus-generator --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.baomidou\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mybatis-plus-generator\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.5.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.apache.velocity/velocity-engine-core --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.velocity\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;velocity-engine-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 在测试类中编写程序让其自动生成\nimport com.baomidou.mybatisplus.generator.FastAutoGenerator; import com.baomidou.mybatisplus.generator.config.DataSourceConfig; import org.apache.ibatis.jdbc.ScriptRunner; import java.io.InputStream; import java.io.InputStreamReader; import java.sql.Connection; import java.sql.SQLException; /** * \u0026lt;p\u0026gt; * 快速生成 * \u0026lt;/p\u0026gt; * * @author lanjerry * @since 2021-09-16 */ public class FastAutoGeneratorTest { /** * 执行初始化数据库脚本 */ public static void before() throws SQLException { Connection conn = DATA_SOURCE_CONFIG.build().getConn(); InputStream inputStream = FastAutoGeneratorTest.class.getResourceAsStream(\u0026#34;/db/schema-mysql.sql\u0026#34;); ScriptRunner scriptRunner = new ScriptRunner(conn); scriptRunner.setAutoCommit(true); scriptRunner.runScript(new InputStreamReader(inputStream)); conn.close(); } /** * 数据源配置 */ private static final DataSourceConfig.Builder DATA_SOURCE_CONFIG = new DataSourceConfig .Builder(\u0026#34;jdbc:mysql://localhost:3306/mybatis_plus_demo?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;allowMultiQueries=true\u0026amp;nullCatalogMeansCurrent=true\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;123456\u0026#34;); /** * 执行 run */ public static void main(String[] args) throws SQLException { before(); FastAutoGenerator.create(DATA_SOURCE_CONFIG) // 全局配置 .globalConfig((scanner, builder) -\u0026gt; builder.author(scanner.apply(\u0026#34;请输入作者名称\u0026#34;))) // 包配置 .packageConfig((scanner, builder) -\u0026gt; builder.parent(scanner.apply(\u0026#34;请输入包名\u0026#34;))) // 策略配置 .strategyConfig((scanner, builder) -\u0026gt; builder.addInclude(scanner.apply(\u0026#34;请输入表名，多个表名用,隔开\u0026#34;))) /* 模板引擎配置，默认 Velocity 可选模板引擎 Beetl 或 Freemarker .templateEngine(new BeetlTemplateEngine()) .templateEngine(new FreemarkerTemplateEngine()) */ .execute(); } } 使用mybats-x插件自动生成代码\n操作 编写controller确定\n@RestController @RequestMapping(\u0026#34;user\u0026#34;) public class UserController { @Autowired private UserService userService; @RequestMapping(\u0026#34;findAll\u0026#34;) public List\u0026lt;User\u0026gt; findAll(){ List\u0026lt;User\u0026gt; list = userService.list(); return list; } } xml文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE mapper PUBLIC \u0026#34;-//mybatis.org//DTD Mapper 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-mapper.dtd\u0026#34;\u0026gt; \u0026lt;mapper namespace=\u0026#34;com.baomidou.mybatisplus.samples.quickstart.mapper.UserMapper\u0026#34;\u0026gt; \u0026lt;resultMap id=\u0026#34;BaseResultMap\u0026#34; type=\u0026#34;com.baomidou.mybatisplus.samples.quickstart.entity.User\u0026#34;\u0026gt; \u0026lt;id property=\u0026#34;id\u0026#34; column=\u0026#34;id\u0026#34; jdbcType=\u0026#34;BIGINT\u0026#34;/\u0026gt; \u0026lt;result property=\u0026#34;name\u0026#34; column=\u0026#34;name\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34;/\u0026gt; \u0026lt;result property=\u0026#34;age\u0026#34; column=\u0026#34;age\u0026#34; jdbcType=\u0026#34;INTEGER\u0026#34;/\u0026gt; \u0026lt;result property=\u0026#34;email\u0026#34; column=\u0026#34;email\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34;/\u0026gt; \u0026lt;/resultMap\u0026gt; \u0026lt;sql id=\u0026#34;Base_Column_List\u0026#34;\u0026gt; id,name,age, email \u0026lt;/sql\u0026gt; \u0026lt;/mapper\u0026gt; entity\n/** * * @TableName user */ @TableName(value =\u0026#34;user\u0026#34;) public class User implements Serializable { /** * 主键ID */ @TableId private Long id; /** * 姓名 */ private String name; /** * 年龄 */ private Integer age; /** * 邮箱 */ private String email; @TableField(exist = false) private static final long serialVersionUID = 1L; /** * 主键ID */ public Long getId() { return id; } /** * 主键ID */ public void setId(Long id) { this.id = id; } /** * 姓名 */ public String getName() { return name; } /** * 姓名 */ public void setName(String name) { this.name = name; } /** * 年龄 */ public Integer getAge() { return age; } /** * 年龄 */ public void setAge(Integer age) { this.age = age; } /** * 邮箱 */ public String getEmail() { return email; } /** * 邮箱 */ public void setEmail(String email) { this.email = email; } @Override public boolean equals(Object that) { if (this == that) { return true; } if (that == null) { return false; } if (getClass() != that.getClass()) { return false; } User other = (User) that; return (this.getId() == null ? other.getId() == null : this.getId().equals(other.getId())) \u0026amp;\u0026amp; (this.getName() == null ? other.getName() == null : this.getName().equals(other.getName())) \u0026amp;\u0026amp; (this.getAge() == null ? other.getAge() == null : this.getAge().equals(other.getAge())) \u0026amp;\u0026amp; (this.getEmail() == null ? other.getEmail() == null : this.getEmail().equals(other.getEmail())); } @Override public int hashCode() { final int prime = 31; int result = 1; result = prime * result + ((getId() == null) ? 0 : getId().hashCode()); result = prime * result + ((getName() == null) ? 0 : getName().hashCode()); result = prime * result + ((getAge() == null) ? 0 : getAge().hashCode()); result = prime * result + ((getEmail() == null) ? 0 : getEmail().hashCode()); return result; } @Override public String toString() { StringBuilder sb = new StringBuilder(); sb.append(getClass().getSimpleName()); sb.append(\u0026#34; [\u0026#34;); sb.append(\u0026#34;Hash = \u0026#34;).append(hashCode()); sb.append(\u0026#34;, id=\u0026#34;).append(id); sb.append(\u0026#34;, name=\u0026#34;).append(name); sb.append(\u0026#34;, age=\u0026#34;).append(age); sb.append(\u0026#34;, email=\u0026#34;).append(email); sb.append(\u0026#34;, serialVersionUID=\u0026#34;).append(serialVersionUID); sb.append(\u0026#34;]\u0026#34;); return sb.toString(); } } service接口类\npublic interface UserService extends IService\u0026lt;User\u0026gt; { } serviceImpl\n@Service public class UserServiceImpl extends ServiceImpl\u0026lt;UserMapper, User\u0026gt; implements UserService{ } mapper\npublic interface UserMapper extends BaseMapper\u0026lt;User\u0026gt; { } controller测试\n@RestController @RequestMapping(\u0026#34;user\u0026#34;) public class UserController { @Autowired private UserService userService; @RequestMapping(\u0026#34;findAll\u0026#34;) public List\u0026lt;User\u0026gt; findAll(){ List\u0026lt;User\u0026gt; list = userService.list(); return list; } } 测试 使用mybatis-x 插件（idea）\n"},{"id":202,"href":"/zh/docs/technology/Flowable/boge_blbl/03-others/","title":"boge-03-其他","section":"基础(波哥)_","content":" 会签 # 流程图绘制 注意上面几个参数\n多实例类型用来判断串行并行 基数（有几个用户处理） 元素变量 集合（集合变量） 完成条件\u0026ndash;这里填的是 ${nrOfCompletedInstances \u0026gt; 1 } 在任务监听器 package org.flowable.listener; import org.flowable.engine.ProcessEngine; import org.flowable.engine.ProcessEngines; import org.flowable.engine.TaskService; import org.flowable.engine.delegate.TaskListener; import org.flowable.task.api.Task; import org.flowable.task.service.delegate.DelegateTask; public class MultiInstanceTaskListener implements TaskListener { @Override public void notify(DelegateTask delegateTask) { System.out.println(\u0026#34;处理aaaa\u0026#34;); if(delegateTask.getEventName().equals(\u0026#34;create\u0026#34;)) { System.out.println(\u0026#34;任务id\u0026#34; + delegateTask.getId()); System.out.println(\u0026#34;哪些人需要会签\u0026#34; + delegateTask.getVariable(\u0026#34;persons\u0026#34;)); System.out.println(\u0026#34;任务处理人\u0026#34; + delegateTask.getVariable(\u0026#34;person\u0026#34;)); ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); Task task = taskService.createTaskQuery().taskId(delegateTask.getId()).singleResult(); task.setAssignee(delegateTask.getVariable(\u0026#34;person\u0026#34;).toString()); taskService.saveTask(task); } } } xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;join-key\u0026#34; name=\u0026#34;会签测试1\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;join-desc\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; name=\u0026#34;申请人\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-477F728E-2F63-43BF-A278-76FBCF58B475\u0026#34; name=\u0026#34;会签人员\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;flowable:taskListener event=\u0026#34;create\u0026#34; class=\u0026#34;org.flowable.listener.MultiInstanceTaskListener\u0026#34;\u0026gt;\u0026lt;/flowable:taskListener\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;multiInstanceLoopCharacteristics isSequential=\u0026#34;false\u0026#34; flowable:collection=\u0026#34;persons\u0026#34; flowable:elementVariable=\u0026#34;person\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt;\u0026lt;/extensionElements\u0026gt; \u0026lt;loopCardinality\u0026gt;3\u0026lt;/loopCardinality\u0026gt; \u0026lt;completionCondition\u0026gt;${nrOfCompletedInstances \u0026gt; 1 }\u0026lt;/completionCondition\u0026gt; \u0026lt;/multiInstanceLoopCharacteristics\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-B5F81E26-E53B-4D10-8328-C5B3C35E0DD5\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-477F728E-2F63-43BF-A278-76FBCF58B475\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-3448D902-AE89-467D-8945-805BDEDE7BCA\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-598B2F86-A13B-48BE-88AF-6B61CDA24EA7\u0026#34; sourceRef=\u0026#34;sid-477F728E-2F63-43BF-A278-76FBCF58B475\u0026#34; targetRef=\u0026#34;sid-3448D902-AE89-467D-8945-805BDEDE7BCA\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_join-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;join-key\u0026#34; id=\u0026#34;BPMNPlane_join-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;105.0\u0026#34; y=\u0026#34;100.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-477F728E-2F63-43BF-A278-76FBCF58B475\u0026#34; id=\u0026#34;BPMNShape_sid-477F728E-2F63-43BF-A278-76FBCF58B475\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;330.0\u0026#34; y=\u0026#34;60.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-3448D902-AE89-467D-8945-805BDEDE7BCA\u0026#34; id=\u0026#34;BPMNShape_sid-3448D902-AE89-467D-8945-805BDEDE7BCA\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;600.0\u0026#34; y=\u0026#34;106.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-B5F81E26-E53B-4D10-8328-C5B3C35E0DD5\u0026#34; id=\u0026#34;BPMNEdge_sid-B5F81E26-E53B-4D10-8328-C5B3C35E0DD5\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;134.94999855629513\u0026#34; y=\u0026#34;115.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;232.5\u0026#34; y=\u0026#34;115.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;232.5\u0026#34; y=\u0026#34;100.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;330.0\u0026#34; y=\u0026#34;100.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-598B2F86-A13B-48BE-88AF-6B61CDA24EA7\u0026#34; id=\u0026#34;BPMNEdge_sid-598B2F86-A13B-48BE-88AF-6B61CDA24EA7\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;429.95000000000005\u0026#34; y=\u0026#34;100.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;515.0\u0026#34; y=\u0026#34;100.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;515.0\u0026#34; y=\u0026#34;120.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;600.0\u0026#34; y=\u0026#34;120.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 将流程部署\n@Test public void deploy() { deleteAll(); ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = engine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;会签测试1.bpmn20.xml\u0026#34;) .deploy(); System.out.println(\u0026#34;部署成功:\u0026#34; + deploy.getId()); } 运行流程\n@Test public void run(){ ProcessEngine defaultProcessEngine = ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = defaultProcessEngine.getRuntimeService(); HashMap\u0026lt;String,Object\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); ArrayList\u0026lt;String\u0026gt; persons=new ArrayList\u0026lt;\u0026gt;(); persons.add(\u0026#34;张三\u0026#34;); persons.add(\u0026#34;李四\u0026#34;); persons.add(\u0026#34;王五\u0026#34;); map.put(\u0026#34;persons\u0026#34;,persons); ProcessInstance processInstance = runtimeService.startProcessInstanceById(\u0026#34;join-key:1:17504\u0026#34;,map); } 此时数据库会有三个任务 完成第一个任务\n@Test public void completeTask(){ //15020 ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); taskService.complete(\u0026#34;20020\u0026#34;); } 再完成一个任务后，流程会直接结束\n@Test public void completeTask(){ //15020 ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); taskService.complete(\u0026#34;20028\u0026#34;); } 流程结束\n"},{"id":203,"href":"/zh/docs/technology/Flowable/boge_blbl/02-advance_6/","title":"boge-02-flowable进阶_6","section":"基础(波哥)_","content":" 任务回退-串行回退 # 流程图绘制 xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;reback-key\u0026#34; name=\u0026#34;回退处理\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;reback-desc\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-D380E41A-48EE-4C08-AD01-1D509C512543\u0026#34; name=\u0026#34;用户1\u0026#34; flowable:assignee=\u0026#34;user1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-E2423FC5-F954-43D3-B57C-8460057CB7D6\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-D380E41A-48EE-4C08-AD01-1D509C512543\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-AF50E3D0-2014-4308-A717-D76586837D70\u0026#34; name=\u0026#34;用户2\u0026#34; flowable:assignee=\u0026#34;user2\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-7C8750DC-E1C1-4AB2-B18C-2C103B61A5E5\u0026#34; sourceRef=\u0026#34;sid-D380E41A-48EE-4C08-AD01-1D509C512543\u0026#34; targetRef=\u0026#34;sid-AF50E3D0-2014-4308-A717-D76586837D70\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-F4CE7565-5977-4B9C-A603-AB3B817B8C8C\u0026#34; name=\u0026#34;用户3\u0026#34; flowable:assignee=\u0026#34;user3\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-F91582FE-D110-48C9-9407-605E503E42B2\u0026#34; sourceRef=\u0026#34;sid-AF50E3D0-2014-4308-A717-D76586837D70\u0026#34; targetRef=\u0026#34;sid-F4CE7565-5977-4B9C-A603-AB3B817B8C8C\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-727C1235-F9C1-4CC5-BC6C-E56ABCA105B0\u0026#34; name=\u0026#34;用户4\u0026#34; flowable:assignee=\u0026#34;user4\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-6D998C20-2A97-44B5-92D0-118E5CB05795\u0026#34; sourceRef=\u0026#34;sid-F4CE7565-5977-4B9C-A603-AB3B817B8C8C\u0026#34; targetRef=\u0026#34;sid-727C1235-F9C1-4CC5-BC6C-E56ABCA105B0\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-6E5F5037-1979-4150-8408-D0BFD0315BCA\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-3ECF3E34-6C07-4AE6-997B-583BF8868AC8\u0026#34; sourceRef=\u0026#34;sid-727C1235-F9C1-4CC5-BC6C-E56ABCA105B0\u0026#34; targetRef=\u0026#34;sid-6E5F5037-1979-4150-8408-D0BFD0315BCA\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_reback-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;reback-key\u0026#34; id=\u0026#34;BPMNPlane_reback-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;163.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-D380E41A-48EE-4C08-AD01-1D509C512543\u0026#34; id=\u0026#34;BPMNShape_sid-D380E41A-48EE-4C08-AD01-1D509C512543\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;165.0\u0026#34; y=\u0026#34;135.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-AF50E3D0-2014-4308-A717-D76586837D70\u0026#34; id=\u0026#34;BPMNShape_sid-AF50E3D0-2014-4308-A717-D76586837D70\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;320.0\u0026#34; y=\u0026#34;138.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-F4CE7565-5977-4B9C-A603-AB3B817B8C8C\u0026#34; id=\u0026#34;BPMNShape_sid-F4CE7565-5977-4B9C-A603-AB3B817B8C8C\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;465.0\u0026#34; y=\u0026#34;138.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-727C1235-F9C1-4CC5-BC6C-E56ABCA105B0\u0026#34; id=\u0026#34;BPMNShape_sid-727C1235-F9C1-4CC5-BC6C-E56ABCA105B0\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;610.0\u0026#34; y=\u0026#34;138.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-6E5F5037-1979-4150-8408-D0BFD0315BCA\u0026#34; id=\u0026#34;BPMNShape_sid-6E5F5037-1979-4150-8408-D0BFD0315BCA\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;755.0\u0026#34; y=\u0026#34;164.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-6D998C20-2A97-44B5-92D0-118E5CB05795\u0026#34; id=\u0026#34;BPMNEdge_sid-6D998C20-2A97-44B5-92D0-118E5CB05795\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;564.9499999999907\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;609.9999999999807\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-7C8750DC-E1C1-4AB2-B18C-2C103B61A5E5\u0026#34; id=\u0026#34;BPMNEdge_sid-7C8750DC-E1C1-4AB2-B18C-2C103B61A5E5\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;264.9499999999882\u0026#34; y=\u0026#34;175.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;292.5\u0026#34; y=\u0026#34;175.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;292.5\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;319.9999999999603\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-3ECF3E34-6C07-4AE6-997B-583BF8868AC8\u0026#34; id=\u0026#34;BPMNEdge_sid-3ECF3E34-6C07-4AE6-997B-583BF8868AC8\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;709.9499999999999\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;755.0\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-E2423FC5-F954-43D3-B57C-8460057CB7D6\u0026#34; id=\u0026#34;BPMNEdge_sid-E2423FC5-F954-43D3-B57C-8460057CB7D6\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.94340692927761\u0026#34; y=\u0026#34;177.55019845363262\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;164.99999999999906\u0026#34; y=\u0026#34;176.4985\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-F91582FE-D110-48C9-9407-605E503E42B2\u0026#34; id=\u0026#34;BPMNEdge_sid-F91582FE-D110-48C9-9407-605E503E42B2\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;419.94999999999067\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;464.9999999999807\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 部署并运行\n依次完成1，2，3\n从任意节点跳转到任意节点\n@Test public void backProcess(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = engine.getRuntimeService(); //从当前流程跳转到任意节点 runtimeService.createChangeActivityStateBuilder() .processInstanceId(\u0026#34;2501\u0026#34;) //4--\u0026gt;3 ，活动id .moveActivityIdTo(\u0026#34;sid-727C1235-F9C1-4CC5-BC6C-E56ABCA105B0\u0026#34;, \u0026#34;sid-F4CE7565-5977-4B9C-A603-AB3B817B8C8C\u0026#34;) .changeState(); } 可以在这个表里让用户选择回退节点 此时让user3再完成任务\n注：用下面的方法，不关心当前节点，只写明要跳转的结点即可 自定义表单 # 内置表单 # 绘制 xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;form1-test-key\u0026#34; name=\u0026#34;form1-test-name\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;form1-test-desc\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;flowable:formProperty id=\u0026#34;days\u0026#34; name=\u0026#34;天数\u0026#34; type=\u0026#34;long\u0026#34; default=\u0026#34;5\u0026#34;\u0026gt;\u0026lt;/flowable:formProperty\u0026gt; \u0026lt;flowable:formProperty id=\u0026#34;start_time\u0026#34; name=\u0026#34;开始时间\u0026#34; type=\u0026#34;date\u0026#34; datePattern=\u0026#34;MM-dd-yyyy\u0026#34;\u0026gt;\u0026lt;/flowable:formProperty\u0026gt; \u0026lt;flowable:formProperty id=\u0026#34;reason\u0026#34; name=\u0026#34;原因\u0026#34; type=\u0026#34;string\u0026#34;\u0026gt;\u0026lt;/flowable:formProperty\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-4C9C8571-1423-4137-93FC-6A138D504E24\u0026#34; name=\u0026#34;用户申请\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;flowable:formProperty id=\u0026#34;days\u0026#34; name=\u0026#34;天数\u0026#34; type=\u0026#34;long\u0026#34;\u0026gt;\u0026lt;/flowable:formProperty\u0026gt; \u0026lt;flowable:formProperty id=\u0026#34;start_time\u0026#34; name=\u0026#34;开始时间\u0026#34; type=\u0026#34;date\u0026#34; datePattern=\u0026#34;MM-dd-yyyy\u0026#34;\u0026gt;\u0026lt;/flowable:formProperty\u0026gt; \u0026lt;flowable:formProperty id=\u0026#34;reason\u0026#34; name=\u0026#34;原因\u0026#34; type=\u0026#34;string\u0026#34;\u0026gt;\u0026lt;/flowable:formProperty\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-8944FE04-D27B-435F-A8A8-4E545AB3D6C0\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-4C9C8571-1423-4137-93FC-6A138D504E24\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;exclusiveGateway id=\u0026#34;sid-35DD948A-C095-486E-98E0-4A0EEC4D9FBC\u0026#34;\u0026gt;\u0026lt;/exclusiveGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-0EA36B83-6115-414F-BC7D-9CB338B03F22\u0026#34; sourceRef=\u0026#34;sid-4C9C8571-1423-4137-93FC-6A138D504E24\u0026#34; targetRef=\u0026#34;sid-35DD948A-C095-486E-98E0-4A0EEC4D9FBC\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-4B6496FE-B5FE-41AC-83F8-4B7224B09FBD\u0026#34; name=\u0026#34;总监审批\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/userTask\u0026gt; \u0026lt;userTask id=\u0026#34;sid-8DE5EA05-89D5-48B0-9359-F8ABFB3A3500\u0026#34; name=\u0026#34;部门经理审批\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/userTask\u0026gt; \u0026lt;exclusiveGateway id=\u0026#34;sid-0EC09183-F41B-4785-83E7-423BB86EB013\u0026#34;\u0026gt;\u0026lt;/exclusiveGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-562C26B5-B634-4771-BF54-C311D56A5317\u0026#34; sourceRef=\u0026#34;sid-4B6496FE-B5FE-41AC-83F8-4B7224B09FBD\u0026#34; targetRef=\u0026#34;sid-0EC09183-F41B-4785-83E7-423BB86EB013\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-9AC3E009-D4D6-4D8B-883C-701E044715E9\u0026#34; sourceRef=\u0026#34;sid-8DE5EA05-89D5-48B0-9359-F8ABFB3A3500\u0026#34; targetRef=\u0026#34;sid-0EC09183-F41B-4785-83E7-423BB86EB013\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-9CD52D35-7874-42F4-B392-466F71316BFE\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-FABB64D1-0182-41D8-90FE-53FE7FE3F024\u0026#34; sourceRef=\u0026#34;sid-0EC09183-F41B-4785-83E7-423BB86EB013\u0026#34; targetRef=\u0026#34;sid-9CD52D35-7874-42F4-B392-466F71316BFE\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-E4DB6764-3EA3-427B-AD00-4D812E404FD6\u0026#34; sourceRef=\u0026#34;sid-35DD948A-C095-486E-98E0-4A0EEC4D9FBC\u0026#34; targetRef=\u0026#34;sid-4B6496FE-B5FE-41AC-83F8-4B7224B09FBD\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${day \u0026gt; 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-585C37CB-61FE-4518-B3B6-5722A90A854F\u0026#34; sourceRef=\u0026#34;sid-35DD948A-C095-486E-98E0-4A0EEC4D9FBC\u0026#34; targetRef=\u0026#34;sid-8DE5EA05-89D5-48B0-9359-F8ABFB3A3500\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${day \u0026lt;= 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_form1-test-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;form1-test-key\u0026#34; id=\u0026#34;BPMNPlane_form1-test-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;163.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-4C9C8571-1423-4137-93FC-6A138D504E24\u0026#34; id=\u0026#34;BPMNShape_sid-4C9C8571-1423-4137-93FC-6A138D504E24\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;175.0\u0026#34; y=\u0026#34;138.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-35DD948A-C095-486E-98E0-4A0EEC4D9FBC\u0026#34; id=\u0026#34;BPMNShape_sid-35DD948A-C095-486E-98E0-4A0EEC4D9FBC\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;315.0\u0026#34; y=\u0026#34;150.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-4B6496FE-B5FE-41AC-83F8-4B7224B09FBD\u0026#34; id=\u0026#34;BPMNShape_sid-4B6496FE-B5FE-41AC-83F8-4B7224B09FBD\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;405.0\u0026#34; y=\u0026#34;30.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-8DE5EA05-89D5-48B0-9359-F8ABFB3A3500\u0026#34; id=\u0026#34;BPMNShape_sid-8DE5EA05-89D5-48B0-9359-F8ABFB3A3500\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;405.0\u0026#34; y=\u0026#34;225.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-0EC09183-F41B-4785-83E7-423BB86EB013\u0026#34; id=\u0026#34;BPMNShape_sid-0EC09183-F41B-4785-83E7-423BB86EB013\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;585.0\u0026#34; y=\u0026#34;165.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-9CD52D35-7874-42F4-B392-466F71316BFE\u0026#34; id=\u0026#34;BPMNShape_sid-9CD52D35-7874-42F4-B392-466F71316BFE\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;670.0\u0026#34; y=\u0026#34;171.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-585C37CB-61FE-4518-B3B6-5722A90A854F\u0026#34; id=\u0026#34;BPMNEdge_sid-585C37CB-61FE-4518-B3B6-5722A90A854F\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;335.5\u0026#34; y=\u0026#34;189.43998414376327\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;335.5\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;405.0\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-E4DB6764-3EA3-427B-AD00-4D812E404FD6\u0026#34; id=\u0026#34;BPMNEdge_sid-E4DB6764-3EA3-427B-AD00-4D812E404FD6\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;336.66824324324324\u0026#34; y=\u0026#34;151.67117117117118\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;342.0\u0026#34; y=\u0026#34;66.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;404.9999999999999\u0026#34; y=\u0026#34;68.23008849557522\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-0EA36B83-6115-414F-BC7D-9CB338B03F22\u0026#34; id=\u0026#34;BPMNEdge_sid-0EA36B83-6115-414F-BC7D-9CB338B03F22\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.5\u0026#34; flowable:targetDockerY=\u0026#34;20.5\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;274.95000000000005\u0026#34; y=\u0026#34;174.60633484162895\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;316.77118644067775\u0026#34; y=\u0026#34;171.76800847457628\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-8944FE04-D27B-435F-A8A8-4E545AB3D6C0\u0026#34; id=\u0026#34;BPMNEdge_sid-8944FE04-D27B-435F-A8A8-4E545AB3D6C0\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.9499984899576\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;174.9999999999917\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-FABB64D1-0182-41D8-90FE-53FE7FE3F024\u0026#34; id=\u0026#34;BPMNEdge_sid-FABB64D1-0182-41D8-90FE-53FE7FE3F024\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;624.5591869398207\u0026#34; y=\u0026#34;185.37820512820514\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;670.0002755524882\u0026#34; y=\u0026#34;185.08885188426405\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-9AC3E009-D4D6-4D8B-883C-701E044715E9\u0026#34; id=\u0026#34;BPMNEdge_sid-9AC3E009-D4D6-4D8B-883C-701E044715E9\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.0\u0026#34; flowable:targetDockerY=\u0026#34;20.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;504.95000000000005\u0026#34; y=\u0026#34;238.33333333333334\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;591.9565217391304\u0026#34; y=\u0026#34;191.93913043478258\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-562C26B5-B634-4771-BF54-C311D56A5317\u0026#34; id=\u0026#34;BPMNEdge_sid-562C26B5-B634-4771-BF54-C311D56A5317\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.5\u0026#34; flowable:targetDockerY=\u0026#34;20.5\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;504.95000000000005\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;605.5\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;605.5\u0026#34; y=\u0026#34;165.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 将流程定义部署\n@Test public void deploy() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = engine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;form1-test-name.bpmn20.xml\u0026#34;) .deploy(); System.out.println(\u0026#34;部署成功:\u0026#34; + deploy.getId()); } 查看部署的流程内置的表单\n@Test public void getStartForm(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); FormService formService = engine.getFormService(); StartFormData startFormData = formService.getStartFormData(\u0026#34;form1-test-key:1:17504\u0026#34;); List\u0026lt;FormProperty\u0026gt; formProperties = startFormData.getFormProperties(); for (FormProperty property:formProperties){ System.out.println(\u0026#34;id==\u0026gt;\u0026#34;+property.getId()); System.out.println(\u0026#34;name==\u0026gt;\u0026#34;+property.getName()); System.out.println(\u0026#34;value==\u0026gt;\u0026#34;+property.getValue()); } } 第一种启动方式，通过map 第二种启动方式\n@Test public void startProcess2(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); FormService formService = engine.getFormService(); Map\u0026lt;String,String\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;days\u0026#34;,\u0026#34;2\u0026#34;); map.put(\u0026#34;startTime\u0026#34;,\u0026#34;22020405\u0026#34;); map.put(\u0026#34;reason\u0026#34;,\u0026#34;想玩\u0026#34;); formService.submitStartFormData(\u0026#34;form1-test-key:1:17504\u0026#34;,map); } 注意查看act_ru_variable变量表 查看任务中的表单数据\n/** * 查看对应的表单数据 */ @Test public void getTaskFormData(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); FormService formService = engine.getFormService(); TaskFormData taskFormData = formService.getTaskFormData(\u0026#34;20012\u0026#34;); List\u0026lt;FormProperty\u0026gt; formProperties = taskFormData.getFormProperties(); for (FormProperty property:formProperties){ System.out.println(\u0026#34;id==\u0026gt;\u0026#34;+property.getId()); System.out.println(\u0026#34;name==\u0026gt;\u0026#34;+property.getName()); System.out.println(\u0026#34;value==\u0026gt;\u0026#34;+property.getValue()); } //这里做一个测试，设置处理人 /*TaskService taskService = engine.getTaskService(); taskService.setAssignee(\u0026#34;20012\u0026#34;,\u0026#34;lalala\u0026#34;);*/ } 查看完成的任务【主要】//有点问题，不管 外置表单 # [flowable-ui中没找到，不知道是不是eclipse独有的]\n"},{"id":204,"href":"/zh/docs/technology/Flowable/boge_blbl/02-advance_5/","title":"boge-02-flowable进阶_5","section":"基础(波哥)_","content":" 网关 # 排他网关 # 会按照所有出口顺序流定义的顺序对它们进行计算，选择第一个条件计算为true的顺序流（当没有设置条件时，认为顺序流为true）继续流程\n排他网关的绘制 xml文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;holiday-exclusive\u0026#34; name=\u0026#34;请假流程-排他网关\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-3D5ED4D4-97F5-4FFD-B160-F00566ECC55E\u0026#34; name=\u0026#34;创建请假单\u0026#34; flowable:assignee=\u0026#34;zhangsan\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-33A73370-751D-413F-9306-39DEAA674DB6\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-3D5ED4D4-97F5-4FFD-B160-F00566ECC55E\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;exclusiveGateway id=\u0026#34;sid-5B2117E6-D341-49F2-85B2-336CA836C7D8\u0026#34;\u0026gt;\u0026lt;/exclusiveGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-D1B1F6E0-EA7F-4FF7-AD0C-5D43DBCEBFD2\u0026#34; sourceRef=\u0026#34;sid-3D5ED4D4-97F5-4FFD-B160-F00566ECC55E\u0026#34; targetRef=\u0026#34;sid-5B2117E6-D341-49F2-85B2-336CA836C7D8\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-08A6CB64-C9BB-4342-852D-444A75315BDE\u0026#34; name=\u0026#34;总经理审批\u0026#34; flowable:assignee=\u0026#34;wangwu\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;userTask id=\u0026#34;sid-EA98D0C3-E41D-4DEB-8933-91A1B7301ABE\u0026#34; name=\u0026#34;部门经理审批\u0026#34; flowable:assignee=\u0026#34;lisi\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;userTask id=\u0026#34;sid-24F73F7F-EB61-484F-A494-686E194D0118\u0026#34; name=\u0026#34;人事审批\u0026#34; flowable:assignee=\u0026#34;zhaoliu\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-8BA0B88C-BA4F-446D-B5E7-6BF0830B1DC8\u0026#34; sourceRef=\u0026#34;sid-EA98D0C3-E41D-4DEB-8933-91A1B7301ABE\u0026#34; targetRef=\u0026#34;sid-24F73F7F-EB61-484F-A494-686E194D0118\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-E748F81F-B0B2-4C34-B993-FBAA2BCD0995\u0026#34; sourceRef=\u0026#34;sid-08A6CB64-C9BB-4342-852D-444A75315BDE\u0026#34; targetRef=\u0026#34;sid-24F73F7F-EB61-484F-A494-686E194D0118\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-928C6C6F-57F1-40F2-BE0F-1A9FF3E6E9E4\u0026#34; sourceRef=\u0026#34;sid-5B2117E6-D341-49F2-85B2-336CA836C7D8\u0026#34; targetRef=\u0026#34;sid-08A6CB64-C9BB-4342-852D-444A75315BDE\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num\u0026gt;3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-4DB25720-11C8-401E-BB4C-83BB25510B2E\u0026#34; sourceRef=\u0026#34;sid-5B2117E6-D341-49F2-85B2-336CA836C7D8\u0026#34; targetRef=\u0026#34;sid-EA98D0C3-E41D-4DEB-8933-91A1B7301ABE\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num\u0026lt;3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_holiday-exclusive\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;holiday-exclusive\u0026#34; id=\u0026#34;BPMNPlane_holiday-exclusive\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;30.0\u0026#34; y=\u0026#34;163.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-3D5ED4D4-97F5-4FFD-B160-F00566ECC55E\u0026#34; id=\u0026#34;BPMNShape_sid-3D5ED4D4-97F5-4FFD-B160-F00566ECC55E\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;150.0\u0026#34; y=\u0026#34;135.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-5B2117E6-D341-49F2-85B2-336CA836C7D8\u0026#34; id=\u0026#34;BPMNShape_sid-5B2117E6-D341-49F2-85B2-336CA836C7D8\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;315.0\u0026#34; y=\u0026#34;155.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-08A6CB64-C9BB-4342-852D-444A75315BDE\u0026#34; id=\u0026#34;BPMNShape_sid-08A6CB64-C9BB-4342-852D-444A75315BDE\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;420.0\u0026#34; y=\u0026#34;225.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-EA98D0C3-E41D-4DEB-8933-91A1B7301ABE\u0026#34; id=\u0026#34;BPMNShape_sid-EA98D0C3-E41D-4DEB-8933-91A1B7301ABE\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;405.0\u0026#34; y=\u0026#34;30.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-24F73F7F-EB61-484F-A494-686E194D0118\u0026#34; id=\u0026#34;BPMNShape_sid-24F73F7F-EB61-484F-A494-686E194D0118\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;630.0\u0026#34; y=\u0026#34;225.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-8BA0B88C-BA4F-446D-B5E7-6BF0830B1DC8\u0026#34; id=\u0026#34;BPMNEdge_sid-8BA0B88C-BA4F-446D-B5E7-6BF0830B1DC8\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;504.95000000000005\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;680.0\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;680.0\u0026#34; y=\u0026#34;225.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-4DB25720-11C8-401E-BB4C-83BB25510B2E\u0026#34; id=\u0026#34;BPMNEdge_sid-4DB25720-11C8-401E-BB4C-83BB25510B2E\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;335.5\u0026#34; y=\u0026#34;155.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;335.5\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;404.99999999996083\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-33A73370-751D-413F-9306-39DEAA674DB6\u0026#34; id=\u0026#34;BPMNEdge_sid-33A73370-751D-413F-9306-39DEAA674DB6\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;59.94725673598754\u0026#34; y=\u0026#34;177.70973069236373\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;150.0\u0026#34; y=\u0026#34;175.96677419354836\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-D1B1F6E0-EA7F-4FF7-AD0C-5D43DBCEBFD2\u0026#34; id=\u0026#34;BPMNEdge_sid-D1B1F6E0-EA7F-4FF7-AD0C-5D43DBCEBFD2\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.5\u0026#34; flowable:targetDockerY=\u0026#34;20.5\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;249.95000000000002\u0026#34; y=\u0026#34;175.18431734317343\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;315.42592592592536\u0026#34; y=\u0026#34;175.42592592592592\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-E748F81F-B0B2-4C34-B993-FBAA2BCD0995\u0026#34; id=\u0026#34;BPMNEdge_sid-E748F81F-B0B2-4C34-B993-FBAA2BCD0995\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;519.95\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;629.9999999998776\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-928C6C6F-57F1-40F2-BE0F-1A9FF3E6E9E4\u0026#34; id=\u0026#34;BPMNEdge_sid-928C6C6F-57F1-40F2-BE0F-1A9FF3E6E9E4\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;335.5\u0026#34; y=\u0026#34;194.43942522321433\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;335.5\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;420.0\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 部署\n@Test public void deploy(){ ProcessEngine engine= ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = engine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;请假流程-排他网关.bpmn20.xml\u0026#34;) .deploy(); System.out.println(\u0026#34;部署成功:\u0026#34;+deploy); } 运行\n@Test public void run() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = engine.getRuntimeService(); Map\u0026lt;String, Object\u0026gt; variables = new HashMap\u0026lt;\u0026gt;(); variables.put(\u0026#34;num\u0026#34;, 2); runtimeService.startProcessInstanceById (\u0026#34;holiday-exclusive:1:4\u0026#34;, variables); } 数据库 张三完成任务\n@Test public void taskComplete(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); Task task = taskService.createTaskQuery() .taskAssignee(\u0026#34;zhangsan\u0026#34;) .processInstanceId(\u0026#34;2501\u0026#34;) .singleResult(); taskService.complete(task.getId()); } //接下来会走到部门经理审批\n此时再ran一个num为4的实例，然后张三完成，此时会走到总经理审批\n注意，如果这里num设置为3，则会报错 两者区别 如果上面的分支都不满足条件，那么会直接异常结束 //如果使用排他网关，如果条件都不满足，流程和任务都还在，只是代码抛异常 //如果两个都满足，那么会找出先定义的线走\n并行网关 # 绘制流程图 xml文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;holiday-parr-key\u0026#34; name=\u0026#34;请假流程-并行网关\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;holiday-parr-descr\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-47EAD72A-932E-4850-9218-08A7335CEEDD\u0026#34; name=\u0026#34;创建请假单\u0026#34; flowable:assignee=\u0026#34;zhangsan\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-8B72154F-6D29-47F8-A81C-A070F82B95F9\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-47EAD72A-932E-4850-9218-08A7335CEEDD\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;parallelGateway id=\u0026#34;sid-8B323A3D-F6DA-4D38-9CAE-D4CDA1031343\u0026#34;\u0026gt;\u0026lt;/parallelGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-5F0BF3BD-BC7C-4AA0-AF87-F679C8EEB40B\u0026#34; sourceRef=\u0026#34;sid-47EAD72A-932E-4850-9218-08A7335CEEDD\u0026#34; targetRef=\u0026#34;sid-8B323A3D-F6DA-4D38-9CAE-D4CDA1031343\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-AEFBD42F-2A10-4630-8E56-EDBD35CC95B1\u0026#34; name=\u0026#34;技术经理\u0026#34; flowable:assignee=\u0026#34;lisi\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-49DBB929-7488-471A-B79C-6BBFF4C810E0\u0026#34; sourceRef=\u0026#34;sid-8B323A3D-F6DA-4D38-9CAE-D4CDA1031343\u0026#34; targetRef=\u0026#34;sid-AEFBD42F-2A10-4630-8E56-EDBD35CC95B1\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-8FB84D20-C946-4988-B4C4-16FFD899AF63\u0026#34; name=\u0026#34;项目经理\u0026#34; flowable:assignee=\u0026#34;wangwu\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-DCF940BC-05D4-4260-8C50-A4C6E291DEA3\u0026#34; sourceRef=\u0026#34;sid-8B323A3D-F6DA-4D38-9CAE-D4CDA1031343\u0026#34; targetRef=\u0026#34;sid-8FB84D20-C946-4988-B4C4-16FFD899AF63\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;parallelGateway id=\u0026#34;sid-B25B9926-873F-46F5-9D62-D155462C1665\u0026#34;\u0026gt;\u0026lt;/parallelGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-18DF81F2-2B7F-4CC7-AD70-8A878FC7B125\u0026#34; sourceRef=\u0026#34;sid-AEFBD42F-2A10-4630-8E56-EDBD35CC95B1\u0026#34; targetRef=\u0026#34;sid-B25B9926-873F-46F5-9D62-D155462C1665\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-B00C2DDD-8A30-4BA0-A2F8-69185D8506F5\u0026#34; sourceRef=\u0026#34;sid-8FB84D20-C946-4988-B4C4-16FFD899AF63\u0026#34; targetRef=\u0026#34;sid-B25B9926-873F-46F5-9D62-D155462C1665\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-143837B7-0687-4268-B381-BA2442E39097\u0026#34; name=\u0026#34;总经理\u0026#34; flowable:assignee=\u0026#34;zjl\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-5ACFE3BE-E094-43A9-85C5-7D438EFE5A97\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-4255A9F7-39A1-46D3-AF14-DBEFF17AE911\u0026#34; sourceRef=\u0026#34;sid-143837B7-0687-4268-B381-BA2442E39097\u0026#34; targetRef=\u0026#34;sid-5ACFE3BE-E094-43A9-85C5-7D438EFE5A97\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-2F49B59A-6860-4101-8156-84780094E6FE\u0026#34; sourceRef=\u0026#34;sid-B25B9926-873F-46F5-9D62-D155462C1665\u0026#34; targetRef=\u0026#34;sid-5ACFE3BE-E094-43A9-85C5-7D438EFE5A97\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num \u0026lt;= 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-A5253FCB-3D23-483F-A511-197811F656D6\u0026#34; sourceRef=\u0026#34;sid-B25B9926-873F-46F5-9D62-D155462C1665\u0026#34; targetRef=\u0026#34;sid-143837B7-0687-4268-B381-BA2442E39097\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num \u0026gt; 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_holiday-parr-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;holiday-parr-key\u0026#34; id=\u0026#34;BPMNPlane_holiday-parr-key\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;163.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-47EAD72A-932E-4850-9218-08A7335CEEDD\u0026#34; id=\u0026#34;BPMNShape_sid-47EAD72A-932E-4850-9218-08A7335CEEDD\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;175.0\u0026#34; y=\u0026#34;138.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-8B323A3D-F6DA-4D38-9CAE-D4CDA1031343\u0026#34; id=\u0026#34;BPMNShape_sid-8B323A3D-F6DA-4D38-9CAE-D4CDA1031343\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;387.0\u0026#34; y=\u0026#34;143.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-AEFBD42F-2A10-4630-8E56-EDBD35CC95B1\u0026#34; id=\u0026#34;BPMNShape_sid-AEFBD42F-2A10-4630-8E56-EDBD35CC95B1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;495.0\u0026#34; y=\u0026#34;45.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-8FB84D20-C946-4988-B4C4-16FFD899AF63\u0026#34; id=\u0026#34;BPMNShape_sid-8FB84D20-C946-4988-B4C4-16FFD899AF63\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;495.0\u0026#34; y=\u0026#34;225.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-B25B9926-873F-46F5-9D62-D155462C1665\u0026#34; id=\u0026#34;BPMNShape_sid-B25B9926-873F-46F5-9D62-D155462C1665\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;695.0\u0026#34; y=\u0026#34;143.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-143837B7-0687-4268-B381-BA2442E39097\u0026#34; id=\u0026#34;BPMNShape_sid-143837B7-0687-4268-B381-BA2442E39097\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;795.0\u0026#34; y=\u0026#34;60.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-5ACFE3BE-E094-43A9-85C5-7D438EFE5A97\u0026#34; id=\u0026#34;BPMNShape_sid-5ACFE3BE-E094-43A9-85C5-7D438EFE5A97\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;840.0\u0026#34; y=\u0026#34;225.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-4255A9F7-39A1-46D3-AF14-DBEFF17AE911\u0026#34; id=\u0026#34;BPMNEdge_sid-4255A9F7-39A1-46D3-AF14-DBEFF17AE911\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;847.586690647482\u0026#34; y=\u0026#34;139.95\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;853.095383523332\u0026#34; y=\u0026#34;225.02614923910227\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-8B72154F-6D29-47F8-A81C-A070F82B95F9\u0026#34; id=\u0026#34;BPMNEdge_sid-8B72154F-6D29-47F8-A81C-A070F82B95F9\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.9499984899576\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;174.9999999999917\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-49DBB929-7488-471A-B79C-6BBFF4C810E0\u0026#34; id=\u0026#34;BPMNEdge_sid-49DBB929-7488-471A-B79C-6BBFF4C810E0\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;404.70744680851067\u0026#34; y=\u0026#34;145.2843450479233\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;395.0\u0026#34; y=\u0026#34;82.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;494.9999999999998\u0026#34; y=\u0026#34;84.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-2F49B59A-6860-4101-8156-84780094E6FE\u0026#34; id=\u0026#34;BPMNEdge_sid-2F49B59A-6860-4101-8156-84780094E6FE\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;715.5\u0026#34; y=\u0026#34;182.43746693121696\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;715.5\u0026#34; y=\u0026#34;239.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;840.0\u0026#34; y=\u0026#34;239.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-DCF940BC-05D4-4260-8C50-A4C6E291DEA3\u0026#34; id=\u0026#34;BPMNEdge_sid-DCF940BC-05D4-4260-8C50-A4C6E291DEA3\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;407.5\u0026#34; y=\u0026#34;182.44067421259845\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;407.5\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;494.9999999999674\u0026#34; y=\u0026#34;265.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-A5253FCB-3D23-483F-A511-197811F656D6\u0026#34; id=\u0026#34;BPMNEdge_sid-A5253FCB-3D23-483F-A511-197811F656D6\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;715.5\u0026#34; y=\u0026#34;143.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;715.5\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;795.0\u0026#34; y=\u0026#34;96.13899613899613\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-5F0BF3BD-BC7C-4AA0-AF87-F679C8EEB40B\u0026#34; id=\u0026#34;BPMNEdge_sid-5F0BF3BD-BC7C-4AA0-AF87-F679C8EEB40B\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.0\u0026#34; flowable:targetDockerY=\u0026#34;20.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;274.9499999999998\u0026#34; y=\u0026#34;173.87912087912088\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;388.52284263959393\u0026#34; y=\u0026#34;164.5190355329949\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-18DF81F2-2B7F-4CC7-AD70-8A878FC7B125\u0026#34; id=\u0026#34;BPMNEdge_sid-18DF81F2-2B7F-4CC7-AD70-8A878FC7B125\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.0\u0026#34; flowable:targetDockerY=\u0026#34;20.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;594.95\u0026#34; y=\u0026#34;107.91823529411766\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;701.273276904474\u0026#34; y=\u0026#34;156.70967741935485\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-B00C2DDD-8A30-4BA0-A2F8-69185D8506F5\u0026#34; id=\u0026#34;BPMNEdge_sid-B00C2DDD-8A30-4BA0-A2F8-69185D8506F5\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.5\u0026#34; flowable:targetDockerY=\u0026#34;20.5\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;594.95\u0026#34; y=\u0026#34;235.23460410557183\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;702.9632352941177\u0026#34; y=\u0026#34;170.94457720588235\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 并行网关的条件会被忽略 代码测试\n//部署并运行 @Test public void deploy() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = engine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;请假流程-并行网关.bpmn20.xml\u0026#34;) .deploy(); System.out.println(\u0026#34;部署成功:\u0026#34; + deploy.getId()); } @Test public void run() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = engine.getRuntimeService(); Map\u0026lt;String, Object\u0026gt; variables = new HashMap\u0026lt;\u0026gt;(); variables.put(\u0026#34;num\u0026#34;, 4); runtimeService.startProcessInstanceById (\u0026#34;holiday-parr-key:1:12504\u0026#34;, variables); } 此时任务停留在zhangsan 让zhangsan完成任务\n@Test public void taskComplete(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); Task task = taskService.createTaskQuery() .taskAssignee(\u0026#34;zhangsan\u0026#34;) .processInstanceId(\u0026#34;15001\u0026#34;) .singleResult(); taskService.complete(task.getId()); } 查看表数据(一个任务包含多个执行实例) 让王五和李四进行审批 查看数据库，wangwu审批后，act_ru_task就少了一条记录 此时走到总经理节点 图解 包容网关 # 包容网关可以选择多于一条顺序流。即固定几条必走，其他几条走条件\n流程图 xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;holiday-inclusive\u0026#34; name=\u0026#34;holiday-inclusive-name\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;holiday-inclusive-desc\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-6C2C29AA-C1D2-4B09-A542-ED194A13F5F2\u0026#34; name=\u0026#34;创建请假单\u0026#34; flowable:assignee=\u0026#34;i0\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-CAD92170-984F-49E0-BB6D-589B11F7FB8B\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-6C2C29AA-C1D2-4B09-A542-ED194A13F5F2\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;inclusiveGateway id=\u0026#34;sid-46FAF12A-7430-4AFA-AABB-99B2D875C9CD\u0026#34;\u0026gt;\u0026lt;/inclusiveGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-CCD38C3B-C06F-4646-B979-F65C0CA26321\u0026#34; sourceRef=\u0026#34;sid-6C2C29AA-C1D2-4B09-A542-ED194A13F5F2\u0026#34; targetRef=\u0026#34;sid-46FAF12A-7430-4AFA-AABB-99B2D875C9CD\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-9AD9C288-F114-4AC6-9366-A09A786B068E\u0026#34; name=\u0026#34;项目经理\u0026#34; flowable:assignee=\u0026#34;i1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;userTask id=\u0026#34;sid-764DC717-439D-425E-83FF-D81BD08A2562\u0026#34; name=\u0026#34;人事\u0026#34; flowable:assignee=\u0026#34;i2\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-B8DE143C-4636-4F2C-99C9-8949E23B0042\u0026#34; sourceRef=\u0026#34;sid-46FAF12A-7430-4AFA-AABB-99B2D875C9CD\u0026#34; targetRef=\u0026#34;sid-764DC717-439D-425E-83FF-D81BD08A2562\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-AC8D2717-5BCD-4C5B-81BB-2FF66CFFC615\u0026#34; name=\u0026#34;技术经理\u0026#34; flowable:assignee=\u0026#34;i3\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;inclusiveGateway id=\u0026#34;sid-6449A9C8-B7A3-44EE-BEDF-154AF323B1A8\u0026#34;\u0026gt;\u0026lt;/inclusiveGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-A52331B4-3769-46D8-AAC1-C34214C729BD\u0026#34; sourceRef=\u0026#34;sid-9AD9C288-F114-4AC6-9366-A09A786B068E\u0026#34; targetRef=\u0026#34;sid-6449A9C8-B7A3-44EE-BEDF-154AF323B1A8\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-681E9C5D-AD4B-45DD-BF12-E2CD5304ADFB\u0026#34; sourceRef=\u0026#34;sid-764DC717-439D-425E-83FF-D81BD08A2562\u0026#34; targetRef=\u0026#34;sid-6449A9C8-B7A3-44EE-BEDF-154AF323B1A8\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-78E79754-E64A-4ADE-A9BB-F9B224D3A5A0\u0026#34; sourceRef=\u0026#34;sid-AC8D2717-5BCD-4C5B-81BB-2FF66CFFC615\u0026#34; targetRef=\u0026#34;sid-6449A9C8-B7A3-44EE-BEDF-154AF323B1A8\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;exclusiveGateway id=\u0026#34;sid-65D4D76B-AD2B-4AE9-8E78-7B8C33BD9E55\u0026#34;\u0026gt;\u0026lt;/exclusiveGateway\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-422FC4A8-B667-4271-9CB3-A1D2CFEFC5E1\u0026#34; sourceRef=\u0026#34;sid-6449A9C8-B7A3-44EE-BEDF-154AF323B1A8\u0026#34; targetRef=\u0026#34;sid-65D4D76B-AD2B-4AE9-8E78-7B8C33BD9E55\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-4B834200-7995-453B-BC08-AF93C9F29FCF\u0026#34; name=\u0026#34;总经理\u0026#34; flowable:assignee=\u0026#34;wz\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-7296D067-FF72-49F9-B416-2452640A0FBC\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-AD0571E9-839D-4F1F-89ED-05BE60F841FD\u0026#34; sourceRef=\u0026#34;sid-4B834200-7995-453B-BC08-AF93C9F29FCF\u0026#34; targetRef=\u0026#34;sid-7296D067-FF72-49F9-B416-2452640A0FBC\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-E808AF78-E258-4997-B4FE-C393D8EBA3B9\u0026#34; sourceRef=\u0026#34;sid-46FAF12A-7430-4AFA-AABB-99B2D875C9CD\u0026#34; targetRef=\u0026#34;sid-9AD9C288-F114-4AC6-9366-A09A786B068E\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num\u0026gt;3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-E4AD02E7-A69A-4684-9A00-DE9B11711348\u0026#34; sourceRef=\u0026#34;sid-46FAF12A-7430-4AFA-AABB-99B2D875C9CD\u0026#34; targetRef=\u0026#34;sid-AC8D2717-5BCD-4C5B-81BB-2FF66CFFC615\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num \u0026lt;= 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-A6760B6A-B74F-4D35-93C2-6653751F8873\u0026#34; sourceRef=\u0026#34;sid-65D4D76B-AD2B-4AE9-8E78-7B8C33BD9E55\u0026#34; targetRef=\u0026#34;sid-4B834200-7995-453B-BC08-AF93C9F29FCF\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num \u0026gt; 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-97A0DAB9-564D-4A62-92A4-26C7056CD347\u0026#34; sourceRef=\u0026#34;sid-65D4D76B-AD2B-4AE9-8E78-7B8C33BD9E55\u0026#34; targetRef=\u0026#34;sid-7296D067-FF72-49F9-B416-2452640A0FBC\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num\u0026lt;=3 }]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_holiday-inclusive\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;holiday-inclusive\u0026#34; id=\u0026#34;BPMNPlane_holiday-inclusive\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;163.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-6C2C29AA-C1D2-4B09-A542-ED194A13F5F2\u0026#34; id=\u0026#34;BPMNShape_sid-6C2C29AA-C1D2-4B09-A542-ED194A13F5F2\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;195.0\u0026#34; y=\u0026#34;135.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-46FAF12A-7430-4AFA-AABB-99B2D875C9CD\u0026#34; id=\u0026#34;BPMNShape_sid-46FAF12A-7430-4AFA-AABB-99B2D875C9CD\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;366.0\u0026#34; y=\u0026#34;145.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-9AD9C288-F114-4AC6-9366-A09A786B068E\u0026#34; id=\u0026#34;BPMNShape_sid-9AD9C288-F114-4AC6-9366-A09A786B068E\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;451.0\u0026#34; y=\u0026#34;30.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-764DC717-439D-425E-83FF-D81BD08A2562\u0026#34; id=\u0026#34;BPMNShape_sid-764DC717-439D-425E-83FF-D81BD08A2562\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;450.0\u0026#34; y=\u0026#34;120.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-AC8D2717-5BCD-4C5B-81BB-2FF66CFFC615\u0026#34; id=\u0026#34;BPMNShape_sid-AC8D2717-5BCD-4C5B-81BB-2FF66CFFC615\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;465.0\u0026#34; y=\u0026#34;255.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-6449A9C8-B7A3-44EE-BEDF-154AF323B1A8\u0026#34; id=\u0026#34;BPMNShape_sid-6449A9C8-B7A3-44EE-BEDF-154AF323B1A8\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;656.0\u0026#34; y=\u0026#34;137.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-65D4D76B-AD2B-4AE9-8E78-7B8C33BD9E55\u0026#34; id=\u0026#34;BPMNShape_sid-65D4D76B-AD2B-4AE9-8E78-7B8C33BD9E55\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;40.0\u0026#34; width=\u0026#34;40.0\u0026#34; x=\u0026#34;750.0\u0026#34; y=\u0026#34;137.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-4B834200-7995-453B-BC08-AF93C9F29FCF\u0026#34; id=\u0026#34;BPMNShape_sid-4B834200-7995-453B-BC08-AF93C9F29FCF\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;855.0\u0026#34; y=\u0026#34;60.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-7296D067-FF72-49F9-B416-2452640A0FBC\u0026#34; id=\u0026#34;BPMNShape_sid-7296D067-FF72-49F9-B416-2452640A0FBC\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;900.0\u0026#34; y=\u0026#34;240.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-681E9C5D-AD4B-45DD-BF12-E2CD5304ADFB\u0026#34; id=\u0026#34;BPMNEdge_sid-681E9C5D-AD4B-45DD-BF12-E2CD5304ADFB\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.0\u0026#34; flowable:targetDockerY=\u0026#34;20.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;549.9499999999988\u0026#34; y=\u0026#34;159.14772727272728\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;656.3351955307262\u0026#34; y=\u0026#34;157.33435754189946\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-CCD38C3B-C06F-4646-B979-F65C0CA26321\u0026#34; id=\u0026#34;BPMNEdge_sid-CCD38C3B-C06F-4646-B979-F65C0CA26321\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.0\u0026#34; flowable:targetDockerY=\u0026#34;20.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;294.94999999999993\u0026#34; y=\u0026#34;171.45390070921985\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;367.32450331125824\u0026#34; y=\u0026#34;166.32119205298014\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-AD0571E9-839D-4F1F-89ED-05BE60F841FD\u0026#34; id=\u0026#34;BPMNEdge_sid-AD0571E9-839D-4F1F-89ED-05BE60F841FD\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;907.3347402597402\u0026#34; y=\u0026#34;139.95\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;913.1831773972388\u0026#34; y=\u0026#34;240.02104379436742\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-A6760B6A-B74F-4D35-93C2-6653751F8873\u0026#34; id=\u0026#34;BPMNEdge_sid-A6760B6A-B74F-4D35-93C2-6653751F8873\u0026#34; flowable:sourceDockerX=\u0026#34;22.5\u0026#34; flowable:sourceDockerY=\u0026#34;7.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;775.8406515580737\u0026#34; y=\u0026#34;142.87818696883852\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;855.0\u0026#34; y=\u0026#34;116.58716981132078\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-B8DE143C-4636-4F2C-99C9-8949E23B0042\u0026#34; id=\u0026#34;BPMNEdge_sid-B8DE143C-4636-4F2C-99C9-8949E23B0042\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;405.4272235576724\u0026#34; y=\u0026#34;165.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;428.0\u0026#34; y=\u0026#34;165.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;428.0\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;449.99999999999346\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-422FC4A8-B667-4271-9CB3-A1D2CFEFC5E1\u0026#34; id=\u0026#34;BPMNEdge_sid-422FC4A8-B667-4271-9CB3-A1D2CFEFC5E1\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;20.5\u0026#34; flowable:targetDockerY=\u0026#34;20.5\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;695.4399309245483\u0026#34; y=\u0026#34;157.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;750.5\u0026#34; y=\u0026#34;157.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-97A0DAB9-564D-4A62-92A4-26C7056CD347\u0026#34; id=\u0026#34;BPMNEdge_sid-97A0DAB9-564D-4A62-92A4-26C7056CD347\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;770.5\u0026#34; y=\u0026#34;176.44111163227018\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;770.5\u0026#34; y=\u0026#34;264.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;900.033302364888\u0026#34; y=\u0026#34;254.96981315483313\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-A52331B4-3769-46D8-AAC1-C34214C729BD\u0026#34; id=\u0026#34;BPMNEdge_sid-A52331B4-3769-46D8-AAC1-C34214C729BD\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.0\u0026#34; flowable:targetDockerY=\u0026#34;20.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;550.95\u0026#34; y=\u0026#34;94.83228571428573\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;662.6257153758107\u0026#34; y=\u0026#34;150.3587786259542\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-E808AF78-E258-4997-B4FE-C393D8EBA3B9\u0026#34; id=\u0026#34;BPMNEdge_sid-E808AF78-E258-4997-B4FE-C393D8EBA3B9\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;386.5\u0026#34; y=\u0026#34;145.5\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;386.5\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;451.0\u0026#34; y=\u0026#34;70.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-CAD92170-984F-49E0-BB6D-589B11F7FB8B\u0026#34; id=\u0026#34;BPMNEdge_sid-CAD92170-984F-49E0-BB6D-589B11F7FB8B\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.94999191137833\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;162.5\u0026#34; y=\u0026#34;178.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;162.5\u0026#34; y=\u0026#34;175.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;194.99999999998522\u0026#34; y=\u0026#34;175.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-E4AD02E7-A69A-4684-9A00-DE9B11711348\u0026#34; id=\u0026#34;BPMNEdge_sid-E4AD02E7-A69A-4684-9A00-DE9B11711348\u0026#34; flowable:sourceDockerX=\u0026#34;20.5\u0026#34; flowable:sourceDockerY=\u0026#34;20.5\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;386.5\u0026#34; y=\u0026#34;184.4426890432099\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;386.5\u0026#34; y=\u0026#34;295.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;465.0\u0026#34; y=\u0026#34;295.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-78E79754-E64A-4ADE-A9BB-F9B224D3A5A0\u0026#34; id=\u0026#34;BPMNEdge_sid-78E79754-E64A-4ADE-A9BB-F9B224D3A5A0\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;20.0\u0026#34; flowable:targetDockerY=\u0026#34;20.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;561.6083333333333\u0026#34; y=\u0026#34;255.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;665.2307692307692\u0026#34; y=\u0026#34;166.20769230769233\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 部署并运行\n@Test public void deploy() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = engine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;holiday-inclusive-name.bpmn20.xml\u0026#34;) .deploy(); System.out.println(\u0026#34;部署成功:\u0026#34; + deploy.getId()); } @Test public void run() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = engine.getRuntimeService(); Map\u0026lt;String, Object\u0026gt; variables = new HashMap\u0026lt;\u0026gt;(); variables.put(\u0026#34;num\u0026#34;, 4); runtimeService.startProcessInstanceById (\u0026#34;holiday-inclusive:1:4\u0026#34;, variables); } i0完成任务\n@Test public void taskComplete(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); Task task = taskService.createTaskQuery() .taskAssignee(\u0026#34;i0\u0026#34;) .processInstanceId(\u0026#34;2501\u0026#34;) .singleResult(); taskService.complete(task.getId()); } 看数据，默认走人事和项目经理 i1,i2所在任务执行完后，会发现走总经理 i1走完之后 i2走的时候，把num设为1，直接结束\n@Test public void taskComplete(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); Task task = taskService.createTaskQuery() .taskAssignee(\u0026#34;i2\u0026#34;) .processInstanceId(\u0026#34;2501\u0026#34;) .singleResult(); taskService.setVariable(task.getId(), \u0026#34;num\u0026#34;,1); taskService.complete(task.getId()); } 事件网关 # "},{"id":205,"href":"/zh/docs/technology/Flowable/boge_blbl/02-advance_4/","title":"boge-02-flowable进阶_4","section":"基础(波哥)_","content":" 候选人 # 流程图设计\n总体 具体 部署并启动流程\n@Test public void deploy(){ ProcessEngine processEngine= ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = processEngine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment().name(\u0026#34;ly画的请假流程-候选人\u0026#34;) .addClasspathResource(\u0026#34;请假流程-候选人.bpmn20.xml\u0026#34;) .deploy(); } @Test public void runProcess(){ //设置候选人 Map\u0026lt;String,Object\u0026gt; variables=new HashMap\u0026lt;\u0026gt;(); variables.put(\u0026#34;candidate1\u0026#34;,\u0026#34;张三\u0026#34;); variables.put(\u0026#34;candidate2\u0026#34;,\u0026#34;李四\u0026#34;); variables.put(\u0026#34;candidate3\u0026#34;,\u0026#34;王五\u0026#34;); ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); //获取流程运行服务 RuntimeService runtimeService = engine.getRuntimeService(); //运行流程 ProcessInstance processInstance = runtimeService.startProcessInstanceById( \u0026#34;holiday-candidate:1:4\u0026#34;,variables); System.out.println(\u0026#34;processInstance--\u0026#34;+processInstance); } 查看数据库表数据\n处理人为空 变量 图解 实际，作为登录用户如果是张三/李四或者王五，那它可以查看它自己是候选人的任务\n/** * 查询候选任务 */ @Test public void queryCandidate(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService=processEngine.getTaskService(); List\u0026lt;Task\u0026gt; tasks = taskService.createTaskQuery() .processInstanceId(\u0026#34;5001\u0026#34;) .taskCandidateUser(\u0026#34;张三\u0026#34;) .list(); for(Task task:tasks){ System.out.println(\u0026#34;id--\u0026#34;+task.getId()+\u0026#34;--\u0026#34;+task.getName()); } } 拾取任务\n/** * 拾取任务 */ @Test public void claimTaskCandidate(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService=engine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;5001\u0026#34;) .taskCandidateUser(\u0026#34;张三\u0026#34;) .singleResult(); if(task != null ){ //拾取任务 taskService.claim(task.getId(),\u0026#34;张三\u0026#34;); System.out.println(\u0026#34;拾取任务成功\u0026#34;); } } 数据库数据 此时查询李四候选任务，就查询不到了 归还任务\n/** * 拾取任务 */ @Test public void unclaimTaskCandidate(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService=engine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;5001\u0026#34;) .taskAssignee(\u0026#34;张三\u0026#34;) .singleResult(); if(task != null ){ //归还任务 taskService.unclaim(task.getId()); System.out.println(\u0026#34;归还任务成功\u0026#34;); } } 数据库数据 此时用李四，拾取成功 任务交接(委托)\n/** * 任务交接(委托) */ @Test public void taskCandidate(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService=engine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;5001\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); if(task != null ){ taskService.setAssignee(task.getId(),\u0026#34;赵六\u0026#34;); System.out.println(\u0026#34;任务交接给赵六\u0026#34;); } } 结果 完成任务\n/** * 完成任务 */ @Test public void taskComplete(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;5001\u0026#34;) .taskAssignee(\u0026#34;赵六\u0026#34;) .singleResult(); if(task!=null){ taskService.complete(task.getId()); System.out.println(\u0026#34;完成任务\u0026#34;); } } 此时任务给wz了 候选人组 # 当候选人很多的情况下，可以分组。（先创建组，然后将用户放到组中）\n维护用户和组\n/** * 创建用户 */ @Test public void createUser(){ ProcessEngine engine= ProcessEngines.getDefaultProcessEngine(); IdentityService identityService = engine.getIdentityService(); User user1 = identityService.newUser(\u0026#34;李飞\u0026#34;); user1.setFirstName(\u0026#34;li\u0026#34;); user1.setLastName(\u0026#34;fei\u0026#34;); identityService.saveUser(user1); User user2 = identityService.newUser(\u0026#34;灯标\u0026#34;); user2.setFirstName(\u0026#34;deng\u0026#34;); user2.setLastName(\u0026#34;biao\u0026#34;); identityService.saveUser(user2); User user3 = identityService.newUser(\u0026#34;田家\u0026#34;); user3.setFirstName(\u0026#34;tian\u0026#34;); user3.setLastName(\u0026#34;jia\u0026#34;); identityService.saveUser(user3); } /** * 创建组 */ @Test public void createGroup(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); IdentityService identityService = engine.getIdentityService(); Group group1 = identityService.newGroup(\u0026#34;group1\u0026#34;); group1.setName(\u0026#34;销售部\u0026#34;); group1.setType(\u0026#34;typ1\u0026#34;); identityService.saveGroup(group1); Group group2 = identityService.newGroup(\u0026#34;group2\u0026#34;); group2.setName(\u0026#34;开发部\u0026#34;); group2.setType(\u0026#34;typ2\u0026#34;); identityService.saveGroup(group2); } /** * 分配 */ @Test public void userGroup(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); IdentityService identityService = engine.getIdentityService(); //找到组 Group group1 = identityService.createGroupQuery().groupId(\u0026#34;group1\u0026#34;) .singleResult(); //找到所有用户 List\u0026lt;User\u0026gt; list = identityService.createUserQuery().list(); for(User user:list){ identityService.createMembership(user.getId(),group1.getId()); System.out.println(user.getId()); } } 表结构\n应用，创建流程图 xml文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;holiday-group\u0026#34; name=\u0026#34;请求流程-候选人组\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-B4CAA6EE-47C0-4C51-AB0F-7A347AA88CF9\u0026#34; name=\u0026#34;创建请假单\u0026#34; flowable:candidateGroups=\u0026#34;${g1}\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-FAA16FF3-BFC5-49AA-8BB5-7DF1918F67FF\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-B4CAA6EE-47C0-4C51-AB0F-7A347AA88CF9\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-C3C15BE2-2D50-4178-AD36-D6BAC5C47526\u0026#34; name=\u0026#34;总经理审批\u0026#34; flowable:assignee=\u0026#34;wz\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-9821E7E5-DB4A-4BE5-95C7-2721E98D6BD6\u0026#34; sourceRef=\u0026#34;sid-B4CAA6EE-47C0-4C51-AB0F-7A347AA88CF9\u0026#34; targetRef=\u0026#34;sid-C3C15BE2-2D50-4178-AD36-D6BAC5C47526\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-BF42EC91-584D-4C19-8EC0-9658CD948CDE\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-6F5E54EF-5767-4E22-8AC7-322C7E332B6B\u0026#34; sourceRef=\u0026#34;sid-C3C15BE2-2D50-4178-AD36-D6BAC5C47526\u0026#34; targetRef=\u0026#34;sid-BF42EC91-584D-4C19-8EC0-9658CD948CDE\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_holiday-group\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;holiday-group\u0026#34; id=\u0026#34;BPMNPlane_holiday-group\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;163.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-B4CAA6EE-47C0-4C51-AB0F-7A347AA88CF9\u0026#34; id=\u0026#34;BPMNShape_sid-B4CAA6EE-47C0-4C51-AB0F-7A347AA88CF9\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;165.0\u0026#34; y=\u0026#34;135.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-C3C15BE2-2D50-4178-AD36-D6BAC5C47526\u0026#34; id=\u0026#34;BPMNShape_sid-C3C15BE2-2D50-4178-AD36-D6BAC5C47526\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;330.0\u0026#34; y=\u0026#34;135.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-BF42EC91-584D-4C19-8EC0-9658CD948CDE\u0026#34; id=\u0026#34;BPMNShape_sid-BF42EC91-584D-4C19-8EC0-9658CD948CDE\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;510.0\u0026#34; y=\u0026#34;164.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-9821E7E5-DB4A-4BE5-95C7-2721E98D6BD6\u0026#34; id=\u0026#34;BPMNEdge_sid-9821E7E5-DB4A-4BE5-95C7-2721E98D6BD6\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;264.94999999998356\u0026#34; y=\u0026#34;175.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;330.0\u0026#34; y=\u0026#34;175.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-FAA16FF3-BFC5-49AA-8BB5-7DF1918F67FF\u0026#34; id=\u0026#34;BPMNEdge_sid-FAA16FF3-BFC5-49AA-8BB5-7DF1918F67FF\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.94340692927761\u0026#34; y=\u0026#34;177.55019845363262\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;164.99999999999906\u0026#34; y=\u0026#34;176.4985\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-6F5E54EF-5767-4E22-8AC7-322C7E332B6B\u0026#34; id=\u0026#34;BPMNEdge_sid-6F5E54EF-5767-4E22-8AC7-322C7E332B6B\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;429.9499999999989\u0026#34; y=\u0026#34;176.04062499999998\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;510.0021426561354\u0026#34; y=\u0026#34;177.70839534661596\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 部署并启动流程\n@Test public void deploy(){ ProcessEngine processEngine= ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = processEngine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment().name(\u0026#34;ly画的请假流程-候选人\u0026#34;) .addClasspathResource(\u0026#34;请求流程-候选人组.bpmn20.xml\u0026#34;) .deploy(); } @Test public void runProcess(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); //实际开发，应该按下面代码让用户选 IdentityService identityService = engine.getIdentityService(); List\u0026lt;Group\u0026gt; list = identityService.createGroupQuery().list(); //获取流程运行服务 RuntimeService runtimeService = engine.getRuntimeService(); //设置候选人 Map\u0026lt;String,Object\u0026gt; variables=new HashMap\u0026lt;\u0026gt;(); variables.put(\u0026#34;g1\u0026#34;,\u0026#34;group1\u0026#34;); //运行流程 ProcessInstance processInstance = runtimeService. startProcessInstanceById( \u0026#34;holiday-group:1:25004\u0026#34;,variables); System.out.println(\u0026#34;processInstance--\u0026#34;+processInstance); } 表 variables 查找当前用户所在组的任务，并拾取\n/** * 查询候选组任务 */ @Test public void queryCandidateGroup(){ String userId=\u0026#34;灯标\u0026#34;; ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); IdentityService identityService = processEngine.getIdentityService(); Group group = identityService.createGroupQuery(). groupMember(userId) .singleResult(); System.out.println(\u0026#34;灯标组id\u0026#34;+group.getId()); TaskService taskService=processEngine.getTaskService(); List\u0026lt;Task\u0026gt; tasks = taskService.createTaskQuery() .processInstanceId(\u0026#34;27501\u0026#34;) .taskCandidateGroup(group.getId()) .list(); for(Task task:tasks){ System.out.println(\u0026#34;id--\u0026#34;+task.getId()+\u0026#34;--\u0026#34;+task.getName()); } Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;27501\u0026#34;) .taskCandidateGroup(group.getId()) .singleResult(); if(task!=null){ System.out.println(\u0026#34;拾取任务--\u0026#34;+task.getId() +\u0026#34;任务名--\u0026#34;+task.getName()); taskService.claim(task.getId(),userId); } } 数据库数据 完成任务\n/** * 完成任务 */ @Test public void taskComplete(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = engine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;27501\u0026#34;) .taskAssignee(\u0026#34;灯标\u0026#34;) .singleResult(); if(task!=null){ taskService.complete(task.getId()); System.out.println(\u0026#34;完成任务\u0026#34;); } } # "},{"id":206,"href":"/zh/docs/technology/Flowable/boge_blbl/02-advance_3/","title":"boge-02-flowable进阶_3","section":"基础(波哥)_","content":" 任务分配-uel表达式 # 通过变量指定来进行分配\n首先绘制流程图（定义） 变量处理 之后将xml文件导出\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;holiday-new\u0026#34; name=\u0026#34;新请假流程\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;new-description\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34; name=\u0026#34;创建请假流程\u0026#34; flowable:assignee=\u0026#34;${assignee0}\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;userTask id=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34; name=\u0026#34;审批请假流程\u0026#34; flowable:assignee=\u0026#34;${assignee1}\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-631EFFB0-795A-4777-B49E-CF7D015BFF15\u0026#34; sourceRef=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34; targetRef=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-001CA567-6169-4F8A-A0E5-010721D52508\u0026#34; sourceRef=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34; targetRef=\u0026#34;sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-0A4A52F2-ECF6-44B2-AA41-F926AA7F5932\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_holiday-new\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;holiday-new\u0026#34; id=\u0026#34;BPMNPlane_holiday-new\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;145.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34; id=\u0026#34;BPMNShape_sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;225.0\u0026#34; y=\u0026#34;120.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34; id=\u0026#34;BPMNShape_sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;370.0\u0026#34; y=\u0026#34;120.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34; id=\u0026#34;BPMNShape_sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;555.0\u0026#34; y=\u0026#34;146.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-001CA567-6169-4F8A-A0E5-010721D52508\u0026#34; id=\u0026#34;BPMNEdge_sid-001CA567-6169-4F8A-A0E5-010721D52508\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;469.94999999997356\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;555.0\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-0A4A52F2-ECF6-44B2-AA41-F926AA7F5932\u0026#34; id=\u0026#34;BPMNEdge_sid-0A4A52F2-ECF6-44B2-AA41-F926AA7F5932\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.94999928606217\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;224.99999999995185\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-631EFFB0-795A-4777-B49E-CF7D015BFF15\u0026#34; id=\u0026#34;BPMNEdge_sid-631EFFB0-795A-4777-B49E-CF7D015BFF15\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;324.9499999999907\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;369.9999999999807\u0026#34; y=\u0026#34;160.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 流程定义的部署\n/** * 流程的部署 */ @Test public void testDeploy() { //获取ProcessEngine对象 ProcessEngine processEngine = configuration.buildProcessEngine(); //获取服务(repository，流程定义) RepositoryService repositoryService = processEngine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;新请假流程.bpmn20.xml\u0026#34;) .name(\u0026#34;请求流程\u0026#34;) //流程名 .deploy(); System.out.println(\u0026#34;部署id\u0026#34; + deploy.getId()); System.out.println(\u0026#34;部署名\u0026#34; + deploy.getName()); } 流程的启动（在流程启动时就已经处理好了各个节点的处理人）\n/** * 流程实例的启动 */ @Test public void testRunProcess2(){ ProcessEngine engine=ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = engine.getRuntimeService(); //启动流程时，发起人就已经设置好了 Map\u0026lt;String,Object\u0026gt; variables=new HashMap\u0026lt;\u0026gt;(); variables.put(\u0026#34;assignee0\u0026#34;,\u0026#34;张三\u0026#34;); variables.put(\u0026#34;assignee1\u0026#34;,\u0026#34;李四\u0026#34;); ProcessInstance processInstance = runtimeService.startProcessInstanceById(\u0026#34;holiday-new:1:4\u0026#34;,variables); System.out.println(processInstance); } 查看数据库表数据\nact_ru_variable\nact_ru_task 让张三完成处理\n@Test public void testComplete(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery().taskAssignee(\u0026#34;张三\u0026#34;) .processInstanceId(\u0026#34;2501\u0026#34;) .singleResult(); taskService.complete(task.getId()); } 此时观察task和identity这两张表\n任务变成了李四，而identity多了张三的记录\n任务分配-监听器分配 # 首先，java代码中，自定义一个监听器 【注意，这里给任务分配assignee是在create中分配才是有用的】\npackage org.flowable.listener; import org.flowable.engine.delegate.TaskListener; import org.flowable.task.service.delegate.DelegateTask; public class MyTaskListener implements TaskListener { /** * 监听器触发的方法 * @param delegateTask */ @Override public void notify(DelegateTask delegateTask) { System.out.println(\u0026#34;MyTaskListener触发：\u0026#34;+delegateTask .getName()); if(\u0026#34;创建请假流程\u0026#34;.equals(delegateTask.getName()) \u0026amp;\u0026amp;\u0026#34;create\u0026#34;.equals(delegateTask.getEventName())){ delegateTask.setAssignee(\u0026#34;小明\u0026#34;); }else { delegateTask.setAssignee(\u0026#34;小李\u0026#34;); } } } 两个节点走的是同一个监听器\nxml定义中任务监听器的配置(两个节点都配置了) \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;holiday-new\u0026#34; name=\u0026#34;新请假流程\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;new-description\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34; name=\u0026#34;创建请假流程\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;flowable:taskListener event=\u0026#34;create\u0026#34; class=\u0026#34;org.flowable.listener.MyTaskListener\u0026#34;\u0026gt;\u0026lt;/flowable:taskListener\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;userTask id=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34; name=\u0026#34;审批请假流程\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;flowable:taskListener event=\u0026#34;create\u0026#34; class=\u0026#34;org.flowable.listener.MyTaskListener\u0026#34;\u0026gt;\u0026lt;/flowable:taskListener\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-631EFFB0-795A-4777-B49E-CF7D015BFF15\u0026#34; sourceRef=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34; targetRef=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-001CA567-6169-4F8A-A0E5-010721D52508\u0026#34; sourceRef=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34; targetRef=\u0026#34;sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-0A4A52F2-ECF6-44B2-AA41-F926AA7F5932\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_holiday-new\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;holiday-new\u0026#34; id=\u0026#34;BPMNPlane_holiday-new\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;115.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34; id=\u0026#34;BPMNShape_sid-8D901410-5BD7-4EED-B988-5E40D12298C7\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;195.0\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34; id=\u0026#34;BPMNShape_sid-5EB8F68B-7876-42AF-98E1-FCA27F99D8CE\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;370.0\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34; id=\u0026#34;BPMNShape_sid-15CAD0D3-7F8B-404C-9346-A8D2A456D47B\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;570.0\u0026#34; y=\u0026#34;116.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-001CA567-6169-4F8A-A0E5-010721D52508\u0026#34; id=\u0026#34;BPMNEdge_sid-001CA567-6169-4F8A-A0E5-010721D52508\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;469.9499999999809\u0026#34; y=\u0026#34;130.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;570.0\u0026#34; y=\u0026#34;130.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-0A4A52F2-ECF6-44B2-AA41-F926AA7F5932\u0026#34; id=\u0026#34;BPMNEdge_sid-0A4A52F2-ECF6-44B2-AA41-F926AA7F5932\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.94999891869114\u0026#34; y=\u0026#34;130.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;195.0\u0026#34; y=\u0026#34;130.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-631EFFB0-795A-4777-B49E-CF7D015BFF15\u0026#34; id=\u0026#34;BPMNEdge_sid-631EFFB0-795A-4777-B49E-CF7D015BFF15\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;294.95000000000005\u0026#34; y=\u0026#34;130.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;369.99999999993753\u0026#34; y=\u0026#34;130.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 之后将流程再重新部署一遍\n/** * 流程的部署 */ @Test public void testDeploy() { //获取ProcessEngine对象 ProcessEngine processEngine = configuration.buildProcessEngine(); //获取服务(repository，流程定义) RepositoryService repositoryService = processEngine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;新请假流程.bpmn20.xml\u0026#34;) .name(\u0026#34;请求流程\u0026#34;) //流程名 .deploy(); System.out.println(\u0026#34;部署id\u0026#34; + deploy.getId()); System.out.println(\u0026#34;部署名\u0026#34; + deploy.getName()); } 流程运行\n/** * 流程实例的启动 */ @Test public void testRunProcess3() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = engine.getRuntimeService(); ProcessInstance processInstance = runtimeService.startProcessInstanceById( \u0026#34;holiday-new:1:4\u0026#34;); System.out.println(processInstance); } 控制台查看 数据库查看 让小明处理任务\n@Test public void testComplete(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery().taskAssignee(\u0026#34;小明\u0026#34;) .processInstanceId(\u0026#34;2501\u0026#34;) .singleResult(); taskService.complete(task.getId()); } 数据库查看 流程变量 # 全局变量（跟流程有关）和局部变量（跟task有关）\n一个流程定义，可以运行多个流程实例； 当用到子流程时，就会出现一对多的关系 全局变量被重复赋值时后面会覆盖前面\n流程图的创建 这里还设置了条件，详见xm文件 sequenceFlow.conditionExpression 属性\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34; exporter=\u0026#34;Flowable Open Source Modeler\u0026#34; exporterVersion=\u0026#34;6.7.2\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;evection\u0026#34; name=\u0026#34;出差申请单\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;documentation\u0026gt;出差申请单\u0026lt;/documentation\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent1\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt;\u0026lt;/startEvent\u0026gt; \u0026lt;userTask id=\u0026#34;sid-BFB6D699-D3B5-4C6C-A0F2-00584EAAF207\u0026#34; name=\u0026#34;创建出差申请单\u0026#34; flowable:assignee=\u0026#34;${assignee0}\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-EE410204-0433-4FE6-A958-48585A2A7B4B\u0026#34; sourceRef=\u0026#34;startEvent1\u0026#34; targetRef=\u0026#34;sid-BFB6D699-D3B5-4C6C-A0F2-00584EAAF207\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-D10C4F45-B429-4E24-B474-5354F1661645\u0026#34; name=\u0026#34;部门经理审批\u0026#34; flowable:assignee=\u0026#34;${assignee1}\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-752CE2F2-40EC-4140-AF60-BEACD06D43A7\u0026#34; sourceRef=\u0026#34;sid-BFB6D699-D3B5-4C6C-A0F2-00584EAAF207\u0026#34; targetRef=\u0026#34;sid-D10C4F45-B429-4E24-B474-5354F1661645\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;userTask id=\u0026#34;sid-35AB278B-E16D-4CEC-98B1-FBB139FB5AC1\u0026#34; name=\u0026#34;总经理审批\u0026#34; flowable:assignee=\u0026#34;${assignee2}\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;userTask id=\u0026#34;sid-4C26DA5C-A4CC-48A5-ABA9-853E82FC2413\u0026#34; name=\u0026#34;财务审批 \u0026#34; flowable:assignee=\u0026#34;${assignee3}\u0026#34; flowable:formFieldValidation=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;extensionElements\u0026gt; \u0026lt;modeler:initiator-can-complete xmlns:modeler=\u0026#34;http://flowable.org/modeler\u0026#34;\u0026gt;\u0026lt;![CDATA[false]]\u0026gt;\u0026lt;/modeler:initiator-can-complete\u0026gt; \u0026lt;/extensionElements\u0026gt; \u0026lt;/userTask\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-BE043A23-0F38-4ED9-A0D1-F4C2F7908A50\u0026#34; sourceRef=\u0026#34;sid-35AB278B-E16D-4CEC-98B1-FBB139FB5AC1\u0026#34; targetRef=\u0026#34;sid-4C26DA5C-A4CC-48A5-ABA9-853E82FC2413\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;endEvent id=\u0026#34;sid-B3A1D5D4-E1FD-4599-A482-762C7C617844\u0026#34;\u0026gt;\u0026lt;/endEvent\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-6C0130A8-E078-486B-9B6E-D8C14BBCD8EF\u0026#34; sourceRef=\u0026#34;sid-4C26DA5C-A4CC-48A5-ABA9-853E82FC2413\u0026#34; targetRef=\u0026#34;sid-B3A1D5D4-E1FD-4599-A482-762C7C617844\u0026#34;\u0026gt;\u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-F85B2D44-1B42-4748-AB35-123C7CCD2F75\u0026#34; sourceRef=\u0026#34;sid-D10C4F45-B429-4E24-B474-5354F1661645\u0026#34; targetRef=\u0026#34;sid-35AB278B-E16D-4CEC-98B1-FBB139FB5AC1\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num \u0026gt;= 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;sid-B12793A8-FC65-408C-81AD-EC81FEEF6E46\u0026#34; sourceRef=\u0026#34;sid-D10C4F45-B429-4E24-B474-5354F1661645\u0026#34; targetRef=\u0026#34;sid-4C26DA5C-A4CC-48A5-ABA9-853E82FC2413\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;\u0026lt;![CDATA[${num \u0026lt; 3}]]\u0026gt;\u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;bpmndi:BPMNDiagram id=\u0026#34;BPMNDiagram_evection\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNPlane bpmnElement=\u0026#34;evection\u0026#34; id=\u0026#34;BPMNPlane_evection\u0026#34;\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;startEvent1\u0026#34; id=\u0026#34;BPMNShape_startEvent1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;30.0\u0026#34; width=\u0026#34;30.0\u0026#34; x=\u0026#34;100.0\u0026#34; y=\u0026#34;75.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-BFB6D699-D3B5-4C6C-A0F2-00584EAAF207\u0026#34; id=\u0026#34;BPMNShape_sid-BFB6D699-D3B5-4C6C-A0F2-00584EAAF207\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;175.0\u0026#34; y=\u0026#34;50.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-D10C4F45-B429-4E24-B474-5354F1661645\u0026#34; id=\u0026#34;BPMNShape_sid-D10C4F45-B429-4E24-B474-5354F1661645\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;320.0\u0026#34; y=\u0026#34;50.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-35AB278B-E16D-4CEC-98B1-FBB139FB5AC1\u0026#34; id=\u0026#34;BPMNShape_sid-35AB278B-E16D-4CEC-98B1-FBB139FB5AC1\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;555.0\u0026#34; y=\u0026#34;50.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-4C26DA5C-A4CC-48A5-ABA9-853E82FC2413\u0026#34; id=\u0026#34;BPMNShape_sid-4C26DA5C-A4CC-48A5-ABA9-853E82FC2413\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;80.0\u0026#34; width=\u0026#34;100.0\u0026#34; x=\u0026#34;555.0\u0026#34; y=\u0026#34;210.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNShape bpmnElement=\u0026#34;sid-B3A1D5D4-E1FD-4599-A482-762C7C617844\u0026#34; id=\u0026#34;BPMNShape_sid-B3A1D5D4-E1FD-4599-A482-762C7C617844\u0026#34;\u0026gt; \u0026lt;omgdc:Bounds height=\u0026#34;28.0\u0026#34; width=\u0026#34;28.0\u0026#34; x=\u0026#34;750.0\u0026#34; y=\u0026#34;236.0\u0026#34;\u0026gt;\u0026lt;/omgdc:Bounds\u0026gt; \u0026lt;/bpmndi:BPMNShape\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-EE410204-0433-4FE6-A958-48585A2A7B4B\u0026#34; id=\u0026#34;BPMNEdge_sid-EE410204-0433-4FE6-A958-48585A2A7B4B\u0026#34; flowable:sourceDockerX=\u0026#34;15.0\u0026#34; flowable:sourceDockerY=\u0026#34;15.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;129.9499984899576\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;175.0\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-752CE2F2-40EC-4140-AF60-BEACD06D43A7\u0026#34; id=\u0026#34;BPMNEdge_sid-752CE2F2-40EC-4140-AF60-BEACD06D43A7\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;274.95000000000005\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;320.0\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-B12793A8-FC65-408C-81AD-EC81FEEF6E46\u0026#34; id=\u0026#34;BPMNEdge_sid-B12793A8-FC65-408C-81AD-EC81FEEF6E46\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;419.95000000000005\u0026#34; y=\u0026#34;124.0085106382979\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;555.0\u0026#34; y=\u0026#34;215.95744680851067\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-6C0130A8-E078-486B-9B6E-D8C14BBCD8EF\u0026#34; id=\u0026#34;BPMNEdge_sid-6C0130A8-E078-486B-9B6E-D8C14BBCD8EF\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;14.0\u0026#34; flowable:targetDockerY=\u0026#34;14.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;654.9499999998701\u0026#34; y=\u0026#34;250.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;750.0\u0026#34; y=\u0026#34;250.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-BE043A23-0F38-4ED9-A0D1-F4C2F7908A50\u0026#34; id=\u0026#34;BPMNEdge_sid-BE043A23-0F38-4ED9-A0D1-F4C2F7908A50\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;605.0\u0026#34; y=\u0026#34;129.95\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;605.0\u0026#34; y=\u0026#34;210.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;bpmndi:BPMNEdge bpmnElement=\u0026#34;sid-F85B2D44-1B42-4748-AB35-123C7CCD2F75\u0026#34; id=\u0026#34;BPMNEdge_sid-F85B2D44-1B42-4748-AB35-123C7CCD2F75\u0026#34; flowable:sourceDockerX=\u0026#34;50.0\u0026#34; flowable:sourceDockerY=\u0026#34;40.0\u0026#34; flowable:targetDockerX=\u0026#34;50.0\u0026#34; flowable:targetDockerY=\u0026#34;40.0\u0026#34;\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;419.95000000000005\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;omgdi:waypoint x=\u0026#34;555.0\u0026#34; y=\u0026#34;90.0\u0026#34;\u0026gt;\u0026lt;/omgdi:waypoint\u0026gt; \u0026lt;/bpmndi:BPMNEdge\u0026gt; \u0026lt;/bpmndi:BPMNPlane\u0026gt; \u0026lt;/bpmndi:BPMNDiagram\u0026gt; \u0026lt;/definitions\u0026gt; 流程进行部署\n/** * 流程的部署 */ @Test public void testDeploy() { //获取ProcessEngine对象 ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); //获取服务(repository，流程定义) RepositoryService repositoryService = processEngine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;出差申请单.bpmn20.xml\u0026#34;) .name(\u0026#34;请假流程\u0026#34;) //流程名 .deploy(); System.out.println(\u0026#34;部署id\u0026#34; + deploy.getId()); System.out.println(\u0026#34;部署名\u0026#34; + deploy.getName()); } 流程运行\n/** * 流程实例的定义 */ @Test public void runProcess(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = processEngine.getRuntimeService(); Map\u0026lt;String,Object\u0026gt; variables=new HashMap\u0026lt;\u0026gt;(); variables.put(\u0026#34;assignee0\u0026#34;,\u0026#34;张三\u0026#34;); variables.put(\u0026#34;assignee1\u0026#34;,\u0026#34;李四\u0026#34;); variables.put(\u0026#34;assignee2\u0026#34;,\u0026#34;王五\u0026#34;); variables.put(\u0026#34;assignee3\u0026#34;,\u0026#34;赵财务\u0026#34;); ProcessInstance processInstance = runtimeService. startProcessInstanceById(\u0026#34;evection:1:4\u0026#34;, variables); } //这时候节点走到张三了，让张三处理\n/** * 任务完成 */ @Test public void taskComplete(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;张三\u0026#34;) .singleResult(); Map\u0026lt;String,Object\u0026gt; processVariables=task.getProcessVariables(); processVariables.put(\u0026#34;num\u0026#34;,3); taskService.complete(task.getId(),processVariables); } 下面修改num的值，修改之前 全局变量的查询\n@Test public void getVariables(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() //注意，这个一定要加的不然获取不到全局变量 .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;张三\u0026#34;) .singleResult(); //这里只能获取到任务的局部变量 Map\u0026lt;String, Object\u0026gt; processVariables = task.getProcessVariables(); System.out.println(\u0026#34;当前流程变量--start\u0026#34;); Set\u0026lt;String\u0026gt; keySet1 = processVariables.keySet(); for(String key:keySet1){ System.out.println(\u0026#34;key--\u0026#34;+key+\u0026#34;value--\u0026#34;+processVariables.get(key)); } System.out.println(\u0026#34;当前流程变量--end\u0026#34;); } 修改\n@Test public void updateVariables(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() //注意，这个一定要加的不然获取不到全局变量 .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); Map\u0026lt;String, Object\u0026gt; processVariables = task.getProcessVariables(); System.out.println(\u0026#34;当前流程变量--start\u0026#34;); Set\u0026lt;String\u0026gt; keySet = processVariables.keySet(); for(String key:keySet){ System.out.println(\u0026#34;key--\u0026#34;+key+\u0026#34;value--\u0026#34;+processVariables.get(key)); } System.out.println(\u0026#34;当前流程变量--end\u0026#34;); processVariables.put(\u0026#34;num\u0026#34;,5); taskService.setVariablesLocal(task.getId(),processVariables); } 结果\n按照视频的说法，这里错了，应该是会多了5条记录 局部变量的再次测试\n@Test public void updateVariables(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;张三\u0026#34;) .singleResult(); //流程还没开始运行的情况下，取到的是全局变量 Map\u0026lt;String, Object\u0026gt; processVariables = task.getProcessVariables(); System.out.println(\u0026#34;当前流程变量--start\u0026#34;); Set\u0026lt;String\u0026gt; keySet = processVariables.keySet(); for(String key:keySet){ System.out.println(\u0026#34;key--\u0026#34;+key+\u0026#34;value--\u0026#34;+processVariables.get(key)); } System.out.println(\u0026#34;当前流程变量--end\u0026#34;); Map\u0026lt;String,Object\u0026gt; varLocalInsert=new HashMap\u0026lt;\u0026gt;(); varLocalInsert.put(\u0026#34;num\u0026#34;,5); Map\u0026lt;String,Object\u0026gt; varUpdate=new HashMap\u0026lt;\u0026gt;(); varUpdate.put(\u0026#34;a\u0026#34;,\u0026#34;嘿嘿\u0026#34;); //这里测试会不会把全局变量全部覆盖 taskService.setVariables(task.getId(),varUpdate); taskService.setVariablesLocal(task.getId(),varLocalInsert); } 修改前 修改后 结果表明这是批量增加/修改，而不是覆盖 当前数据库的数据 1个局部变量num，5个全局变量 接下来在张三节点设置一个局部变量\n/** * 任务完成 */ @Test public void taskComplete(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;张三\u0026#34;) .singleResult(); Map\u0026lt;String,Object\u0026gt; processVariables=task.getProcessVariables(); processVariables.put(\u0026#34;num\u0026#34;,2); taskService.complete(task.getId(),processVariables); } 查看数据库表，发现num已经被修改成2 这时李四设置了一个局部变量num=6\n@Test public void updateVariables2(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); Map\u0026lt;String,Object\u0026gt; varLocalInsert=new HashMap\u0026lt;\u0026gt;(); varLocalInsert.put(\u0026#34;num\u0026#34;,6); Map\u0026lt;String,Object\u0026gt; varUpdate=new HashMap\u0026lt;\u0026gt;(); varUpdate.put(\u0026#34;a\u0026#34;,\u0026#34;嘿嘿\u0026#34;); //这里测试会不会把全局变量全部覆盖 //taskService.setVariables(task.getId(),varUpdate); taskService.setVariablesLocal(task.getId(),varLocalInsert); } 仅仅多了一条记录 修改全局变量\n@Test public void updateVariables3(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); Map\u0026lt;String,Object\u0026gt; varLocalInsert=new HashMap\u0026lt;\u0026gt;(); varLocalInsert.put(\u0026#34;num\u0026#34;,18); varLocalInsert.put(\u0026#34;a\u0026#34;,\u0026#34;a被修改了\u0026#34;); //这里测试会不会把全局变量全部覆盖 //taskService.setVariables(task.getId(),varUpdate); taskService.setVariables(task.getId(),varLocalInsert); } 结果如下，当局部变量和全局变量的名称一样时，只能修改局部变量 让李四完成审批 这里存在局部变量num=18，且完成时设置了局部变量20\n@Test public void taskComplete4(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); // System.out.println(\u0026#34;taskId\u0026#34;+task.getId()); Map\u0026lt;String,Object\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;num\u0026#34;,20); taskService.complete(task.getId(),map); } 注意，这里全局变量被改成20了，局部变量被删除了 走到了总经理审批\n再测试 将数据清空，重新部署并运行流程\n现在在赵四节点，局部变量为 @Test public void taskComplete4(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); // System.out.println(\u0026#34;taskId\u0026#34;+task.getId()); Map\u0026lt;String,Object\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;num\u0026#34;,20); taskService.setVariablesLocal(task.getId(),map); taskService.complete(task.getId()); } 运行完之后，局部变量变成20了，但是流程走不下去 稍作更改，添加一个全局变量(但是由于存在局部变量a，所以这里全局变量没设置成功)\n@Test public void taskComplete4(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); // System.out.println(\u0026#34;taskId\u0026#34;+task.getId()); Map\u0026lt;String,Object\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;num\u0026#34;,20); taskService.setVariablesLocal(task.getId(),map); Map\u0026lt;String,Object\u0026gt; map1=new HashMap\u0026lt;\u0026gt;(); map1.put(\u0026#34;num\u0026#34;,1); taskService.setVariables(task.getId(),map1); taskService.complete(task.getId()); } 现在只能通过在complete中设置，来使得全局变量生效\n@Test public void taskComplete4(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); // System.out.println(\u0026#34;taskId\u0026#34;+task.getId()); Map\u0026lt;String,Object\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;num\u0026#34;,null); taskService.setVariablesLocal(task.getId(),map); Map\u0026lt;String,Object\u0026gt; map1=new HashMap\u0026lt;\u0026gt;(); map1.put(\u0026#34;num\u0026#34;,1); //taskService.setVariables(task.getId(),map1); taskService.complete(task.getId(),map1); } 结果，全局变量设置成功，且任务流转到了财务那 再测试\n在存在局部变量num=2的情况下执行下面代码\n@Test public void taskComplete5() { ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); // System.out.println(\u0026#34;taskId\u0026#34; + task.getId()); Map\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;num\u0026#34;, 15); taskService.setVariables(task.getId(), map); taskService.complete(task.getId()); /*Map\u0026lt;String,Object\u0026gt; map1=new HashMap\u0026lt;\u0026gt;(); map1.put(\u0026#34;num\u0026#34;,1); taskService.complete(task.getId(),map1);*/ } 会提示报错，Unknown property used in expression: ${num \u0026gt;= 3}\n//说明线条中查找的是全局变量\n在不存在局部变量num的情况下执行上面代码，会走总经理审批（num\u0026gt;3)\n在complete中加上map参数，验证明线条查找的是全局变量的值，complete带上variables会设置全局变量\n@Test public void taskComplete5() { ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .includeProcessVariables() .processInstanceId(\u0026#34;2501\u0026#34;) .taskAssignee(\u0026#34;李四\u0026#34;) .singleResult(); // System.out.println(\u0026#34;taskId\u0026#34; + task.getId()); Map\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;num\u0026#34;, 15); // taskService.setVariables(task.getId(), map); taskService.complete(task.getId(),map); /*Map\u0026lt;String,Object\u0026gt; map1=new HashMap\u0026lt;\u0026gt;(); map1.put(\u0026#34;num\u0026#34;,1); taskService.complete(task.getId(),map1);*/ } 数据库表 act_hi_varinst 里面看得到局部变量\n"},{"id":207,"href":"/zh/docs/technology/Flowable/boge_blbl/02-advance_2/","title":"boge-02-flowable进阶_2","section":"基础(波哥)_","content":" Service服务接口 # 各个Service类 RepositoryService 资源管理类，流程定义、部署、文件 RuntimeService 流程运行管理类，运行过程中（执行） TaskService 任务管理类 HistoryService 历史管理类 ManagerService 引擎管理类 Flowable图标 # BPMN2.0定义的一些图标\n时间 活动 网关 流程部署深入解析 # 使用eclipse打包部署(没有eclipse环境，所以这里只有截图) 将两个流程，打包为bar文件，然后放到项目resources文件夹中 这里是为了测试一次部署多个流程（定义，图） 代码如下 部署完成后查看表结构\nact_re_procdef\n部署id一样 act_re_deployment 结论：部署和定义是1对多的关系\n每次部署所涉及到的资源文件 涉及到的三张表\nact_ge_bytearray act_re_procdef category\u0026ndash;\u0026gt;xml中的namespace name\u0026ndash;\u0026gt;定义时起的名称 key_\u0026mdash;\u0026gt;xml中定义的id resource_name\u0026mdash;\u0026gt;xml文件名称 dgrm_resource_name\u0026ndash;\u0026gt;生成图片名称 suspension_state \u0026ndash;\u0026gt; 是否被挂起\ntenant_id \u0026ndash; \u0026gt;谁部署的流程\nact_re_deployment name_部署名\n代码 主要源码 DeployCmd.class DeploymentEntityManagerImpl.java insert()方法 插入并执行资源 点开里面的insert方法 AbstractDataManger.insert() 回到test类，deploy()方法最终就是完成了表结构的数据的操作（通过Mybatis）\n流程的挂起和激活 # xml文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34;\u0026gt; \u0026lt;!--id process key--\u0026gt; \u0026lt;process id=\u0026#34;holidayRequest\u0026#34; name=\u0026#34;请假流程\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent\u0026#34;/\u0026gt; \u0026lt;!--sequenceFlow表示的是线条箭头--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;startEvent\u0026#34; targetRef=\u0026#34;approveTask\u0026#34;/\u0026gt; \u0026lt;userTask id=\u0026#34;approveTask\u0026#34; name=\u0026#34;同意或者拒绝请假\u0026#34; flowable:assignee=\u0026#34;zhangsan\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;approveTask\u0026#34; targetRef=\u0026#34;decision\u0026#34;/\u0026gt; \u0026lt;!--网关--\u0026gt; \u0026lt;exclusiveGateway id=\u0026#34;decision\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;decision\u0026#34; targetRef=\u0026#34;externalSystemCall\u0026#34;\u0026gt; \u0026lt;!--条件--\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt; \u0026lt;![CDATA[ ${approved} ]]\u0026gt; \u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;decision\u0026#34; targetRef=\u0026#34;sendRejectionMail\u0026#34;\u0026gt; \u0026lt;!--条件--\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt; \u0026lt;![CDATA[ ${!approved} ]]\u0026gt; \u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;serviceTask id=\u0026#34;externalSystemCall\u0026#34; name=\u0026#34;Enter holidays in external system\u0026#34; flowable:class=\u0026#34;org.flowable.CallExternalSystemDelegate\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;externalSystemCall\u0026#34; targetRef=\u0026#34;holidayApprovedTask\u0026#34;/\u0026gt; \u0026lt;userTask id=\u0026#34;holidayApprovedTask\u0026#34; name=\u0026#34;Holiday approved\u0026#34; flowable:assignee=\u0026#34;lisi\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;holidayApprovedTask\u0026#34; targetRef=\u0026#34;approveEnd\u0026#34;/\u0026gt; \u0026lt;!--发送一个邮件--\u0026gt; \u0026lt;serviceTask id=\u0026#34;sendRejectionMail\u0026#34; name=\u0026#34;Send out rejection email\u0026#34; flowable:class=\u0026#34;org.flowable.SendRejectionMail\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;sendRejectionMail\u0026#34; targetRef=\u0026#34;rejectEnd\u0026#34;/\u0026gt; \u0026lt;endEvent id=\u0026#34;approveEnd\u0026#34;/\u0026gt; \u0026lt;endEvent id=\u0026#34;rejectEnd\u0026#34;/\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;/definitions\u0026gt; 部署的流程默认情况下为激活，如果不想使用该定义的流程，那么可以挂起该流程，当然该流程定义下边所有的流程实例全部暂停。\n流程定义被定义为挂起，该流程定义将不允许启动新的流程实例，且该流程定义下所有的流程实例将被全部挂起暂停执行\n表结构 act_re_procdef表中的SUSPENSION_STATE字段来表示1激活，2挂起\n挂起流程\n@Test public void testSuspend() { ProcessEngine engine = ProcessEngines.getDefaultProcessEngine(); RepositoryService repositoryService = engine.getRepositoryService(); //找到流程定义 ProcessDefinition processDefinition = repositoryService. createProcessDefinitionQuery().processDefinitionId(\u0026#34;holidayRequest:1:7503\u0026#34;) .singleResult(); //当前流程定义的状态 boolean suspended = processDefinition.isSuspended(); if (suspended) { //如果挂起则激活 System.out.println(\u0026#34;激活流程(定义)\u0026#34; + processDefinition.getId() + \u0026#34;name:\u0026#34; + processDefinition .getName()); repositoryService.activateProcessDefinitionById(processDefinition.getId()); } else { //如果激活则挂起 System.out.println(\u0026#34;挂起流程(定义)\u0026#34; + processDefinition.getId() + \u0026#34;name:\u0026#34; + processDefinition .getName()); repositoryService.suspendProcessDefinitionById(processDefinition.getId()); } } 执行后 如果这时启动流程\n/** * 流程运行 */ @Test public void testRunProcess() { ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();//configuration.buildProcessEngine(); RuntimeService runtimeService = processEngine.getRuntimeService(); //这边模拟表单数据(表单数据有多种处理方式，这只是其中一种) Map\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;employee\u0026#34;, \u0026#34;张三\u0026#34;); map.put(\u0026#34;nrOfHolidays\u0026#34;, 3); map.put(\u0026#34;description\u0026#34;, \u0026#34;工作累了想出去玩\u0026#34;); ProcessInstance holidayRequest = runtimeService.startProcessInstanceByKey( \u0026#34;holidayRequest\u0026#34;, map); System.out.println(\u0026#34;流程定义的id:\u0026#34; + holidayRequest.getProcessDefinitionId()); System.out.println(\u0026#34;当前活跃id:\u0026#34; + holidayRequest.getActivityId()); System.out.println(\u0026#34;流程运行id:\u0026#34; + holidayRequest.getId()); } 则会出现异常报错信息\norg.flowable.common.engine.api.FlowableException: Cannot start process instance. Process definition 请假流程 (id = holidayRequest:1:7503) is suspended 此时再运行一次testSuspend()，将流程定义激活，此时数据库act_re_procdef表中的SUSPENSION_STATE字段值为1 再运行testRunProcess()，流程正常启动 启动流程的原理 # 流程启动\n/** * 流程运行 */ @Test public void testRunProcess() { ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine();//configuration.buildProcessEngine(); RepositoryService repositoryService = processEngine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment() .addClasspathResource(\u0026#34;holiday-request.bpmn20.xml\u0026#34;) .name(\u0026#34;ly05150817部署的请假流程\u0026#34;) .deploy(); //通过部署id查找流程定义 ProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery(). deploymentId(deploy.getId()) .singleResult(); RuntimeService runtimeService = processEngine.getRuntimeService(); //这边模拟表单数据(表单数据有多种处理方式，这只是其中一种) Map\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;employee\u0026#34;, \u0026#34;张三\u0026#34;); map.put(\u0026#34;nrOfHolidays\u0026#34;, 3); map.put(\u0026#34;description\u0026#34;, \u0026#34;工作累了想出去玩\u0026#34;); ProcessInstance holidayRequest = runtimeService.startProcessInstanceById( processDefinition.getId(), \u0026#34;order1000\u0026#34;, map); System.out.println(\u0026#34;流程定义的id:\u0026#34; + holidayRequest.getProcessDefinitionId()); System.out.println(\u0026#34;当前活跃id:\u0026#34; + holidayRequest.getActivityId()); System.out.println(\u0026#34;流程运行id:\u0026#34; + holidayRequest.getId()); } 涉及到的表：(HI中也有对应的表)\nACT_RU_EXECUTION 运行时流程执行实例 当启动一个实例的时候，这里会有两个流程执行\nACT_RU_IDENTITYLINK 运行时用户关系信息\n记录流程实例当前所处的节点\n数据库表 有几种任务处理人的类型\npublic class IdentityLinkType { public static final String ASSIGNEE = \u0026#34;assignee\u0026#34;; //指派 public static final String CANDIDATE = \u0026#34;candidate\u0026#34;;//候选 public static final String OWNER = \u0026#34;owner\u0026#34;;//拥有者 public static final String STARTER = \u0026#34;starter\u0026#34;;//启动者 public static final String PARTICIPANT = \u0026#34;participant\u0026#34;;//参与者 public static final String REACTIVATOR = \u0026#34;reactivator\u0026#34;; } ACT_RU_TASK 运行时任务表 ACT_RU_VARIABLE 运行时变量表\n处理流程的原理 # 流程处理\n@Test public void testCompleted(){ ProcessEngine processEngine=ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;4\u0026#34;) .taskAssignee(\u0026#34;zhangsan\u0026#34;) .singleResult(); //获取当前流程实例绑定的流程变量 Map\u0026lt;String, Object\u0026gt; processVariables = task.getProcessVariables(); Set\u0026lt;String\u0026gt; keySet = processVariables.keySet(); for(String key:keySet){ Object o = processVariables.get(key); System.out.println(\u0026#34;key:\u0026#34;+key+\u0026#34;--value:\u0026#34;+o); } processVariables.put(\u0026#34;approved\u0026#34;,true);//同意 processVariables.put(\u0026#34;description\u0026#34;,\u0026#34;我被修改了\u0026#34;); taskService.complete(task.getId(),processVariables); } 这里用的是之前的xml，所以应该给一个服务监听类\npublic class CallExternalSystemDelegate implements JavaDelegate { @Override public void execute(DelegateExecution execution) { System.out.println(\u0026#34;您的请求通过了！\u0026#34;); } } 任务处理后，这里添加了一个变量，且修改了变量description 可以通过流程变量，它可以在整个流程过程中流转的[注意，这里流程结束后流程变量会不存在的，但是act_hi_variinst里面可以看到流程变量实例] //我感觉应该用表单替代 act_ru_task和act_ru_identitylink\n两者区别 ACT _ RU _ IDENTITYLINK：此表存储有关用户或组的数据及其与（流程/案例/等）实例相关的角色。该表也被其他需要身份链接的引擎使用。【显示全部，包括已完成】 ACT _ RU _ TASK：此表包含一个正在运行的实例的每个未完成用户任务的条目。然后在查询用户的任务列表时使用此表。【这里只显示运行中】 act_ru_task 记录当前实例所运行的当前节点的信息 act_ru_identitylink act_ru_execution这个表的数据不会有变动 流程结束的原理 # 流程走完\n@Test public void testCompleted1() { ProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); TaskService taskService = processEngine.getTaskService(); Task task = taskService.createTaskQuery() .processInstanceId(\u0026#34;4\u0026#34;) .taskAssignee(\u0026#34;lisi\u0026#34;) .singleResult(); //获取当前流程实例绑定的流程变量 Map\u0026lt;String, Object\u0026gt; processVariables = task.getProcessVariables(); Set\u0026lt;String\u0026gt; keySet = processVariables.keySet(); for (String key : keySet) { Object o = processVariables.get(key); System.out.println(\u0026#34;key:\u0026#34; + key + \u0026#34;--value:\u0026#34; + o); } /* processVariables.put(\u0026#34;approved\u0026#34;,true);//拒绝 processVariables.put(\u0026#34;description\u0026#34;,\u0026#34;我被修改了\u0026#34;);*/ taskService.complete(task.getId(), processVariables); } 此时跟流程相关的数据都会被清空掉 历史数据\n变量 任务流转历史 流程实例 涉及到的用户 流程活动\n"},{"id":208,"href":"/zh/docs/problem/Idea/01/","title":"问题01","section":"Idea","content":" Cannot download sources # 在maven项目(根目录)下执行\nmvn dependency:resolve -Dclassifier=sources 会开始下载，有控制台输出，结束后再点即可\n预留 # "},{"id":209,"href":"/zh/docs/technology/Flowable/boge_blbl/02-advance_1/","title":"boge-02-flowable进阶_1","section":"基础(波哥)_","content":" 表结构 # 尽量通过API动数据\nACT_RE：repository，包含流程定义和流程静态资源\nACT_RU: runtime，包含流程实例、任务、变量等，流程结束会删除\nACT_HI: history，包含历史数据，比如历史流程实例、变量、任务等\nACT_GE: general，通用数据\nACT_ID: identity，组织机构。包含标识的信息，如用户、用户组等等\n具体的\n流程历史记录\n流程定义表 运行实例表 用户用户组表\n源码中的体现 默认的配置文件加载 # 对于\nProcessEngine defaultProcessEngine = ProcessEngines.getDefaultProcessEngine(); //--\u0026gt; public static ProcessEngine getDefaultProcessEngine() { return getProcessEngine(NAME_DEFAULT); //NAME_DEFAULT = \u0026#34;default\u0026#34; } //--\u0026gt; public static ProcessEngine getProcessEngine(String processEngineName) { if (!isInitialized()) { init(); } return processEngines.get(processEngineName); } //--\u0026gt;部分 /** * Initializes all process engines that can be found on the classpath for resources \u0026lt;code\u0026gt;flowable.cfg.xml\u0026lt;/code\u0026gt; (plain Flowable style configuration) and for resources * \u0026lt;code\u0026gt;flowable-context.xml\u0026lt;/code\u0026gt; (Spring style configuration). */ public static synchronized void init() { if (!isInitialized()) { if (processEngines == null) { // Create new map to store process-engines if current map is null processEngines = new HashMap\u0026lt;\u0026gt;(); } ClassLoader classLoader = ReflectUtil.getClassLoader(); Enumeration\u0026lt;URL\u0026gt; resources = null; try { resources = classLoader.getResources(\u0026#34;flowable.cfg.xml\u0026#34;); } catch (IOException e) { throw new FlowableIllegalArgumentException(\u0026#34;problem retrieving flowable.cfg.xml resources on the classpath: \u0026#34; + System.getProperty(\u0026#34;java.class.path\u0026#34;), e); } //后面还有，每帖出来 } } 注意这行classLoader.getResources(\u0026quot;flowable.cfg.xml\u0026quot;); 需要在resources根目录下放这么一个文件\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;processEngineConfiguration\u0026#34; class=\u0026#34;org.flowable.engine.impl.cfg.StandaloneProcessEngineConfiguration\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;jdbcUrl\u0026#34; value=\u0026#34;jdbc:mysql://localhost:3306/flow1?useUnicode=true\u0026amp;amp;characterEncoding=utf-8\u0026amp;amp;allowMultiQueries=true\u0026amp;amp;nullCatalogMeansCurrent=true\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;jdbcDriver\u0026#34; value=\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;jdbcUsername\u0026#34; value=\u0026#34;root\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;jdbcPassword\u0026#34; value=\u0026#34;123456\u0026#34;/\u0026gt; \u0026lt;property name=\u0026#34;databaseSchemaUpdate\u0026#34; value=\u0026#34;true\u0026#34;/\u0026gt; \u0026lt;!--异步执行器--\u0026gt; \u0026lt;property name=\u0026#34;asyncExecutorActivate\u0026#34; value=\u0026#34;false\u0026#34;/\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 新建数据库flow1，运行测试代码\n@Test public void processEngine2(){ ProcessEngine defaultProcessEngine = ProcessEngines.getDefaultProcessEngine(); System.out.println(defaultProcessEngine); } 此时数据库已经有表\n加载自定义名称的配置文件 # 把刚才的数据库清空，将flowable的配置文件放到目录custom/lycfg.xml中 代码\n@Test public void processEngine03(){ ProcessEngineConfiguration configuration = ProcessEngineConfiguration.createProcessEngineConfigurationFromResource(\u0026#34;custom/lycfg.xml\u0026#34;); System.out.println(configuration); ProcessEngine processEngine = configuration.buildProcessEngine(); System.out.println(processEngine); } ProcessEngine源码查看 # 源码追溯\nconfiguration.buildProcessEngine() //---\u0026gt;ProcessEngineConfigurationImpl.class @Override public ProcessEngine buildProcessEngine() { init(); ProcessEngineImpl processEngine = new ProcessEngineImpl(this); //... } //----\u0026gt;ProcessEngineImpl.class public class ProcessEngineImpl implements ProcessEngine { private static final Logger LOGGER = LoggerFactory.getLogger(ProcessEngineImpl.class); protected String name; protected RepositoryService repositoryService; protected RuntimeService runtimeService; protected HistoryService historicDataService; protected IdentityService identityService; protected TaskService taskService; protected FormService formService; protected ManagementService managementService; protected DynamicBpmnService dynamicBpmnService; protected ProcessMigrationService processInstanceMigrationService; protected AsyncExecutor asyncExecutor; protected AsyncExecutor asyncHistoryExecutor; protected CommandExecutor commandExecutor; protected Map\u0026lt;Class\u0026lt;?\u0026gt;, SessionFactory\u0026gt; sessionFactories; protected TransactionContextFactory transactionContextFactory; protected ProcessEngineConfigurationImpl processEngineConfiguration; //这里通过ProcessEngineConfigurationImpl获取各种对象 public ProcessEngineImpl(ProcessEngineConfigurationImpl processEngineConfiguration) { this.processEngineConfiguration = processEngineConfiguration; this.name = processEngineConfiguration.getEngineName(); this.repositoryService = processEngineConfiguration.getRepositoryService(); this.runtimeService = processEngineConfiguration.getRuntimeService(); this.historicDataService = processEngineConfiguration.getHistoryService(); this.identityService = processEngineConfiguration.getIdentityService(); this.taskService = processEngineConfiguration.getTaskService(); this.formService = processEngineConfiguration.getFormService(); this.managementService = processEngineConfiguration.getManagementService(); this.dynamicBpmnService = processEngineConfiguration.getDynamicBpmnService(); this.processInstanceMigrationService = processEngineConfiguration.getProcessMigrationService(); this.asyncExecutor = processEngineConfiguration.getAsyncExecutor(); this.asyncHistoryExecutor = processEngineConfiguration.getAsyncHistoryExecutor(); this.commandExecutor = processEngineConfiguration.getCommandExecutor(); this.sessionFactories = processEngineConfiguration.getSessionFactories(); this.transactionContextFactory = processEngineConfiguration.getTransactionContextFactory(); } //... } //----\u0026gt;ProcessEngine.class 获取各个service服务 public interface ProcessEngine extends Engine { /** the version of the flowable library */ String VERSION = FlowableVersions.CURRENT_VERSION; /** * Starts the execuctors (async and async history), if they are configured to be auto-activated. */ void startExecutors(); RepositoryService getRepositoryService(); RuntimeService getRuntimeService(); FormService getFormService(); TaskService getTaskService(); HistoryService getHistoryService(); IdentityService getIdentityService(); ManagementService getManagementService(); DynamicBpmnService getDynamicBpmnService(); ProcessMigrationService getProcessMigrationService(); ProcessEngineConfiguration getProcessEngineConfiguration(); } ProcessEngineConfiguration中的init方法 # 源码追溯\nconfiguration.buildProcessEngine() //---\u0026gt;ProcessEngineConfigurationImpl.class @Override public ProcessEngine buildProcessEngine() { init(); ProcessEngineImpl processEngine = new ProcessEngineImpl(this); //... } //---\u0026gt;ProcessEngineConfigurationImpl.init(); public void init() { initEngineConfigurations(); initConfigurators(); configuratorsBeforeInit(); initClock(); initObjectMapper(); initProcessDiagramGenerator(); initCommandContextFactory(); initTransactionContextFactory(); initCommandExecutors(); initIdGenerator(); initHistoryLevel(); initFunctionDelegates(); initAstFunctionCreators(); initDelegateInterceptor(); initBeans(); initExpressionManager(); initAgendaFactory(); //关系型数据库 if (usingRelationalDatabase) { initDataSource();//下面拿这个举例1 } else { initNonRelationalDataSource(); } if (usingRelationalDatabase || usingSchemaMgmt) { initSchemaManager(); initSchemaManagementCommand(); } configureVariableServiceConfiguration(); configureJobServiceConfiguration(); initHelpers(); initVariableTypes(); initFormEngines(); initFormTypes(); initScriptingEngines(); initBusinessCalendarManager(); initServices(); initWsdlImporterFactory(); initBehaviorFactory(); initListenerFactory(); initBpmnParser(); initProcessDefinitionCache(); initProcessDefinitionInfoCache(); initAppResourceCache(); initKnowledgeBaseCache(); initJobHandlers(); initHistoryJobHandlers(); initTransactionFactory(); if (usingRelationalDatabase) { initSqlSessionFactory();//下面拿这个举例2 } initSessionFactories(); //相关表结构操作 initDataManagers(); //下面拿这个举例2 initEntityManagers(); initCandidateManager(); initVariableAggregator(); initHistoryManager(); initChangeTenantIdManager(); initDynamicStateManager(); initProcessInstanceMigrationValidationManager(); initIdentityLinkInterceptor(); initJpa(); initDeployers(); initEventHandlers(); initFailedJobCommandFactory(); initEventDispatcher(); initProcessValidator(); initFormFieldHandler(); initDatabaseEventLogging(); initFlowable5CompatibilityHandler(); initVariableServiceConfiguration(); //流程变量 initIdentityLinkServiceConfiguration(); initEntityLinkServiceConfiguration(); initEventSubscriptionServiceConfiguration(); initTaskServiceConfiguration(); initJobServiceConfiguration(); initBatchServiceConfiguration(); initAsyncExecutor(); initAsyncHistoryExecutor(); configuratorsAfterInit(); afterInitTaskServiceConfiguration(); afterInitEventRegistryEventBusConsumer(); initHistoryCleaningManager(); initLocalizationManagers(); } //---\u0026gt;AbstractEngineConfiguration //----\u0026gt;AbstractEngineConfiguration.initDataSrouce() public static Properties getDefaultDatabaseTypeMappings() { Properties databaseTypeMappings = new Properties(); databaseTypeMappings.setProperty(\u0026#34;H2\u0026#34;, DATABASE_TYPE_H2); databaseTypeMappings.setProperty(\u0026#34;HSQL Database Engine\u0026#34;, DATABASE_TYPE_HSQL); databaseTypeMappings.setProperty(\u0026#34;MySQL\u0026#34;, DATABASE_TYPE_MYSQL); databaseTypeMappings.setProperty(\u0026#34;MariaDB\u0026#34;, DATABASE_TYPE_MYSQL); databaseTypeMappings.setProperty(\u0026#34;Oracle\u0026#34;, DATABASE_TYPE_ORACLE); databaseTypeMappings.setProperty(PRODUCT_NAME_POSTGRES, DATABASE_TYPE_POSTGRES); databaseTypeMappings.setProperty(\u0026#34;Microsoft SQL Server\u0026#34;, DATABASE_TYPE_MSSQL); databaseTypeMappings.setProperty(DATABASE_TYPE_DB2, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/NT\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/NT64\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2 UDP\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/LINUX\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/LINUX390\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/LINUXX8664\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/LINUXZ64\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/LINUXPPC64\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/LINUXPPC64LE\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/400 SQL\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/6000\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2 UDB iSeries\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/AIX64\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/HPUX\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/HP64\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/SUN\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/SUN64\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/PTX\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2/2\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(\u0026#34;DB2 UDB AS400\u0026#34;, DATABASE_TYPE_DB2); databaseTypeMappings.setProperty(PRODUCT_NAME_CRDB, DATABASE_TYPE_COCKROACHDB); return databaseTypeMappings; } //initDataSource(); protected void initDataSource() { if (dataSource == null) { if (dataSourceJndiName != null) { try { dataSource = (DataSource) new InitialContext().lookup(dataSourceJndiName); } catch (Exception e) { throw new FlowableException(\u0026#34;couldn\u0026#39;t lookup datasource from \u0026#34; + dataSourceJndiName + \u0026#34;: \u0026#34; + e.getMessage(), e); } } else if (jdbcUrl != null) { if ((jdbcDriver == null) || (jdbcUsername == null)) { throw new FlowableException(\u0026#34;DataSource or JDBC properties have to be specified in a process engine configuration\u0026#34;); } logger.debug(\u0026#34;initializing datasource to db: {}\u0026#34;, jdbcUrl); if (logger.isInfoEnabled()) { logger.info(\u0026#34;Configuring Datasource with following properties (omitted password for security)\u0026#34;); logger.info(\u0026#34;datasource driver : {}\u0026#34;, jdbcDriver); logger.info(\u0026#34;datasource url : {}\u0026#34;, jdbcUrl); logger.info(\u0026#34;datasource user name : {}\u0026#34;, jdbcUsername); } PooledDataSource pooledDataSource = new PooledDataSource(this.getClass().getClassLoader(), jdbcDriver, jdbcUrl, jdbcUsername, jdbcPassword); if (jdbcMaxActiveConnections \u0026gt; 0) { pooledDataSource.setPoolMaximumActiveConnections(jdbcMaxActiveConnections); } if (jdbcMaxIdleConnections \u0026gt; 0) { pooledDataSource.setPoolMaximumIdleConnections(jdbcMaxIdleConnections); } if (jdbcMaxCheckoutTime \u0026gt; 0) { pooledDataSource.setPoolMaximumCheckoutTime(jdbcMaxCheckoutTime); } if (jdbcMaxWaitTime \u0026gt; 0) { pooledDataSource.setPoolTimeToWait(jdbcMaxWaitTime); } if (jdbcPingEnabled) { pooledDataSource.setPoolPingEnabled(true); if (jdbcPingQuery != null) { pooledDataSource.setPoolPingQuery(jdbcPingQuery); } pooledDataSource.setPoolPingConnectionsNotUsedFor(jdbcPingConnectionNotUsedFor); } if (jdbcDefaultTransactionIsolationLevel \u0026gt; 0) { pooledDataSource.setDefaultTransactionIsolationLevel(jdbcDefaultTransactionIsolationLevel); } dataSource = pooledDataSource; } } if (databaseType == null) { initDatabaseType(); } } //initSqlSessionFactory(); public void initSqlSessionFactory() { if (sqlSessionFactory == null) { InputStream inputStream = null; try { //获取MyBatis配置文件信息 inputStream = getMyBatisXmlConfigurationStream(); Environment environment = new Environment(\u0026#34;default\u0026#34;, transactionFactory, dataSource); Reader reader = new InputStreamReader(inputStream); Properties properties = new Properties(); properties.put(\u0026#34;prefix\u0026#34;, databaseTablePrefix); String wildcardEscapeClause = \u0026#34;\u0026#34;; if ((databaseWildcardEscapeCharacter != null) \u0026amp;\u0026amp; (databaseWildcardEscapeCharacter.length() != 0)) { wildcardEscapeClause = \u0026#34; escape \u0026#39;\u0026#34; + databaseWildcardEscapeCharacter + \u0026#34;\u0026#39;\u0026#34;; } properties.put(\u0026#34;wildcardEscapeClause\u0026#34;, wildcardEscapeClause); // set default properties properties.put(\u0026#34;limitBefore\u0026#34;, \u0026#34;\u0026#34;); properties.put(\u0026#34;limitAfter\u0026#34;, \u0026#34;\u0026#34;); properties.put(\u0026#34;limitBetween\u0026#34;, \u0026#34;\u0026#34;); properties.put(\u0026#34;limitBeforeNativeQuery\u0026#34;, \u0026#34;\u0026#34;); properties.put(\u0026#34;limitAfterNativeQuery\u0026#34;, \u0026#34;\u0026#34;); properties.put(\u0026#34;blobType\u0026#34;, \u0026#34;BLOB\u0026#34;); properties.put(\u0026#34;boolValue\u0026#34;, \u0026#34;TRUE\u0026#34;); if (databaseType != null) { properties.load(getResourceAsStream(pathToEngineDbProperties())); } //Mybatis相关的配置 Configuration configuration = initMybatisConfiguration(environment, reader, properties); sqlSessionFactory = new DefaultSqlSessionFactory(configuration); } catch (Exception e) { throw new FlowableException(\u0026#34;Error while building ibatis SqlSessionFactory: \u0026#34; + e.getMessage(), e); } finally { IoUtil.closeSilently(inputStream); } } } //ProcessEngineConfigurationImpl.getMyBatisXmlConfigurationStream(); @Override public InputStream getMyBatisXmlConfigurationStream() { return getResourceAsStream(mybatisMappingFile); } //代码往上翻 //构造器中 public ProcessEngineConfigurationImpl() { mybatisMappingFile = DEFAULT_MYBATIS_MAPPING_FILE; } //其中 public static final String DEFAULT_MYBATIS_MAPPING_FILE = \u0026#34;org/flowable/db/mapping/mappings.xml\u0026#34;; 查找映射文件 mappings.xml\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE configuration PUBLIC \u0026#34;-//mybatis.org//DTD Config 3.0//EN\u0026#34; \u0026#34;http://mybatis.org/dtd/mybatis-3-config.dtd\u0026#34;\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;settings\u0026gt; \u0026lt;setting name=\u0026#34;lazyLoadingEnabled\u0026#34; value=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;/settings\u0026gt; \u0026lt;typeAliases\u0026gt; \u0026lt;typeAlias type=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRefTypeHandler\u0026#34; alias=\u0026#34;ByteArrayRefTypeHandler\u0026#34; /\u0026gt; \u0026lt;typeAlias type=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRefTypeHandler\u0026#34; alias=\u0026#34;VariableByteArrayRefTypeHandler\u0026#34; /\u0026gt; \u0026lt;typeAlias type=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRefTypeHandler\u0026#34; alias=\u0026#34;JobByteArrayRefTypeHandler\u0026#34; /\u0026gt; \u0026lt;typeAlias type=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRefTypeHandler\u0026#34; alias=\u0026#34;BatchByteArrayRefTypeHandler\u0026#34; /\u0026gt; \u0026lt;/typeAliases\u0026gt; \u0026lt;typeHandlers\u0026gt; \u0026lt;typeHandler handler=\u0026#34;ByteArrayRefTypeHandler\u0026#34; javaType=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRef\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; /\u0026gt; \u0026lt;typeHandler handler=\u0026#34;VariableByteArrayRefTypeHandler\u0026#34; javaType=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRef\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; /\u0026gt; \u0026lt;typeHandler handler=\u0026#34;JobByteArrayRefTypeHandler\u0026#34; javaType=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRef\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; /\u0026gt; \u0026lt;typeHandler handler=\u0026#34;BatchByteArrayRefTypeHandler\u0026#34; javaType=\u0026#34;org.flowable.common.engine.impl.persistence.entity.ByteArrayRef\u0026#34; jdbcType=\u0026#34;VARCHAR\u0026#34; /\u0026gt; \u0026lt;/typeHandlers\u0026gt; \u0026lt;mappers\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/ChangeTenantBpmn.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/Attachment.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/Comment.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/job/service/db/mapping/entity/DeadLetterJob.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/Deployment.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/Execution.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/ActivityInstance.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/HistoricActivityInstance.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/HistoricDetail.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/HistoricProcessInstance.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/variable/service/db/mapping/entity/HistoricVariableInstance.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/task/service/db/mapping/entity/HistoricTaskInstance.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/task/service/db/mapping/entity/HistoricTaskLogEntry.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/identitylink/service/db/mapping/entity/HistoricIdentityLink.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/entitylink/service/db/mapping/entity/HistoricEntityLink.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/job/service/db/mapping/entity/HistoryJob.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/identitylink/service/db/mapping/entity/IdentityLink.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/entitylink/service/db/mapping/entity/EntityLink.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/job/service/db/mapping/entity/Job.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/Model.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/ProcessDefinition.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/ProcessDefinitionInfo.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/common/db/mapping/entity/Property.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/common/db/mapping/entity/ByteArray.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/common/db/mapping/common.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/Resource.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/job/service/db/mapping/entity/SuspendedJob.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/job/service/db/mapping/entity/ExternalWorkerJob.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/common/db/mapping/entity/TableData.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/task/service/db/mapping/entity/Task.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/job/service/db/mapping/entity/TimerJob.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/variable/service/db/mapping/entity/VariableInstance.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/eventsubscription/service/db/mapping/entity/EventSubscription.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/db/mapping/entity/EventLogEntry.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/batch/service/db/mapping/entity/Batch.xml\u0026#34; /\u0026gt; \u0026lt;mapper resource=\u0026#34;org/flowable/batch/service/db/mapping/entity/BatchPart.xml\u0026#34; /\u0026gt; \u0026lt;/mappers\u0026gt; \u0026lt;/configuration\u0026gt; 源码\n//ProcessEnginConfigurationImpl.init()中的代码 initDataManagers(); //下面拿这个举例3 //-\u0026gt;\u0026gt;\u0026gt; @Override @SuppressWarnings(\u0026#34;rawtypes\u0026#34;) public void initDataManagers() { super.initDataManagers(); if (attachmentDataManager == null) { attachmentDataManager = new MybatisAttachmentDataManager(this); } if (commentDataManager == null) { commentDataManager = new MybatisCommentDataManager(this); } if (deploymentDataManager == null) { //下面拿这个查看 deploymentDataManager = new MybatisDeploymentDataManager(this); } if (eventLogEntryDataManager == null) { eventLogEntryDataManager = new MybatisEventLogEntryDataManager(this); } if (executionDataManager == null) { executionDataManager = new MybatisExecutionDataManager(this); } if (dbSqlSessionFactory != null \u0026amp;\u0026amp; executionDataManager instanceof AbstractDataManager) { dbSqlSessionFactory.addLogicalEntityClassMapping(\u0026#34;execution\u0026#34;, ((AbstractDataManager) executionDataManager).getManagedEntityClass()); } if (historicActivityInstanceDataManager == null) { historicActivityInstanceDataManager = new MybatisHistoricActivityInstanceDataManager(this); } if (activityInstanceDataManager == null) { activityInstanceDataManager = new MybatisActivityInstanceDataManager(this); } if (historicDetailDataManager == null) { historicDetailDataManager = new MybatisHistoricDetailDataManager(this); } if (historicProcessInstanceDataManager == null) { historicProcessInstanceDataManager = new MybatisHistoricProcessInstanceDataManager(this); } if (modelDataManager == null) { modelDataManager = new MybatisModelDataManager(this); } if (processDefinitionDataManager == null) { processDefinitionDataManager = new MybatisProcessDefinitionDataManager(this); } if (processDefinitionInfoDataManager == null) { processDefinitionInfoDataManager = new MybatisProcessDefinitionInfoDataManager(this); } if (resourceDataManager == null) { resourceDataManager = new MybatisResourceDataManager(this); } } //--\u0026gt;MybatisDeploymentDataManager，这个类相当于mybatis中的mapper /** * @author Joram Barrez */ public class MybatisDeploymentDataManager extends AbstractProcessDataManager\u0026lt;DeploymentEntity\u0026gt; implements DeploymentDataManager { public MybatisDeploymentDataManager(ProcessEngineConfigurationImpl processEngineConfiguration) { super(processEngineConfiguration); } @Override public Class\u0026lt;? extends DeploymentEntity\u0026gt; getManagedEntityClass() { return DeploymentEntityImpl.class; } @Override public DeploymentEntity create() { return new DeploymentEntityImpl(); } @Override public long findDeploymentCountByQueryCriteria(DeploymentQueryImpl deploymentQuery) { return (Long) getDbSqlSession().selectOne(\u0026#34;selectDeploymentCountByQueryCriteria\u0026#34;, deploymentQuery); } @Override @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public List\u0026lt;Deployment\u0026gt; findDeploymentsByQueryCriteria(DeploymentQueryImpl deploymentQuery) { final String query = \u0026#34;selectDeploymentsByQueryCriteria\u0026#34;; return getDbSqlSession().selectList(query, deploymentQuery); } @Override public List\u0026lt;String\u0026gt; getDeploymentResourceNames(String deploymentId) { return getDbSqlSession().getSqlSession().selectList(\u0026#34;selectResourceNamesByDeploymentId\u0026#34;, deploymentId); } @Override @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public List\u0026lt;Deployment\u0026gt; findDeploymentsByNativeQuery(Map\u0026lt;String, Object\u0026gt; parameterMap) { return getDbSqlSession().selectListWithRawParameter(\u0026#34;selectDeploymentByNativeQuery\u0026#34;, parameterMap); } @Override public long findDeploymentCountByNativeQuery(Map\u0026lt;String, Object\u0026gt; parameterMap) { return (Long) getDbSqlSession().selectOne(\u0026#34;selectDeploymentCountByNativeQuery\u0026#34;, parameterMap); } } ProcessEngine各种方法对比 # ProcessEngines.getDefaultProcessEngine();的方式\n/** * Initializes all process engines that can be found on the classpath for resources \u0026lt;code\u0026gt;flowable.cfg.xml\u0026lt;/code\u0026gt; (plain Flowable style configuration) and for resources * \u0026lt;code\u0026gt;flowable-context.xml\u0026lt;/code\u0026gt; (Spring style configuration). */ public static synchronized void init() { if (!isInitialized()) { if (processEngines == null) { // Create new map to store process-engines if current map is null processEngines = new HashMap\u0026lt;\u0026gt;(); } ClassLoader classLoader = ReflectUtil.getClassLoader(); Enumeration\u0026lt;URL\u0026gt; resources = null; try { resources = classLoader.getResources(\u0026#34;flowable.cfg.xml\u0026#34;); } catch (IOException e) { throw new FlowableIllegalArgumentException(\u0026#34;problem retrieving flowable.cfg.xml resources on the classpath: \u0026#34; + System.getProperty(\u0026#34;java.class.path\u0026#34;), e); } // Remove duplicated configuration URL\u0026#39;s using set. Some // classloaders may return identical URL\u0026#39;s twice, causing duplicate // startups Set\u0026lt;URL\u0026gt; configUrls = new HashSet\u0026lt;\u0026gt;(); while (resources.hasMoreElements()) { configUrls.add(resources.nextElement()); } for (URL resource : configUrls) { LOGGER.info(\u0026#34;Initializing process engine using configuration \u0026#39;{}\u0026#39;\u0026#34;, resource); initProcessEngineFromResource(resource); //注意这个 } try { resources = classLoader.getResources(\u0026#34;flowable-context.xml\u0026#34;); } catch (IOException e) { throw new FlowableIllegalArgumentException(\u0026#34;problem retrieving flowable-context.xml resources on the classpath: \u0026#34; + System.getProperty(\u0026#34;java.class.path\u0026#34;), e); } while (resources.hasMoreElements()) { URL resource = resources.nextElement(); LOGGER.info(\u0026#34;Initializing process engine using Spring configuration \u0026#39;{}\u0026#39;\u0026#34;, resource); initProcessEngineFromSpringResource(resource); } setInitialized(true); } else { LOGGER.info(\u0026#34;Process engines already initialized\u0026#34;); } } 可以通过Spring配置文件的方式\ninitProcessEngineFromResource(resource); //注意这个 private static EngineInfo initProcessEngineFromResource(URL resourceUrl) { EngineInfo processEngineInfo = processEngineInfosByResourceUrl.get(resourceUrl.toString()); // if there is an existing process engine info if (processEngineInfo != null) { // remove that process engine from the member fields processEngineInfos.remove(processEngineInfo); if (processEngineInfo.getException() == null) { String processEngineName = processEngineInfo.getName(); processEngines.remove(processEngineName); processEngineInfosByName.remove(processEngineName); } processEngineInfosByResourceUrl.remove(processEngineInfo.getResourceUrl()); } String resourceUrlString = resourceUrl.toString(); try { LOGGER.info(\u0026#34;initializing process engine for resource {}\u0026#34;, resourceUrl); //注意这个 ProcessEngine processEngine = buildProcessEngine(resourceUrl); String processEngineName = processEngine.getName(); LOGGER.info(\u0026#34;initialised process engine {}\u0026#34;, processEngineName); processEngineInfo = new EngineInfo(processEngineName, resourceUrlString, null); processEngines.put(processEngineName, processEngine); processEngineInfosByName.put(processEngineName, processEngineInfo); } catch (Throwable e) { LOGGER.error(\u0026#34;Exception while initializing process engine: {}\u0026#34;, e.getMessage(), e); processEngineInfo = new EngineInfo(null, resourceUrlString, ExceptionUtils.getStackTrace(e)); } processEngineInfosByResourceUrl.put(resourceUrlString, processEngineInfo); processEngineInfos.add(processEngineInfo); return processEngineInfo; } 源码\nbuildProcessEngine(resourceUrl); // private static ProcessEngine buildProcessEngine(URL resource) { InputStream inputStream = null; try { inputStream = resource.openStream(); ProcessEngineConfiguration processEngineConfiguration = ProcessEngineConfiguration.createProcessEngineConfigurationFromInputStream(inputStream); return processEngineConfiguration.buildProcessEngine(); } catch (IOException e) { throw new FlowableIllegalArgumentException(\u0026#34;couldn\u0026#39;t open resource stream: \u0026#34; + e.getMessage(), e); } finally { IoUtil.closeSilently(inputStream); } } "},{"id":210,"href":"/zh/docs/technology/Flowable/boge_blbl/01-base/","title":"boge-01-flowable基础","section":"基础(波哥)_","content":" Flowable介绍 # flowable的历史\nflowable是BPNM的一个基于java的软件实现，不仅包括BPMN，还有DMN决策表和CMMNCase管理引擎，并且有自己的用户管理、微服务API等\n获取Engine对象 # maven依赖\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.29\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.flowable/flowable-engine --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.flowable\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flowable-engine\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.7.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/junit/junit --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;junit\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.13.2\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 配置并获取ProcessEngine\nProcessEngineConfiguration configuration= new StandaloneProcessEngineConfiguration(); //配置 configuration.setJdbcDriver(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); configuration.setJdbcUsername(\u0026#34;root\u0026#34;); configuration.setJdbcPassword(\u0026#34;123456\u0026#34;); //nullCatalogMeansCurrent=true 设置为只查当前连接的schema库 configuration.setJdbcUrl(\u0026#34;jdbc:mysql://localhost:3306/flowable-learn?\u0026#34; + \u0026#34;useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; + \u0026#34;\u0026amp;allowMultiQueries=true\u0026#34; + \u0026#34;\u0026amp;nullCatalogMeansCurrent=true\u0026#34;); //如果数据库中表结构不存在则新建 configuration.setDatabaseSchemaUpdate(ProcessEngineConfiguration.DB_SCHEMA_UPDATE_TRUE); //构建ProcessEngine ProcessEngine processEngine=configuration.buildProcessEngine(); 日志和表结构介绍 # 添加slf4j依赖\n\u0026lt;!-- https://mvnrepository.com/artifact/org.slf4j/slf4j-reload4j --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-reload4j\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.36\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/org.apache.logging.log4j/log4j-api --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.logging.log4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;log4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.17.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 添加log配置文件\nlog4j.rootLogger = DEBUG, CA log4j.appender.CA = org.apache.log4j.ConsoleAppender log4j.appender.CA.layout = org.apache.log4j.PatternLayout log4j.appender.CA.layout.ConversionPattern = %d{hh:mm:ss,SSS} {%t} %-5p %c %x - %m%n 此时再次启动就会看到一堆日志 表 流程定义文件解析 # 先通过流程绘制器绘制流程\n案例（官网，请假流程） 设计好流程之后，流程数据保存在holiday-request.bpmn20.xml文件中\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34;\u0026gt; \u0026lt;!--id process key--\u0026gt; \u0026lt;process id=\u0026#34;holidayRequest\u0026#34; name=\u0026#34;请假流程\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent\u0026#34;/\u0026gt; \u0026lt;!--sequenceFlow表示的是线条箭头--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;startEvent\u0026#34; targetRef=\u0026#34;approveTask\u0026#34;/\u0026gt; \u0026lt;userTask id=\u0026#34;approveTask\u0026#34; name=\u0026#34;同意或者拒绝请假\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;approveTask\u0026#34; targetRef=\u0026#34;decision\u0026#34;/\u0026gt; \u0026lt;!--网关--\u0026gt; \u0026lt;exclusiveGateway id=\u0026#34;decision\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;decision\u0026#34; targetRef=\u0026#34;externalSystemCall\u0026#34;\u0026gt; \u0026lt;!--条件--\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt; \u0026lt;![CDATA[ ${approved} ]]\u0026gt; \u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;decision\u0026#34; targetRef=\u0026#34;sendRejectionMail\u0026#34;\u0026gt; \u0026lt;!--条件--\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt; \u0026lt;![CDATA[ ${!approved} ]]\u0026gt; \u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;serviceTask id=\u0026#34;externalSystemCall\u0026#34; name=\u0026#34;Enter holidays in external system\u0026#34; flowable:class=\u0026#34;org.flowable.CallExternalSystemDelegate\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;externalSystemCall\u0026#34; targetRef=\u0026#34;holidayApprovedTask\u0026#34;/\u0026gt; \u0026lt;userTask id=\u0026#34;holidayApprovedTask\u0026#34; name=\u0026#34;Holiday approved\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;holidayApprovedTask\u0026#34; targetRef=\u0026#34;approveEnd\u0026#34;/\u0026gt; \u0026lt;!--发送一个邮件--\u0026gt; \u0026lt;serviceTask id=\u0026#34;sendRejectionMail\u0026#34; name=\u0026#34;Send out rejection email\u0026#34; flowable:class=\u0026#34;org.flowable.SendRejectionMail\u0026#34;/\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;sendRejectionMail\u0026#34; targetRef=\u0026#34;rejectEnd\u0026#34;/\u0026gt; \u0026lt;endEvent id=\u0026#34;approveEnd\u0026#34;/\u0026gt; \u0026lt;endEvent id=\u0026#34;rejectEnd\u0026#34;/\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;/definitions\u0026gt; 部署流程-代码实现 # 使用@bofore 处理测试中繁琐的配置操作\nProcessEngineConfiguration configuration = null; @Before public void before() { configuration = new StandaloneProcessEngineConfiguration(); //配置 configuration.setJdbcDriver(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); configuration.setJdbcUsername(\u0026#34;root\u0026#34;); configuration.setJdbcPassword(\u0026#34;123456\u0026#34;); //nullCatalogMeansCurrent=true 设置为只查当前连接的schema库 configuration.setJdbcUrl(\u0026#34;jdbc:mysql://localhost:3306/flowable-learn?\u0026#34; + \u0026#34;useUnicode=true\u0026amp;characterEncoding=utf-8\u0026#34; + \u0026#34;\u0026amp;allowMultiQueries=true\u0026#34; + \u0026#34;\u0026amp;nullCatalogMeansCurrent=true\u0026#34;); //如果数据库中表结构不存在则新建 configuration.setDatabaseSchemaUpdate(ProcessEngineConfiguration.DB_SCHEMA_UPDATE_TRUE); } ProcessEngine提供的几个服务 流程部署\n/** * 流程的部署 */ @Test public void testDeploy() { //获取ProcessEngine对象 ProcessEngine processEngine = configuration.buildProcessEngine(); //获取服务(repository，流程定义) RepositoryService repositoryService = processEngine.getRepositoryService(); Deployment deploy = repositoryService.createDeployment().addClasspathResource(\u0026#34;holiday-request.bpmn20.xml\u0026#34;) .name(\u0026#34;请求流程\u0026#34;) //流程名 .deploy(); System.out.println(\u0026#34;部署id\u0026#34; + deploy.getId()); System.out.println(\u0026#34;部署名\u0026#34; + deploy.getName()); } 表结构 查询和删除操作 # 查询已经部署的流程定义\n/** * 流程定义及部署的查询 */ @Test public void testDeployQuery(){ ProcessEngine processEngine=configuration.buildProcessEngine(); RepositoryService repositoryService=processEngine.getRepositoryService(); //流程部署查询 //这里只部署了一个流程定义 Deployment deployment = repositoryService.createDeploymentQuery() .deploymentId(\u0026#34;1\u0026#34;).singleResult(); System.out.println(\u0026#34;部署时的名称:\u0026#34;+deployment.getName()); //流程定义查询器 ProcessDefinitionQuery processDefinitionQuery = repositoryService.createProcessDefinitionQuery(); //查询到的流程定义 ProcessDefinition processDefinition = processDefinitionQuery.deploymentId(\u0026#34;1\u0026#34;).singleResult(); System.out.println(\u0026#34;部署id:\u0026#34;+processDefinition.getDeploymentId()); System.out.println(\u0026#34;定义名:\u0026#34;+processDefinition.getName()); System.out.println(\u0026#34;描述:\u0026#34;+processDefinition.getDescription()); System.out.println(\u0026#34;定义id:\u0026#34;+processDefinition.getId()); } 删除流程定义\n代码\n/** * 流程删除 */ @Test public void testDeleteDeploy(){ ProcessEngine processEngine=configuration.buildProcessEngine(); RepositoryService repositoryService=processEngine. getRepositoryService(); //注意：第一个参数时部署id //后面那个参数表示级联删除，如果流程启动了会同时删除任务。 repositoryService.deleteDeployment(\u0026#34;2501\u0026#34;,true); } 下面三个表的数据都会被删除 启动流程实例 # 由于刚才将部署删除了，所以这里再运行testDeploy()重新部署上\n这里通过流程定义key（xml中的id）启动流程\n/** * 流程运行 */ @Test public void testRunProcess(){ ProcessEngine processEngine=configuration.buildProcessEngine(); RuntimeService runtimeService = processEngine.getRuntimeService(); //这边模拟表单数据(表单数据有多种处理方式，这只是其中一种) Map\u0026lt;String,Object\u0026gt; map=new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;employee\u0026#34;,\u0026#34;张三\u0026#34;); map.put(\u0026#34;nrOfHolidays\u0026#34;,3); map.put(\u0026#34;description\u0026#34;,\u0026#34;工作累了想出去玩\u0026#34;); ProcessInstance holidayRequest = runtimeService.startProcessInstanceByKey(\u0026#34;holidayRequest\u0026#34;, map); System.out.println(\u0026#34;流程定义的id:\u0026#34;+holidayRequest.getProcessDefinitionId()); System.out.println(\u0026#34;当前活跃id:\u0026#34;+holidayRequest.getActivityId()); System.out.println(\u0026#34;流程运行id:\u0026#34;+holidayRequest.getId()); } 三个表 act_ru_variable act_ru_task arc_ru_execution\n查询任务 # 这里先指定一下每个任务的候选人，修改xml文件中userTask的节点属性\n修改前先删除一下之前部署的流程图(还是上面的代码)\n/** * 流程删除 */ @Test public void testDeleteDeploy(){ ProcessEngine processEngine=configuration.buildProcessEngine(); RepositoryService repositoryService=processEngine. getRepositoryService(); //注意：第一个参数时部署id //后面那个参数表示级联删除，true表示如果流程启动了会同时删除任务。 repositoryService.deleteDeployment(\u0026#34;2501\u0026#34;,false); } 这里用false参数测试，会提示失败，运行中的流程不允许删除。将第二个参数改为true即可级联删除\n删除后可以发现下面几个表数据全部清空了 然后修改xml定义文件并运行testDeploy()重新部署\n定义修改\n\u0026lt;userTask id=\u0026#34;approveTask\u0026#34; name=\u0026#34;同意或者拒绝请假\u0026#34; flowable:assignee=\u0026#34;zhangsan\u0026#34;/\u0026gt; \u0026lt;!--这里增加了assignee属性值--\u0026gt; 运行流程 testRunProcess()\n运行后节点会跳到给zhangsan的那个任务，查看数据库表 流程变量 查询任务\n/** * 测试任务查询 */ @Test public void testQueryTask(){ ProcessEngine processEngine=configuration.buildProcessEngine(); TaskService taskService = processEngine.getTaskService(); //通过流程定义查询任务 List\u0026lt;Task\u0026gt; list = taskService.createTaskQuery().processDefinitionKey(\u0026#34;holidayRequest\u0026#34;) .taskAssignee(\u0026#34;zhangsan\u0026#34;) .list(); for (Task task:list){ System.out.println(\u0026#34;任务对应的流程定义id\u0026#34;+task.getProcessDefinitionId()); System.out.println(\u0026#34;任务名\u0026#34;+task.getName()); System.out.println(\u0026#34;任务处理人\u0026#34;+task.getAssignee()); System.out.println(\u0026#34;任务描述\u0026#34;+task.getDescription()); System.out.println(\u0026#34;任务id\u0026#34;+task.getId()); } } 处理任务 # 流程图定义的分析 任务A处理后，根据处理结果（这里是拒绝），会走向任务D，然后任务D是一个Service，且通过java的委托对象，自动实现操作\n到了D那个节点，这里指定了一个自定义的java类处理 代码配置，注意类名和xml中的一致\npackage org.flowable; import org.flowable.engine.delegate.DelegateExecution; import org.flowable.engine.delegate.JavaDelegate; public class SendRejectionMail implements JavaDelegate { /** * 这是一个flowable中的触发器 * * @param delegateExecution */ @Override public void execute(DelegateExecution delegateExecution) { //触发执行的逻辑 按照我们在流程中的定义给被拒绝的员工发送通知邮件 System.out.println(\u0026#34;不好意思，你的请假申请被拒绝了\u0026#34;); } } 任务的完成\n@Test public void testCompleteTask() { ProcessEngine engine = configuration.buildProcessEngine(); TaskService taskService = engine.getTaskService(); //查找出张三在这个流程定义中的任务 Task task = taskService.createTaskQuery().processDefinitionKey(\u0026#34;holidayRequest\u0026#34;) .taskAssignee(\u0026#34;zhangsan\u0026#34;) .singleResult(); //创建流程变量 HashMap\u0026lt;String, Object\u0026gt; map = new HashMap\u0026lt;\u0026gt;(); map.put(\u0026#34;approved\u0026#34;, false); //完成任务 taskService.complete(task.getId(), map); } 控制台 数据库 下面几个表的数据都被清空了 历史任务的完成 # Flowable流程引擎可以自动存储所有流程实例的审计数据或历史数据\n先查看一下刚才用的流程定义的id 历史信息查询\n@Test public void testHistory(){ ProcessEngine processEngine=configuration.buildProcessEngine(); HistoryService historyService=processEngine.getHistoryService(); List\u0026lt;HistoricActivityInstance\u0026gt; list = historyService.createHistoricActivityInstanceQuery() .processDefinitionId(\u0026#34;holidayRequest:1:7503\u0026#34;) .finished() //查询已经完成的 .orderByHistoricActivityInstanceEndTime().asc() //指定排序字段和升降序 .list(); for(HistoricActivityInstance history:list){ //注意,和视频不一样的地方，history表还记录了流程箭头流向的那个节点 //_flow_ System.out.println( \u0026#34;活动名--\u0026#34;+history.getActivityName()+ \u0026#34;处理人--\u0026#34;+history.getAssignee()+ \u0026#34;活动id--\u0026#34;+history.getActivityId()+ \u0026#34;处理时长--\u0026#34;+history.getDurationInMillis()+\u0026#34;毫秒\u0026#34;); } 不一样的地方，在旧版本时没有的 流程设计器 # 有eclipse流程设计器，和flowable流程设计器\n使用eclipse的设计，会生成一个bar文件，代码稍微有点不同 接收一个ZipInputStream\nFlowableUI # 使用flowable官方提供的包，里面有一个war，直接用命令 java -jar xx.war启动即可 这个应用分成四个模块 流程图的绘制及用户分配 "},{"id":211,"href":"/zh/docs/technology/Linux/hanshunping/12-20/","title":"linux_韩老师_12-20","section":"韩顺平老师_","content":" 目录结构 # 目录结构很重要\nwindows下 linux下，从根目录开始分支 /，/root （root用户），/home （创建的用户的目录），/bin（常用的指令），/etc（环境配置）\n在linux世界里，一切皆文件\ncpu被映射成文件\n硬盘 具体的目录结构\n/bin 常用，binary的缩写，存放常用的命令 (/usr/bin、/usr/local/bin) /sbin （/usr/sbin、/usr/local/sbin） SuperUser，存放的是系统管理员使用的系统管理程序\n/home 存放普通用户的主目录\nuseradd jack 之后看该目录 删掉 userdel -r jack 目录消失 /root 该目录为系统管理员，也称超级管理员的用户的主目录\n/lib 系统开机所需要的最基本的动态连接共享库，其作用类似于windows里的DLL，几乎所有的应用程序都需要用到这些共享库\nlost+found 一般为空，非法关机后会存放文件\n/etc 系统管理所需要的配置文件和子目录，比如mysql的my.conf\n/usr 用户的应用程序和文件，类似windows的program files\n/boot 启动Linux时使用的核心文件（破坏则无法启动）\n/proc （不能动） 虚拟目录，系统内存的映射，访问这个目录获取系统信息\n/srv （不能动） service的缩写，存放服务启动之后需要提取的数据\n/sys （不能动） 安装了2.6内核中新出现的文件系统 sysfs\n/tmp 这个目录用来存放一些临时文件\n/dev 类似windows设备管理器，将硬件映射成文件\n/media linux系统会自动识别一些设备，u盘、光驱，将识别的设备映射到该目录下\n/mnt 为了让用户挂载别的文件系统，比如将外部的存储挂载到该目录 /opt 给主机额外安装软件所存放的目录\n/usr/local 给主机额外安装软件所安装的目录，一般通过编译源码方式安装的程序\n/var 日志，不断扩充的东西 /selinux [security-enhanced linux] 安全子系统，控制程序只能访问特定文件 (启用之后才能看到)\n远程登陆 # 背景 linux服务器开发小组共享 正式上线项目运行在公网，所以需要远程开发部署 图解 软件 xshell 和xftp https://www.xshell.com/zh/free-for-home-school/ 使用ifconfig 查看ip 先添加网络工具包 yum install net-tools -y 使用 在客户端打开cmd，并使用ping命令 xshell中配置并进行连接 按住ctrl+鼠标滚轴可以放大字体 远程文件传输 # xtfp6 person安装 新建连接配置 文件夹 可以在这里直接复制上传 图解 解决乱码问题 reboot vim快捷键 # vi ：linux内置vi文本编辑器 vim是vi的增强版本，有丰富的字体颜色\n常用的三种模式\n正常模式，使用上下左右、复制粘贴 插入模式 正常模式\u0026ndash;\u0026gt;插入模式 按下i I o O a A r R（一般用i） 命令行模式 插入模式\u0026ndash;\u0026gt;命令行 输入输入esc表示退出，然后输入: 输入wq表示保存并退出 编辑，重新vim Hello.java 下面，这时候按tab可以自动补全 命令 快捷键使用\n正常模式下\n输入yy，拷贝当前行。p进行粘贴 4yy，拷贝当前行(包括)往下4行\n输入dd，删除当前行 4dd，删除当前行（包括）往下4行\n定位到首行(gg)或者末行G\n使用u，撤回刚才的输入(lalala将被撤回) 定位到20行 （20+shift+g）【其实是20+G】\n命令模式 ：切换到命令行)\n命令行模式下（：下），输入 /搜索内容\n或者（/）下，直接输入搜索内容\n再次输入 / ，就会清空前面的搜索\n设置文件行号（：下） set nu 设置；set nonu 取消 如果修改太多，需要先拷贝到windows下，然后再传上来\nvim/vi 快捷键 关机重启 # 命令 halt 停止\nshutdown -h now #立刻关机 shutdown -h 1 #给出提示并关机 shutdown -r now #现在重启计算机 halt #立刻关机(虚拟机好像只是把cpu关闭？) reboot #立刻重启 sync #将内存的数据同步到磁盘 sync #将内存的数据同步到磁盘 shutdown/reboot/halt等命令都会在执行前执行sync 登录注销 # 尽量不要用root账号登录\n普通用户登陆后，用su - 用户名 切换成系统管理员身份 logout 注销用户（图形页面没效果） 在运行级别3下有效 "},{"id":212,"href":"/zh/docs/technology/Linux/hanshunping/07-11/","title":"linux_韩老师_07-11","section":"韩顺平老师_","content":" 网络连接 # 网络连接的三种模式 同一个教室的三个主机 此时三个同学可以正常通讯 桥接模式 这是张三的虚拟机和外部互通；但是如果这样设置，ip会不够用； NAT模式 如图，虚拟机可以跟虚拟的网卡(192.168.100.99)互通，且通过这个虚拟网卡，及（192.168.0.50代理），与外界(192.168.0.X)互通 NAT模式，网络地址转换模式，虚拟系统和外部系统通讯，不造成IP冲突 注意，这里外部其他主机(除0.50和100.99)是访问不到100.88的 主机模式：独立的系统 虚拟机克隆 # 方式1，直接拷贝整个文件夹 方式2，使用VMWare 克隆前先把克隆目标关闭 克隆虚拟机当前状态\u0026ndash;创建完整克隆 虚拟机快照 # 为什么需要虚拟机快照 快照a 之后创建了文件夹hello 然后拍摄快照b 之后创建了文件夹hello2 然后拍摄快照c\n目前 回到快照A 之后会重启，效果（两个文件夹都没有了)\n如果恢复到B，然后再创建一个快照，就会变成 虚拟机迁移 # 直接剪切、删除，即可 vmtools工具 # 如下步骤，注意，这里只是在有界面的情况下进行安装 安装完毕后 在vm上面设置 共享文件夹在linux中的路径 /mnt/hgfs/myshare "},{"id":213,"href":"/zh/docs/technology/Flowable/offical/05/","title":"Flowable-05-spring-boot","section":"官方文档","content":" 入门 # 需要两个依赖\n\u0026lt;properties\u0026gt; \u0026lt;flowable.version\u0026gt;6.7.2\u0026lt;/flowable.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.flowable\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flowable-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${flowable.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- https://mvnrepository.com/artifact/com.h2database/h2 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.h2database\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;h2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.1.212\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 结合Spring：\n只需将依赖项添加到类路径并使用*@SpringBootApplication*注释，幕后就会发生很多事情：\n自动创建内存数据源（因为 H2 驱动程序位于类路径中）并传递给 Flowable 流程引擎配置\n已创建并公开了 Flowable ProcessEngine、CmmnEngine、DmnEngine、FormEngine、ContentEngine 和 IdmEngine bean\n所有 Flowable 服务都暴露为 Spring bean\nSpring Job Executor 已创建\n将自动部署流程文件夹中的任何 BPMN 2.0 流程定义。创建一个文件夹processes并将一个虚拟进程定义（名为one-task-process.bpmn20.xml）添加到此文件夹。该文件的内容如下所示。\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; targetNamespace=\u0026#34;Examples\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;oneTaskProcess\u0026#34; name=\u0026#34;The One Task Process\u0026#34;\u0026gt; \u0026lt;startEvent id=\u0026#34;theStart\u0026#34; /\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;flow1\u0026#34; sourceRef=\u0026#34;theStart\u0026#34; targetRef=\u0026#34;theTask\u0026#34; /\u0026gt; \u0026lt;userTask id=\u0026#34;theTask\u0026#34; name=\u0026#34;my task\u0026#34; flowable:assignee=\u0026#34;kermit\u0026#34; /\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;flow2\u0026#34; sourceRef=\u0026#34;theTask\u0026#34; targetRef=\u0026#34;theEnd\u0026#34; /\u0026gt; \u0026lt;endEvent id=\u0026#34;theEnd\u0026#34; /\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;/definitions\u0026gt; 案例文件夹中的任何 CMMN 1.1 案例定义都将自动部署。\n将自动部署dmn文件夹中的任何 DMN 1.1 dmn 定义。\n表单文件夹中的任何表单定义都将自动部署。\njava代码 在项目服务启动的时候就去加载一些数据\n@SpringBootApplication(proxyBeanMethods = false) public class MyApplication { public static void main(String[] args) { SpringApplication.run(MyApplication.class, args); } @Bean public CommandLineRunner init(final RepositoryService repositoryService, final RuntimeService runtimeService, final TaskService taskService) { //该bean在项目服务启动的时候就去加载一些数据 return new CommandLineRunner() { @Override public void run(String... strings) throws Exception { //有几个流程定义 System.out.println(\u0026#34;Number of process definitions : \u0026#34; + repositoryService.createProcessDefinitionQuery().count()); //有多少个任务 System.out.println(\u0026#34;Number of tasks : \u0026#34; + taskService.createTaskQuery().count()); runtimeService.startProcessInstanceByKey(\u0026#34;oneTaskProcess\u0026#34;); //开启流程后有多少个任务（+1） System.out.println(\u0026#34;Number of tasks after process start: \u0026#34; + taskService.createTaskQuery().count()); } }; } } 更改数据库 # 添加依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.29\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; application.yml中添加配置\nspring: datasource: url: jdbc:mysql://localhost:3306/flowable-spring-boot?useUnicode=true\u0026amp;characterEncoding=utf-8\u0026amp;allowMultiQueries=true\u0026amp;nullCatalogMeansCurrent=true username: root password: 123456 driver-class-name: com.mysql.jdbc.Driver Rest支持 # web支持\n\u0026lt;parent\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.6.7\u0026lt;/version\u0026gt; \u0026lt;/parent\u0026gt; 添加依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 使用Service启动流程及获取给定受让人的任务\n@Service public class MyService { @Autowired private RuntimeService runtimeService; @Autowired private TaskService taskService; @Transactional public void startProcess() { runtimeService.startProcessInstanceByKey(\u0026#34;oneTaskProcess\u0026#34;); } @Transactional public List\u0026lt;Task\u0026gt; getTasks(String assignee) { return taskService.createTaskQuery().taskAssignee(assignee).list(); } } 创建REST端点\n@RestController public class MyRestController { @Autowired private MyService myService; @PostMapping(value=\u0026#34;/process\u0026#34;) public void startProcessInstance() { myService.startProcess(); } @RequestMapping(value=\u0026#34;/tasks\u0026#34;, method= RequestMethod.GET, produces=MediaType.APPLICATION_JSON_VALUE) public List\u0026lt;TaskRepresentation\u0026gt; getTasks(@RequestParam String assignee) { List\u0026lt;Task\u0026gt; tasks = myService.getTasks(assignee); List\u0026lt;TaskRepresentation\u0026gt; dtos = new ArrayList\u0026lt;TaskRepresentation\u0026gt;(); for (Task task : tasks) { dtos.add(new TaskRepresentation(task.getId(), task.getName())); } return dtos; } static class TaskRepresentation { private String id; private String name; public TaskRepresentation(String id, String name) { this.id = id; this.name = name; } public String getId() { return id; } public void setId(String id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } } } 使用下面语句进行测试\ncurl http://localhost:8080/tasks?assignee=kermit [] curl -X POST http://localhost:8080/process curl http://localhost:8080/tasks?assignee=kermit [{\u0026#34;id\u0026#34;:\u0026#34;10004\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;my task\u0026#34;}] JPA支持 # 添加依赖\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-data-jpa\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 创建一个实体类\n@Entity class Person { @Id @GeneratedValue private Long id; private String username; private String firstName; private String lastName; private Date birthDate; public Person() { } public Person(String username, String firstName, String lastName, Date birthDate) { this.username = username; this.firstName = firstName; this.lastName = lastName; this.birthDate = birthDate; } public Long getId() { return id; } public void setId(Long id) { this.id = id; } public String getUsername() { return username; } public void setUsername(String username) { this.username = username; } public String getFirstName() { return firstName; } public void setFirstName(String firstName) { this.firstName = firstName; } public String getLastName() { return lastName; } public void setLastName(String lastName) { this.lastName = lastName; } public Date getBirthDate() { return birthDate; } public void setBirthDate(Date birthDate) { this.birthDate = birthDate; } } 属性文件添加\nspring.jpa.hibernate.ddl-auto=update 添加Repository类\n@Repository public interface PersonRepository extends JpaRepository\u0026lt;Person, Long\u0026gt; { Person findByUsername(String username); } 代码\n添加事务\nstartProcess现在修改成：获取传入的受理人用户名，查找Person，并将PersonJPA对象作为流程变量放入流程实例中\n在CommandLineRunner中初始化时创建用户\n@Service @Transactional public class MyService { @Autowired private RuntimeService runtimeService; @Autowired private TaskService taskService; @Autowired private PersonRepository personRepository; public void startProcess(String assignee) { Person person = personRepository.findByUsername(assignee); Map\u0026lt;String, Object\u0026gt; variables = new HashMap\u0026lt;String, Object\u0026gt;(); variables.put(\u0026#34;person\u0026#34;, person); runtimeService.startProcessInstanceByKey(\u0026#34;oneTaskProcess\u0026#34;, variables); } public List\u0026lt;Task\u0026gt; getTasks(String assignee) { return taskService.createTaskQuery().taskAssignee(assignee).list(); } public void createDemoUsers() { if (personRepository.findAll().size() == 0) { personRepository.save(new Person(\u0026#34;jbarrez\u0026#34;, \u0026#34;Joram\u0026#34;, \u0026#34;Barrez\u0026#34;, new Date())); personRepository.save(new Person(\u0026#34;trademakers\u0026#34;, \u0026#34;Tijs\u0026#34;, \u0026#34;Rademakers\u0026#34;, new Date())); } } } CommandRunner修改\n@Bean public CommandLineRunner init(final MyService myService) { return new CommandLineRunner() { public void run(String... strings) throws Exception { myService.createDemoUsers(); } }; } RestController修改\n@RestController public class MyRestController { @Autowired private MyService myService; @PostMapping(value=\u0026#34;/process\u0026#34;) public void startProcessInstance(@RequestBody StartProcessRepresentation startProcessRepresentation) { myService.startProcess(startProcessRepresentation.getAssignee()); } ... static class StartProcessRepresentation { private String assignee; public String getAssignee() { return assignee; } public void setAssignee(String assignee) { this.assignee = assignee; } } 修改流程定义\n\u0026lt;userTask id=\u0026#34;theTask\u0026#34; name=\u0026#34;my task\u0026#34; flowable:assignee=\u0026#34;${person.id}\u0026#34;/\u0026gt; 测试\n启动spring boot之后person表会有两条数据\n启动流程实例\n此时会把从数据库查找到的person传入流程图(变量)\ncurl -H \u0026#34;Content-Type: application/json\u0026#34; -d \u0026#39;{\u0026#34;assignee\u0026#34; : \u0026#34;jbarrez\u0026#34;}\u0026#39; http://localhost:8080/process 使用id获取任务列表\ncurl http://localhost:8080/tasks?assignee=1 [{\u0026#34;id\u0026#34;:\u0026#34;12505\u0026#34;,\u0026#34;name\u0026#34;:\u0026#34;my task\u0026#34;}] 可流动的执行器端点 # "},{"id":214,"href":"/zh/docs/technology/Flowable/offical/04/","title":"Flowable-04-spring","section":"官方文档","content":" ProcessEngineFactoryBean # 将ProcessEngine配置为常规的SpringBean\n\u0026lt;bean id=\u0026#34;processEngineConfiguration\u0026#34; class=\u0026#34;org.flowable.spring.SpringProcessEngineConfiguration\u0026#34;\u0026gt; ... \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;processEngine\u0026#34; class=\u0026#34;org.flowable.spring.ProcessEngineFactoryBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;processEngineConfiguration\u0026#34; ref=\u0026#34;processEngineConfiguration\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 使用transaction\n\u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:context=\u0026#34;http://www.springframework.org/schema/context\u0026#34; xmlns:tx=\u0026#34;http://www.springframework.org/schema/tx\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-2.5.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx-3.0.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;dataSource\u0026#34; class=\u0026#34;org.springframework.jdbc.datasource.SimpleDriverDataSource\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;driverClass\u0026#34; value=\u0026#34;org.h2.Driver\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;url\u0026#34; value=\u0026#34;jdbc:h2:mem:flowable;DB_CLOSE_DELAY=1000\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;username\u0026#34; value=\u0026#34;sa\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;password\u0026#34; value=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;transactionManager\u0026#34; class=\u0026#34;org.springframework.jdbc.datasource.DataSourceTransactionManager\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;processEngineConfiguration\u0026#34; class=\u0026#34;org.flowable.spring.SpringProcessEngineConfiguration\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;dataSource\u0026#34; ref=\u0026#34;dataSource\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;transactionManager\u0026#34; ref=\u0026#34;transactionManager\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;databaseSchemaUpdate\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;asyncExecutorActivate\u0026#34; value=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;processEngine\u0026#34; class=\u0026#34;org.flowable.spring.ProcessEngineFactoryBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;processEngineConfiguration\u0026#34; ref=\u0026#34;processEngineConfiguration\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;repositoryService\u0026#34; factory-bean=\u0026#34;processEngine\u0026#34; factory-method=\u0026#34;getRepositoryService\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;runtimeService\u0026#34; factory-bean=\u0026#34;processEngine\u0026#34; factory-method=\u0026#34;getRuntimeService\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;taskService\u0026#34; factory-bean=\u0026#34;processEngine\u0026#34; factory-method=\u0026#34;getTaskService\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;historyService\u0026#34; factory-bean=\u0026#34;processEngine\u0026#34; factory-method=\u0026#34;getHistoryService\u0026#34; /\u0026gt; \u0026lt;bean id=\u0026#34;managementService\u0026#34; factory-bean=\u0026#34;processEngine\u0026#34; factory-method=\u0026#34;getManagementService\u0026#34; /\u0026gt; ... 还包括了其他的一些bean\n\u0026lt;beans\u0026gt; ... \u0026lt;tx:annotation-driven transaction-manager=\u0026#34;transactionManager\u0026#34;/\u0026gt; \u0026lt;bean id=\u0026#34;userBean\u0026#34; class=\u0026#34;org.flowable.spring.test.UserBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;runtimeService\u0026#34; ref=\u0026#34;runtimeService\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;printer\u0026#34; class=\u0026#34;org.flowable.spring.test.Printer\u0026#34; /\u0026gt; \u0026lt;/beans\u0026gt; 使用\n使用XML资源方式类配置Spring应用程序上下文\nClassPathXmlApplicationContext applicationContext = new ClassPathXmlApplicationContext( \u0026#34;org/flowable/examples/spring/SpringTransactionIntegrationTest-context.xml\u0026#34;); 或者添加注解\n@ContextConfiguration( \u0026#34;classpath:org/flowable/spring/test/transaction/SpringTransactionIntegrationTest-context.xml\u0026#34;) 获取服务bean并进行部署流程\nRepositoryService repositoryService = (RepositoryService) applicationContext.getBean(\u0026#34;repositoryService\u0026#34;); String deploymentId = repositoryService .createDeployment() .addClasspathResource(\u0026#34;org/flowable/spring/test/hello.bpmn20.xml\u0026#34;) .deploy() .getId(); 下面看userBean类，使用了Transaction事务\npublic class UserBean { /** injected by Spring */ private RuntimeService runtimeService; @Transactional public void hello() { // here you can do transactional stuff in your domain model // and it will be combined in the same transaction as // the startProcessInstanceByKey to the Flowable RuntimeService runtimeService.startProcessInstanceByKey(\u0026#34;helloProcess\u0026#34;); } public void setRuntimeService(RuntimeService runtimeService) { this.runtimeService = runtimeService; } } 使用userBean\nUserBean userBean = (UserBean) applicationContext.getBean(\u0026#34;userBean\u0026#34;); userBean.hello(); 表达式 # BPMN 流程中的所有表达式也将默认“看到”所有 Spring bean\n要完全不暴露任何 bean，只需将一个空列表作为 SpringProcessEngineConfiguration 上的“beans”属性传递。当没有设置 \u0026lsquo;beans\u0026rsquo; 属性时，上下文中的所有 Spring beans 都将可用\n如下，可以设置暴露的bean\n\u0026lt;bean id=\u0026#34;processEngineConfiguration\u0026#34; class=\u0026#34;org.flowable.spring.SpringProcessEngineConfiguration\u0026#34;\u0026gt; ... \u0026lt;property name=\u0026#34;beans\u0026#34;\u0026gt; \u0026lt;map\u0026gt; \u0026lt;entry key=\u0026#34;printer\u0026#34; value-ref=\u0026#34;printer\u0026#34; /\u0026gt; \u0026lt;/map\u0026gt; \u0026lt;/property\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;printer\u0026#34; class=\u0026#34;org.flowable.examples.spring.Printer\u0026#34; /\u0026gt; 现在的bean进行公开了，在.bpmn20.xml中可以使用\n\u0026lt;definitions id=\u0026#34;definitions\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;helloProcess\u0026#34;\u0026gt; \u0026lt;startEvent id=\u0026#34;start\u0026#34; /\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;flow1\u0026#34; sourceRef=\u0026#34;start\u0026#34; targetRef=\u0026#34;print\u0026#34; /\u0026gt; \u0026lt;serviceTask id=\u0026#34;print\u0026#34; flowable:expression=\u0026#34;#{printer.printMessage()}\u0026#34; /\u0026gt; \u0026lt;sequenceFlow id=\u0026#34;flow2\u0026#34; sourceRef=\u0026#34;print\u0026#34; targetRef=\u0026#34;end\u0026#34; /\u0026gt; \u0026lt;endEvent id=\u0026#34;end\u0026#34; /\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;/definitions\u0026gt; Print类\npublic class Printer { public void printMessage() { System.out.println(\u0026#34;hello world\u0026#34;); } } spring配置bean\n\u0026lt;beans\u0026gt; ... \u0026lt;bean id=\u0026#34;printer\u0026#34; class=\u0026#34;org.flowable.examples.spring.Printer\u0026#34; /\u0026gt; \u0026lt;/beans\u0026gt; 自动资源部署 # \u0026lt;bean id=\u0026#34;processEngineConfiguration\u0026#34; class=\u0026#34;org.flowable.spring.SpringProcessEngineConfiguration\u0026#34;\u0026gt; ... \u0026lt;property name=\u0026#34;deploymentResources\u0026#34; value=\u0026#34;classpath*:/org/flowable/spring/test/autodeployment/autodeploy.*.bpmn20.xml\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;bean id=\u0026#34;processEngine\u0026#34; class=\u0026#34;org.flowable.spring.ProcessEngineFactoryBean\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;processEngineConfiguration\u0026#34; ref=\u0026#34;processEngineConfiguration\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; 单元测试 # @ExtendWith(FlowableSpringExtension.class) @ExtendWith(SpringExtension.class) @ContextConfiguration(classes = SpringJunitJupiterTest.TestConfiguration.class) public class MyBusinessProcessTest { @Autowired private RuntimeService runtimeService; @Autowired private TaskService taskService; @Test @Deployment void simpleProcessTest() { runtimeService.startProcessInstanceByKey(\u0026#34;simpleProcess\u0026#34;); Task task = taskService.createTaskQuery().singleResult(); assertEquals(\u0026#34;My Task\u0026#34;, task.getName()); taskService.complete(task.getId()); assertEquals(0, runtimeService.createProcessInstanceQuery().count()); } } "},{"id":215,"href":"/zh/docs/technology/Flowable/offical/03/","title":"Flowable-03-api","section":"官方文档","content":" 流程引擎API和服务 # 引擎API是与Flowable交互的常见方式，主要起点是ProcessEngine，可以通过配置（Configuration章节）中描述的多种方式创建。\n从ProcessEngine获取包含工作流/BPM方法的各种服务。ProcessEngine和服务对象是线程安全的\n下面是通过processEngine获取各种服务的方法\nProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); RuntimeService runtimeService = processEngine.getRuntimeService(); RepositoryService repositoryService = processEngine.getRepositoryService(); TaskService taskService = processEngine.getTaskService(); ManagementService managementService = processEngine.getManagementService(); IdentityService identityService = processEngine.getIdentityService(); HistoryService historyService = processEngine.getHistoryService(); FormService formService = processEngine.getFormService(); DynamicBpmnService dynamicBpmnService = processEngine.getDynamicBpmnService(); ProcessEngines.getDefaultProcessEngine()在第一次调用时初始化并构建流程引擎，然后返回相同的流程引擎\nProcessEngines类将扫描所有flowable.cfg.xml和flowable-context.xml文件。\n对于所有 flowable.cfg.xml 文件，流程引擎将以典型的 Flowable 方式构建：ProcessEngineConfiguration.createProcessEngineConfigurationFromInputStream(inputStream).buildProcessEngine()。\n对于所有 flowable-context.xml 文件，流程引擎将以 Spring 方式构建：首先创建 Spring 应用程序上下文，然后从该应用程序上下文中获取流程引擎。\nThe RepositoryService is probably the first service needed when working with the Flowable engine.\n该服务**(RepositoryService)提供用于管理和操作部署deployments**和流程定义的操作\n查询引擎已知的部署和流程定义 暂停和激活作为一个整体或特定流程定义的部署。挂起意味着不能对它们执行进一步的操作，而激活则相反并再次启用操作 检索各种资源，例如引擎自动生成的部署或流程图中包含的文件 检索流程定义的 POJO 版本，该版本可用于使用 Java 而不是 XML 来内省流程 RepositoryService主要是关于静态信息（不会改变的数据，或者至少不会改变太多），而RuntimeService处理启动流程定义的新流程实例\n流程定义定义了流程中不同步骤的结构和行为，流程实例是此类流程定义的一次执行\n对于每个流程定义，通常有许多实例同时运行\nRuntime也用于检索和存储流程变量\nRuntimeservice还可以用来查询流程实例和执行(executions)\nExecutions are a representation of the \u0026rsquo;token\u0026rsquo; concept of BPMN 2.0. 执行是指向流程实例当前所在位置的指针\n只要流程实例正在等待外部触发器并且流程需要继续，就会使用 RuntimeService\n流程实例可以有各种等待状态，并且该服务包含各种操作以向实例发出“信号”，即接收到外部触发器并且流程实例可以继续\n需要由系统的人类用户执行的任务是BPM引擎（如Floable）的核心，围绕任务的所有内容都在TaskService中进行分组\n查询分配给用户或组的任务 创建新的独立任务（与流程实例无关） 任务被分配给哪个用户或哪些用户，以及让这些用户以某种方式参与该任务 要求并完成一项任务，声明意味着某人决定成为该任务的受让人assignee IdentityService支持组和用户的管理（创建、更新、删除、查询）\nFormService是可选服务，引入了启动表单（start form）和任务表单(a task form)的概念\nHistoryService公开了 Flowable 引擎收集的所有历史数据。在执行流程时，引擎可以保留很多数据（这是可配置的），例如流程实例的启动时间，谁做了哪些任务，完成任务花了多长时间，每个流程实例中遵循的路径，等等。\n使用Flowable 编写自定义应用程序时，通常不需要**ManagementService 。**它允许检索有关数据库表和表元数据的信息。此外，它还公开了作业的查询功能和管理操作\nDynamicBpmnService可用于更改流程定义的一部分，而无需重新部署它。例如，您可以更改流程定义中用户任务的受理人定义，或更改服务任务的类名。\n异常策略 # Flowable 中的基本异常是 org.flowable.engine.FlowableException\nFlowable的一些异常子类\nFlowableWrongDbException：当 Flowable 引擎发现数据库架构版本和引擎版本不匹配时抛出。 FlowableOptimisticLockingException：当并发访问同一数据条目导致数据存储发生乐观锁定时抛出。 FlowableClassLoadingException：当请求加载的类未找到或加载时发生错误时抛出（例如 JavaDelegates、TaskListeners \u0026hellip;\u0026hellip;）。 FlowableObjectNotFoundException：当请求或操作的对象不存在时抛出。 FlowableIllegalArgumentException：异常表明在 Flowable API 调用中提供了非法参数，在引擎配置中配置了非法值，或者提供了非法值，或者在流程定义中使用了非法值。 FlowableTaskAlreadyClaimedException：当任务已被声明时抛出，当 taskService.claim(\u0026hellip;) 被调用时 查询接口 # 引擎查询数据有两种方式：the query API and native queries\nqueryAPi允许使用fluent API编写完全类型安全的查询，例如\nList\u0026lt;Task\u0026gt; tasks = taskService.createTaskQuery() .taskAssignee(\u0026#34;kermit\u0026#34;) .processVariableValueEquals(\u0026#34;orderId\u0026#34;, \u0026#34;0815\u0026#34;) .orderByDueDate().asc() .list(); native queries （返回类型由您使用的查询对象定义，数据映射到正确的对象[比如任务、流程实例、执行等，且您必须使用在数据库中定义的表明和列名]）。如下，可以通过api检索表名等，使依赖关系尽可能小\nList\u0026lt;Task\u0026gt; tasks = taskService.createNativeTaskQuery() .sql(\u0026#34;SELECT count(*) FROM \u0026#34; + managementService.getTableName(Task.class) + \u0026#34; T WHERE T.NAME_ = #{taskName}\u0026#34;) .parameter(\u0026#34;taskName\u0026#34;, \u0026#34;gonzoTask\u0026#34;) .list(); long count = taskService.createNativeTaskQuery() .sql(\u0026#34;SELECT count(*) FROM \u0026#34; + managementService.getTableName(Task.class) + \u0026#34; T1, \u0026#34; + managementService.getTableName(VariableInstanceEntity.class) + \u0026#34; V1 WHERE V1.TASK_ID_ = T1.ID_\u0026#34;) .count(); 变量 # 每个流程实例都需要并使用数据来执行其组成的步骤。在 Flowable 中，这些数据称为变量，存储在数据库中\n流程实例可以有变量（称为流程变量），也可以有执行（指向流程处于活动状态的特定指针）。用户任务也可以有变量，变量存储在ACT_RU_VARIABLE数据库表中\n所有startProcessInstanceXXX方法都有一个可选参数，用于在创建和启动流程实例时提供变量\nProcessInstance startProcessInstanceByKey(String processDefinitionKey, Map\u0026lt;String, Object\u0026gt; variables); 可以在流程执行期间添加变量。例如，（RuntimeService）\nvoid setVariable(String executionId, String variableName, Object value); void setVariableLocal(String executionId, String variableName, Object value); void setVariables(String executionId, Map\u0026lt;String, ? extends Object\u0026gt; variables); void setVariablesLocal(String executionId, Map\u0026lt;String, ? extends Object\u0026gt; variables); 检索变量 TaskService上存在类似的方法。这意味着任务（如执行）可以具有仅在任务期间“活动”的局部变量\nMap\u0026lt;String, Object\u0026gt; getVariables(String executionId); Map\u0026lt;String, Object\u0026gt; getVariablesLocal(String executionId); Map\u0026lt;String, Object\u0026gt; getVariables(String executionId, Collection\u0026lt;String\u0026gt; variableNames); Map\u0026lt;String, Object\u0026gt; getVariablesLocal(String executionId, Collection\u0026lt;String\u0026gt; variableNames); Object getVariable(String executionId, String variableName); \u0026lt;T\u0026gt; T getVariable(String executionId, String variableName, Class\u0026lt;T\u0026gt; variableClass); 当前执行或任务对象是可用的，它可以用于变量设置和/或检索\nexecution.getVariables(); execution.getVariables(Collection\u0026lt;String\u0026gt; variableNames); execution.getVariable(String variableName); execution.setVariables(Map\u0026lt;String, object\u0026gt; variables); execution.setVariable(String variableName, Object value); 在执行上述任何调用时，所有变量都会在后台从数据库中获取。这意味着，如果您有 10 个变量，但只能通过*getVariable(\u0026ldquo;myVariable\u0026rdquo;)*获得一个，那么在幕后将获取并缓存其他 9 个\n接上述，可以设置是否缓存所有变量\nMap\u0026lt;String, Object\u0026gt; getVariables(Collection\u0026lt;String\u0026gt; variableNames, boolean fetchAllVariables); Object getVariable(String variableName, boolean fetchAllVariables); void setVariable(String variableName, Object value, boolean fetchAllVariables); 瞬态变量 # 瞬态变量是行为类似于常规变量但不持久的变量。通常，瞬态变量用于高级用例\n对于瞬态变量，根本没有存储历史记录。 与常规变量一样，瞬态变量在设置时放在最高父级。这意味着在执行时设置变量时，瞬态变量实际上存储在流程实例执行中。与常规变量一样，如果在特定执行或任务上设置变量，则存在方法的局部变体。 只能在流程定义中的下一个“等待状态”之前访问瞬态变量。在那之后，他们就走了。在这里，等待状态是指流程实例中它被持久化到数据存储中的点。请注意，在此定义中，异步活动也是“等待状态”！ 瞬态变量只能由setTransientVariable(name, value)设置，但调用getVariable(name)时也会返回瞬态变量（也存在一个getTransientVariable(name)，它只检查瞬态变量）。这样做的原因是使表达式的编写变得容易，并且使用变量的现有逻辑适用于这两种类型。 瞬态变量会隐藏同名的持久变量。这意味着当在流程实例上同时设置持久变量和瞬态变量并*调用 getVariable(\u0026ldquo;someVariable\u0026rdquo;)*时，将返回瞬态变量值。 可以在大多数地方设置和获取瞬态变量\n关于JavaDelegate实现中的DelegateExecution\n关于ExecutionListener实现中的DelegateExecution和关于TaskListener实现的DelegateTask\n通过执行对象在脚本任务中\n通过运行时服务启动流程实例时\n完成任务时\n调用runtimeService.trigger方法时\n方法\nvoid setTransientVariable(String variableName, Object variableValue); void setTransientVariableLocal(String variableName, Object variableValue); void setTransientVariables(Map\u0026lt;String, Object\u0026gt; transientVariables); void setTransientVariablesLocal(Map\u0026lt;String, Object\u0026gt; transientVariables); Object getTransientVariable(String variableName); Object getTransientVariableLocal(String variableName); Map\u0026lt;String, Object\u0026gt; getTransientVariables(); Map\u0026lt;String, Object\u0026gt; getTransientVariablesLocal(); void removeTransientVariable(String variableName); void removeTransientVariableLocal(String variableName); 典型示例 瞬态变量传递\nProcessInstance processInstance = runtimeService.createProcessInstanceBuilder() .processDefinitionKey(\u0026#34;someKey\u0026#34;) .transientVariable(\u0026#34;configParam01\u0026#34;, \u0026#34;A\u0026#34;) .transientVariable(\u0026#34;configParam02\u0026#34;, \u0026#34;B\u0026#34;) .transientVariable(\u0026#34;configParam03\u0026#34;, \u0026#34;C\u0026#34;) .start(); 获取数据\npublic static class FetchDataServiceTask implements JavaDelegate { public void execute(DelegateExecution execution) { String configParam01 = (String) execution.getVariable(configParam01); // ... RestResponse restResponse = executeRestCall(); execution.setTransientVariable(\u0026#34;response\u0026#34;, restResponse.getBody()); execution.setTransientVariable(\u0026#34;status\u0026#34;, restResponse.getStatus()); } } 离开独占网关的序列流的条件不知道使用的是持久变量还是瞬态变量（在本例中为状态瞬态变量）：\n\u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt;${status == 200}\u0026lt;/conditionExpression\u0026gt; 表达式 # Flowable使用UEL进行表达式解析，UEL代表统一表达式语言，是EE6规范的一部分。两种类型的表达式（值表达式和方法表达式），都可以在需要表达式的地方使用\n值表达式，解析为一个值\n${myVar} ${myBean.myProperty} 方法表达式：调用带或不带参数的方法\n${printer.print()} ${myBean.addNewOrder(\u0026#39;orderName\u0026#39;)} ${myBean.doSomething(myVar, execution)} 表达式函数 # 一些开箱即用的函数\nvariables:get(varName)：检索变量的值。与直接在表达式中写变量名的主要区别在于，当变量不存在时，使用这个函数不会抛出异常。例如，如果myVariable不存在，*${myVariable == \u0026ldquo;hello\u0026rdquo;}会抛出异常，但${var:get(myVariable) == \u0026lsquo;hello\u0026rsquo;}*会正常工作。 variables:getOrDefault(varName, defaultValue)：类似于get，但可以选择提供默认值，当变量未设置或值为null时返回。 variables:exists(varName) ：如果变量具有非空值，则返回true 。 variables:isEmpty(varName) (alias :empty ) : 检查变量值是否不为空。根据变量类型，行为如下： 对于字符串变量，如果变量是空字符串，则认为该变量为空。 对于 java.util.Collection 变量，如果集合没有元素，则返回true 。 对于 ArrayNode 变量，如果没有元素则返回true 如果变量为null，则始终返回true variables:isNotEmpty(varName) (alias : notEmpty) : isEmpty的逆运算。 variables:equals(varName, value)（别名*:eq*）：检查变量是否等于给定值。这是表达式的简写函数，否则将被写为*${execution.getVariable(\u0026ldquo;varName\u0026rdquo;) != null \u0026amp;\u0026amp; execution.getVariable(\u0026ldquo;varName\u0026rdquo;) == value}*。 如果变量值为 null，则返回 false（除非与 null 比较）。 variables:notEquals(varName, value)（别名*:ne ）：* equals的反向比较。 variables:contains(varName, value1, value2, \u0026hellip;)：检查提供的所有值是否包含在变量中。根据变量类型，行为如下： 对于字符串变量，传递的值用作需要成为变量一部分的子字符串 对于 java.util.Collection 变量，所有传递的值都需要是集合的一个元素（正则包含语义）。 对于 ArrayNode 变量：支持检查 arraynode 是否包含作为变量类型支持的类型的 JsonNode 当变量值为 null 时，在所有情况下都返回 false。当变量值不为null，且实例类型不是上述类型之一时，会返回false。 variables:containsAny(varName, value1, value2, \u0026hellip;)：类似于contains函数，但如果任何（而非全部）传递的值包含在变量中，则将返回true 。 variables:base64(varName)：将二进制或字符串变量转换为 Base64 字符串 比较器功能： variables:lowerThan(varName, value) (别名*:lessThan或:lt* ) : ${execution.getVariable(\u0026ldquo;varName\u0026rdquo;) != null \u0026amp;\u0026amp; execution.getVariable(\u0026ldquo;varName\u0026rdquo;) \u0026lt; value}的简写 变量：lowerThanOrEquals(varName, value)（别名*:lessThanOrEquals或:lte*）：类似，但现在用于*\u0026lt; =* variables:greaterThan(varName, value) (alias :gt ) : 类似，但现在用于*\u0026gt;* variables:greaterThanOrEquals(varName, value) (alias :gte ) : 类似，但现在用于*\u0026gt; =* 单元测试 # 使用自定义资源进行单元测试\n@FlowableTest public class MyBusinessProcessTest { private ProcessEngine processEngine; private RuntimeService runtimeService; private TaskService taskService; @BeforeEach void setUp(ProcessEngine processEngine) { this.processEngine = processEngine; this.runtimeService = processEngine.getRuntimeService(); this.taskService = processEngine.getTaskService(); } @Test @Deployment(resources = \u0026#34;holiday-request.bpmn20.xml\u0026#34;) void testSimpleProcess() { HashMap\u0026lt;String, Object\u0026gt; employeeInfo = new HashMap\u0026lt;\u0026gt;(); employeeInfo.put(\u0026#34;employee\u0026#34;, \u0026#34;wangwu1028930\u0026#34;); //employeeInfo.put() runtimeService.startProcessInstanceByKey( \u0026#34;holidayRequest\u0026#34;, employeeInfo ); Task task = taskService.createTaskQuery().singleResult(); assertEquals(\u0026#34;Approve or reject request\u0026#34;, task.getName()); HashMap\u0026lt;String, Object\u0026gt; hashMap = new HashMap\u0026lt;\u0026gt;(); hashMap.put(\u0026#34;approved\u0026#34;, true); taskService.complete(task.getId(), hashMap); assertEquals(1, runtimeService .createProcessInstanceQuery().count()); } } 调试单元测试 # Web应用程序中的流程引擎 # 编写一个简单的ServletContextListener来初始化和销毁普通Servlet环境中的流程引擎\npublic class ProcessEnginesServletContextListener implements ServletContextListener { public void contextInitialized(ServletContextEvent servletContextEvent) { ProcessEngines.init(); } public void contextDestroyed(ServletContextEvent servletContextEvent) { ProcessEngines.destroy(); } } 其中，ProcessEngines.init()将在类路径中查找flowable.cfg.xml资源文件，并为给定的配置创建一个ProcessEngine，使用下面两种方式来获取他\nProcessEngines.getDefaultProcessEngine() //或者下面的方式 ProcessEngines.getProcessEngine(\u0026#34;myName\u0026#34;); "},{"id":216,"href":"/zh/docs/technology/Flowable/offical/02/","title":"Flowable-02-Configuration","section":"官方文档","content":" 创建流程引擎 # Flowable 流程引擎通过一个名为 flowable.cfg.xml 的 XML 文件进行配置\n现在类路径下放置floable.cfg.xml文件\n\u0026lt;beans xmlns=\u0026#34;http://www.springframework.org/schema/beans\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\u0026#34;\u0026gt; \u0026lt;bean id=\u0026#34;processEngineConfiguration\u0026#34; class=\u0026#34;org.flowable.engine.impl.cfg.StandaloneProcessEngineConfiguration\u0026#34;\u0026gt; \u0026lt;property name=\u0026#34;jdbcUrl\u0026#34; value=\u0026#34;jdbc:h2:mem:flowable;DB_CLOSE_DELAY=1000\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;jdbcDriver\u0026#34; value=\u0026#34;org.h2.Driver\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;jdbcUsername\u0026#34; value=\u0026#34;sa\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;jdbcPassword\u0026#34; value=\u0026#34;\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;databaseSchemaUpdate\u0026#34; value=\u0026#34;true\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;asyncExecutorActivate\u0026#34; value=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;mailServerHost\u0026#34; value=\u0026#34;mail.my-corp.com\u0026#34; /\u0026gt; \u0026lt;property name=\u0026#34;mailServerPort\u0026#34; value=\u0026#34;5025\u0026#34; /\u0026gt; \u0026lt;/bean\u0026gt; \u0026lt;/beans\u0026gt; 然后使用静态方法进行获取ProcessEngine\nProcessEngine processEngine = ProcessEngines.getDefaultProcessEngine(); 还有其他配置，这里不一一列举，详见文档地址 https://www.flowable.com/open-source/docs/bpmn/ch03-Configuration\n大致目录如下 "},{"id":217,"href":"/zh/docs/technology/Flowable/offical/01/","title":"Flowable-01-GettingStarted","section":"官方文档","content":" 入门 # 什么是流动性 # Flowable 是一个用 Java 编写的轻量级业务流程引擎。Flowable 流程引擎允许您部署 BPMN 2.0 流程定义（用于定义流程的行业 XML 标准）、创建这些流程定义的流程实例、运行查询、访问活动或历史流程实例和相关数据等等。\n可以使用 Flowable REST API 通过 HTTP 进行通信。还有几个 Flowable 应用程序（Flowable Modeler、Flowable Admin、Flowable IDM 和 Flowable Task）提供开箱即用的示例 UI，用于处理流程和任务。\nFlowable和Activiti # Flowable是Activiti的一个分支\n构建命令行命令 # 创建流程引擎 # 请假流程如下\n员工要求休假数次 经理批准或拒绝请求 之后将模拟再某个外部系统中注册请求，并向员工发送一封包含结果的邮件 创建一个空的Mave项目，并添加依赖\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.flowable\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;flowable-engine\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;6.6.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.h2database\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;h2\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.3.176\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.29\u0026lt;/version\u0026gt; \u0026lt;!--当版本号\u0026gt;=8.0.22时会报date转字符串的错误--\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; 添加一个带有Main方法的类\n这里实例化一个ProcessEngine实例，一般只需要实例化一次，是通过ProcessEngineConfiguration创建的，用来配置和调整流程引擎的配置\nProcessEngineConfiguration也可以使用配置 XML 文件创建 ProcessEngineConfiguration需要的最低配置是与数据库的 JDBC 连接 package org.flowable; import org.flowable.engine.ProcessEngine; import org.flowable.engine.ProcessEngineConfiguration; import org.flowable.engine.impl.cfg.StandaloneProcessEngineConfiguration; public class HolidayRequest { public static void main(String[] args) { //这里改用mysql，注意后面的nullCatalogMeansCurrent=true //注意，pom需要添加mysql驱动依赖 ProcessEngineConfiguration cfg = new StandaloneProcessEngineConfiguration() .setJdbcUrl(\u0026#34;jdbc:mysql://localhost:3306/flowable_official?useUnicode=true\u0026#34; + \u0026#34;\u0026amp;characterEncoding=utf-8\u0026amp;serverTimezone=Asia/Shanghai\u0026amp;allowMultiQueries=true\u0026#34; +\u0026#34;\u0026amp;nullCatalogMeansCurrent=true\u0026#34; ) .setJdbcUsername(\u0026#34;root\u0026#34;) .setJdbcPassword(\u0026#34;123456\u0026#34;) .setJdbcDriver(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;) .setDatabaseSchemaUpdate(ProcessEngineConfiguration.DB_SCHEMA_UPDATE_TRUE); /* //这是官网，用的h2 ProcessEngineConfiguration cfg = new StandaloneProcessEngineConfiguration() .setJdbcUrl(\u0026#34;jdbc:h2:mem:flowable;DB_CLOSE_DELAY=-1\u0026#34;) .setJdbcUsername(\u0026#34;sa\u0026#34;) .setJdbcPassword(\u0026#34;\u0026#34;) .setJdbcDriver(\u0026#34;org.h2.Driver\u0026#34;) .setDatabaseSchemaUpdate(ProcessEngineConfiguration.DB_SCHEMA_UPDATE_TRUE);*/ ProcessEngine processEngine = cfg.buildProcessEngine(); } } 运行后会出现slf4j的警告，添加依赖并编写配置文件即可\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-log4j12\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 配置文件\nlog4j.rootLogger=DEBUG, CA log4j.appender.CA=org.apache.log4j.ConsoleAppender log4j.appender.CA.layout=org.apache.log4j.PatternLayout log4j.appender.CA.layout.ConversionPattern=%d{hh:mm:ss,SSS} [%t] %-5p %c %x - %m%n 重运行程序无警告\n会自动往mysql添加一些表及数据\n部署流程定义 # flowable 引擎希望以 BPMN 2.0 格式定义流程，这是一种在行业中被广泛接受的 XML 标准。Flowable术语称之为流程定义 （可以理解成许多执行的蓝图），从流程定义中可以启动许多流程实例\n流程定义了请假假期所涉及的不同步骤，而一个流程实例与一位特定员工的假期请相匹配。\nBPMN 2.0 存储为 XML，但它也有一个可视化部分：它以标准方式定义每个不同的步骤类型（人工任务、自动服务调用等）如何表示，以及如何将这些不同的步骤连接到彼此。通过这种方式，BPMN 2.0 标准允许技术人员和业务人员以双方都理解的方式就业务流程进行交流。\n我们将使用的流程定义\n假设该过程是通过提供一些信息开始的 左边的圆圈称为开始事件 第一个矩形是用户任务（经理必须执行，批准或拒绝） 根据经理决定，专用网关 （带有十字菱形）会将流程实例路由到批准或拒绝路径 如果获得批准，必须在某个外部系统中注册请求，然后再次为原始员工执行用户任务，通知他们该决定 如果被拒绝，则会向员工发送一封电子邮件，通知他们这一点 此类流程定义使用可视化建模工具建模，例如Flowable Designer（Eclipse）或FlowableModeler（Web应用程序）\nBPMN 2.0 及其概念 下面的holiday-request.bmpn20.xm文件放在src/main/resouces中\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;definitions xmlns=\u0026#34;http://www.omg.org/spec/BPMN/20100524/MODEL\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xsd=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; xmlns:bpmndi=\u0026#34;http://www.omg.org/spec/BPMN/20100524/DI\u0026#34; xmlns:omgdc=\u0026#34;http://www.omg.org/spec/DD/20100524/DC\u0026#34; xmlns:omgdi=\u0026#34;http://www.omg.org/spec/DD/20100524/DI\u0026#34; xmlns:flowable=\u0026#34;http://flowable.org/bpmn\u0026#34; typeLanguage=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34; expressionLanguage=\u0026#34;http://www.w3.org/1999/XPath\u0026#34; targetNamespace=\u0026#34;http://www.flowable.org/processdef\u0026#34;\u0026gt; \u0026lt;process id=\u0026#34;holidayRequest\u0026#34; name=\u0026#34;Holiday Request\u0026#34; isExecutable=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;startEvent id=\u0026#34;startEvent\u0026#34;/\u0026gt; \u0026lt;!--线条指向--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;startEvent\u0026#34; targetRef=\u0026#34;approveTask\u0026#34;/\u0026gt; \u0026lt;userTask id=\u0026#34;approveTask\u0026#34; name=\u0026#34;Approve or reject request\u0026#34;/\u0026gt; \u0026lt;!--线条指向--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;approveTask\u0026#34; targetRef=\u0026#34;decision\u0026#34;/\u0026gt; \u0026lt;!--网关--\u0026gt; \u0026lt;exclusiveGateway id=\u0026#34;decision\u0026#34;/\u0026gt; \u0026lt;!--线条指向，下面有两个分支--\u0026gt; \u0026lt;!--线条指向approved--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;decision\u0026#34; targetRef=\u0026#34;externalSystemCall\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt; \u0026lt;![CDATA[ ${approved} ]]\u0026gt; \u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;!--线条指向!approved--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;decision\u0026#34; targetRef=\u0026#34;sendRejectionMail\u0026#34;\u0026gt; \u0026lt;conditionExpression xsi:type=\u0026#34;tFormalExpression\u0026#34;\u0026gt; \u0026lt;![CDATA[ ${!approved} ]]\u0026gt; \u0026lt;/conditionExpression\u0026gt; \u0026lt;/sequenceFlow\u0026gt; \u0026lt;!--分支1--\u0026gt; \u0026lt;serviceTask id=\u0026#34;externalSystemCall\u0026#34; name=\u0026#34;Enter holidays in external system\u0026#34; flowable:class=\u0026#34;org.flowable.CallExternalSystemDelegate\u0026#34;/\u0026gt; \u0026lt;!--线条指向--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;externalSystemCall\u0026#34; targetRef=\u0026#34;holidayApprovedTask\u0026#34;/\u0026gt; \u0026lt;!--用户任务--\u0026gt; \u0026lt;userTask id=\u0026#34;holidayApprovedTask\u0026#34; name=\u0026#34;Holiday approved\u0026#34;/\u0026gt; \u0026lt;!--线条指向--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;holidayApprovedTask\u0026#34; targetRef=\u0026#34;approveEnd\u0026#34;/\u0026gt; \u0026lt;!--服务任务--\u0026gt; \u0026lt;serviceTask id=\u0026#34;sendRejectionMail\u0026#34; name=\u0026#34;Send out rejection email\u0026#34; flowable:class=\u0026#34;org.flowable.SendRejectionMail\u0026#34;/\u0026gt; \u0026lt;!--线条指向--\u0026gt; \u0026lt;sequenceFlow sourceRef=\u0026#34;sendRejectionMail\u0026#34; targetRef=\u0026#34;rejectEnd\u0026#34;/\u0026gt; \u0026lt;!--分支2结束--\u0026gt; \u0026lt;endEvent id=\u0026#34;approveEnd\u0026#34;/\u0026gt; \u0026lt;!--分支2结束--\u0026gt; \u0026lt;endEvent id=\u0026#34;rejectEnd\u0026#34;/\u0026gt; \u0026lt;/process\u0026gt; \u0026lt;/definitions\u0026gt; 解释\n该文件与BPMN2.0标准规范完全兼容 每个步骤（活动 activity），都有一个id属性，在XML中，该属性提供唯一标识符 name属性为可选的名称，增加了可视化图表的可读性 活动通过**顺序流(sequenceFlow)**连接，即可视图中的定向箭头。执行流程实例时，执行将从开始事件流向下一个活动，且遵循顺序流 离开专有网关的序列流（带有 X 的菱形）显然是特殊的：两者都有一个以表达式形式定义的条件（见第 25 和 32 行）。当流程实例执行到达此gateway时，将评估条件并采用第一个解析为true的条件。这就是这里独有的含义：只选择一个。如果需要不同的路由行为，当然也可以使用其他类型的网关 表达式以${approved}的形式，是${approved == true}的简写 approved称为过程变量，他与流程实例一起存储（持久数据为，在流程实例的声明周期内使用），意味着必须在流程实例的某个时间点（提交经理用户任务时，即结点\u0026lt;userTask id=\u0026quot;approveTask\u0026quot; /\u0026gt;[Flowable术语，完成])设置此流程变量） 部署流程 使用RepositoryService，它可以从ProcessEngine对象中检索，通过传递XML文件的位置并调用deploy()方法来执行它来创建一个新的Deployment\nRepositoryService repositoryService = processEngine.getRepositoryService(); //部署流程 Deployment deployment = repositoryService.createDeployment() .addClasspathResource(\u0026#34;holiday-request.bpmn20.xml\u0026#34;) .deploy(); //打印部署id System.out.println(\u0026#34;Found deployment id : \u0026#34; + deployment.getId()); 每次部署的id存在act_re_deployment表中 通过API查询来验证引擎是否知道流程定义\nProcessDefinition processDefinition = repositoryService.createProcessDefinitionQuery() .deploymentId(deployment.getId()) .singleResult(); System.out.println(\u0026#34;Found process definition : \u0026#34; + processDefinition.getName()); 启动流程实例 # 现在已经将流程定义部署到流程引擎中了，所以可以将此流程定义作为“蓝图”来启动流程实例\n启动前提供一些初始流程变量 ，通常，当流程自动触发时，将通过呈现给用户的表单或者通过REST API获得这些信息，本例为保持简单使用java.util.Scanner在命令中简单输入一些数据\nScanner scanner= new Scanner(System.in); System.out.println(\u0026#34;Who are you?\u0026#34;); String employee = scanner.nextLine(); System.out.println(\u0026#34;How many holidays do you want to request?\u0026#34;); Integer nrOfHolidays = Integer.valueOf(scanner.nextLine()); System.out.println(\u0026#34;Why do you need them?\u0026#34;); String description = scanner.nextLine(); 接下来，通过RuntimeService启动一个流程实例，流程实例使用key启动，此键与BPMN2.0 XML文件中设置的id属性匹配\nRuntimeService runtimeService = processEngine.getRuntimeService(); Map\u0026lt;String, Object\u0026gt; variables = new HashMap\u0026lt;String, Object\u0026gt;(); variables.put(\u0026#34;employee\u0026#34;, employee); variables.put(\u0026#34;nrOfHolidays\u0026#34;, nrOfHolidays); variables.put(\u0026#34;description\u0026#34;, description); ProcessInstance processInstance = runtimeService.startProcessInstanceByKey(\u0026#34;holidayRequest\u0026#34;, variables); 流程实例启动时，会创建一个执行(execution)并将其放入start event启动事件中。之后，此执行(execution)遵守user task 用户任务的序列流 sequence flow以供经理批准并执行用户任务user task行为 此行为将在数据库中创建一个任务，稍后可以使用查询找到该任务 用户任务处于等待状态，引擎将停止进一步执行任何操作，返回 API 调用 支线：交易性 (Sidetrack: transactionality) # 当您进行 Flowable API 调用时，默认情况下，一切都是同步synchronous的，并且是同一事务的一部分。这意味着，当方法调用返回时，将启动并提交事务。 当一个流程实例启动时，从流程实例启动到下一个等待状态会有一个数据库事务。在本例中，这是第一个用户任务。当引擎到达这个用户任务时，状态被持久化到数据库中并且事务被提交并且API调用返回 在 Flowable 中，当继续一个流程实例时，总会有一个数据库事务从前一个等待状态转到下一个等待状态。 查询和完成任务 # 为用户任务配置分配\n[第一个任务进入\u0026quot;经理\u0026quot;组]\n\u0026lt;userTask id=\u0026#34;approveTask\u0026#34; name=\u0026#34;Approve or reject request\u0026#34; flowable:candidateGroups=\u0026#34;managers\u0026#34;/\u0026gt; 第二个任务的受让人assignee属性 基于我们在流程实例启动时传递的流程变量的动态分配\n\u0026lt;userTask id=\u0026#34;holidayApprovedTask\u0026#34; name=\u0026#34;Holiday approved\u0026#34; flowable:assignee=\u0026#34;${employee}\u0026#34;/\u0026gt; 查询并返回\u0026quot;managers\u0026quot;组的任务\nTaskService taskService = processEngine.getTaskService(); List\u0026lt;Task\u0026gt; tasks = taskService.createTaskQuery().taskCandidateGroup(\u0026#34;managers\u0026#34;).list(); System.out.println(\u0026#34;You have \u0026#34; + tasks.size() + \u0026#34; tasks:\u0026#34;); for (int i=0; i\u0026lt;tasks.size(); i++) { System.out.println((i+1) + \u0026#34;) \u0026#34; + tasks.get(i).getName());// } 有三个是因为启动了三个实例\n获取特定的流程实例变量，并在屏幕上显示实际请求\nSystem.out.println(\u0026#34;Which task would you like to complete?\u0026#34;); int taskIndex = Integer.valueOf(scanner.nextLine()); Task task = tasks.get(taskIndex - 1); Map\u0026lt;String, Object\u0026gt; processVariables = taskService.getVariables(task.getId()); System.out.println(processVariables.get(\u0026#34;employee\u0026#34;) + \u0026#34; wants \u0026#34; + processVariables.get(\u0026#34;nrOfHolidays\u0026#34;) + \u0026#34; of holidays. Do you approve this?\u0026#34;); 设置variables让经理批准\nboolean approved = scanner.nextLine().toLowerCase().equals(\u0026#34;y\u0026#34;); variables = new HashMap\u0026lt;String, Object\u0026gt;(); variables.put(\u0026#34;approved\u0026#34;, approved); //经理完成任务 taskService.complete(task.getId(), variables); $\\color{red}该任务现已完成，并且基于\u0026quot;approved\u0026quot;流程变量选择离开专用网关的两条路径之一$\n编写JavaDelegate # 实现在请求被批准时将执行的自动逻辑，在BPMN2.0 XML中，这是一个服务任务\n\u0026lt;serviceTask id=\u0026#34;externalSystemCall\u0026#34; name=\u0026#34;Enter holidays in external system\u0026#34; flowable:class=\u0026#34;org.flowable.CallExternalSystemDelegate\u0026#34;/\u0026gt; 这里指定了具体实现类\npackage org.flowable; import org.flowable.engine.delegate.DelegateExecution; import org.flowable.engine.delegate.JavaDelegate; public class CallExternalSystemDelegate implements JavaDelegate { public void execute(DelegateExecution execution) { System.out.println(\u0026#34;Calling the external system for employee \u0026#34; + execution.getVariable(\u0026#34;employee\u0026#34;)); } } 当执行execution到达service tast服务任务时，BPMN 2.0 XML中引用的类被实例化并被调用\n运行，发现自定义逻辑确实已执行\n处理历史数据 # Flowable引擎会自动存储所有流程实例的审计数据audit data 或历史数据historical data\n下面，显示一直在执行的流程实例的持续时间，从ProcessEngine获取HistoryService并创建历史活动查询。这里添加了过滤\u0026ndash;1 仅针对一个特定流程实例的活动 \u0026ndash;2 只有已经完成的活动\nHistoryService historyService = processEngine.getHistoryService(); List\u0026lt;HistoricActivityInstance\u0026gt; activities = historyService.createHistoricActivityInstanceQuery() .processInstanceId(processInstance.getId()) .finished() .orderByHistoricActivityInstanceEndTime().asc() .list(); for (HistoricActivityInstance activity : activities) { System.out.println(activity.getActivityId() + \u0026#34; took \u0026#34; + activity.getDurationInMillis() + \u0026#34; milliseconds\u0026#34;); } 结论 # 本教程介绍了各种 Flowable 和 BPMN 2.0 概念和术语，同时还演示了如何以编程方式使用 Flowable API。\nFlowable REST API入门 # 设置REST应用程序 # 使用flowable-rest.war , java -jar flowable-rest.war\n测试是否运行成功\ncurl --user rest-admin:test http://localhost:8080/flowable-rest/service/management/engine 部署流程定义 # 先切到该文件夹下 使用下面命令启动flowable-rest\njava -jar flowable-rest.war 部署流程定义\ncurl --user rest-admin:test -F \u0026#34;file=@holiday-request.bpmn20.xml\u0026#34; http://localhost:8080/flowable-rest/service/repository/deployments 查看流程是否部署\ncurl --user rest-admin:test http://localhost:8080/flowable-rest/service/repository/process-definitions 将返回一个列表，列表每个元素是当前部署到引擎的所有流程定义 启动流程实例 # 命令\ncurl --user rest-admin:test -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{ \u0026#34;processDefinitionKey\u0026#34;:\u0026#34;holidayRequest\u0026#34;, \u0026#34;variables\u0026#34;: [ { \u0026#34;name\u0026#34;:\u0026#34;employee\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;John Doe\u0026#34; }, { \u0026#34;name\u0026#34;:\u0026#34;nrOfHolidays\u0026#34;, \u0026#34;value\u0026#34;: 7 }]}\u0026#39; http://localhost:8080/flowable-rest/service/runtime/process-instances windows中会报错\u0026hellip;估计是没转义啥的原因 将返回\n{\u0026#34;id\u0026#34;:\u0026#34;43\u0026#34;,\u0026#34;url\u0026#34;:\u0026#34;http://localhost:8080/flowable-rest/service/runtime/process-instances/43\u0026#34;,\u0026#34;businessKey\u0026#34;:null,\u0026#34;suspended\u0026#34;:false,\u0026#34;ended\u0026#34;:false,\u0026#34;processDefinitionId\u0026#34;:\u0026#34;holidayRequest:1:42\u0026#34;,\u0026#34;processDefinitionUrl\u0026#34;:\u0026#34;http://localhost:8080/flowable-rest/service/repository/process-definitions/holidayRequest:1:42\u0026#34;,\u0026#34;activityId\u0026#34;:null,\u0026#34;variables\u0026#34;:[],\u0026#34;tenantId\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;completed\u0026#34;:false} 任务列表和完成任务 # 获取manager经理组的所有任务\ncurl --user rest-admin:test -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{ \u0026#34;candidateGroup\u0026#34; : \u0026#34;managers\u0026#34; }\u0026#39; http://localhost:8080/flowable-rest/service/query/tasks 使用命令完成一个任务\ncurl --user rest-admin:test -H \u0026#34;Content-Type: application/json\u0026#34; -X POST -d \u0026#39;{ \u0026#34;action\u0026#34; : \u0026#34;complete\u0026#34;, \u0026#34;variables\u0026#34; : [ { \u0026#34;name\u0026#34; : \u0026#34;approved\u0026#34;, \u0026#34;value\u0026#34; : true} ] }\u0026#39; http://localhost:8080/flowable-rest/service/runtime/tasks/25 这里会报下面的错\n{\u0026#34;message\u0026#34;:\u0026#34;Internal server error\u0026#34;,\u0026#34;exception\u0026#34;:\u0026#34;couldn\u0026#39;t instantiate class org.flowable.CallExternalSystemDelegate\u0026#34;} 解决办法\n这意味着引擎找不到服务任务中引用的 CallExternalSystemDelegate 类。为了解决这个问题，需要将该类放在应用程序的类路径中（这将需要重新启动）。按照本节所述创建类，将其打包为JAR，并将其放在Tomcat的webapps文件夹下的flowable-rest文件夹的WEB-INF/lib文件夹中。\n"},{"id":218,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/2.1.2-2.1.3/","title":"算法红皮书 2.1.2-2.1.3","section":"_算法(第四版)_","content":" 排序 # 初级排序算法 # 选择排序 # 命题A。对于长度为N 的数组，选择排序需要大约 N^2/2 次比较和N 次交换。\n代码\npublic class Selection { public static void sort(Comparable[] a) { // 将a[]按升序排列 int N = a.length; // 数组长度 for (int i = 0; i \u0026lt; N; i++) { // 将a[i]和a[i+1..N]中最小的元素交换 int min = i; // 最小元素的索引 for (int j = i+1; j \u0026lt; N; j++) if (less(a[j], a[min])) min = j; exch(a, i, min); } } // less()、exch()、isSorted()和main()方法见“排序算法类模板” } 特点\n运行时间与输入无关，即输入数据的初始状态（比如是否已排序好等等）不影响排序时间 数据移动是最少的（只使用了N次交换，交换次数和数组的大小是线性关系 插入排序 # 命题B。对于随机排列的长度为N 且主键不重复的数组，平均情况下插入排序需要～ N^2/4 次比较以及～ N^2/4 次交换。最坏情况下需要～ N^2/2 次比较和～ N^2/2 次交换，最好情况下需要N-1次比较和0 次交换。\n代码\npublic static void sort(Comparable[] a) { int N = a.length; //将下表为 n-1的数，依次和n-2,n-3一直到0比较， //所以第二层for只走到1，因为0前面没有值 //如果比前面的值小，就进行交换 for (int i = 1; i \u0026lt; N; i++) { for (int j = i; j \u0026gt; 0 \u0026amp;\u0026amp; less(a[j], a[j - 1]); j--) { exch(a, j, j - 1); } } } 当倒置的数量很小时，插入排序比本章中的其他任何算法都快\n命题C。插入排序需要的交换操作和数组中倒置的数量相同，需要的比较次数大于等于倒置的数量，小于等于倒置的数量加上数组的大小再减一。\n性质D。对于随机排序的无重复主键的数组，插入排序和选择排序的运行时间是平方级别的，两者之比应该是一个较小的常数\n希尔排序 # 希尔排序的思想是使数组中任意间隔为h的元素都是有序的，这样的数组称为h有序数组，一个h有序数组就是h个互相独立的有序数组编制在一起组成的数组\n算法2.3 的实现使用了序列1/2（3k-1），从N/3 开始递减至1。我们把这个序列称为递增序列\n详述\n实现希尔排序的一种方法是对于每个h，用插入排序将h 个子数组独立地排序。但因为子数组是相互独立的，一个更简单的方法是在h- 子数组中将每个元素交换到比它大的元素之前去（将比它大的元素向右移动一格）。只需要在插入排序的代码中将移动元素的距离由1 改为h 即可。这样，希尔排序的实现就转化为了一个类似于插入排序但使用不同增量的过程。\n代码\npublic class Shell { public static void sort(Comparable[] a) { // 将a[]按升序排列 int N = a.length; int h = 1; while (h \u0026lt; N/3) h = 3*h + 1; // 1, 4, 13, 40, 121, 364, 1093, ... while (h \u0026gt;= 1) { // 将数组变为h有序 for (int i = h; i \u0026lt; N; i++) { // 将a[i]插入到a[i-h], a[i-2*h], a[i-3*h]... 之中 for (int j = i; j \u0026gt;= h \u0026amp;\u0026amp; less(a[j], a[j-h]); j -= h) exch(a, j, j-h); } h = h/3; } } // less()、exch()、isSorted()和main()方法见“排序算法类模板” } 通过提升速度来解决其他方式无法解决的问题是研究算法的设计和性能的主要原因之一\n归并排序 # 归并排序最吸引人的性质是它能够保证将任意长度为N的数组排序所需时间和NlogN成正比，主要缺点是他所需的额外空间和N成正比\n归并排序示意图 自顶向下的归并排序 # 原地归并的抽象方法\n/** * 这里有一个前提，就是a[i..mid]是有序的， * a[mid..hi]是有序的 * * @param a * @param lo * @param mid * @param hi */ public static void merge(Comparable[] a, int lo, int mid, int hi) { int i = lo, j = mid + 1; //先在辅助数组赋上需要的值 for (int k = lo; k \u0026lt;= hi; k++) { aux[k] = a[k]; } //最坏情况下这里时需要比较hi-lo+1次的，也就是数组长度 for (int k = lo; k \u0026lt;= hi; k++) { if (i \u0026gt; mid) { //说明i(左边）比较完了，直接拿右边的值放进去 a[k] = aux[j++]; } else if (j \u0026gt; hi) { //说明j(右边)比较完了，直接拿左边的值放进去 a[k] = aux[i++]; } else if (less(aux[j], aux[i])) { //左右都还有值的情况下，取出最小的值放进去 a[k] = aux[j++]; } else { a[k] = aux[i++]; } } } 递归进行归并排序\nprivate static void sort(Comparable[] a, int lo, int hi) { if (hi \u0026lt;= lo) { return; } int mid = lo + (hi - lo) / 2; //保证左边有序 sort(a, lo, mid); //保证右边有序 sort(a, mid + 1, hi); //归并数组有序的两部分 merge(a, lo, mid, hi); } 辅助数组的一次性初始化\nprivate static Comparable[] aux; public static void sort(Comparable[] a) { aux = new Comparable[a.length];//辅助数组，一次性分配空间 sort(a, 0, a.length - 1); } 自顶向下的归并排序的调用轨迹 N=16时归并排序中子数组的依赖树 每个结点都表示一个sort() 方法通过merge() 方法归并而成的子数组。这棵树正好有n 层。对于0 到n-1 之间的任意k，自顶向下的第k 层有2^k 个子数组，每个数组的长度为 $2^{(n-k)}$，归并最多需要$2^{(n-k)}$次比较。因此每层的比较次数为$ 2^k * 2 ^ {( n - 1 )} = 2 ^ n $ ，n层总共为 $n*2^n = lg N * (2 ^ { lg N}) = lg N * N$\n命题F。对于长度为N 的任意数组，自顶向下的归并排序需要(1/2)N lgN 至N lgN 次比较。\n注：因为归并所需要的比较次数最少为N/2\n命题G。对于长度为N 的任意数组，自顶向下的归并排序最多需要访问数组6NlgN 次。 证明。每次归并最多需要访问数组6N 次（2N 次用来复制，2N 次用来将排好序的元素移动回去，另外最多比较2N 次），根据命题F 即可得到这个命题的结果。\n自底向上的归并排序 # 递归实现的归并排序时算法设计中分治思想 的典型应用\n自底向上的归并排序的可视轨迹\n源代码\nprivate static Comparable[] aux; private static void sort(Comparable[] a) { int N = a.length; aux = new Comparable[N]; //每次合并的子数组长度翻倍 for (int sz = 1; sz \u0026lt; N; sz = sz + sz) { //lo:子数组索引 //边界问题， 假设是N为2^n，则倒数第二个数组的元素的下标，一定在倒数第一个元素下标(n-sz)之前 for (int lo = 0; lo \u0026lt; N - sz; lo += sz + sz) { //循环合并一个个的小数组 merge(a, lo, lo + sz - 1, Math.min(lo + sz + sz - 1, N - 1)); } } } 子数组的大小sz的初始值为1，每次加倍\n最后一个子数组的大小只有在数组大小是sz的偶数倍的时候才会等于sz（否则比sz小)\n命题H。对于长度为N 的任意数组，自底向上的归并排序需要1/2NlgN 至NlgN 次比较，最多访问数组6NlgN 次。\n自底向上的归并排序比较适合用链表组织的数据。想象一下将链表先按大小为1 的子链表进行排序，然后是大小为2 的子链表，然后是大小为4 的子链表等。这种方法只需要重新组织链表链接就能将链表原地排序（不需要创建任何新的链表结点）\n归并排序告诉我们，当能够用其中一种方法解决一个问题时，都应该试试另一种，可以像Merge.sort()那样化整为零（然后递归地解决）问题，或者像MergeBU.sort()那样循序渐进的解决问题\n命题I。没有任何基于比较的算法能够保证使用少于lg（N!）～ NlgN 次比较将长度为N 的数组排序\n命题J。归并排序是一种渐进最优的基于比较排序的算法。\n快速排序 # 快速排序是应用最广泛的排序算法\n基本算法 # 是一种分治的排序算法，将一个数组分成两个子数组，将两部分独立的排序\n归并排序将数组分成两个子数组分别排序，并将有序的子数组归并以将两个数组排序；快速排序将数组排序的方式是当两个子数组都有序时整个数组也都有序了\n归并排序：递归调用发生在处理数组之前；快速排序：递归调用发生在处理数组之后\n归并排序中数组被分为两半；快速排序中切分取决于数组内容\n快速排序示意图 递归代码\npublic static void sort(Comparable[] a, int lo, int hi) { if (hi \u0026lt;= lo) return; int j = partition(a, lo, hi); //切分 sort(a, lo, j - 1); /// 将左半部分a[lo .. j-1]排序 sort(a, j + 1, hi);//将右半部分a[j+1..hi]排序 } 快速排序递归的将子数组a[lo..hi]排序，先用partition()方法将a[j]放到一个合适的位置，然后再用递归调用将其他位置的元素排序 切分后使得数组满足三个条件\n对于某个j，a[j]已经排定 a[lo]到a[j-1]的所有元素都不大于a[j] a[j+1]的所有元素都不小于a[j] 归纳法证明数组有序：\n如果左子数组和右子数组都是有序的，那么由左子数组（有序且没有任何元素大于切分元素）、切分元素和右子数组（有序且没有任何元素小于切分元素）组成的结果数组也一定是有序的\n一般策略是先随意地取a[lo] 作为切分元素，即那个将会被排定的元素，然后我们从数组的左端开始向右扫描直到找到一个大于等于它的元素，再从数组的右端开始向左扫描直到找到一个小于等于它的元素。这两个元素显然是没有排定的，因此我们交换它们的位置。如此继续，我们就可以保证左指针i 的左侧元素都不大于切分元素，右指针j 的右侧元素都不小于切分元素。当两个指针相遇时，我们只需要将切分元素a[lo] 和左子数组最右侧的元素（a[j]）交换然后返回j 即可\n代码如下\nprivate static int partition(Comparable[] a, int lo, int hi) { int i = lo, j = hi + 1; //左右扫描指针 Comparable v = a[lo]; //切分元素 while (true) { //从左往右扫描，如果找到了大于等于v值的数，就退出循环 while (less(a[++i], v)) { if (i == hi) break; } //从右往左扫描，如果找到了小于等于v值得数，就退出循环 while (less(a[--j], v)) { if (j == lo) break; } if (i \u0026gt;= j) break;//如果i，j相遇则退出循环 //将左边大于等于v值的数与右边小于等于v值的数交换 exch(a, i, j); } //上面的遍历结束后，a[lo+1...j]和a[i..hi]都已经分别有序 //且a[j]\u0026lt;=a[i]\u0026lt;=a[lo]，所以应该交换a[lo]和a[j](而不是a[i)，因为 //a[i]有可能大于a[lo] exch(a, lo, j); //返回a[lo]被交换的位置 return j; } 切分轨迹 性能特点 # 将长度为N的无重复数组排序，快速排序平均需要~2N lnN 次比较（以及1/6的交换）\n算法改进 # 三向切分\n"},{"id":219,"href":"/zh/docs/problem/Git/01/","title":"git使用ssh连不上","section":"Git","content":" 处理方式 在系统的host文件中，添加ip指定\n199.232.69.194 github.global.ssl.fastly.net 140.82.114.4 github.com "},{"id":220,"href":"/zh/docs/life/archive/20220416/","title":"《作酒》有感","section":"往日归档","content":"最近几天吃饭，经常听到一首很嗨的歌。旋律很轻快，其实本来也就一听而过，可能是耳闻目染次数多了，好奇心上来了，查了下歌词。\n听这首歌期间我居然联想了很多，果然是老emo了。不知道怎么回事，我这种与世无争的心态，听完后居然也让我幻想了一下这歌描述的爱情模样。我又突然想到，如今社会上离婚率居高不下，也许与网络信息的传输有密切关联。如果是古代，嫁错人或者娶错人，大家也都都认了，有什么小打小闹都互相包含。而如今，生活压力不断增大，加上网络上爆炸式（至少效果是）的宣传爱情，对比显著，很让人一着魔就陷进去，就摒弃几年甚至十几年的夫妻之情，去追求所谓的真爱、自由。\n每个人对自己的过往，或多或少都会不甘。如果这种不甘自己没有办法化解，那么就会在某一刻爆发。每个人都应该，也必定会为自己曾经的所作所为负责。不要懵懵懂懂地进入(现代)婚姻，这样对自己和它人都极其不负责。 爆炸式的信息接收会激发你所有的冲动与不甘。\n"},{"id":221,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/2.1.1/","title":"算法红皮书 2.1.1","section":"_算法(第四版)_","content":" 排序 # 排序就是将一组对象按照某种逻辑顺序重新排序的过程\n对排序算法的分析有助于理解本书中比较算法性能的方法 类似技术能解决其他类型问题 排序算法常常是我们解决其他问题的第一步 初级排序算法 # 熟悉术语及技巧 某些情况下初级算法更有效 有助于改进复杂算法的效率 游戏规则 # 主要关注重新排序数组元素的算法，每个元素都会有一个主键\n排序后索引较大的主键大于索引较小的主键\n一般情况下排序算法通过两个方法操作数据，less()进行比较，exch()进行交换\n排序算法类的模板\npublic class Example { public static void sort(Comparable[] a) { /* 请见算法2.1、算法2.2、算法2.3、算法2.4、算法2.5或算法2.7*/ } private static Boolean less(Comparable v, Comparable w) { return v.compareTo(w) \u0026lt; 0; } private static void exch(Comparable[] a, int i, int j) { Comparable t = a[i]; a[i] = a[j]; a[j] = t; } private static void show(Comparable[] a) { // 在单行中打印数组 for (int i = 0; i \u0026lt; a.length; i++) StdOut.print(a[i] + \u0026#34; \u0026#34;); StdOut.println(); } public static Boolean isSorted(Comparable[] a) { // 测试数组元素是否有序 for (int i = 1; i \u0026lt; a.length; i++) if (less(a[i], a[i-1])) return false; return true; } public static void main(String[] args) { // 从标准输入读取字符串，将它们排序并输出 String[] a = In.readStrings(); sort(a); assert isSorted(a); show(a); } } 使用\n% more tiny.txt S O R T E X A M P L E % java Example \u0026lt; tiny.txt A E E L M O P R S T X % more words3.txt bed bug dad yes zoo ... all bad yet % java Example \u0026lt; words.txt all bad bed bug dad ... yes yet zoo 使用assert验证\n排序成本模型：在研究排序算法时，我们需要计算比较和交换的数量。对于不交换元素的算法，我们会比较访问数组的次数\n额外内存开销和运行时间同等重要，排序算法分为\n除了函数调用需要的栈和固定数目的实例变量之外，无需额外内存的原地排序算法 需要额外内存空间来存储另一份数组副本的其他排序算法 数据类型\n排序模板适用于任何实现了Comparable接口的数据类型\n对于自己的数据类型，实现Comparable接口即可\npublic class Date implements Comparable\u0026lt;Date\u0026gt; { private final int day; private final int month; private final int year; public Date(int d, int m, int y) { day = d; month = m; year = y; } public int day() { return day; } public int month() { return month; } public int year() { return year; } public int compareTo(Date that) { if (this.year \u0026gt; that.year ) return +1; if (this.year \u0026lt; that.year ) return -1; if (this.month \u0026gt; that.month) return +1; if (this.month \u0026lt; that.month) return -1; if (this.day \u0026gt; that.day ) return +1; if (this.day \u0026lt; that.day ) return -1; return 0; } public String toString() { return month + \u0026#34;/\u0026#34; + day + \u0026#34;/\u0026#34; + year; } } compareTo()必须实现全序关系 自反性，反对称性及传递性 经典算法，包括选择排序、插入排序、希尔排序、归并排序、快速排序和堆排序\n"},{"id":222,"href":"/zh/docs/technology/RocketMQ/heima/05advance/","title":"05高级功能","section":"基础(黑马)_","content":" 学习来源 https://www.bilibili.com/video/BV1L4411y7mn（添加小部分笔记）感谢作者！\n消息存储 # 流程 # 存储介质 # 关系型数据库DB # 适合数据量不够大，比如ActiveMQ可选用JDBC方式作为消息持久化\n文件系统 # 关系型数据库最终也是要存到文件系统中的，不如直接存到文件系统，绕过关系型数据库 常见的RocketMQ/RabbitMQ/Kafka都是采用消息刷盘到计算机的文件系统来做持久化(同步刷盘/异步刷盘) 消息发送 # 顺序写：600MB/s，随机写：100KB/s\n系统运行一段时间后，我们对文件的增删改会导致磁盘上数据无法连续，非常的分散。\n顺序读也只是逻辑上的顺序，也就是按照当前文件的相对偏移量顺序读取，并非磁盘上连续空间读取\n对于磁盘的读写分为两种模式，顺序IO和随机IO。 随机IO存在一个寻址的过程，所以效率比较低。而顺序IO，相当于有一个物理索引，在读取的时候不需要寻找地址，效率很高。\n来源： https://www.cnblogs.com/liuche/p/15455808.html\n数据网络传输\n零拷贝技术MappedByteBuffer，省去了用户态，由内核态直接拷贝到网络驱动内核。 RocketMQ默认设置单个CommitLog日志数据文件为1G\n消息存储 # 三个概念：commitLog、ConsumerQueue、index\nCommitLog # 默认大小1G\n存储消息的元数据，包括了Topic、QueueId、Message 还存储了ConsumerQueue相关信息，所以ConsumerQueue丢了也没事 ConsumerQueue # 存储了消息在CommitLog的索引（几百K，Linux会事先加载到内存中） 包括最小/最大偏移量、已经消费的偏移量 一个Topic多个队列，每个队列对应一个ConsumerQueue\nIndex # 也是索引文件，为消息查询服务，通过key或时间区间查询消息\n总结 # 刷盘机制 # 同步刷盘 异步刷盘 高可用性机制 # 消费高可用及发送高可用 # 消息主从复制 # 负载均衡 # 消息重试 # 下面都是针对消费失败的重试\n顺序消息 # RocketMQ会自动不断重试，且为了保证顺序性，会导致消息消费被阻塞。使用时要及时监控并处理消费失败现象\n无序消息（普通、定时、延时、事务） # 通过设置返回状态达到消息重试的结果 重试只对集群消费方式生效，广播方式不提供重试特性 重试次数 如果16次后还是消费失败，会进入死信队列，不再被消费 配置是否重试 # 重试 # 不重试，认为消费成功 # 修改重试次数 # 在创建消费者的时候，传入Properties即可\n注意事项 # messge.getReconsumeTimes()获取消息已经重试的次数\n死信队列 # 特性 # 针对的是消费者组；不再被正常消费；有过期时间；\n查看 # 通过admin的控制台查看\n可重发；可指定后特殊消费\n可以重发，也可以写一个消费者，指定死信队列里面的消息\n消费幂等 # 同一条消息不论消费多少次，结果应该都是一样的\n发送时发送的消息重复 # "},{"id":223,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/1.5.1-1.5.3/","title":"算法红皮书 1.5.1-1.5.3","section":"_算法(第四版)_","content":" 案例研究：union-find 算法 # 设计和分析算法的基本方法 优秀的算法能解决实际问题 高效的算法也可以很简单 理解某个实现的性能特点是一项有趣的挑战 在解决同一个问题的多种算法间选择，科学方法是一种重要工具 迭代式改进能让算法效率越来越高 动态连通性 # 从输入中读取整数对p q，如果已知的所有整数对都不能说明p,q相连，就打印出pq 网络：整个程序能够判定是否需要在pq之间架设一条新的连接才能进行通信 变量名等价性（即指向同一个对象的多个引用） 数学集合：在处理一个整数对pq时，我们是在判断它们是否属于相同的集合 本节中，将对象称为触点，整数对称为连接，等价类称为连通分量或是简称分量 连通性 问题只要求我们的程序能够判别给定的整数对pq是否相连，并没有要求给两者之间的通路上的所有连接 union-find算法的API\n数据结构和算法的设计影响到算法的效率 实现 # public class UF { private int[]\tid; /* 分量id（以触点作为索引） */ private int\tcount; /* 分量数量 */ public UF( int N ) { /* 初始化分量id数组 */ count\t= N; id\t= new int[N]; for ( int i = 0; i \u0026lt; N; i++ ) id[i] = i; } public int count() { return(count); } public Boolean connected( int p, int q ) { return(find( p ) == find( q ) ); } public int find( int p ) public void union( int p, int q ) /* 请见1.5.2.1节用例（quick-find）、1.5.2.3节用例（quick-union）和算法1.5（加权quick-union） */ public static void main( String[] args ) { /* 解决由StdIn得到的动态连通性问题 */ int\tN\t= StdIn.readint(); /* 读取触点数量 */ UF\tuf\t= new UF( N ); /* 初始化N个分量 */ while ( !StdIn.isEmpty() ) { int\tp\t= StdIn.readint(); int\tq\t= StdIn.readint(); /* 读取整数对 */ if ( uf.connected( p, q ) ) continue; /* 如果已经连通则忽略 */ uf.union( p, q ); /* 归并分量 */ StdOut.println( p + \u0026#34; \u0026#34; + q ); /* 打印连接 */ } StdOut.println( uf.count() + \u0026#34;components\u0026#34; ); } } union-find的成本模型：union-find API的各种算法，统计的是数组的访问次数，不论读写\n以下有三种实现\n且仅当id[p] 等于id[q] 时p 和q 是连通的\npublic int find(int p) { return id[p]; } public void union(int p, int q) { // 将p和q归并到相同的分量中 int pID = find(p);mi int qID = find(q); // 如果p和q已经在相同的分量之中则不需要采取任何行动 if (pID == qID) return; // 将p的分量重命名为q的名称 for (int i = 0; i \u0026lt; id.length; i++) if (id[i] == pID) id[i] = qID; count--; } 命题F：在quick-find 算法中，每次find() 调用只需要访问数组一次，而归并两个分量的union() 操作访问数组的次数在(N+3) 到(2N+1) 之间。\n证明：由代码马上可以知道，每次connected() 调用都会检查id[] 数组中的两个元素是否相等，即会调用两次find() 方法。归并两个分量的union() 操作会调用两次find()，检查id[] 数组中的全部N 个元素并改变它们中1 到N-1 个元素的值。\n假设我们使用quick-find 算法来解决动态连通性问题并且最后只得到了一个连通分量，那么这至少需要调用N-1 次union()，即至少(N+3)(N-1) ～ N2 次数组访问——我们马上可以猜想动态连通性的quick-find 算法是平方级别的\n以触点作为索引的id[]数组，每个触点所对应的id[]元素都是同一个分量中的另一个触点的名称 如下图： private int find(int p) { // 找出分量的名称 while (p != id[p]) p = id[p]; return p; } public void union(int p, int q) { // 将p和q的根节点统一 int pRoot = find(p); int qRoot = find(q); if (pRoot == qRoot) return; id[pRoot] = qRoot; count--; } quick-union算法的最坏情况 加权quick-union算法（减少树的高度） 用一个数组来表示各个节点对应的分量的大小\npublic class WeightedQuickUnionUF { private int[] id; // 父链接数组（由触点索引） private int[] sz; // （由触点索引的）各个根节点所对应的分量的大小 private int count; // 连通分量的数量 public WeightedQuickUnionUF(int N) { count = N; id = new int[N]; for (int i = 0; i \u0026lt; N; i++) id[i] = i; sz = new int[N]; for (int i = 0; i \u0026lt; N; i++) sz[i] = 1; } public int count() { return count; } public Boolean connected(int p, int q) { return find(p) == find(q); } public int find(int p) { // 跟随链接找到根节点 while (p != id[p]) p = id[p]; return p; } public void union(int p, int q) { int i = find(p); int j = find(q); if (i == j) return; // 将小树的根节点连接到大树的根节点 if (sz[i] \u0026lt; sz[j]) { id[i] = j; sz[j] += sz[i]; } else { id[j] = i; sz[i] += sz[j]; } count--; } } quick-union 算法与加权quick-union 算法的对比（100 个触点，88 次union() 操作） 所有操作的总成本 展望 # 研究问题的步骤\n完整而详细地定义问题，找出解决问题所必需的基本抽象操作并定义一份 API。 简洁地实现一种初级算法，给出一个精心组织的开发用例并使用实际数据作为输入。 当实现所能解决的问题的最大规模达不到期望时决定改进还是放弃。 逐步改进实现，通过经验性分析或（和）数学分析验证改进后的效果。 用更高层次的抽象表示数据结构或算法来设计更高级的改进版本。 如果可能尽量为最坏情况下的性能提供保证，但在处理普通数据时也要有良好的性能。 在适当的时候将更细致的深入研究留给有经验的研究者并继续解决下一个问题。 "},{"id":224,"href":"/zh/docs/technology/RocketMQ/heima/04case/","title":"04案例","section":"基础(黑马)_","content":" 学习来源 https://www.bilibili.com/video/BV1L4411y7mn（添加小部分笔记）感谢作者!基本架构\n架构 # 流程图 # 下单流程 # 支付流程 # SpringBoot整合RocketMQ # 依赖包 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.rocketmq\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rocketmq-spring-boot-starter\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 生产者 # yaml # rocketmq: name-server: 192.168.1.135:9876;192.168.1.138:9876 producer: group: my-group 使用 # @Autowired private RocketMQTemplate template; @RequestMapping(\u0026#34;rocketmq\u0026#34;) public String rocketmq(){ log.info(\u0026#34;我被调用了-rocketmq\u0026#34;); //主题+内容 template.convertAndSend(\u0026#34;mytopic-ly\u0026#34;,\u0026#34;hello1231\u0026#34;); return \u0026#34;hello world\u0026#34;+serverPort; } 消费者 # yaml # rocketmq: name-server: 192.168.1.135:9876;192.168.1.138:9876 consumer: group: my-group2 使用 # 创建监听器\n@RocketMQMessageListener(topic = \u0026#34;mytopic-ly\u0026#34;, consumeMode = ConsumeMode.CONCURRENTLY,consumerGroup = \u0026#34;${rocketmq.producer.group}\u0026#34;) @Slf4j @Component public class Consumer implements RocketMQListener\u0026lt;String\u0026gt; { @Override public void onMessage(String s) { log.info(\u0026#34;消费了\u0026#34;+s); } } 下单流程利用MQ进行回退处理，保证数据一致性 # 库存回退的消费者，代码如下：\n@Slf4j @Component @RocketMQMessageListener(topic = \u0026#34;${mq.order.topic}\u0026#34;,consumerGroup = \u0026#34;${mq.order.consumer.group.name}\u0026#34;,messageModel = MessageModel.BROADCASTING ) public class CancelMQListener implements RocketMQListener\u0026lt;MessageExt\u0026gt;{ @Value(\u0026#34;${mq.order.consumer.group.name}\u0026#34;) private String groupName; @Autowired private TradeGoodsMapper goodsMapper; @Autowired private TradeMqConsumerLogMapper mqConsumerLogMapper; @Autowired private TradeGoodsNumberLogMapper goodsNumberLogMapper; @Override public void onMessage(MessageExt messageExt) { String msgId=null; String tags=null; String keys=null; String body=null; try { //1. 解析消息内容 msgId = messageExt.getMsgId(); tags= messageExt.getTags(); keys= messageExt.getKeys(); body= new String(messageExt.getBody(),\u0026#34;UTF-8\u0026#34;); log.info(\u0026#34;接受消息成功\u0026#34;); //2. 查询消息消费记录 TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey(); primaryKey.setMsgTag(tags); primaryKey.setMsgKey(keys); primaryKey.setGroupName(groupName); TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey); if(mqConsumerLog!=null){ //3. 判断如果消费过... //3.1 获得消息处理状态 Integer status = mqConsumerLog.getConsumerStatus(); //处理过...返回 if(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode().intValue()==status.intValue()){ log.info(\u0026#34;消息:\u0026#34;+msgId+\u0026#34;,已经处理过\u0026#34;); return; } //正在处理...返回 if(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode().intValue()==status.intValue()){ log.info(\u0026#34;消息:\u0026#34;+msgId+\u0026#34;,正在处理\u0026#34;); return; } //处理失败 if(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode().intValue()==status.intValue()){ //获得消息处理次数 Integer times = mqConsumerLog.getConsumerTimes(); if(times\u0026gt;3){ log.info(\u0026#34;消息:\u0026#34;+msgId+\u0026#34;,消息处理超过3次,不能再进行处理了\u0026#34;); return; } mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode()); //使用数据库乐观锁更新 TradeMqConsumerLogExample example = new TradeMqConsumerLogExample(); TradeMqConsumerLogExample.Criteria criteria = example.createCriteria(); criteria.andMsgTagEqualTo(mqConsumerLog.getMsgTag()); criteria.andMsgKeyEqualTo(mqConsumerLog.getMsgKey()); criteria.andGroupNameEqualTo(groupName); criteria.andConsumerTimesEqualTo(mqConsumerLog.getConsumerTimes()); int r = mqConsumerLogMapper.updateByExampleSelective(mqConsumerLog, example); if(r\u0026lt;=0){ //未修改成功,其他线程并发修改 log.info(\u0026#34;并发修改,稍后处理\u0026#34;); } } }else{ //4. 判断如果没有消费过... mqConsumerLog = new TradeMqConsumerLog(); mqConsumerLog.setMsgTag(tags); mqConsumerLog.setMsgKey(keys); mqConsumerLog.setGroupName(groupName); mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_PROCESSING.getCode()); mqConsumerLog.setMsgBody(body); mqConsumerLog.setMsgId(msgId); mqConsumerLog.setConsumerTimes(0); //将消息处理信息添加到数据库 mqConsumerLogMapper.insert(mqConsumerLog); } //5. 回退库存 MQEntity mqEntity = JSON.parseObject(body, MQEntity.class); Long goodsId = mqEntity.getGoodsId(); TradeGoods goods = goodsMapper.selectByPrimaryKey(goodsId); goods.setGoodsNumber(goods.getGoodsNumber()+mqEntity.getGoodsNum()); goodsMapper.updateByPrimaryKey(goods); //6. 将消息的处理状态改为成功 mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_SUCCESS.getCode()); mqConsumerLog.setConsumerTimestamp(new Date()); mqConsumerLogMapper.updateByPrimaryKey(mqConsumerLog); log.info(\u0026#34;回退库存成功\u0026#34;); } catch (Exception e) { e.printStackTrace(); TradeMqConsumerLogKey primaryKey = new TradeMqConsumerLogKey(); primaryKey.setMsgTag(tags); primaryKey.setMsgKey(keys); primaryKey.setGroupName(groupName); TradeMqConsumerLog mqConsumerLog = mqConsumerLogMapper.selectByPrimaryKey(primaryKey); if(mqConsumerLog==null){ //数据库未有记录 mqConsumerLog = new TradeMqConsumerLog(); mqConsumerLog.setMsgTag(tags); mqConsumerLog.setMsgKey(keys); mqConsumerLog.setGroupName(groupName); mqConsumerLog.setConsumerStatus(ShopCode.SHOP_MQ_MESSAGE_STATUS_FAIL.getCode()); mqConsumerLog.setMsgBody(body); mqConsumerLog.setMsgId(msgId); mqConsumerLog.setConsumerTimes(1); mqConsumerLogMapper.insert(mqConsumerLog); }else{ mqConsumerLog.setConsumerTimes(mqConsumerLog.getConsumerTimes()+1); mqConsumerLogMapper.updateByPrimaryKeySelective(mqConsumerLog); } } } } "},{"id":225,"href":"/zh/docs/technology/RocketMQ/heima/03messagetype/","title":"03收发消息","section":"基础(黑马)_","content":" 学习来源 https://www.bilibili.com/video/BV1L4411y7mn（添加小部分笔记）感谢作者!前提\n依赖包 # \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.rocketmq\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;rocketmq-client\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.4.0\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 消息生产者步骤 # 创建生产者，生产者组名\u0026ndash;\u0026gt;指定nameserver地址\u0026ndash;\u0026gt;启动producer\u0026ndash;\u0026gt;\n创建消息对象(Topic、Tag、消息体)\n发送消息、关闭生产者producer\n消息消费者步骤 # 创建消费者，制定消费者组名\u0026ndash;\u0026gt;指定nameserver地址\n订阅Topic和Tag，设置回调函数处理消息\n启动消费者consumer\n消息发送 # 同步消息 # 发送消息后客户端会进行阻塞，直到得到结果后，客户端才会继续执行\npublic static void main(String[] args) throws MQClientException, MQBrokerException, RemotingException, InterruptedException { //创建Producer，并指定生产者组 DefaultMQProducer producer = new DefaultMQProducer(\u0026#34;group1\u0026#34;); producer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); producer.start(); for (int i = 0; i \u0026lt; 10; i++) { Message msg = new Message(); msg.setTopic(\u0026#34;base\u0026#34;); msg.setTags(\u0026#34;Tag1\u0026#34;); msg.setBody((\u0026#34;hello world\u0026#34; + i).getBytes()); //发送消息 SendResult result = producer.send(msg); //发送状态 SendStatus sendStatus = result.getSendStatus(); //消息id String msgId = result.getMsgId(); //消息接收队列id MessageQueue messageQueue = result.getMessageQueue(); int queueId = messageQueue.getQueueId(); log.info(result.toString()); log.info(messageQueue.toString()); log.info(\u0026#34;status:\u0026#34; + sendStatus + \u0026#34;msgId:\u0026#34; + msgId + \u0026#34;queueId\u0026#34; + queueId); TimeUnit.SECONDS.sleep(1); } log.info(\u0026#34;发送结束===================\u0026#34;); producer.shutdown(); } 异步消息 # 发送消息后不会导致阻塞，当broker返回结果时，会调用回调函数进行处理\npublic static void main(String[] args) throws MQClientException, MQBrokerException, RemotingException, InterruptedException { //创建Producer，并指定生产者组 DefaultMQProducer producer = new DefaultMQProducer(\u0026#34;group1\u0026#34;); producer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); producer.start(); for (int i = 0; i \u0026lt; 10; i++) { Message msg = new Message(); msg.setTopic(\u0026#34;base\u0026#34;); msg.setTags(\u0026#34;Tag1\u0026#34;); msg.setBody((\u0026#34;hello world\u0026#34; + i).getBytes()); //发送消息 producer.send(msg, new SendCallback() { @Override public void onSuccess(SendResult result) { //发送状态 SendStatus sendStatus = result.getSendStatus(); //消息id String msgId = result.getMsgId(); //消息接收队列id MessageQueue messageQueue = result.getMessageQueue(); int queueId = messageQueue.getQueueId(); log.info(result.toString()); log.info(messageQueue.toString()); log.info(\u0026#34;status:\u0026#34; + sendStatus + \u0026#34;msgId:\u0026#34; + msgId + \u0026#34;queueId\u0026#34; + queueId); } @Override public void onException(Throwable throwable) { log.error(\u0026#34;发送异常\u0026#34; + throwable); } }); //TimeUnit.SECONDS.sleep(1); } log.info(\u0026#34;发送结束===================\u0026#34;); TimeUnit.SECONDS.sleep(3); } 单向消息 # 不关心发送结果\npublic static void main(String[] args) throws MQClientException, MQBrokerException, RemotingException, InterruptedException { //创建Producer，并指定生产者组 DefaultMQProducer producer = new DefaultMQProducer(\u0026#34;group1\u0026#34;); producer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); producer.start(); for (int i = 0; i \u0026lt; 10; i++) { Message msg = new Message(); msg.setTopic(\u0026#34;base\u0026#34;); msg.setTags(\u0026#34;Tag3\u0026#34;); msg.setBody((\u0026#34;hello world danxiang\u0026#34; + i).getBytes()); //发送消息 producer.sendOneway(msg); //TimeUnit.SECONDS.sleep(1); } log.info(\u0026#34;发送结束===================\u0026#34;); TimeUnit.SECONDS.sleep(3); } 消费消息 # public static void main(String[] args) throws MQClientException { DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\u0026#34;group1\u0026#34;); consumer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); consumer.subscribe(\u0026#34;base\u0026#34;, \u0026#34;Tag3\u0026#34;); consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List\u0026lt;MessageExt\u0026gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { for (MessageExt messageExt : list) { log.info(messageExt.toString()); String s = new String(messageExt.getBody()); log.info(s); } return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); consumer.start(); } 消费模式 # 注意事项 # 如果一个消息在广播消费模式下被消费过，之后再启动一个消费者，那么它可以在集群消费模式下再被消费一次。或者：\n如果一个消息在集群消费模式下被消费过，之后再启动一个消费者，那么它可以在广播消费模式下再被消费一次 如果一个消息在广播消费模式下被消费过，之后再启动一个消费者，那么它不能在广播模式下再被消费。或者\n如果一个消息在集群消费模式下被消费过，之后再启动一个消费者，那么它不能在集群模式下再被消费。 顺序消息 # 消息实体 # @Data @AllArgsConstructor @NoArgsConstructor @ToString public class OrderStep { private int orderId; private String desc; public static List\u0026lt;OrderStep\u0026gt; getData(){ List\u0026lt;OrderStep\u0026gt; orderSteps=new ArrayList\u0026lt;\u0026gt;(); OrderStep orderStep=new OrderStep(123,\u0026#34;创建\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(125,\u0026#34;创建\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(123,\u0026#34;付款\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(125,\u0026#34;付款\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(123,\u0026#34;推送\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(124,\u0026#34;创建\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(124,\u0026#34;付款\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(124,\u0026#34;推送\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(123,\u0026#34;完成\u0026#34;); orderSteps.add(orderStep); orderStep=new OrderStep(125,\u0026#34;推送\u0026#34;); orderSteps.add(orderStep); return orderSteps; } } 发送消息 # //同一个订单的消息，放在同一个topic的同一个queue里面 public static void main(String[] args) throws MQClientException { DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\u0026#34;group1\u0026#34;); consumer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); consumer.subscribe(\u0026#34;base\u0026#34;, \u0026#34;Tag1\u0026#34;); consumer.setMessageModel(MessageModel.BROADCASTING); consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List\u0026lt;MessageExt\u0026gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { for (MessageExt messageExt : list) { //log.info(messageExt.toString()); String s = new String(messageExt.getBody()); log.info(s); } return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); consumer.start(); } 顺序消费消息 # public class ConsumerOrder { public static void main(String[] args) throws MQClientException { DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\u0026#34;group1\u0026#34;); consumer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); consumer.subscribe(\u0026#34;OrderTopic\u0026#34;, \u0026#34;*\u0026#34;); consumer.registerMessageListener(new MessageListenerOrderly() { @Override public ConsumeOrderlyStatus consumeMessage(List\u0026lt;MessageExt\u0026gt; list, ConsumeOrderlyContext consumeOrderlyContext) { for (MessageExt messageExt : list) { //log.info(messageExt.toString()); String s = new String(messageExt.getBody()); log.info(s); } return ConsumeOrderlyStatus.SUCCESS; } }); consumer.start(); } } MessageListenerOrderly 保证了同一时刻只有一个线程去消费这个queue，但不能保证每次消费queue的会是同一个线程\n由于queue具有先进先出的有序性，所以这并不影响消费queue中消息的顺序性\n延时消息 # 在生产者端设置，可以设置一个消息在一定延时后才能消费\nmessage.setDelayTimLevel(2) //级别2，即延时10秒//1s 5s 10s 30s 1m\n批量消息发送 # producer.send(List\u0026lt;Message\u0026gt; messages)\n事务消息 # 事务消息的架构图 # 生产者 # public class SyncProducer { public static void main(String[] args) throws MQClientException, MQBrokerException, RemotingException, InterruptedException { //创建Producer，并指定生产者组 TransactionMQProducer producer = new TransactionMQProducer(\u0026#34;group1\u0026#34;); producer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); producer.setTransactionListener(new TransactionListener() { /** * 在该方法中执行本地事务 * @param message * @param o * @return */ @Override public LocalTransactionState executeLocalTransaction(Message message, Object o) { if(\u0026#34;TAGA\u0026#34;.equals(message.getTags())){ return LocalTransactionState.COMMIT_MESSAGE; }else if(\u0026#34;TAGB\u0026#34;.equals(message.getTags())){ return LocalTransactionState.ROLLBACK_MESSAGE; }else if(\u0026#34;TAGC\u0026#34;.equals(message.getTags())){ return LocalTransactionState.UNKNOW; } return LocalTransactionState.UNKNOW; } /** * 该方法时MQ进行消息是无状态的回查 * @param messageExt * @return */ @Override public LocalTransactionState checkLocalTransaction(MessageExt messageExt) { log.info(\u0026#34;消息的回查:\u0026#34;+messageExt.getTags()); try { log.info(\u0026#34;5s后告诉mq可以提交了\u0026#34;); TimeUnit.SECONDS.sleep(5); } catch (InterruptedException e) { e.printStackTrace(); } //可以提交 return LocalTransactionState.COMMIT_MESSAGE; } }); producer.start(); String[] tags={\u0026#34;TAGA\u0026#34;,\u0026#34;TAGB\u0026#34;,\u0026#34;TAGC\u0026#34;}; for (int i = 0; i \u0026lt; 3; i++) { Message msg = new Message(); msg.setTopic(\u0026#34;TransactionTopic\u0026#34;); msg.setTags(tags[i]); msg.setBody((\u0026#34;hello world\u0026#34; + i).getBytes()); //发送消息 //参数：针对某一个消息进行事务控制 SendResult result = producer.sendMessageInTransaction(msg,null); //发送状态 SendStatus sendStatus = result.getSendStatus(); log.info(result.toString()); log.info(\u0026#34;status:\u0026#34; + sendStatus ); } log.info(\u0026#34;发送结束===================\u0026#34;); //producer.shutdown(); } } 消费者 # @Slf4j public class Consumer { public static void main(String[] args) throws MQClientException { DefaultMQPushConsumer consumer = new DefaultMQPushConsumer(\u0026#34;group1\u0026#34;); consumer.setNamesrvAddr(\u0026#34;192.168.1.135:9876;192.168.1.138:9876\u0026#34;); consumer.subscribe(\u0026#34;TransactionTopic\u0026#34;, \u0026#34;*\u0026#34;); consumer.registerMessageListener(new MessageListenerConcurrently() { @Override public ConsumeConcurrentlyStatus consumeMessage(List\u0026lt;MessageExt\u0026gt; list, ConsumeConcurrentlyContext consumeConcurrentlyContext) { for (MessageExt messageExt : list) { String s = new String(messageExt.getBody()); log.info(s); } return ConsumeConcurrentlyStatus.CONSUME_SUCCESS; } }); consumer.start(); log.info(\u0026#34;生产者启动----\u0026#34;); } } "},{"id":226,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/1.4.1-1.4.10/","title":"算法红皮书 1.4.1-1.4.10","section":"_算法(第四版)_","content":" 算法分析 # 使用数学分析为算法成本建立简洁的模型，并使用实验数据验证这些模型\n科学方法 # 观察、假设、预测、观察并核实预测、反复确认预测和观察 原则：实验可重现 观察 # 计算性任务的困难程度可以用问题的规模来衡量\n问题规模可以是输入的大小或某个命令行参数的值\n研究问题规模和运行时间的关系\n使用计时器得到大概的运行时间 典型用例\npublic static void main(String[] args) { int N = Integer.parseInt(args[0]); int[] a = new int[N]; for (int i = 0; i \u0026lt; N; i++) a[i] = StdRandom.uniform(-1000000, 1000000); Stopwatch timer = new Stopwatch(); int cnt = ThreeSum.count(a); double time = timer.elapsedTime(); StdOut.println(cnt + \u0026#34; triples \u0026#34; + time + \u0026#34; seconds\u0026#34;); } 使用方法 数据类型的实现\npublic class Stopwatch { private final long start; public Stopwatch() { start = System.currentTimeMillis(); } public double elapsedTime() { long now = System.currentTimeMillis(); return (now - start) / 1000.0; } } 数学模型 # 程序运行的总时间主要和两点有关：执行每条语句的耗时；执行每条语句的频率\n定义：我们用~f(N) 表示所有随着N 的增大除以f(N) 的结果趋近于1 的函数。我们用g(N) ~ f(N) 表示g(N)/f(N) 随着N 的增大趋近于1。 即使用曰等号忽略较小的项\n$$ f(N)=N^b(logN)^c $$将f(N)称为g(N)的增长的数量级\n常见的增长数量级函数 本书用性质表示需要用实验验证的猜想\nThreeSum分析 执行最频繁的指令决定了程序执行的总时间\u0026ndash;我们将这些指令称为程序的内循环\n程序运行时间的分析 算法的分析 ThreeSum的运行时间增长数量级为N^3，与在哪台机器无关\n成本模型 3-sum的成本模型：数组的访问次数（访问数组元素的次数，无论读写）\n总结-得到运行时间的数学模型所需的步骤\n确定输入模型，定义问题的规模 识别内循环 根据内循环中的操作确定成本模型 对于给定的输入，判断这些操作的执行效率 增长数量级的分类 # 成长增长的数量级一般都是问题规模N的若干函数之一，如下表 常数级别表示运行时间不依赖于N 对数级别，经典例子是二分查找 线性级别（常见的for循环） 线性对数级别 ，其中，对数的底数和增长的数量级无关 平方级别，一般指两个嵌套的for循环 立方级别，一般含有三个嵌套的for循环 指数级别 问题规模（图） 典型的增长数量级函数（图） 典型的增长数量级函数 在某个成本模型下可以提出精确的命题 比如，归并排序所需的比较次数在$1/2NlgN$~$NlgN$之间 ，即归并排序所需的运行时间的增长数量级是线性对数的，也就是：归并排序是线性对数的 设计更快的算法 # 前提，目前已知归并排序是线性对数级别的，二分查找是对数级别的\n将3-sum问题简化为2-sum问题，即找出一个输入文件中所有和为0的整数对的数量，为了简化问题，题设所有整数均不相同\n可以使用双层循环，以平方级别来解决\n改进后的算法，当且仅当-a[i]存在于数组中且a[i]非零时，a[i]存在于某个和为0的整数对之中\n代码如下\nimport java.util.Arrays; public class TwoSumFast { public static int count(int[] a) { // 计算和为0的整数对的数目 Arrays.sort(a); int N = a.length; int cnt = 0; for (int i = 0; i \u0026lt; N; i++) if (BinarySearch.rank(-a[i], a) \u0026gt; i) cnt++; return cnt; } public static void main(String[] args) { int[] a = In.readInts(args[0]); StdOut.println(count(a)); } } 3-sum问题的快速算法\n当且仅当-(a[i]+a[j])在数组中,且不是a[i]也不是a[j]时，整数对(a[i]和a[j])为某个和为0的三元组的一部分\n总运行时间和$N^2logN$成正比\n代码如下\nimport java.util.Arrays; public class ThreeSumFast { public static int count(int[] a) { // 计算和为０的三元组的数目 Arrays.sort(a); int N = a.length; int cnt = 0; for (int i = 0; i \u0026lt; N; i++) for (int j = i + 1; j \u0026lt; N; j++) if (BinarySearch.rank(-a[i] - a[j], a) \u0026gt; j) { cnt++; } return cnt; } public static void main(String[] args) { int[] a = In.readInts(args[0]); StdOut.println(count(a)); } } 下界\n为算法在最坏情况下的运行时间给出一个下界的思 想是非常有意义的\n运行时间的总结\n图1 图2 实现并分析该问题的一种简单解法，我们称之为暴力算法\n算法的改进，能降低算法所需的运行时间的增长数量级\n倍率实验 # 翻倍后运行时间，与没翻倍时的运行时间成正比\n代码\npublic class DoublingRatio { public static double timeTrial(int N) // 参见DoublingTest（请见1.4.2.3 节实验程序） public static void main(String[] args) { double prev = timeTrial(125); for (int N = 250; true; N += N) { double time = timeTrial(N); StdOut.printf(\u0026#34;%6d %7.1f \u0026#34;, N, time); StdOut.printf(\u0026#34;%5.1fn\u0026#34;, time/prev); prev = time; } } } 试验结果 预测 倍率定理（没看懂，不管） 评估它解决大型问题的可行性 评估使用更快的计算机所产生的价值 注意事项 # 大常数，$c = 10^3或10^6$ 非决定性的内循环 指令时间 系统因素 不分伯仲（相同任务在不同场景效率不一样） 对输入的强烈依赖 多个问题参量 处理对于输入的依赖 # 输入模型，例如假设ThreeSum的所有输入均为随机int值，可能不切实际 输入的分析，需要数学几千 对最坏情况下的性能保证 命题（这里只针对之前的代码） 对计划算法，有时候对输入需要进行打乱 操作序列 均摊分析 通过记录所有操作的总成本并除以操作总数来将成本均摊 内存 # Java的内存分配系统 原始数据类型的常见内存、需求 这里漏了，short也是2字节。总结boolean、byte 1字节；char、short 2字节；int、float 4字节；long、double 8字节 对象（跳过） 要知道一个对象所使用的内存量，需要将所有实例变量使用的内存与内存本身的开销（一般是16字节）\n一般内存的使用都会被填充为8字节的倍数（注意，说的是64位计算机中的机器字）\n引用存储需要8字节\n典型对象的内存需求 例如第一个，16+4=20；20+4 = 24为8的倍数\n链表，嵌套的非静态（内部）类，如上面的Node，需要额外的8字节（用于外部类的引用）\n数组 int值、double值、对象和数组的数组对内存的典型需求 比如一个原始数据类型的数组，需要24字节的头信息（16字节的对象开销，4字节用于保存长度[数组长度]，以及4填充字节，再加上保存值需要的内存） Date对象需要的：一个含有N 个Date 对象（请见表1.2.12）的数 组需要使用24 字节（数组开销）加上8N 字节（所有引用）加上每个对象的32 字节，总共（24 +40N）字节 【这里说的是需要，和本身存储是两回事】\n![ly-20241212142056395](img/ly-20241212142056395.png) 字符串对象\nString 的标准实现含有4 个实例变量：一个指向字符数组的引用（8 字节）和三 个int 值（各4 字节）。第一个int 值描述的是字符数组中的偏移量，第二个int 值是一个计数器（字符串的长度）。按照图1.4.10 中所示的实例变量名，对象所表示的字符串由value[offset]到value[offset + count - 1] 中的字符组成。String 对象中的第三个int 值是一个散列值，它在某些情况下可以节省一些计算，我们现在可以忽略它。因此，每个String 对象总共会使用40字节（16 字节表示对象，三个int 实例变量各需4 字节，加上数组引用的8 字节和4 个填充字节）\n字符串的值和子字符串\n一个长度为N 的String 对象一般需要使用40 字节（String 对象本身）加上（24+2N）字节（字符数组），总共（64+2N）字节 Java 对字符串的表示希望能够避免复制字符串中的字符 一个子字符串所需的额外内存是一个常数，构造一个子字符串所需的时间也是常数 关于子字符串 展望 # 最重要的是代码正确，其次才是性能 "},{"id":227,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/1.3.3.1-1.3.4/","title":"算法红皮书1.3.3.1-1.3.4","section":"_算法(第四版)_","content":" 背包、队列和栈 # 链表 # 链表是一种递归的数据结构，它或者为空(null)，或者是一个指向一个结点（node）的引用，该节点含有一个泛型的元素和一个指向另一条链表的引用。 结点记录 # 使用嵌套类定义结点的抽象数据类型\nprivate class Node { Item item; Node next; } 该类没有其它任何方法，且会在代码中直接引用实例变量，这种类型的变量称为记录 构造链表 # 需要一个Node类型的变量，保证它的值是null或者指向另一个Node对象的next域指向了另一个链表 如下图 链表表示的是一列元素 链式结构在本书中的可视化表示 长方形表示对象；实例变量的值写在长方形中；用指向被引用对象的箭头表示引用关系 术语链接表示对结点的引用 在表头插入结点 # 在首结点为first 的给定链表开头插入字符串not，我们先将first 保存在oldfirst 中， 然后将一个新结点赋予first，并将它的item 域设为not，next 域设为oldfirst\n时间复杂度为O(1)\n如图 从表头删除结点 # 将first指向first.next\n原先的结点称为孤儿，Java的内存管理系统最终将回收它所占用的内存\n如图 在表尾插入结点 # 每个修改链表的操作都需要增加检查是否要修改该变量（以及做出相应修改）的代码\n例如，当删除链表首结点时可能改变指向链表的尾结点的引用，因为链表中只有一个结点时它既是首结点又是尾结点\n如图 其他位置的插入和删除操作 # 删除指定结点；在指定节点插入新结点\n需要将链表尾结点的前一个节点中的链接（它指向的是last）值改为null 为了找到指向last的结点，需要遍历链表，时间复杂度为O(n) 实现任意插入和删除操作的标准解决方案是双向链表 遍历 # 将x初始化为链表首结点，然后通过x.item访问和x相关联的元素，并将x设为x.next来访问链表中的下一个结点，知道x=null(没有下一个结点了，到达链表结尾)\nfor (Node x = first; x != null; x = x.next) { // 处理x.item } 栈的实现 # 使用链表实现栈\n将栈保存为一条链表，栈的顶部即为表头，实例变量first 指向栈顶。这样，当使用push() 压入一个元素时，我们会按照1.3.3.3 节所讨论的代码将该元素添加在表头；当使用pop() 删除一个元素时，我们会按照1.3.3.4 节讨论的代码将该元素从表头删除。要实现size() 方法，我们用实例变量N 保存元素的个数，在压入元素时将N 加1，在弹出元素时将N 减1。要实现isEmpty() 方法，只需检查first 是否为null（或者可以检查N 是否为0）\n实现上述几个操作的时间复杂度为O(1)\n下压堆栈（链表的实现）\npublic class Stack\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private Node first; // 栈顶（最近添加的元素） private int N; // 元素数量 private class Node { // 定义了结点的嵌套类 Item item; Node next; } public Boolean isEmpty() { return first == null; } // 或：N == 0 public int size() { return N; } public void push(Item item) { // 向栈顶添加元素 Node oldfirst = first; first = new Node(); first.item = item; first.next = oldfirst; N++; } public Item pop() { // 从栈顶删除元素 Item item = first.item; first = first.next; N--; return item; } // iterator() 的实现请见算法1.4 // 测试用例main() 的实现请见本节前面部分 } 测试用例(pop()之前测试用例做了判断)\npublic static void main(String[] args) { // 创建一个栈并根据StdIn中的指示压入或弹出字符串 Stack\u0026lt;String\u0026gt; s = new Stack\u0026lt;String\u0026gt;(); while (!StdIn.isEmpty()) { String item = StdIn.readString(); if (!item.equals(\u0026#34;-\u0026#34;)) s.push(item); else if (!s.isEmpty()) StdOut.print(s.pop() + \u0026#34; \u0026#34;); } StdOut.println(\u0026#34;(\u0026#34; + s.size() + \u0026#34; left on stack)\u0026#34;); } 队列的实现 # 这里维护了first和last两个变量\nQueue实现使用的数据结构和Stack都是链表，但实现了不同的添加和删除元素的算法，所以前者是先入先出，后者是后进先出\nQueue的测试用例\npublic static void main(String[] args) { // 创建一个队列并操作字符串入列或出列 Queue\u0026lt;String\u0026gt; q = new Queue\u0026lt;String\u0026gt;(); while (!StdIn.isEmpty()) { String item = StdIn.readString(); if (!item.equals(\u0026#34;-\u0026#34;)) q.enqueue(item); else if (!q.isEmpty()) StdOut.print(q.dequeue() + \u0026#34; \u0026#34;); } StdOut.println(\u0026#34;(\u0026#34; + q.size() + \u0026#34; left on queue)\u0026#34;); } Queue的测试用例\npublic static void main(String[] args) { // 创建一个队列并操作字符串入列或出列 Queue\u0026lt;String\u0026gt; q = new Queue\u0026lt;String\u0026gt;(); while (!StdIn.isEmpty()) { String item = StdIn.readString(); if (!item.equals(\u0026#34;-\u0026#34;)) q.enqueue(item); else if (!q.isEmpty()) StdOut.print(q.dequeue() + \u0026#34; \u0026#34;); } StdOut.println(\u0026#34;(\u0026#34; + q.size() + \u0026#34; left on queue)\u0026#34;); } Queue的实现\n如下，enqueue()需要额外考虑first，dequeue()需要额外考虑last 如果原队列没有结点，那么增加后last指向了新的元素，应该把first也指向新元素 如果原对队列只有一个元素，那么删除后first确实指向null，而last没有更新，所以需要下面的判断手动更新 public class Queue\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private Node first; // 指向最早添加的结点的链接 private Node last; // 指向最近添加的结点的链接 private int N; // 队列中的元素数量 private class Node { // 定义了结点的嵌套类 Item item; Node next; } public Boolean isEmpty() { return first == null; } // 或： N == 0. public int size() { return N; } public void enqueue(Item item) { // 向表尾添加元素 Node oldlast = last; last = new Node(); last.item = item; last.next = null; if (isEmpty()) first = last; else oldlast.next = last; N++; } public Item dequeue() { // 从表头删除元素 Item item = first.item; first = first.next; if (isEmpty()) last = null; N--; return item; } // iterator() 的实现请见算法1.4 // 测试用例main() 的实现请见前面 } 在结构化数据集时，链表是数组的一种重要替代方法\n背包的实现 # 只需要将Stack中的push()改为add()即可，并去掉pop()\n下面添加了Iterator实现类，以及iterator()具体方法 其中，嵌套类ListIterator 维护了一个实例变量current来记录链表的当前结点。hasNext() 方法会检测current 是否为null，next() 方法会保存当前元素的引用，将current 变量指向链表中的下个结点并返回所保存的引用。\nimport java.util.Iterator; public class Bag\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private Node first; // 链表的首结点 private class Node { Item item; Node next; } public void add(Item item) { // 和Stack 的push() 方法完全相同 Node oldfirst = first; first = new Node(); first.item = item; first.next = oldfirst; } public Iterator\u0026lt;Item\u0026gt; iterator() { return new ListIterator(); } private class ListIterator implements Iterator\u0026lt;Item\u0026gt; { private Node current = first; public Boolean hasNext() { return current != null; } public void remove() { } public Item next() { Item item = current.item; current = current.next; return item; } } } 综述 # 学习了支持泛型和迭代的背包、队列和栈\n现在拥有两种表示对象集合的方式，即数组和链表\u0026mdash;\u0026gt;顺序存储和链式存储\n各种含有多个链接的数据结构，如二叉树的数据结构，由含有两个链接的节点组成 复合型的数据结构：背包存储栈，队列存储数组等，例如用数组的背包表示图 基础数据结构 研究新领域时，按以下步骤识别并使用数据抽象解决问题\n定义API 根据应用场景开发用例代码 描述数据结构（一组值的表示），并在API所对应的抽象数据类型的实现中根据它定义类的实例变量 描述算法（实现一组操作的方式），实现类的实例方法 分析算法的性能特点 本书的数据结构举例 End # "},{"id":228,"href":"/zh/docs/technology/RocketMQ/heima/02buildcluster/","title":"02双主双从集群搭建","section":"基础(黑马)_","content":" 学习来源 https://www.bilibili.com/video/BV1L4411y7mn（添加小部分笔记）感谢作者!\n服务器信息修改 # 在.135和.138均进行下面的操作\n解压 # rocketmq解压到/usr/local/rocketmq目录下\nhost添加 # #添加host vim /etc/hosts ##添加内容 192.168.1.135 rocketmq-nameserver1 192.168.1.138 rocketmq-nameserver2 192.168.1.135 rocketmq-master1 192.168.1.135 rocketmq-slave2 192.168.1.138 rocketmq-master2 192.168.1.138 rocketmq-slave1 ## 保存后 systemctl restart network 防火墙 # 直接关闭 # ## 防火墙关闭 systemctl stop firewalld.service ## 防火墙状态查看 firewall-cmd --state ##禁止开机启动 systemctl disable firewalld.service 或者直接关闭对应端口即可 # 环境变量配置 # 为了执行rocketmq命令方便\n#添加环境变量 vim /etc/profile #添加 ROCKETMQ_HOME=/usr/local/rocketmq/rocketmq-all-4.4.0-bin-release PATH=$PATH:$ROCKETMQ_HOME/bin export ROCKETMQ_HOME PATH #使配置生效 source /etc/profile 消息存储路径创建 # a # mkdir /usr/local/rocketmq/store-a mkdir /usr/local/rocketmq/store-a/commitlog mkdir /usr/local/rocketmq/store-a/consumequeue mkdir /usr/local/rocketmq/store-a/index b # mkdir /usr/local/rocketmq/store-b mkdir /usr/local/rocketmq/store-b/commitlog mkdir /usr/local/rocketmq/store-b/consumequeue mkdir /usr/local/rocketmq/store-b/index 双主双从配置文件的修改 # master-a # #所属集群名字 brokerClusterName=rocketmq-cluster #broker名字，注意此处不同的配置文件填写的不一样 brokerName=broker-a #0 表示 Master，\u0026gt;0 表示 Slave brokerId=0 #nameServer地址，分号分割 namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876 #在发送消息时，自动创建服务器不存在的topic，默认创建的队列数 defaultTopicQueueNums=4 #是否允许 Broker 自动创建Topic，建议线下开启，线上关闭 autoCreateTopicEnable=true #是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭 autoCreateSubscriptionGroup=true #Broker 对外服务的监听端口 listenPort=10911 #删除文件时间点，默认凌晨 4点 deleteWhen=04 #文件保留时间，默认 48 小时 fileReservedTime=120 #commitLog每个文件的大小默认1G mapedFileSizeCommitLog=1073741824 #ConsumeQueue每个文件默认存30W条，根据业务情况调整 mapedFileSizeConsumeQueue=300000 #destroyMapedFileIntervalForcibly=120000 #redeleteHangedFileInterval=120000 #检测物理文件磁盘空间 diskMaxUsedSpaceRatio=88 #存储路径 storePathRootDir=/usr/local/rocketmq/store-a #commitLog 存储路径 storePathCommitLog=/usr/local/rocketmq/store-a/commitlog #消费队列存储路径存储路径 storePathConsumeQueue=/usr/local/rocketmq/store-a/consumequeue #消息索引存储路径 storePathIndex=/usr/local/rocketmq/store-a/index #checkpoint 文件存储路径 storeCheckpoint=/usr/local/rocketmq/store-a/checkpoint #abort 文件存储路径 abortFile=/usr/local/rocketmq/store-a/abort #限制的消息大小 maxMessageSize=65536 #flushCommitLogLeastPages=4 #flushConsumeQueueLeastPages=2 #flushCommitLogThoroughInterval=10000 #flushConsumeQueueThoroughInterval=60000 #Broker 的角色 #- ASYNC_MASTER 异步复制Master #- SYNC_MASTER 同步双写Master #- SLAVE brokerRole=SYNC_MASTER #刷盘方式 #- ASYNC_FLUSH 异步刷盘 #- SYNC_FLUSH 同步刷盘 flushDiskType=SYNC_FLUSH #checkTransactionMessageEnable=false #发消息线程池数量 #sendMessageThreadPoolNums=128 #拉消息线程池数量 #pullMessageThreadPoolNums=128 slave-b # #所属集群名字 brokerClusterName=rocketmq-cluster #broker名字，注意此处不同的配置文件填写的不一样 brokerName=broker-b #0 表示 Master，\u0026gt;0 表示 Slave brokerId=1 #nameServer地址，分号分割 namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876 #在发送消息时，自动创建服务器不存在的topic，默认创建的队列数 defaultTopicQueueNums=4 #是否允许 Broker 自动创建Topic，建议线下开启，线上关闭 autoCreateTopicEnable=true #是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭 autoCreateSubscriptionGroup=true #Broker 对外服务的监听端口 listenPort=11011 #删除文件时间点，默认凌晨 4点 deleteWhen=04 #文件保留时间，默认 48 小时 fileReservedTime=120 #commitLog每个文件的大小默认1G mapedFileSizeCommitLog=1073741824 #ConsumeQueue每个文件默认存30W条，根据业务情况调整 mapedFileSizeConsumeQueue=300000 #destroyMapedFileIntervalForcibly=120000 #redeleteHangedFileInterval=120000 #检测物理文件磁盘空间 diskMaxUsedSpaceRatio=88 #存储路径 storePathRootDir=/usr/local/rocketmq/store-b #commitLog 存储路径 storePathCommitLog=/usr/local/rocketmq/store-b/commitlog #消费队列存储路径存储路径 storePathConsumeQueue=/usr/local/rocketmq/store-b/consumequeue #消息索引存储路径 storePathIndex=/usr/local/rocketmq/store-b/index #checkpoint 文件存储路径 storeCheckpoint=/usr/local/rocketmq/store-b/checkpoint #abort 文件存储路径 abortFile=/usr/local/rocketmq/store-b/abort #限制的消息大小 maxMessageSize=65536 #flushCommitLogLeastPages=4 #flushConsumeQueueLeastPages=2 #flushCommitLogThoroughInterval=10000 #flushConsumeQueueThoroughInterval=60000 #Broker 的角色 #- ASYNC_MASTER 异步复制Master #- SYNC_MASTER 同步双写Master #- SLAVE brokerRole=SLAVE #刷盘方式 #- ASYNC_FLUSH 异步刷盘 #- SYNC_FLUSH 同步刷盘 flushDiskType=ASYNC_FLUSH #checkTransactionMessageEnable=false #发消息线程池数量 #sendMessageThreadPoolNums=128 #拉消息线程池数量 #pullMessageThreadPoolNums=128 master-b # #所属集群名字 brokerClusterName=rocketmq-cluster #broker名字，注意此处不同的配置文件填写的不一样 brokerName=broker-b #0 表示 Master，\u0026gt;0 表示 Slave brokerId=0 #nameServer地址，分号分割 namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876 #在发送消息时，自动创建服务器不存在的topic，默认创建的队列数 defaultTopicQueueNums=4 #是否允许 Broker 自动创建Topic，建议线下开启，线上关闭 autoCreateTopicEnable=true #是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭 autoCreateSubscriptionGroup=true #Broker 对外服务的监听端口 listenPort=10911 #删除文件时间点，默认凌晨 4点 deleteWhen=04 #文件保留时间，默认 48 小时 fileReservedTime=120 #commitLog每个文件的大小默认1G mapedFileSizeCommitLog=1073741824 #ConsumeQueue每个文件默认存30W条，根据业务情况调整 mapedFileSizeConsumeQueue=300000 #destroyMapedFileIntervalForcibly=120000 #redeleteHangedFileInterval=120000 #检测物理文件磁盘空间 diskMaxUsedSpaceRatio=88 #存储路径 storePathRootDir=/usr/local/rocketmq/store-b #commitLog 存储路径 storePathCommitLog=/usr/local/rocketmq/store-b/commitlog #消费队列存储路径存储路径 storePathConsumeQueue=/usr/local/rocketmq/store-b/consumequeue #消息索引存储路径 storePathIndex=/usr/local/rocketmq/store-b/index #checkpoint 文件存储路径 storeCheckpoint=/usr/local/rocketmq/store-b/checkpoint #abort 文件存储路径 abortFile=/usr/local/rocketmq/store-b/abort #限制的消息大小 maxMessageSize=65536 #flushCommitLogLeastPages=4 #flushConsumeQueueLeastPages=2 #flushCommitLogThoroughInterval=10000 #flushConsumeQueueThoroughInterval=60000 #Broker 的角色 #- ASYNC_MASTER 异步复制Master #- SYNC_MASTER 同步双写Master #- SLAVE brokerRole=SYNC_MASTER #刷盘方式 #- ASYNC_FLUSH 异步刷盘 #- SYNC_FLUSH 同步刷盘 flushDiskType=SYNC_FLUSH #checkTransactionMessageEnable=false #发消息线程池数量 #sendMessageThreadPoolNums=128 #拉消息线程池数量 #pullMessageThreadPoolNums=128 slave-a # #所属集群名字 brokerClusterName=rocketmq-cluster #broker名字，注意此处不同的配置文件填写的不一样 brokerName=broker-a #0 表示 Master，\u0026gt;0 表示 Slave brokerId=1 #nameServer地址，分号分割 namesrvAddr=rocketmq-nameserver1:9876;rocketmq-nameserver2:9876 #在发送消息时，自动创建服务器不存在的topic，默认创建的队列数 defaultTopicQueueNums=4 #是否允许 Broker 自动创建Topic，建议线下开启，线上关闭 autoCreateTopicEnable=true #是否允许 Broker 自动创建订阅组，建议线下开启，线上关闭 autoCreateSubscriptionGroup=true #Broker 对外服务的监听端口 listenPort=11011 #删除文件时间点，默认凌晨 4点 deleteWhen=04 #文件保留时间，默认 48 小时 fileReservedTime=120 #commitLog每个文件的大小默认1G mapedFileSizeCommitLog=1073741824 #ConsumeQueue每个文件默认存30W条，根据业务情况调整 mapedFileSizeConsumeQueue=300000 #destroyMapedFileIntervalForcibly=120000 #redeleteHangedFileInterval=120000 #检测物理文件磁盘空间 diskMaxUsedSpaceRatio=88 #存储路径 storePathRootDir=/usr/local/rocketmq/store-a #commitLog 存储路径 storePathCommitLog=/usr/local/rocketmq/store-a/commitlog #消费队列存储路径存储路径 storePathConsumeQueue=/usr/local/rocketmq/store-a/consumequeue #消息索引存储路径 storePathIndex=/usr/local/rocketmq/store-a/index #checkpoint 文件存储路径 storeCheckpoint=/usr/local/rocketmq/store-a/checkpoint #abort 文件存储路径 abortFile=/usr/local/rocketmq/store-a/abort #限制的消息大小 maxMessageSize=65536 #flushCommitLogLeastPages=4 #flushConsumeQueueLeastPages=2 #flushCommitLogThoroughInterval=10000 #flushConsumeQueueThoroughInterval=60000 #Broker 的角色 #- ASYNC_MASTER 异步复制Master #- SYNC_MASTER 同步双写Master #- SLAVE brokerRole=SLAVE #刷盘方式 #- ASYNC_FLUSH 异步刷盘 #- SYNC_FLUSH 同步刷盘 flushDiskType=ASYNC_FLUSH #checkTransactionMessageEnable=false #发消息线程池数量 #sendMessageThreadPoolNums=128 #拉消息线程池数量 #pullMessageThreadPoolNums=128 修改两台主机的runserver.sh及runbroker.sh修改 # 修改runbroker.sh # JAVA_OPT=\u0026#34;${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m\u0026#34; 修改runserver.sh # JAVA_OPT=\u0026#34;${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m\u0026#34; 两台主机分别启动nameserver和Brocker # ## 在两台主机分别启动nameserver nohup sh mqnamesrv \u0026amp; #135启动master1 nohup sh mqbroker -c /usr/local/rocketmq/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-a.properties \u0026amp; #135启动slave2 nohup sh mqbroker -c /usr/local/rocketmq/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-b-s.properties \u0026amp; #查看 jps 3478 Jps 3366 BrokerStartup 3446 BrokerStartup 3334 NamesrvStartup #138启动master2 nohup sh mqbroker -c /usr/local/rocketmq/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-b.properties \u0026amp; #135启动slave1 nohup sh mqbroker -c /usr/local/rocketmq/rocketmq-all-4.4.0-bin-release/conf/2m-2s-sync/broker-a-s.properties \u0026amp; #查看 jps 3376 Jps 3360 BrokerStartup 3251 NamesrvStartup 3295 BrokerStartup 双主双从集群搭建完毕！\n集群管理工具 # mqadmin # cd bin 进入bin目录：mqadmin，对下面的信息进行操作\ntopic、集群、Broker、消息、消费者组、生产者组、连接相关、namesrv相关、其他\nrocket-console # //已经弃用，现在改用rocketmq-dashboard\nhttps://github.com/apache/rocketmq-dashboard\n"},{"id":229,"href":"/zh/docs/technology/RocketMQ/heima/01base/","title":"01rocketmq学习","section":"基础(黑马)_","content":" 学习来源 https://www.bilibili.com/video/BV1L4411y7mn（添加小部分笔记）感谢作者!\n基本操作 # 下载 # https://rocketmq.apache.org/download/ 选择Binary下载即可，放到Linux主机中\n前提java运行环境 # yum search java | grep jdk yum install -y java-1.8.0-openjdk-devel.x86_64 # java -version 正常 # javac -version 正常 启动 # #nameserver启动 nohup sh bin/mqnamesrv \u0026amp; #nameserver日志查看 tail -f ~/logs/rocketmqlogs/namesrv.log #输出 2023-04-06 00:08:34 INFO main - tls.client.certPath = null 2023-04-06 00:08:34 INFO main - tls.client.authServer = false 2023-04-06 00:08:34 INFO main - tls.client.trustCertPath = null 2023-04-06 00:08:35 INFO main - Using OpenSSL provider 2023-04-06 00:08:35 INFO main - SSLContext created for server 2023-04-06 00:08:36 INFO NettyEventExecutor - NettyEventExecutor service started 2023-04-06 00:08:36 INFO main - The Name Server boot success. serializeType=JSON 2023-04-06 00:08:36 INFO FileWatchService - FileWatchService service started 2023-04-06 00:09:35 INFO NSScheduledThread1 - -------------------------------------------------------- 2023-04-06 00:09:35 INFO NSScheduledThread1 - configTable SIZE: 0 #broker启动 nohup sh bin/mqbroker -n localhost:9876 \u0026amp; #查看broker日志 tail -f ~/logs/rocketmqlogs/broker.log #日志如下 tail: 无法打开\u0026#34;/root/logs/rocketmqlogs/broker.log\u0026#34; 读取数据: 没有那个文件或目录 tail: 没有剩余文件 👇 #jps查看 2465 Jps 2430 NamesrvStartup #说明没有启动成功,因为默认配置的虚拟机内存较大 vim bin/runbroker.sh 以及 vim runserver.sh #修改 JAVA_OPT=\u0026#34;${JAVA_OPT} -server -Xms8g -Xmx8g -Xmn4g\u0026#34; #修改为 JAVA_OPT=\u0026#34;${JAVA_OPT} -server -Xms256m -Xmx256m -Xmn128m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=320m\u0026#34; #修改完毕后启动 #先关闭namesrv后 #按上述启动namesrv以及broker sh bin/mqshutdown namesrv # jsp命令查看进程 2612 Jps 2551 BrokerStartup 2524 NamesrvStartup 测试 # 同一台机器上，两个cmd窗口\n发送端 # #配置namesrv为环境变量 export NAMESRV_ADDR=localhost:9876 #运行程序（发送消息） sh bin/tools.sh org.apache.rocketmq.example.quickstart.Producer #结果 SendResult [sendStatus=SEND_OK, msgId=C0A801640B012503DBD319DEF7D203E1, offsetMsgId=C0A8010300002A9F0000000000057878, messageQueue=MessageQueue [topic=TopicTest, brokerName=rheCentos700, queueId=3], queueOffset=498] SendResult [sendStatus=SEND_OK, msgId=C0A801640B012503DBD319DEF7D803E2, offsetMsgId=C0A8010300002A9F000000000005792C, messageQueue=MessageQueue [topic=TopicTest, brokerName=rheCentos700, queueId=0], queueOffset=498] SendResult [sendStatus=SEND_OK, msgId=C0A801640B012503DBD319DEF7DB03E3, offsetMsgId=C0A8010300002A9F00000000000579E0, messageQueue=MessageQueue [topic=TopicTest, brokerName=rheCentos700, queueId=1], queueOffset=498] 接收端 # #配置namesrv为环境变量 export NAMESRV_ADDR=localhost:9876 #运行程序（发送消息） sh bin/tools.sh org.apache.rocketmq.example.quickstart.Consumer #结果 ConsumeMessageThread_5 Receive New Messages: [MessageExt [queueId=0, storeSize=180, queueOffset=499, sysFlag=0, bornTimestamp=1680712442864, bornHost=/192.168.1.3:45716, storeTimestamp=1680712442878, storeHost=/192.168.1.3:10911, msgId=C0A8010300002A9F0000000000057BFC, commitLogOffset=359420, bodyCRC=1359908749, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic=\u0026#39;TopicTest\u0026#39;, flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=500, CONSUME_START_TIME=1680712442881, UNIQ_KEY=C0A801640B012503DBD319DEF7F003E6, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 57, 57, 56], transactionId=\u0026#39;null\u0026#39;}]] ConsumeMessageThread_2 Receive New Messages: [MessageExt [queueId=1, storeSize=180, queueOffset=499, sysFlag=0, bornTimestamp=1680712442879, bornHost=/192.168.1.3:45716, storeTimestamp=1680712442883, storeHost=/192.168.1.3:10911, msgId=C0A8010300002A9F0000000000057CB0, commitLogOffset=359600, bodyCRC=638172955, reconsumeTimes=0, preparedTransactionOffset=0, toString()=Message{topic=\u0026#39;TopicTest\u0026#39;, flag=0, properties={MIN_OFFSET=0, MAX_OFFSET=500, CONSUME_START_TIME=1680712442889, UNIQ_KEY=C0A801640B012503DBD319DEF7FF03E7, WAIT=true, TAGS=TagA}, body=[72, 101, 108, 108, 111, 32, 82, 111, 99, 107, 101, 116, 77, 81, 32, 57, 57, 57], transactionId=\u0026#39;null\u0026#39;}]] RocketMQ基本架构 # 简单解释 # nameserver：broker的管理者 broker：自己找nameserer上报 broker：真正存储消息的地方 nameserver是无状态的，即nameserver之间不用同步broker信息，由broker自己上报 Producer集群之间也不需要同步；Consumer集群之间也不需要同步 BrokerMaster和BrokerSlave之间信息是有同步的 如图 # "},{"id":230,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/1.3.1.1-1.3.2.5/","title":"算法红皮书 1.3.1.1-1.3.2.5","section":"_算法(第四版)_","content":" 背包、队列和栈 # 数据类型的值就是一组对象的集合，所有操作都是关于添加、删除或是访问集合中的对象 本章将学习三种数据类型：背包Bag、队列Queue、栈Stack 对集合中的对象的表示方式直接影响各种操作的效率 介绍泛型和迭代 介绍并说明链式数据结构的重要性(链表) API # 泛型可迭代的基础集合数据类型的API\n背包\n队列(先进先出FIFO)\n下压(后进先出,LIFO)栈 泛型\n泛型，参数化类型 在每份API 中，类名后的\u0026lt;Item\u0026gt; 记号将Item 定义为一个类型参数，它是一个象征性的占位符，表示的是用例将会使用的某种具体数据类型 自动装箱\n用来处理原始类型 Boolean、Byte、Character、Double、Float、Integer、Long 和Short 分别对应着boolean、byte、char、double、float、int、long 和short 自动将一个原始数据类型转换为一个封装类型称为自动装箱，自动将一个封装类型转换为一个原始数据类型被称为自动拆箱 可迭代的集合类型\n迭代访问集合中的所有元素 背包是一种不支持从中删除元素的集合数据类型\u0026ndash;帮助用例收集元素并迭代遍历所有收集到的元素（无序遍历）\n典型用例，计算标准差\n先进先出队列\n是一种基于先进先出(FIFO)策略的集合类型 使用队列的主要原因：集合保存元素的同时保存它们的相对顺序 如图\nQueue用例(先进先出) 下压栈\n简称栈，是一种基于后进先出LIFO策略的集合类型 比如，收邮件等，如图\nStack的用例\n用栈解决算数表达式的问题\n（双栈算数表达式求值算法）\n集合类数据类型的实现 # 定容栈，表示容量固定的字符串栈的抽象数据类型\n只能处理String值，支持push和pop\n抽象数据类型\n测试用例\n使用方法\n数据类型的实现\n泛型\npublic class FixedCapacityStack\u0026lt;Item\u0026gt; 由于不允许直接创建泛型数组，所以 a =new Item[cap] 不允许，应该改为\na=(Item[])new Object[cap]; 泛型定容栈的抽象数据类型\n测试用例\n使用方法\n数据类型的实现\n调整数组大小\nN为当前元素的数量\n使用resize创建新的数组\n当元素满了的时候进行扩容\n当元素过少(1/4)的时候，进行减半\n对象游离\nJava的垃圾回收策略是回收所有无法被访问的对象的内存\n示例中，被弹出的元素不再需要，但由于数组中的引用仍然让它可以继续存在（垃圾回收器无法回收），这种情况（保存了一个不需要的对象的引用）称为游离，避免游离的做法就是将数组元素设为null\n迭代\nforeach和while\n集合数据类型必须实现iterator()并返回Iterator对象 Iterator类必须包括两个方法,hasNext()和next() 让类继承Iterable\u0026lt;Item\u0026gt;使类可迭代 使用一个嵌套类\n下压栈的代码\nimport java.util.Iterator; public class ResizingArrayStack\u0026lt;Item\u0026gt; implements Iterable\u0026lt;Item\u0026gt; { private\tItem[] a = (Item[]) new Object[1]; /* 栈元素 */ private int\tN = 0; /* 元素数量 */ public boolean isEmpty() { return(N == 0); } public int size() { return(N); } private void resize( int max ) { /* 将栈移动到一个大小为max 的新数组 */ Item[] temp = (Item[]) new Object[max]; for ( int i = 0; i \u0026lt; N; i++ ) temp[i] = a[i]; a = temp; } public void push( Item item ) { /* 将元素添加到栈顶 */ if ( N == a.length ) resize( 2 * a.length ); a[N++] = item; } public Item pop() { /* 从栈顶删除元素 */ Item item = a[--N]; a[N] = null; /* 避免对象游离（请见1.3.2.4 节） */ if ( N \u0026gt; 0 \u0026amp;\u0026amp; N == a.length / 4 ) resize( a.length / 2 ); return(item); } public Iterator\u0026lt;Item\u0026gt; iterator() { return(new ReverseArrayIterator() ); } private class ReverseArrayIterator implements Iterator\u0026lt;Item\u0026gt; { /* 支持后进先出的迭代 */ private int i = N; public boolean hasNext() { return(i \u0026gt; 0); } public Item next() { return(a[--i]); } public void remove() { } } } End # "},{"id":231,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/1.2.1-1.2.5/","title":"算法红皮书 1.2.1-1.2.5","section":"_算法(第四版)_","content":" 数据抽象 # 数据类型指的是一组值和一组对这些值的操作的集合\n定义和使用数据类型的过程，也被称为数据抽象 Java编程的基础是使用class关键字构造被称为引用类型的数据类型，也称面向对象编程 定义自己的数据类型来抽象任意对象 抽象数据类型（ADT）是一种能够对使用者隐藏数据表示的数据类型 抽象数据类型将数据和函数的实现相关联，将数据的表示方式隐藏起来 抽象数据类型使用时，关注API描述的操作上而不会去关心数据的表示；实现抽象数据类型时，关注数据本身并将实现对数据的各种操作 研究同一个问题的不同算法的主要原因是他们的性能不同 使用抽象数据类型 # 使用一种数据类型并不一定非得知道它是如何实现的 使用Counter(计数器)的简单数据类型的程序，操作有 创建对象并初始化为0 当前值加1 获取当前值 场景，用于电子计票 抽象数据类型的API(应用程序编程接口) API用来说明抽象数据类型的行为 将列出所有构造函数和实例方法(即操作) 计算器的API\n继承的方法 所有数据类型都会继承toString()方法 Java会在用+运算符将任意数据类型的值和String值连接时调用toString() 默认实现：返回该数据类型值的内存地址 用例代码 可以在用例代码中，声明变量、创建对象来保存数据类型的值并允许通过实例方法来操作它们 对象 对象是能够承载数据类型的值的实体 对象三大特性：状态、标识和行为 状态：数据类型中的值 标识：在内存中的地址 行为：数据类型的操作 Java使用\u0026quot;引用类型\u0026quot;和原始数据类型区别 创建对象 每种数据类型中的值都存储于一个对象中 构造函数总是返回他的数据类型的对象的引用 使用new()，会为新的对象分配内存空间，调用构造函数初始化对象中的值，返回该对象的一个引用 抽象数据类型向用例隐藏了值的表示细节 实例方法：参数按值传递 方法每次触发都和一个对象相关 静态方法的主要作用是实现函数；非静态(实例)方法的主要作用是实现数据类型的操作 使用对象\n开发某种数据类型的用例 声明该类型的变量，以引用对象 使用new触发能够创建该类型的对象的一个构造函数 使用变量名调用实例方法 赋值语句(对象赋值) 别名：两个变量同时指向同一个对象 将对象作为参数 Java将参数值的一个副本从调用端传递给了方法，这种方式称为按值传递 当使用引用类型作为参数时我们创建的都是别名，这种约定会传递引用的值(复制引用)，也就是传递对象的引用 虽然无法改变原始的引用(将原变量指向另一个Counter对象)，但能够改变该对象的值 将对象作为返回值 由于Java只由一个返回值，有了对象实际上就能返回多个值 数组也是对象 将数组传递给一个方法或是将一个数组变量放在赋值语句的右侧时，我们都是在创建数组引用的一个副本，而非数组的副本 对象的数组\n创建一个对象的数组 使用方括号语法调用数组的构造函数创建数组 对于每个数组元素调用它的构造函数创建相应的对象\n如下图\n运用数据抽象的思想编写代码(定义和使用数据类型，将数据类型的值封装在对象中)的方式称为面向对象编程 总结 数据类型指的是一组值和一组对值的操作的集合 我们会在数据类型的Java类中编写用理 对象是能够存储任意该数据类型的值的实体 对象有三个关键性质：状态、标识和行为 抽象数据类型举例 # 本书中将会用到或开发的所有数据类型 java.lang.* Java标准库中的抽象数据类型，需要import，比如java.io、java.net等 I/O处理嘞抽象数据类型,StdIn和StdOut 面向数据类抽象数据类型，计算机和和信息处理 集合类抽象数据类型，主要是为了简化对同一类型的一组数据的操作，包括Bag、Stack和Queue，PQ(优先队列)、ST(符号表)、SET(集合) 面向操作的抽象数据类型(用来分析各种算法) 图算法相关的抽象数据类型，用来封装各种图的表示的面向数据的抽象数据类型，和一些提供图的处理算法的面向操作的抽象数据类型 几何对象(画图(图形)的)[跳过] 信息处理 抽象数据类型是组织信息的一种自然方式 定义和真实世界中的物体相对应的对象 字符串 java的String 一个String值是一串可以由索引访问的char值 有了String类型可以写出清晰干净的用例代码而无需关心字符串的表示方式 抽象数据类型的实现 # 使用Java的类(class)实现抽象数据类型并将所有代码放入一个和类名相同并带有.java扩展名的文件 如下图\n实例变量\n用来定义数据类型的值(每个对象的状态) 构造函数 每个Java类都至少有一个构造函数以创建一个对象的标识 每个构造函数将创建一个对象并向调用者返回一个该对象的引用 实例方法 如图\n作用域 参数变量、局部变量、实例变量 范围(如图)\nAPI、用例与实现 我们要学习的每个抽象数据类型的实现，都会是一个含有若干私有实例变量、构造函数、实例方法和一个测试用例的Java类 用例和实现分离(一般将用例独立成含有静态方法main()的类) 做法如下 定义一份API，APi的作用是将使用和实现分离，以实现模块化编程 用一个Java类实现API的定义 实现多个测试用例来验证前两步做出的设计决定 例子如下 API\n典型用例\n数据类型的实现\n使用方法(执行程序)\n更多抽象数据类型的实现 # 日期 两种实现方式\n本书反复出现的主题，即理解各种实现对空间和时间的需求 维护多个实现 比较同一份API的两种实现在同一个用例中的性能表现，需要下面非正式的命名约定 使用前缀的描述性修饰符，比如BasicDate和SmallDate,以及是否合法的SmartDate 适合大多数用力的需求的实现，比如Date 累加器 数据类型的设计 # 抽象数据类型是一种向用例隐藏内部表示的数据类型 封装(数据封装) 设计APi 算法与抽象数据类型 能够准确地说明一个算法的目的及其他程序应该如何使用该算法 每个Java程序都是一组静态方法和(或)一种数据类型的实现的集合 本书中关注的是抽象数据类型的实现中的操作和向用例隐藏其中的数据表示 例子，将二分法封装 API\n典型的用例\n数据类型的实现\n接口继承 Java语言为定义对象之间的关系提供了支持，称为接口 接口继承使得我们的程序能够通过调用接口中的方法操作实现该接口的任意类型的对象 本书中使用到的接口\n继承 由Object类继承得到的方法\n继承toString()并自定义 封装类型(内置的引用类型，包括Boolean、Byte、Character、Double、Float、Integer、Long和Short) 等价性 如图\n例子，在Date中重写equals\n内存管理\nJava具有自动内存管理，通过记录孤儿对象并将它们的内存释放到内存池中 不可变性\n使用final保证数据不可变\n使用final修饰的引用类型，不能再引用(指向)其他对象，但对象本身的值可改变 契约式设计 Java语言能够在程序运行时检测程序状态 异常(Exception)+断言(Assertion) 异常与错误\n允许抛出异常或抛出错误 断言\n程序不应该依赖断言 End # "},{"id":232,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/1.1.6-1.1.11/","title":"算法红皮书 1.1.6-1.1.11","section":"_算法(第四版)_","content":" 基础编程模型 # 静态方法 # 本书中所有的Java程序要么是数据类型的定义，要么是一个静态方法库 当讨论静态方法和实体方法共有的属性时，我们会使用不加定语的方法一词 方法需要参数(某种数据类型的值)并根据参数计算出某种数据类型的返回值(例如数学函数的结果)或者产生某种副作用(例如打印一个值) 静态方法由签名(public static 以及函数的返回值,方法名及一串参数)和函数体组成 调用静态方法(写出方法名并在后面的括号中列出数值) 方法的性质 方法的参数按值传递，方法中使用的参数变量能够引用调用者的参数并改变其内容(只是不能改变原数组变量本身) 方法名可以被重载 方法只能返回一个值，但能包含多个返回语句 方法可以产生副作用 递归：方法可以调用自己 可以使用数学归纳法证明所解释算法的正确性,编写递归重要的三点 递归总有一个最简单的情况(方法第一条总包含return的条件语句) 递归调用总是去尝试解决一个规模更小的子问题 递归调用的父问题和尝试解决的子问题之间不应该由交集 如下图中，两个子问题各自操作的数组部分是不同的\n基础编程模型 静态方法库是定义在一个Java类中的一组静态方法 Java开发的基本模式是编写一个静态方法库(包含一个main()方法)类完成一个任务 在本书中，当我们提到用于执行一项人物的Java程序时，我们指的就是用这种模式开发的代码(还包括对数据类型的定义) 模块化编程 通过静态方法库实现了模块化编程 一个库中的静态方法也能够调用另一个库中定义的静态方法 单元测试 Java编程最佳实践之一就是每个静态方法库中都包含一个main()函数来测试库中所有的方法 本书中使用main()来说明模块的功能并将测试用例留作练习 外部库 系统标准库 java.lang.*:包括Math库;String和StringBuilder库 导入的系统库 java.util.Arrays 本书中其他库 本书使用了作者开发的标准库Std* API # 模块化编程重要组成部分，记录库方法的用法并供其他人参考的文档 会统一使用应用程序编程接口API的方法列出每个库方法、签名及简述 用例(调用另一个库中的方法的程序)，实现(实现了某个API方法的Java代码) 作者自己的两个库，一个扩展Math.random(),一个支持各种统计 随机静态方法库(StdRandom)的API\n数据分析方法库(StdStats)的API\nStdRandom库中的静态方法的实现 编写自己的库 编写用例，实现中将计算过程分解 明确静态方法库和与之对应的API 实现API和一个能够对方法进行独立测试的main()函数 API的目的是将调用和实现分离 字符串 # 字符串拼接，使用 + 类型转换(将用户从键盘输入的内容转换成相应数据类型的值以及将各种数据类型的值转换成能够在屏幕上显示的值)\n如果数字跟在+后面，那么会将数据类型的值自动转换为字符串 命令行参数 Java中字符串的存在，使程序能够接收到从命令行传递来的信息 当输入命令java和一个库名及一系列字符串后，Java系统会调用库的main()方法并将后面的一系列字符串变成一个数组作为参数传递给它 输入输出 # Java程序可以从命令行参数或者一个名为标准输入流的抽象字符流中获得输入，并将输出写入另一个名为标准输出流的字符流中 默认情况下，命令行参数、标准输入和标准输出是和应用程序绑定的，而应用程序是由能够接受命令输入的操作系统或是开发环境所支持 使用终端来指代这个应用程序提供的供输入和显示的窗口,如图\n命令和参数 终端窗口包含一个提示符，通过它我们能够向操作系统输入命令和参数 操作系统常用命令\n标准输出 StdOut库的作用是支持标准输出 标准输出库的静态方法的API\n格式化输出 字符%并紧跟一个字符表示的转换代码(包括d,f和s)。%和转换代码之间可以插入证书表示值的宽度，且转换后会在字符串左边添加空格以达到需要的宽度。负数表示空格从右边加 宽度后用小数点及数值可以指定精度(或String字符串所截取的长度) 格式中转换代码和对应参数的数据类型必须匹配 标准输入 StdIn库从标准输入流中获取数据，然后将标准输出定向到终端窗口 标准输入流最重要的特点，这些值会在程序读取后消失 例子\n标准输入库中的静态方法API\n重定向和管道 将标准输出重定向到一个文件 java RandomSeq 1000 100.0 200.0 \u0026gt; data.txt 从文件而不是终端应用程序中读取数据 java Average \u0026lt; data.txt 将一个程序的输出重定向为另一个程序的输入，叫做管道 java RandomSeq 1000 100.0 200.0 | java Average 突破了我们能够处理的输入输出流的长度限制 即使计算机没有足够的空间来存储十亿个数， 我们仍然可以将例子中的1000 换成1 000 000 000 （当然我们还是需要一些时间来处理它们）。当RandomSeq 调用StdOut.println() 时，它就向输出流的末尾添加了一个字符串；当Average 调用StdIn.readInt() 时，它就从输入流的开头删除了一个字符串。这些动作发生的实际顺序取决于操作系统 命令行的重定向及管道\n基于文件的输入输出 In和Out库提供了一些静态方法,来实现向文件中写入或从文件中读取一个原始数据类型的数组的抽象 用于读取和写入数组的静态方法的API\n标准绘图库(基本方法和控制方法)\u0026ndash;这里跳过 二分查找 # 如图，在终端接收需要判断的数字，如果不存在于白名单(文件中的int数组)中则输出 开发用例以及使用测试文件(数组长度很大的白名单) 模拟实际情况来展示当前算法的必要性，比如 将客户的账号保存在一个文件中，我们称它为白名单； 从标准输入中得到每笔交易的账号； 使用这个测试用例在标准输出中打印所有与任何客户无关的账号，公司很可能拒绝此类交易。 使用顺序查找 public static int rank(int key, int[] a) { for (int i = 0; i \u0026lt; a.length; i++) if (a[i] == key) return i; return -1; } 当处理大量输入的时候，顺序查找的效率极其低 展望 # 下一节，鼓励使用数据抽象，或称面向对象编程，而不是操作预定义的数据类型的静态方法 使用数据抽象的好处 复用性 链式数据结构比数组更灵活 可以准确地定义锁面对的算法问题 1.1 End # "},{"id":233,"href":"/zh/docs/technology/Other/pc_base/","title":"电脑基础操作","section":"其他","content":"\n"},{"id":234,"href":"/zh/docs/technology/Algorithm/algorithhms_4th/1.1.1-1.1.5/","title":"算法红皮书 1.1.1-1.1.5","section":"_算法(第四版)_","content":" 基础编程模型 # Java程序的基本结构 # 本书学习算法的方法：用Java编程语言编写的程序来实现算法(相比用自然语言有很多优势) 劣势：编程语言特定，使算法的思想和实现细节变得困难(所以本书尽量使用大部分语言都必须的语法) 把描述和实现算法所用到的语言特性、软件库和操作系统特定总称为基础编程模型 Java程序的基本结构 一段Java程序或者是一个静态方法库，或者定义了一个数据类型，需要用到的语法\n原始数据类型(在计算机中精确地定义整数浮点数布尔值等) 语句(创建变量并赋值，控制运行流程或引发副作用来进行计算，包括声明、赋值、条件、循环、调用和返回) 数组(多个同种数据类型值的集合) 静态方法(封装并重用代码) 字符串(一连串的字符，内置一些对他们的操作) 标准输入/输出(是程序与外界联系的桥梁) 数据抽象(数据抽象封装和重用代码，可以定义非原始数据类型，进而面向对象编程) 把这种输入命令执行程序的环境称为 虚拟终端\n要执行一条Java程序，需要先用javac命令编译，然后用java命令运行，比如下面的文件，需要使用命令\njavac BinarySearch.java java BinarySearch 原始数据类型与表达式 # 数据类型就是一组数据和其所能进行的操作的集合 Java中最基础的数据类型(整型int，双精度实数类型double,布尔值boolean,字符型char) Java程序控制用标识符命名的变量 对于原始类型，用标识符引用变量，+-*/指定操作，用字面量来表示值(如1或3.14),用表达式表示对值的操作( 表达式:(x+2.334)/2 ) 只要能够指定值域和在此值域上的操作，就能定义一个数据类型(很像数学上函数的定义) +-*/是被重载过的 运算产生的数据的数据类型和参与运算的数据的数据类型是相同的(5/3=1,5.0/3.0=1.6667等) 如下图(图歪了亿点点..) 表达式 表达式具有优先级，Java使用的是中缀表达式(一个字面量紧接运算符，然后是另一个字面量)。逻辑运算中优先级 ! \u0026amp;\u0026amp; || ,运算符中 * / % 高于+ - 。括号能改变这些规则。代码中尽量使用括号消除对优先级的依赖 类型转换 数值会自动提升为高级数据类型，如1+2.5 1会被先转为double 1.0，值也为double的3.5 强转(把类型名放在括号里讲其转换为括号中的类型) 讲高级数据类型转为低级可能会导致精度的缺失，尽量少使用 比较 ==、!=、\u0026lt;、\u0026lt;=、\u0026gt;、\u0026gt;=，这些运算符称为 混合类型运算符，因为结果是布尔型而不是参与比较的数据类型 结果是布尔型的表达式称为布尔表达式 其他原始类型(int为32位，double为64位) long,64位整数 short,16位整数 char,16位字符 byte,8位整数 32位单精度实数,float 语句 # 语句用来创建和操作变量、对变量赋值并控制操作的执行流程 包括声明语句、赋值语句、条件语句、循环语句、调用和返回语句 声明：让一个变量名和一个类型在编译时关联起来 赋值：将(由一个表达式定义的)某个数据类型额值和一个变量关联起来 条件语句： if (\u0026lt;boolean expression\u0026gt;) { \u0026lt;block statement\u0026gt; } 循环语句 while(\u0026lt;boolean expression\u0026gt;) { \u0026lt;block statement\u0026gt; } 其中循环语句中的代码段称为循环体 break与continue语句 break，立即退出循环 continue，立即开始下一轮循环 简便记法 # 声明并初始化 隐式赋值 ++i;\u0026ndash;i i/=2;i+=1 单语句代码段(省略if/while代码段的花括号) for语句 for(\u0026lt;initialize\u0026gt;;\u0026lt;boolean expression\u0026gt;;\u0026lt;increment\u0026gt;) { \u0026lt;block statements\u0026gt; } 这段代码等价于后面的 \u0026lt;initialize\u0026gt;; while(\u0026lt;boolean expression\u0026gt;) { \u0026lt;block statments\u0026gt; \u0026lt;increment\u0026gt;; } java语句总结\n数组 # 数组能够存储相同类型的多个数据 N个数组的数组编号为0至N-1；这种数组在Java中称为一维数组 创建并初始化数组 需要三个步骤，声明数组名字和类型，创建数组，初始化数组元素 声明并初始化一个数组\n简化写法\ndouble[] a = new double[N]; 使用数组(访问的索引小于0或者大于N-1时会抛出ArrayIndexOutOfBoundsException) 典型的数组处理代码\n起别名 下面的情况并没有将数组新复制一份，而是a，b指向了同一个数组\n二维数组 Java中二维数组就是一堆数组的数组 二维数组可以是参差不齐，比如a[0]=new double[5],a[1]=new double[6]之类 二维数组的创建及初始化 double[][] a; a = new double[M][N]; for (int i = 0; i \u0026lt; M; i++) for (int j = 0; j \u0026lt; N; j++) a[i][j] = 0.0; 精简后的代码 double[][] a=new double[M][N]; "},{"id":235,"href":"/zh/docs/technology/Linux/hanshunping/01-06/","title":"linux_韩老师_01-06","section":"韩顺平老师_","content":" 基础介绍 # 本套课程内容\n基础篇: linux入门、vm和Linux的安装、linux目录结构 实操篇 远程登录（xshell，xftp）、实用指令、进程管理、用户管理 vi和vim编辑器、定时任务调度、RPM和YUM 开机、重启和用户登录注销、磁盘分区及挂载、网络配置 linux使用的地方 在linux下开发项目(需要把javaee项目部署到linux下运行) linux运维工程师(服务器规划、优化、监控等) linux嵌入式工程师(linux下驱动开发[c,c++]) linux应用领域 个人桌面 服务器(免费稳定高效) 嵌入式领域(对软件裁剪，内核最小可达几百kb等) linux介绍 # linux是一个开源免费操作系统 linux吉祥物\ntux(/tu\u0026rsquo;ks/唾可si)，没找到音标，将就一下\nlinux之父，linus，也是git的创作者\n主要发行版：Ubuntu、RedHat，Centos，Debian等\nRedHat和Centos使用同样的源码，但是RedHat收费 Linux和Unix的关系\nunix也是一个操作系统，贝尔实验室。做一个多用户分时操作系统， multics，但是没完成。其中一个后来在这基础上，完成的操作系统为unix （原本是B语言写的），后面和另一个人用unix用c语言改写了。\nunix源码是公开的，后面商业公司拿来包装做成自己的系统， 后面有个人提倡自由时代用户应该对源码享有读写权利而非垄断\n后面RichardStallman发起GNU计划（开源计划），Linus参加该计划，并共享出linux内核，于是大家在此基础上开发出各种软件。linux又称GNU/linux Linux和Unix关系\nVMWare安装Centos7.6 # 在windows中安装Linux系统\nVM和Linux系统在pc中的关系\n安装过程中，网络模式使用NAT模式\n选择最小安装，且选择CompatibilityLibraries和DevelopmentTools\nlinux分区\n一般分为三个\n一般boot1G,swap分区一般跟内存大小一致，这里是2G，所以根分区就是剩下的，也就是20-1-2=17G\n如图，boot，/，swap都是标准分区。且boot和/是ext4的文件格式，swap是swap的文件格式\n修改主机名\n修改密码及增加除root外的普通用户\n修改网络为固定ip(NAT模式下)\n先在VM里面把子网ip改了，这里改成 192.168.200.0\n然后改网关为192.168.200.200\n使用yum install -y vim 安装文本编辑工具 最后在linux中改配置文件 vim /etc/sysconfig/network-scripts/ifcfg-ens33 其中先修改BOOTPROTO=\u0026ldquo;static\u0026rdquo; 然后设置ip地址、网关和DNS， 下面是添加到上面的ifcfg-ens33后面，不是直接执行代码 IPADDR=192.168.200.200 GATEWAY=192.168.200.2 DNS1=192.168.200.2 使用命令重启网络 service network restart # 或者直接重启电脑 reboot 这里顺便装一下zsx\nsh -c \u0026#34;$(wget https://raw.github.com/ohmyzsh/ohmyzsh/master/tools/install.sh -O -)\u0026#34; "},{"id":236,"href":"/zh/docs/technology/Redis/shangguigu_BV1Rv41177Af/19-A/","title":"redis_尚硅谷_19-A","section":"基础(尚硅谷)_","content":" 验证码模拟 # 首先需要一个MyRedis单例类 /** * MyRedis单例类 */ public class MyJedis { private static Jedis myJedis; public static Jedis getInstance() { //如果是空则进行初始化 if (myJedis == null) { //由于synchronized同步是在条件判断内，所以同步 //并不会一直都执行，增加了效率 synchronized (MyJedis.class) { if (myJedis == null) { //设置密码 DefaultJedisClientConfig.Builder builder = DefaultJedisClientConfig.builder() .password(\u0026#34;hello.lwm\u0026#34;); DefaultJedisClientConfig config = builder.build(); Jedis jedis = new redis.clients.jedis.Jedis(\u0026#34;192.168.200.200\u0026#34;, 6379, config); return jedis; } } } return myJedis; } } "},{"id":237,"href":"/zh/docs/technology/Redis/shangguigu_BV1Rv41177Af/18/","title":"redis_尚硅谷_18","section":"基础(尚硅谷)_","content":" Jedis操作Redis6 # 插曲:本地项目关联github远程库 git init git add README.md git commit -m \u0026#34;first commit\u0026#34; #-m表示强制重命名 git branch -M main #使用别名 git remote add origin git@github.com:lwmfjc/jedis_demo.git #用了-u之后以后可以直接用git push替代整行 git push -u origin main jedis pom依赖 \u0026lt;!-- https://mvnrepository.com/artifact/redis.clients/jedis --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;redis.clients\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jedis\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.0.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; jedis使用 public class Main { public static void main(String[] args) { //设置密码 DefaultJedisClientConfig.Builder builder = DefaultJedisClientConfig.builder() .password(\u0026#34;hello.lwm\u0026#34;); DefaultJedisClientConfig config = builder.build(); Jedis jedis = new Jedis(\u0026#34;192.168.200.200\u0026#34;, 6379, config); //ping String value = jedis.ping(); System.out.println(value); //返回所有key Set\u0026lt;String\u0026gt; keys = jedis.keys(\u0026#34;*\u0026#34;); System.out.println(\u0026#34;key count: \u0026#34; + keys.size()); for (String key : keys) { System.out.printf(\u0026#34;key--:%s---value:%s\\n\u0026#34;, key, jedis.get(key)); } System.out.println(\u0026#34;操作list\u0026#34;); //操作list jedis.lpush(\u0026#34;ly-list\u0026#34;, \u0026#34;java\u0026#34;, \u0026#34;c++\u0026#34;, \u0026#34;css\u0026#34;); List\u0026lt;String\u0026gt; lrange = jedis.lrange(\u0026#34;ly-list\u0026#34;, 0, -1); for (String v : lrange) { System.out.println(\u0026#34;value:\u0026#34; + v); } //操作set System.out.println(\u0026#34;操作set\u0026#34;); jedis.sadd(\u0026#34;ly-set\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;3\u0026#34;, \u0026#34;5\u0026#34;, \u0026#34;1\u0026#34;); Set\u0026lt;String\u0026gt; smembers = jedis.smembers(\u0026#34;ly-set\u0026#34;); for (String v : smembers) { System.out.println(\u0026#34;value:\u0026#34; + v); } //操作hash System.out.println(\u0026#34;操作hash\u0026#34;); jedis.hset(\u0026#34;ly-hash\u0026#34;, \u0026#34;name\u0026#34;, \u0026#34;lidian\u0026#34;); jedis.hset(\u0026#34;ly-hash\u0026#34;, \u0026#34;age\u0026#34;, \u0026#34;30\u0026#34;); jedis.hset(\u0026#34;ly-hash\u0026#34;, \u0026#34;sex\u0026#34;, \u0026#34;man\u0026#34;); Map\u0026lt;String, String\u0026gt; lyHash = jedis.hgetAll(\u0026#34;ly-hash\u0026#34;); for (String key : lyHash.keySet()) { System.out.println(key + \u0026#34;:\u0026#34; + lyHash.get(key)); } //操作zset System.out.println(\u0026#34;操作zset\u0026#34;); jedis.zadd(\u0026#34;person\u0026#34;, 100, \u0026#34;xiaohong\u0026#34;); jedis.zadd(\u0026#34;person\u0026#34;, 80, \u0026#34;xiaoli\u0026#34;); jedis.zadd(\u0026#34;person\u0026#34;, 90, \u0026#34;xiaochen\u0026#34;); List\u0026lt;String\u0026gt; person = jedis.zrange(\u0026#34;person\u0026#34;, 0, -1); for (String name : person) { System.out.println(name); } //结束操作 jedis.flushDB(); jedis.close(); } } "},{"id":238,"href":"/zh/docs/technology/Redis/shangguigu_BV1Rv41177Af/12-17/","title":"redis_尚硅谷_12-17","section":"基础(尚硅谷)_","content":" Redis配置文件 # redis中单位的设置，支持k,kb,m,mb,g,gb，且不区分大小写\ninclude (包含其他文件，比如公共部分)\nbind bind 127.0.0.1 ::1 #listens on loopback IPv4 and IPv6 后面这个::1，相当于ipv6版的127.0.0.1。在redis配置文件中，整句表示只允许本地网卡的某个ip连接(但是它并不能指定某个主机连接到redis中。比如本机有两个网卡，两个ip，可以限定只有其中一个ip可以连接) 如果注释掉了/或者bind 0.0.0.0，表示允许所有主机连接 protected-mode protected-mode yes 设置保护模式为yes，protected是redis本身的一个安全层，这个安全层在同时满足下面三个条件的时候会开启，开启后只有本机可以访问redis protected-mode yes 没有bind指令(bind 0.0.0.0不属于这个条件) 没有设置密码 (没有设置requirepass password) 只要上面一个条件不满足，就不会开启保护模式。换言之，只要设置了bind 0.0.0.0或者没有设置bind，且不满足上面三个条件之一，就能够进行远程访问(当然，linux/windows的6379端口要开放) tcp-backlog 表示未连接队列总和 timeout 秒为单位，时间内没操作则断开连接 tcp-keepalive 300 心跳检测，每隔300s检测连接是否存在 pidfile /var/run/redis_6379.pid 将进程号保存到文件中 loglevel 表示日志的级别/debug/verbose/notice/warning logfile \u0026quot;\u0026quot; 设置日志的路径 database 16 默认有16个库 requirepass password 设置密码 maxclients 设置最大连接数 maxmemory 设置最大内存量，达到则会根据移除策略进行移除操作 Redis的发布和订阅 # 发布订阅，pub/sub，是一种消息通信模式：发送者pub发送消息，订阅器sub接收消息 发布者能发布消息，订阅者可以订阅/接收消息\n操作 subscribe channel1 #客户端A订阅频道 publish channel1 helloly #向频道发送消息 此时订阅channel1频道的客户端就会接收到消息\nredis新数据类型 # Bitmaps # 进行二进制操作\n可以把Bitmaps想象成一个以位为单位的数组，数组的每个单元只能存储0和1，数组的下标在Bitmaps中叫做偏移量\nbitcount:统计字符串被设置为1的bit数，这里结果是5\nbitcount u1 0 1 #统计字符串第0个字节到第1个字节1的bit数\n(1,6,11,15,19bit值为1)[也就是统计第0到第15位的1的个数]\nsetbit u1 1 1 setbit u1 2 1 setbit u1 5 1 setbit u1 9 1 setbit u2 0 1 setbit u2 1 1 setbit u2 4 1 setbit u2 9 1 获取u1，u2共同位为1的个数，如上1,9都是1，所以返回2，且 bitcount u1\u0026ndash;u2的值为2（第1和第9位为1），其实就是u1和u2进行\u0026amp;操作\nbitop and u1-and-u2 u1 u2 获取u1或u2存在值为1的位的个数，如上结果为8-2=6，结果存在u1-or-u2中，即1，2，5，9，0，4的位 值为1(的字符串)，其实就是u1和u2进行或操作\n性能比较，假设有一亿个用户，用户id数值递增，需求是存储每个用户是否活跃。下面是使用hashMap和bitmaps的比较\nbitmaps主要用来进行位操作计算\nHyperLogLog # 解决基数问题\n从{1,3,5,5,7,8,8,7,9}找出基数：基数为5，即不重复元素的个数 解决方案 mysql中可以用distinct count redis中可以用hash,set,bitmaps 使用 pfadd a 1 2 3 4 3 3 3 2 1 6 7 pfcount a #得到基数 6 pfadd b 1 10 7 15 #基数4 pfmerge c a b #将a，b合并到c pfcount c #得到基数8 GEO类型 (geographic) # 基本命令 geoadd china:city 121.47 31.43 shanghai geoadd china:city 166.50 29.53 chongqing 114.05 22.52 shenzhen geoadd china:city 16.38 39.90 beijing 不支持南北极，所以有效经度在-180到180度，有效纬度从-85.05xxx度到85.05xxx度 获取坐标值及直线距离 geopos china:city beijing #获取beijing经纬度 geodist china:city beijing shenzhen km #获取beijing到shenzhen的直线距离 # 单位有m,km,ft,mi 以给定的经纬度为中心，找出某一半径内的元素 georadius china:city 110 30 1000 km End # "},{"id":239,"href":"/zh/docs/technology/Redis/shangguigu_BV1Rv41177Af/06-11/","title":"redis_尚硅谷_06-11","section":"基础(尚硅谷)_","content":" Redis针对key的基本操作 # 常用命令 keys * #查找当前库所有库 exists key1 #key1是否存在 1存在；0不存在 type key2 #key2的类型 del key3 #删除key3 unlink key3 #删除key3(选择非阻塞删除。会先从元数据删除，而真正删除是异步删除) expire key1 10 #设置key1的过期时间，单位秒 ttl key1 #获取key1的剩余存活时间，-2表示key已过期或不存在，-1表示永不过期 select 1 #切换到1号库(redis中有15个库，默认在库1) dbsize #查找当前redis库中有多少个key flushdb #清空当前库 flushall #清空所有库 Redis中常用数据类型 # 字符串（String） # String是二进制安全的，可以包含jpg图片或序列化的对象 一个Redis中字符串value最多可以只能是512M 常用命令 set key1 value1 get key1 set key1 value11 #将覆盖上一个值 append key1 abc #在key1的值追加\u0026#34;abc\u0026#34; strlen key1 #key值的长度 setnx key1 value #当key不存在时才设置key incr n1 #将n1的值加一,,如果n1不存在则会创建key n1 并改为1(0+1) decr n1 #将n1的值减一,如果n1不存在则会创建key n1 并改为-1(0-1) incrby n1 20 #将n1的值加20，其他同上 decrby n1 20 #将n1的值减20，其他同上 redis原子性\nincr具有原子性操作\njava中的i++不是原子操作 其他命令 mset k1 v1 k2 v2 mget k1 k2 msetnx k1 v1 k2 v2 #仅当所有的key都不存在时才会进行设置 getrange name 0 3 #截断字符串[0,3] setrange name 3 123 #从下标[3]开始替换字符串（换成123） setex k1 20 v1 #设置过期时间为20s expire k1 30 #设置过期时间为30s getset k1 123 #获取旧值，并设置一个新值 数据结构，SimpleDynamicString，SDS，简单动态字符串，内部结构类似Java的ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配\n列表 (List) # 单键多值 底层是双向链表 从左放 lpush k1 v1 v2 v3 #从左边放(从左往右推) lrange k1 0 -1 #从左边取(v3 v2 v1) lpush:\n从右放 rpush k2 v1 v2 v3 brpush:\nlpop/rpop lpop k2 #从左边弹出一个值 lpop k2 2 #从左边弹出两个值，当键没有包含值时，键被删除 rpoplpush lpush a a1 a2 a3 rpush b b1 b2 b3 rpoplpush a b #此时a:a1 a2，b:a3 b1 b2 b3 lrange lrange b 1 2 #获取b中下标[1,2]的所有值 lrange b 1 -1 #获取所有值[1,最大下标]的所有值 lindex,llen lindex b 1 #直接取第一个下标的元素 llen b #获取列表的长度 linsert linsert b before b2 myinsert linsert b after b2 myinsert #在某个列表的值(如果重复取第一个)的位置之前/之后插入值 lrem,lset lrem b 2 a #从b列表中，删除两个a（从左往右） lset b 2 AA #把下标2的值设置为AA list数据结构是一个快速列表，quicklist\n当元素较少的时候，会使用连续的内存存储，结构时ziplist，即压缩列表；当数据多的时候会有多个压缩列表，然后会链接到一起(使用双向指针)\n集合(Set) # 特点：无序，不重复 Set:string类型的无序集合，底层是一个value为null的hash表；添加/删除时间复杂度为O(1) 常用命令 sadd k1 v1 v2 v3 v2 v2 v1 #设置集合中的值 smembers k1 #取出集合中的值 sismember k1 v3 #k1是否存在v3，存在返回1，不存在返回0 scard k1 #返回集合中元素的个数 srem k1 v2 v3 #删除集合中的v2和v3 spop k1 #从k1中随机取出一个值 srandmember k1 2 #从k1中随机取出2个值 smove a k a1 #从a中将a1移动到k中 sinter a k #取a，k的交集 sunion a k #取a，k的并集 sdiff a k #返回两个集合的差集（从集合a中，去除存在集合k中的元素，即a-k） Set数据结构时dict字典，字典使用哈希表实现的 哈希（Hash) # 是String类型的field和value的映射表，用来存储对象,类似java中的Map\u0026lt;String,Object\u0026gt; 常用命令 hset user:1001 id 1 #设置(对象)user:1001的id属性值 hset user:1001 name zhangsan hget user:1001 name #取出user:1001的name hmset user:1001 id 1 name zhangsan #批量设置（现在hset也可以批量设置了，hmset已弃用） hexists user:1001 id 1 #判断属性id是否存在 hkeys user:1001 #查看hash结构中的所有filed hvals user:1001 #查看hash结构中所有value hincrby user:1001 age 2 #给hash结构的age属性值加2 hsetnx user:1001 age 10 #给hash结构的age属性设置值为10（如果age属性不存在） hash类型数据结构，当field-value长度较短时用的是ziplist，否则使用的是hashtable 有序集合(ZSet) # 与set很相似，但是是有序的 有序集合的所有元素（成员）都关联一个评分(score)，score用来从最低到最高方式进行排序，成员唯一但评分是重复的 常用命令 zadd topn 100 xiaoming 120 xiaohong 60 xiaochen #添加key并为每个成员添加评分 zadd topn xiaoli 200 zrange topn 0 -1 #查找出所有成员(按排名由小到大) zrange topn 0 -1 withscores #从小到大查找所有成员并显示分数 zrangebyscore topn 130 200 #查找所有在130-200的成员 zrevrangebyscore topn 200 130 #从大到小查找所有成员（注意，从大到小时第一个值必须大于等于第二个） zincrby topn 15 xiaohong #给小红添加15分 zrem topn xiaohong #删除元素 zcount topn 10 200 #统计该集合，分数区间内的元素个数 zrank topn xiaohong #xiaohong的排名，从0开始 zset底层数据结构 hash结构\n跳跃表 给元素value排序，根据score的范围获取元素列表 对比有序链表和跳跃表 查找51元素\n跳跃表\n按图中的顺序查找，查找四次就能找到\nEnd "},{"id":240,"href":"/zh/docs/problem/Hugo/p1/","title":"hugo踩坑","section":"Hugo","content":" 对于访问文件资源\nhugo的文件夹名不能以-结尾。 一个文件夹(比如这里是hugo文件夹)中，其中的index.md文件中引用图片时，是以index.md所在文件夹(也就是hugo文件夹)为根目录访问图片；而其中的01a.md文件中引用图片时，是以和该文件同级的01a文件夹(也就是hugo/01a/)为根目录，访问图片\n当一个文件夹下存在index.md文件时，其他文件(代表的文章)不显示在网站的文章列表\n为了某些文件预览功能，我建议使用下面的文件夹结构处理文章及资源\n"},{"id":241,"href":"/zh/docs/problem/Hugo/01a/","title":"图片测试(hugo踩坑)","section":"Hugo","content":" 图片测试 # "},{"id":242,"href":"/zh/docs/technology/Redis/shangguigu_BV1Rv41177Af/01-05/","title":"redis_尚硅谷_01-05","section":"基础(尚硅谷)_","content":" 课程简介 # NoSQL数据库简介、Redis概述与安装、常用五大数据结构、配置文件详解、发布与订阅、Redis6新数据类型、Redis与spring boot整合、事务操作、持久化之RDB、持久化之AOF、主从复制及集群、Redis6应用问题(缓存穿透、击穿、雪崩以及分布式锁)、Redis6新增功能\nNoSQL数据库简介 # Redis属于NoSQL数据库 技术分为三大类 解决功能性问题：Java、Jsp、RDBMS、Tomcat、Linux、JDBC、SVN 解决扩展性问题：Struts、Spring、SpringMVC、Hibernate、Mybatis 解决性能问题：NoSQL、Java线程、Nginx、MQ、ElasticSearch 缓存数据库的好处 完全在内存中，速度快，结构简单 作为缓存数据库：减少io的读操作 NoSQL＝Not Only SQL,不仅仅是SQL，泛指非泛型数据库 不支持ACID(但是NoSQL支持事务) 选超于SQL的性能 NoSQL适用场景 对数据高并发的读写 海量数据的读写 对数据高可扩展性 NoSQL不适用的场景 需要事务支持 基于sql的结构化查询存储 多种NoSQL数据库介绍 Memcache 不支持持久化，数据类型单一，一般作为辅助持久化的数据库 Redis 支持持久化，除了k-v模式还有其他多种数据结构，一般作为辅助持久化的数据库 MongoDB，是文档型数据类型；k-v模型，但是对value提供了丰富的查询功能；支持二进制数据及大型对象；替代RDBMS，成为独立数据库 大数据时代（行式数据库、列式数据库） 行式数据库\n查询某一块数据的时候效率高\n列式数据库\n查询某一列统计信息快\n其他\nHbase，Cassandra，图关系数据库(比如社会关系，公共交通网等) 小计\nNoSQL数据库是为提高性能而产生的非关系型数据库 Redis概述与安装 # 简单概述 Redis是一个开源的kv存储系统 相比Mencached，支持存储的数据类型更多，包括string，list，set，zset以及hash，这些类型都支持(pop、add/remove及取交并集和差集等)，操作都是原子性的 Redis数据都是缓存在内存中 Redis会周期性地把数据写入磁盘或修改操作写入追加的记录文件 能在此基础上实现master-slave(主从)同步 Redis功能 配合关系型数据库做高速缓存 Redis具有多样的数据结构存储持久化数据 其他部分功能\nRedis安装 从官网中下载redis-6.xx.tar.gz包(该教程在linux中使用redis6教学) 编译redis需要gcc环境 使用gcc \u0026ndash;version查看服务器是否有gcc环境 如果没有需要进行安装 apt install -y gcc 或者 yum install -y gcc 将redis压缩文件进行解压 tar -zxvf redis-6xx.tar.gz 进入解压后的文件夹，并使用make命令进行编译 make 如果报错了，需要先用下面命令清理，之后再进行编译 make distclean 安装redis make install 进入/usr/local/bin目录，查看目录\nRedis启动 前台启动 redis-server 后台启动 在刚才解压的文件夹中，拷贝出redis.conf文件(这里拷贝到/etc/目录下) cp redis.conf /etc/redis.conf 到etc中修改redis.conf文件 vim /etc/redis.conf # 进入编辑器后使用下面命令进行搜索并回车 /daemonize no 将no改为yes并保存 进入/usr/local/bin目录启动redis redis-server /etc/redis.conf 查看进程，发现redis已经启动 ps -ef | grep redis 使用redis-cli 客户端连接redis redis-cli keys * 相关知识 # Redis6379的由来 人名Merz 在九宫格对应的数字就是6379\nRedis默认有15个库，默认数据都在数据库0中，所有库的密码都是相同的 Redis是单线程+多路复用技术 Redis是串行操作\n火车站的例子\n当1，2，3没有票的时候，不用一直等待买票，可以继续做自己的事情，黄牛买到票就会通知123进行取票\nMemcached和Redis区别 Memcached支持单一数据类型，Redis支持多数据类型 Memcached不支持持久化 Memcached用的多线程+锁的机制，Redis用的是单线程+多路复用程序 End # "},{"id":243,"href":"/zh/docs/life/archive/20121226/","title":"2021年最后一个周日","section":"往日归档","content":" 装宽带 # 太晚了，不想写了- -。简单写几个字吧，满心期待的装了宽带，但是并没有我想像的那么快乐。反而打了两把游戏更难过了，难过的是浪费了时间也什么都没得到\n图书馆 # 下午跑去图书馆收获倒是挺多，可能是我不太熟悉，对于书架上的书没有太大的感触。但是环境真的太棒了，很安静，感觉多发出点声音我都会觉得不好意思，大家都很自觉。也许对经常网上都能找到电子书看(程序员的事怎么能是盗呢)的人帮助不会特别大，但对于很大一部分人绝对帮助特别大，包括学生、老年人、还有一些文学类书籍阅读者等等(我一直认为文学类的一定要纸质的看起来才有味道~)\n当然，从图书馆回来我又打了两把游戏 o_O，dota2 yyds!! 打完日常卸载，哈哈\n每次去图书馆我都会想起那句话，\u0026quot;一个国家为其年轻人所提供的教育，决定了这个国家未来的样子\u0026quot;。\n希望能多办点这样的图书馆，大家都能少点浮躁，多点沉淀；虽然我并不是热心公益人士，但我还是希望咱们国家的人民都生活的越来越好。不要辜负我们曾经受过的苦难。\n"},{"id":244,"href":"/zh/docs/life/archive/20231021/","title":"沉沦","section":"往日归档","content":"玩物丧志并非是错的，如果你命里是的话。可惜我不是，我明显有其他更为重要的事等着我去做。我应该是骨子里的老实人。如果顺利的话我应该属于研究所那种老干部，至少现在思维已经老化得跟他们差不多了。\n"}]